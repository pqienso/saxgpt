# Stronger regularization, turned off embedding scaling. Used LR Plateau Scheduler.
# Better validation loss (5.75), but training loss suffered (3.0 -> 4.0).
# At this training loss, audio produced is not viable (<20% train accuracy)
# Looks like data is not sufficient. But due to hardware constraints,
# I think it would be better to overfit the model just so I have some viable audio
# to showcase.

data:
  train_path: "data/main/datasets/train.pt"
  val_path: "data/main/datasets/val.pt"

# 4.8M Param model
model:
  vocab_size: 2049
  num_codebooks: 4
  d_model: 128
  nhead: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  dim_feedforward: 512
  dropout: 0.3
  activation: "relu"
  norm_first: true
  max_seq_len: 1505
  padding_idx: 2048
  scale_embeddings: false


training:
  num_epochs: 500
  # Effective batch size = batch_size * gradient_accumulation_steps
  batch_size: 8
  gradient_accumulation_steps: 8
  max_grad_norm: 2.0
  
  # Learning rate scheduler
  scheduler:
    # type: "cosine"           # Options: cosine, linear, step, plateau
    # warmup_steps: 1000       # Warmup for N steps
    # total_steps:  null       # Total training steps (auto-calculated if omitted)
    # min_lr: 1.0e-6           # Minimum learning rate
    
    # For 'step' scheduler:
    # step_size: 10          # Decay every N epochs
    # gamma: 0.5             # Multiply LR by this factor
    
    # For 'plateau' scheduler:
    type: "plateau"
    patience: 5            # Wait N epochs before reducing
    factor: 0.5            # Multiply LR by this factor
    min_lr: 1.0e-7

  # Optimizer
  optimizer:
    type: "adamw"
    lr: 0.0005
    embedding_lr: null
    betas: [0.9, 0.98]
    eps: 1.0e-9
    weight_decay: 1.0e-2
  
  # Logging and checkpointing
  output_dir: "experiments/121025_small"
  log_interval: 2           # Log metrics every N steps
  save_interval: 50      # Save checkpoint every N epochs
  num_workers: 8             # DataLoader workers
  
  # Resume training (optional)
  resume_from_checkpoint: "experiments/121025_small/checkpoints/checkpoint_interrupt.pt" # Path to checkpoint, or null to start fresh
