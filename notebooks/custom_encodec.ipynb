{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a742c58-9a90-472f-9183-566773cf1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/yuan/Desktop/SaxGPT/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1832282-481c-4515-85bb-a5236185524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "from transformers import EncodecModel, EncodecConfig\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "from datetime import timedelta\n",
    "import torchaudio\n",
    "\n",
    "from src.data.audio_util import trim_wav_file\n",
    "from src.data.augmentation import AudioAugmenter, augment_examples\n",
    "from src.data.tokenization import tokenize, detokenize\n",
    "\n",
    "\n",
    "def clip_valid_windows(metadata: List[Dict]) -> List[Tuple[Tensor, Tensor]]:\n",
    "   examples = []\n",
    "   for metadata_entry in tqdm(metadata):\n",
    "       video_id = metadata_entry[\"video_id\"]\n",
    "       windows = json.loads(metadata_entry[\"valid_windows\"])\n",
    "       for window in windows:\n",
    "           start, end = window[0], window[1]\n",
    "           lead_audio = trim_wav_file(\n",
    "               stem_path / f\"sax_{video_id}.wav\",\n",
    "               timedelta(seconds=start),\n",
    "               timedelta(seconds=end),\n",
    "           )\n",
    "           backing_audio = trim_wav_file(\n",
    "               stem_path / f\"rhythm_{video_id}.wav\",\n",
    "               timedelta(seconds=start),\n",
    "               timedelta(seconds=end),\n",
    "           )\n",
    "           examples.append((backing_audio, lead_audio))\n",
    "   return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "629cafe8-a39d-4573-8c2a-e927ede5405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"config/data/main.yaml\", \"r\") as file:\n",
    "   config = yaml.safe_load(file)\n",
    "try:\n",
    "   stem_path_str = config[\"data_paths\"][\"stem_dest\"]\n",
    "   metadata_path_str = config[\"data_paths\"][\"metadata_path\"]\n",
    "   codes_dest_str = config[\"data_paths\"][\"codes_dest\"]\n",
    "   aug_cfg = config[\"augmentation\"]\n",
    "   encodec_config_override = config[\"encodec\"]\n",
    "except KeyError as e:\n",
    "   print(f\"Error: Missing key in configuration file: {e}\")\n",
    "   raise\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = EncodecModel.from_pretrained(\"facebook/encodec_32khz\").to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_32khz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dcc82d7-8517-44d2-9a36-cb71ead5210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuan/Desktop/SaxGPT/.venv/encodec/lib/python3.13/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m audio, sr = torchaudio.load(\u001b[33m\"\u001b[39m\u001b[33m/home/yuan/Desktop/SaxGPT/data/main/stems/rhythm_3RShrYcbCxs.wav\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m codes = \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_len_s\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m28.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m codes.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SaxGPT/src/data/tokenization.py:37\u001b[39m, in \u001b[36mtokenize\u001b[39m\u001b[34m(audio_values, processor, model, chunk_len_s)\u001b[39m\n\u001b[32m     31\u001b[39m     inputs = processor(\n\u001b[32m     32\u001b[39m         raw_audio=chunk.squeeze().cpu(),\n\u001b[32m     33\u001b[39m         sampling_rate=processor.sampling_rate,\n\u001b[32m     34\u001b[39m         return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     35\u001b[39m     )\n\u001b[32m     36\u001b[39m     device = model.device\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpadding_mask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     code_chunks.append(output.audio_codes.squeeze())\n\u001b[32m     42\u001b[39m codes = torch.cat(code_chunks, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SaxGPT/.venv/encodec/lib/python3.13/site-packages/transformers/models/encodec/modeling_encodec.py:593\u001b[39m, in \u001b[36mEncodecModel.encode\u001b[39m\u001b[34m(self, input_values, padding_mask, bandwidth, return_dict)\u001b[39m\n\u001b[32m    591\u001b[39m mask = padding_mask[..., offset : offset + chunk_length].bool()\n\u001b[32m    592\u001b[39m frame = mask * input_values[..., offset : offset + chunk_length]\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m encoded_frame, scale = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbandwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m encoded_frames.append(encoded_frame)\n\u001b[32m    595\u001b[39m scales.append(scale)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SaxGPT/.venv/encodec/lib/python3.13/site-packages/transformers/models/encodec/modeling_encodec.py:523\u001b[39m, in \u001b[36mEncodecModel._encode_frame\u001b[39m\u001b[34m(self, input_values, bandwidth)\u001b[39m\n\u001b[32m    520\u001b[39m     scale = scale.view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    522\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.encoder(input_values)\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m codes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbandwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m codes = codes.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m codes, scale\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SaxGPT/.venv/encodec/lib/python3.13/site-packages/transformers/models/encodec/modeling_encodec.py:434\u001b[39m, in \u001b[36mEncodecResidualVectorQuantizer.encode\u001b[39m\u001b[34m(self, embeddings, bandwidth)\u001b[39m\n\u001b[32m    432\u001b[39m all_indices = []\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[:num_quantizers]:\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     indices = \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    435\u001b[39m     quantized = layer.decode(indices)\n\u001b[32m    436\u001b[39m     residual = residual - quantized\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SaxGPT/.venv/encodec/lib/python3.13/site-packages/transformers/models/encodec/modeling_encodec.py:398\u001b[39m, in \u001b[36mEncodecVectorQuantization.encode\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[32m    397\u001b[39m     hidden_states = hidden_states.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     embed_in = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcodebook\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m embed_in\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SaxGPT/.venv/encodec/lib/python3.13/site-packages/transformers/models/encodec/modeling_encodec.py:377\u001b[39m, in \u001b[36mEncodecEuclideanCodebook.encode\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    375\u001b[39m hidden_states = hidden_states.reshape((-\u001b[32m1\u001b[39m, shape[-\u001b[32m1\u001b[39m]))\n\u001b[32m    376\u001b[39m \u001b[38;5;66;03m# quantize\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m embed_ind = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# post-process\u001b[39;00m\n\u001b[32m    379\u001b[39m embed_ind = embed_ind.view(*shape[:-\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SaxGPT/.venv/encodec/lib/python3.13/site-packages/transformers/models/encodec/modeling_encodec.py:368\u001b[39m, in \u001b[36mEncodecEuclideanCodebook.quantize\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    366\u001b[39m embed = \u001b[38;5;28mself\u001b[39m.embed.t()\n\u001b[32m    367\u001b[39m scaled_states = hidden_states.pow(\u001b[32m2\u001b[39m).sum(\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m dist = \u001b[43m-\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m embed_ind = dist.max(dim=-\u001b[32m1\u001b[39m).indices\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embed_ind\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "audio, sr = torchaudio.load(\"/home/yuan/Desktop/SaxGPT/data/main/stems/rhythm_3RShrYcbCxs.wav\")\n",
    "codes = tokenize(audio, processor, model, chunk_len_s=28.9)\n",
    "codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25eda8f0-7db1-48cc-a512-3f7eda8c6cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuan/Desktop/SaxGPT/.venv/encodec/lib/python3.13/site-packages/torchaudio/_backend/utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torchaudio.save(\n",
    "    \"notebooks/outputs/chunked_decoded.wav\",\n",
    "    detokenize(codes.cpu(), EncodecModel.from_pretrained(\"facebook/encodec_32khz\")),\n",
    "    sample_rate=32000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeaba7a-809c-4db8-9d0e-6f26912fb996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
