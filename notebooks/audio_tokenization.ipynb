{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing audio tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "from transformers import EncodecModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "\n",
    "from data_processing.data_pipeline import AudioExample\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bucks\\anaconda3\\envs\\encodec\\Lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "model = EncodecModel.from_pretrained(\"facebook/encodec_32khz\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_32khz\")\n",
    "\n",
    "\n",
    "def convert_audio_to_codes(audio_values: torch.Tensor):\n",
    "    inputs = processor(\n",
    "        raw_audio=audio_values.squeeze(),\n",
    "        sampling_rate=processor.sampling_rate,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    output = model.encode(\n",
    "        inputs[\"input_values\"].to(device), inputs[\"padding_mask\"].to(device)\n",
    "    )\n",
    "    return output.audio_codes.squeeze()\n",
    "\n",
    "\n",
    "def convert_codes_to_audio(audio_codes: torch.Tensor):\n",
    "    audio_codes = audio_codes.unsqueeze(0).unsqueeze(0)\n",
    "    waveform = model.decode(audio_codes, [None])\n",
    "    return waveform.audio_values.detach()[0]\n",
    "\n",
    "\n",
    "# waveform, sample_rate = torchaudio.load(\n",
    "#     \"audio_dataset/shift2repeat2/sax_sk9rbWPDLxw.wav\"\n",
    "# )\n",
    "# token_tensor = convert_wav_to_tensor(waveform)\n",
    "# recreated_wav = convert_tensor_to_wav(token_tensor)\n",
    "# torchaudio.save(\"recreated.wav\", recreated_wav, sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9304064])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.load(\"audio_dataset/augmented_audio_small.pt\", map_location=\"cpu\", weights_only=False)\n",
    "eg: AudioExample = out[0]\n",
    "eg.lead.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save(\"bking_recreated.wav\", convert_codes_to_audio(backing[0]), 32000)\n",
    "torchaudio.save(\"lead_recreated.wav\", convert_codes_to_audio(lead[0]), 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = torch.load(\"audio_dataset/augmented_audio_small.pt\", weights_only=False)\n",
    "torchaudio.save(\"bking.wav\", examples[1].backing, 32000)\n",
    "torchaudio.save(\"lead.wav\", examples[1].lead, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "backing_full = []\n",
    "lead_full = []\n",
    "for indices in [(0, 400), (400, 1000), (1000, \"\")]:\n",
    "    start, end = indices\n",
    "    backing, lead = torch.load(f\"audio_dataset/codes/augmented_codes_{start}_{end}.pt\")\n",
    "    backing_full.extend(backing)\n",
    "    lead_full.extend(lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1710, 1710)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lead_full), len(backing_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((backing_full, lead_full), \"audio_dataset/codes/aug_codes.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encodec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
