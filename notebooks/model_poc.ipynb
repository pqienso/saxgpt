{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "import typing as tp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1368, 171, 171)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 364298472\n",
    "TRAIN = 0.8\n",
    "VALIDATION = 0.1\n",
    "\n",
    "dataset = torch.load(\"audio_dataset/codes/aug_codes.pt\")\n",
    "train_len = int(TRAIN * len(dataset))\n",
    "val_len = int(VALIDATION * len(dataset))\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_codes, val_codes, test_codes = (\n",
    "    dataset[:train_len],\n",
    "    dataset[train_len : train_len + val_len],\n",
    "    dataset[train_len + val_len :],\n",
    ")\n",
    "\n",
    "# FOR TESTING ONLY\n",
    "train_codes = train_codes\n",
    "\n",
    "len(train_codes), len(val_codes), len(test_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 60 has seq_len 1320, skipping\n",
      "Index 89 has seq_len 1264, skipping\n",
      "Index 134 has seq_len 1432, skipping\n",
      "Index 147 has seq_len 1320, skipping\n",
      "Index 154 has seq_len 1288, skipping\n",
      "Index 208 has seq_len 1408, skipping\n",
      "Index 260 has seq_len 1248, skipping\n",
      "Index 310 has seq_len 1288, skipping\n",
      "Index 334 has seq_len 1432, skipping\n",
      "Index 431 has seq_len 1392, skipping\n",
      "Index 453 has seq_len 1392, skipping\n",
      "Index 463 has seq_len 1488, skipping\n",
      "Index 474 has seq_len 1320, skipping\n",
      "Index 496 has seq_len 1248, skipping\n",
      "Index 498 has seq_len 1368, skipping\n",
      "Index 538 has seq_len 1496, skipping\n",
      "Index 565 has seq_len 1304, skipping\n",
      "Index 611 has seq_len 1488, skipping\n",
      "Index 703 has seq_len 1264, skipping\n",
      "Index 729 has seq_len 1248, skipping\n",
      "Index 796 has seq_len 1384, skipping\n",
      "Index 860 has seq_len 1304, skipping\n",
      "Index 866 has seq_len 1304, skipping\n",
      "Index 942 has seq_len 1376, skipping\n",
      "Index 981 has seq_len 1376, skipping\n",
      "Index 999 has seq_len 1408, skipping\n",
      "Index 1026 has seq_len 1496, skipping\n",
      "Index 1044 has seq_len 1496, skipping\n",
      "Index 1046 has seq_len 1408, skipping\n",
      "Index 1126 has seq_len 1368, skipping\n",
      "Index 1134 has seq_len 1432, skipping\n",
      "Index 1184 has seq_len 1392, skipping\n",
      "Index 1190 has seq_len 1408, skipping\n",
      "Index 1229 has seq_len 1288, skipping\n",
      "Index 1236 has seq_len 1336, skipping\n",
      "Index 1258 has seq_len 1336, skipping\n",
      "Index 1287 has seq_len 1264, skipping\n",
      "Index 14 has seq_len 1488, skipping\n",
      "Index 86 has seq_len 1376, skipping\n",
      "Index 121 has seq_len 1464, skipping\n",
      "Index 141 has seq_len 1464, skipping\n",
      "Index 164 has seq_len 1408, skipping\n",
      "Index 29 has seq_len 1408, skipping\n",
      "Index 69 has seq_len 1392, skipping\n",
      "Index 86 has seq_len 1368, skipping\n",
      "Index 92 has seq_len 1392, skipping\n",
      "Index 117 has seq_len 1384, skipping\n",
      "Index 142 has seq_len 1384, skipping\n",
      "Index 156 has seq_len 1336, skipping\n",
      "Index 157 has seq_len 1464, skipping\n",
      "Index 163 has seq_len 1392, skipping\n"
     ]
    }
   ],
   "source": [
    "class SequenceDataset(TensorDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: tp.List[tp.Tuple[torch.Tensor, torch.Tensor]],\n",
    "        device: torch.device,\n",
    "        seq_len: int = 1500,\n",
    "        stride: int = 750,\n",
    "    ):\n",
    "        src = []\n",
    "        tgt = []\n",
    "        for index, (backing, lead) in enumerate(data):\n",
    "            if backing.shape[-1] < seq_len:\n",
    "                print(f\"Index {index} has seq_len {backing.shape[-1]}, skipping\")\n",
    "                continue\n",
    "            src.append(backing.unfold(-1, seq_len, stride).transpose(0, 1))\n",
    "            tgt.append(lead.unfold(-1, seq_len, stride).transpose(0, 1))\n",
    "        src = torch.concat(src)\n",
    "        tgt = torch.concat(tgt)\n",
    "        src = torch.vmap(self.add_delay_interleaving)(src).to(device)\n",
    "        tgt = torch.vmap(self.add_delay_interleaving)(tgt).to(device)\n",
    "        return super().__init__(src, tgt)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_delay_interleaving(\n",
    "        streams: torch.Tensor, padding_idx: int = 2048\n",
    "    ) -> torch.Tensor:\n",
    "        num_streams = len(streams)\n",
    "        new_streams = []\n",
    "        for index, stream in enumerate(streams):\n",
    "            new_streams.append(\n",
    "                F.pad(stream, (index + 1, num_streams - index), value=padding_idx)\n",
    "            )\n",
    "        return torch.stack(new_streams)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_delay_interleaving(streams: torch.Tensor) -> torch.Tensor:\n",
    "        num_streams = len(streams)\n",
    "        stream_length = streams.shape[-1]\n",
    "        new_streams = []\n",
    "        for index, stream in enumerate(streams):\n",
    "            new_streams.append(\n",
    "                torch.narrow(\n",
    "                    stream, -1, 1 + index, stream_length - (num_streams - 1) - 2\n",
    "                )\n",
    "            )\n",
    "        return torch.stack(new_streams)\n",
    "\n",
    "\n",
    "train_ds = SequenceDataset(train_codes, device=device)\n",
    "val_ds = SequenceDataset(val_codes, device=device)\n",
    "test_ds = SequenceDataset(test_codes, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MHAModel\n",
    "\n",
    "\n",
    "# Save model state, optimizer state, and other info to a .pth file\n",
    "def save_checkpoint(\n",
    "    model, optimizer, epoch, loss, config, filepath=\"model_checkpoint/checkpoint.pth\"\n",
    "):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss,\n",
    "        \"config\": config,\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    lr: int, optimizer=None, filepath=\"model_checkpoint/checkpoint.pth\"\n",
    "):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    config = checkpoint[\"config\"]\n",
    "    model = MHAModel(**config)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])  # Load model state\n",
    "    if optimizer is None:\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    optimizer.load_state_dict(\n",
    "        checkpoint[\"optimizer_state_dict\"]\n",
    "    )  # Load optimizer state\n",
    "    epoch = checkpoint[\"epoch\"]  # Get saved epoch\n",
    "    loss = checkpoint[\"loss\"]  # Get saved loss (optional)\n",
    "    return model, optimizer, epoch, loss, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bucks\\anaconda3\\envs\\encodec\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "FROM_CHECKPOINT = False\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 1e-2\n",
    "NUM_EPOCHS = 10000\n",
    "MODEL_CONFIG = dict(\n",
    "    d_model=256,\n",
    "    nhead=8,\n",
    "    num_decoder_layers=24,\n",
    "    num_encoder_layers=24,\n",
    "    dim_feedforward=1024,\n",
    "    device=device,\n",
    ")\n",
    "FILEPATH = \"model_checkpoint/checkpoint.pth\"\n",
    "\n",
    "previous_epochs = 0\n",
    "model_config = None\n",
    "\n",
    "if FROM_CHECKPOINT:\n",
    "    model, optimizer, previous_epochs, loss, model_config = load_checkpoint(\n",
    "        lr=LEARNING_RATE, filepath=FILEPATH\n",
    "    )\n",
    "else:\n",
    "    model = MHAModel(**MODEL_CONFIG)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    model_config = MODEL_CONFIG\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=2048).to(device)\n",
    "\n",
    "\n",
    "# Learning rate scheduler (optional, for better convergence)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/10044], Batch [1/7], Loss: 3.3504, Accuracy: 35.77%, Grad Norm: 0.50841\n",
      "Epoch [45/10044], Batch [2/7], Loss: 3.8057, Accuracy: 28.18%, Grad Norm: 0.88023\n",
      "Epoch [45/10044], Batch [3/7], Loss: 4.2415, Accuracy: 19.26%, Grad Norm: 0.88402\n",
      "Epoch [45/10044], Batch [4/7], Loss: 4.2268, Accuracy: 20.17%, Grad Norm: 0.59992\n",
      "Epoch [45/10044], Batch [5/7], Loss: 3.8164, Accuracy: 26.83%, Grad Norm: 0.67382\n",
      "Epoch [45/10044], Batch [6/7], Loss: 3.7653, Accuracy: 26.77%, Grad Norm: 0.89185\n",
      "Epoch [45/10044], Batch [7/7], Loss: 3.4678, Accuracy: 31.58%, Grad Norm: 0.77221\n",
      "Epoch [45/10044], Loss: 3.4678\n",
      "Epoch [46/10044], Batch [1/7], Loss: 3.3603, Accuracy: 36.10%, Grad Norm: 0.76256\n",
      "Epoch [46/10044], Batch [2/7], Loss: 3.7768, Accuracy: 28.57%, Grad Norm: 0.87446\n",
      "Epoch [46/10044], Batch [3/7], Loss: 4.2225, Accuracy: 19.14%, Grad Norm: 0.84858\n",
      "Epoch [46/10044], Batch [4/7], Loss: 4.2336, Accuracy: 19.58%, Grad Norm: 0.71968\n",
      "Epoch [46/10044], Batch [5/7], Loss: 3.7969, Accuracy: 26.42%, Grad Norm: 0.62421\n",
      "Epoch [46/10044], Batch [6/7], Loss: 3.7384, Accuracy: 26.97%, Grad Norm: 0.78058\n",
      "Epoch [46/10044], Batch [7/7], Loss: 3.4654, Accuracy: 31.22%, Grad Norm: 0.86838\n",
      "Epoch [46/10044], Loss: 3.4654\n",
      "Epoch [47/10044], Batch [1/7], Loss: 3.3358, Accuracy: 36.24%, Grad Norm: 0.72241\n",
      "Epoch [47/10044], Batch [2/7], Loss: 3.7626, Accuracy: 28.54%, Grad Norm: 0.99304\n",
      "Epoch [47/10044], Batch [3/7], Loss: 4.1871, Accuracy: 20.19%, Grad Norm: 0.87097\n",
      "Epoch [47/10044], Batch [4/7], Loss: 4.1784, Accuracy: 20.94%, Grad Norm: 0.69627\n",
      "Epoch [47/10044], Batch [5/7], Loss: 3.7544, Accuracy: 27.77%, Grad Norm: 0.53164\n",
      "Epoch [47/10044], Batch [6/7], Loss: 3.7103, Accuracy: 27.41%, Grad Norm: 0.68191\n",
      "Epoch [47/10044], Batch [7/7], Loss: 3.4384, Accuracy: 31.05%, Grad Norm: 0.91041\n",
      "Epoch [47/10044], Loss: 3.4384\n",
      "Epoch [48/10044], Batch [1/7], Loss: 3.3058, Accuracy: 36.77%, Grad Norm: 0.59274\n",
      "Epoch [48/10044], Batch [2/7], Loss: 3.7243, Accuracy: 28.93%, Grad Norm: 0.75316\n",
      "Epoch [48/10044], Batch [3/7], Loss: 4.1675, Accuracy: 19.99%, Grad Norm: 0.88548\n",
      "Epoch [48/10044], Batch [4/7], Loss: 4.1664, Accuracy: 21.12%, Grad Norm: 0.77147\n",
      "Epoch [48/10044], Batch [5/7], Loss: 3.7285, Accuracy: 28.10%, Grad Norm: 0.52587\n",
      "Epoch [48/10044], Batch [6/7], Loss: 3.6791, Accuracy: 27.60%, Grad Norm: 0.65724\n",
      "Epoch [48/10044], Batch [7/7], Loss: 3.4028, Accuracy: 31.50%, Grad Norm: 0.79141\n",
      "Epoch [48/10044], Loss: 3.4028\n",
      "Epoch [49/10044], Batch [1/7], Loss: 3.2980, Accuracy: 36.16%, Grad Norm: 0.71708\n",
      "Epoch [49/10044], Batch [2/7], Loss: 3.6962, Accuracy: 28.87%, Grad Norm: 0.63958\n",
      "Epoch [49/10044], Batch [3/7], Loss: 4.1119, Accuracy: 20.02%, Grad Norm: 0.65972\n",
      "Epoch [49/10044], Batch [4/7], Loss: 4.1414, Accuracy: 21.10%, Grad Norm: 0.90522\n",
      "Epoch [49/10044], Batch [5/7], Loss: 3.7121, Accuracy: 28.51%, Grad Norm: 0.81596\n",
      "Epoch [49/10044], Batch [6/7], Loss: 3.6408, Accuracy: 28.87%, Grad Norm: 0.74991\n",
      "Epoch [49/10044], Batch [7/7], Loss: 3.3468, Accuracy: 33.28%, Grad Norm: 0.64918\n",
      "Epoch [49/10044], Loss: 3.3468\n",
      "Epoch [50/10044], Batch [1/7], Loss: 3.2531, Accuracy: 36.90%, Grad Norm: 0.61701\n",
      "Epoch [50/10044], Batch [2/7], Loss: 3.6921, Accuracy: 28.17%, Grad Norm: 0.95408\n",
      "Epoch [50/10044], Batch [3/7], Loss: 4.1189, Accuracy: 19.66%, Grad Norm: 0.96104\n",
      "Epoch [50/10044], Batch [4/7], Loss: 4.0988, Accuracy: 21.47%, Grad Norm: 0.61738\n",
      "Epoch [50/10044], Batch [5/7], Loss: 3.6857, Accuracy: 28.37%, Grad Norm: 0.69833\n",
      "Epoch [50/10044], Batch [6/7], Loss: 3.6668, Accuracy: 28.76%, Grad Norm: 1.27516\n",
      "Epoch [50/10044], Batch [7/7], Loss: 3.3468, Accuracy: 33.75%, Grad Norm: 1.03833\n",
      "Epoch [50/10044], Loss: 3.3468\n",
      "Epoch [51/10044], Batch [1/7], Loss: 3.2506, Accuracy: 37.82%, Grad Norm: 0.83504\n",
      "Epoch [51/10044], Batch [2/7], Loss: 3.6551, Accuracy: 29.14%, Grad Norm: 0.88589\n",
      "Epoch [51/10044], Batch [3/7], Loss: 4.1018, Accuracy: 19.59%, Grad Norm: 1.26167\n",
      "Epoch [51/10044], Batch [4/7], Loss: 4.1135, Accuracy: 20.60%, Grad Norm: 1.12506\n",
      "Epoch [51/10044], Batch [5/7], Loss: 3.6829, Accuracy: 27.98%, Grad Norm: 0.67048\n",
      "Epoch [51/10044], Batch [6/7], Loss: 3.6085, Accuracy: 28.57%, Grad Norm: 0.90015\n",
      "Epoch [51/10044], Batch [7/7], Loss: 3.3137, Accuracy: 34.12%, Grad Norm: 1.07939\n",
      "Epoch [51/10044], Loss: 3.3137\n",
      "Epoch [52/10044], Batch [1/7], Loss: 3.2168, Accuracy: 38.22%, Grad Norm: 0.80300\n",
      "Epoch [52/10044], Batch [2/7], Loss: 3.6190, Accuracy: 29.77%, Grad Norm: 0.66126\n",
      "Epoch [52/10044], Batch [3/7], Loss: 4.0481, Accuracy: 20.77%, Grad Norm: 0.75477\n",
      "Epoch [52/10044], Batch [4/7], Loss: 4.0638, Accuracy: 21.79%, Grad Norm: 0.96959\n",
      "Epoch [52/10044], Batch [5/7], Loss: 3.6370, Accuracy: 28.48%, Grad Norm: 0.63715\n",
      "Epoch [52/10044], Batch [6/7], Loss: 3.5714, Accuracy: 28.90%, Grad Norm: 0.66141\n",
      "Epoch [52/10044], Batch [7/7], Loss: 3.2687, Accuracy: 33.92%, Grad Norm: 0.69587\n",
      "Epoch [52/10044], Loss: 3.2687\n",
      "Epoch [53/10044], Batch [1/7], Loss: 3.1872, Accuracy: 37.89%, Grad Norm: 0.69564\n",
      "Epoch [53/10044], Batch [2/7], Loss: 3.6281, Accuracy: 29.80%, Grad Norm: 1.19548\n",
      "Epoch [53/10044], Batch [3/7], Loss: 4.0390, Accuracy: 21.68%, Grad Norm: 1.15463\n",
      "Epoch [53/10044], Batch [4/7], Loss: 4.0193, Accuracy: 22.14%, Grad Norm: 0.55538\n",
      "Epoch [53/10044], Batch [5/7], Loss: 3.6161, Accuracy: 28.18%, Grad Norm: 0.65758\n",
      "Epoch [53/10044], Batch [6/7], Loss: 3.5728, Accuracy: 28.62%, Grad Norm: 1.07066\n",
      "Epoch [53/10044], Batch [7/7], Loss: 3.2637, Accuracy: 33.08%, Grad Norm: 0.94633\n",
      "Epoch [53/10044], Loss: 3.2637\n",
      "Epoch [54/10044], Batch [1/7], Loss: 3.1778, Accuracy: 37.45%, Grad Norm: 0.73945\n",
      "Epoch [54/10044], Batch [2/7], Loss: 3.5741, Accuracy: 30.76%, Grad Norm: 0.71222\n",
      "Epoch [54/10044], Batch [3/7], Loss: 4.0064, Accuracy: 22.08%, Grad Norm: 0.88355\n",
      "Epoch [54/10044], Batch [4/7], Loss: 3.9987, Accuracy: 22.42%, Grad Norm: 0.75082\n",
      "Epoch [54/10044], Batch [5/7], Loss: 3.5903, Accuracy: 29.45%, Grad Norm: 0.55678\n",
      "Epoch [54/10044], Batch [6/7], Loss: 3.5454, Accuracy: 29.61%, Grad Norm: 0.79366\n",
      "Epoch [54/10044], Batch [7/7], Loss: 3.2124, Accuracy: 34.05%, Grad Norm: 0.71678\n",
      "Epoch [54/10044], Loss: 3.2124\n",
      "Epoch [55/10044], Batch [1/7], Loss: 3.1464, Accuracy: 37.42%, Grad Norm: 0.69349\n",
      "Epoch [55/10044], Batch [2/7], Loss: 3.5541, Accuracy: 30.09%, Grad Norm: 0.63818\n",
      "Epoch [55/10044], Batch [3/7], Loss: 3.9691, Accuracy: 21.74%, Grad Norm: 0.67251\n",
      "Epoch [55/10044], Batch [4/7], Loss: 3.9768, Accuracy: 22.72%, Grad Norm: 0.78671\n",
      "Epoch [55/10044], Batch [5/7], Loss: 3.5607, Accuracy: 29.77%, Grad Norm: 0.59924\n",
      "Epoch [55/10044], Batch [6/7], Loss: 3.4988, Accuracy: 30.57%, Grad Norm: 0.76810\n",
      "Epoch [55/10044], Batch [7/7], Loss: 3.1736, Accuracy: 35.47%, Grad Norm: 0.63436\n",
      "Epoch [55/10044], Loss: 3.1736\n",
      "Epoch [56/10044], Batch [1/7], Loss: 3.1120, Accuracy: 38.98%, Grad Norm: 0.55865\n",
      "Epoch [56/10044], Batch [2/7], Loss: 3.5363, Accuracy: 30.00%, Grad Norm: 0.76276\n",
      "Epoch [56/10044], Batch [3/7], Loss: 3.9425, Accuracy: 21.59%, Grad Norm: 0.76882\n",
      "Epoch [56/10044], Batch [4/7], Loss: 3.9338, Accuracy: 23.27%, Grad Norm: 0.61350\n",
      "Epoch [56/10044], Batch [5/7], Loss: 3.5317, Accuracy: 30.02%, Grad Norm: 0.56725\n",
      "Epoch [56/10044], Batch [6/7], Loss: 3.4877, Accuracy: 30.62%, Grad Norm: 0.96367\n",
      "Epoch [56/10044], Batch [7/7], Loss: 3.1616, Accuracy: 36.05%, Grad Norm: 0.83801\n",
      "Epoch [56/10044], Loss: 3.1616\n",
      "Epoch [57/10044], Batch [1/7], Loss: 3.0979, Accuracy: 39.42%, Grad Norm: 0.66320\n",
      "Epoch [57/10044], Batch [2/7], Loss: 3.5074, Accuracy: 30.75%, Grad Norm: 0.84844\n",
      "Epoch [57/10044], Batch [3/7], Loss: 3.9245, Accuracy: 22.08%, Grad Norm: 0.94657\n",
      "Epoch [57/10044], Batch [4/7], Loss: 3.9176, Accuracy: 22.77%, Grad Norm: 0.80412\n",
      "Epoch [57/10044], Batch [5/7], Loss: 3.5221, Accuracy: 29.68%, Grad Norm: 0.59822\n",
      "Epoch [57/10044], Batch [6/7], Loss: 3.4657, Accuracy: 30.47%, Grad Norm: 0.86254\n",
      "Epoch [57/10044], Batch [7/7], Loss: 3.1277, Accuracy: 35.90%, Grad Norm: 0.86060\n",
      "Epoch [57/10044], Loss: 3.1277\n",
      "Epoch [58/10044], Batch [1/7], Loss: 3.0972, Accuracy: 39.40%, Grad Norm: 0.90711\n",
      "Epoch [58/10044], Batch [2/7], Loss: 3.4815, Accuracy: 31.00%, Grad Norm: 0.69025\n",
      "Epoch [58/10044], Batch [3/7], Loss: 3.8879, Accuracy: 23.15%, Grad Norm: 0.83610\n",
      "Epoch [58/10044], Batch [4/7], Loss: 3.9022, Accuracy: 23.37%, Grad Norm: 0.94154\n",
      "Epoch [58/10044], Batch [5/7], Loss: 3.4966, Accuracy: 29.74%, Grad Norm: 0.62319\n",
      "Epoch [58/10044], Batch [6/7], Loss: 3.4157, Accuracy: 31.07%, Grad Norm: 0.60285\n",
      "Epoch [58/10044], Batch [7/7], Loss: 3.0926, Accuracy: 36.08%, Grad Norm: 0.65795\n",
      "Epoch [58/10044], Loss: 3.0926\n",
      "Epoch [59/10044], Batch [1/7], Loss: 3.0564, Accuracy: 39.29%, Grad Norm: 0.61763\n",
      "Epoch [59/10044], Batch [2/7], Loss: 3.4696, Accuracy: 31.66%, Grad Norm: 0.89070\n",
      "Epoch [59/10044], Batch [3/7], Loss: 3.8580, Accuracy: 23.72%, Grad Norm: 0.81266\n",
      "Epoch [59/10044], Batch [4/7], Loss: 3.8527, Accuracy: 23.80%, Grad Norm: 0.60990\n",
      "Epoch [59/10044], Batch [5/7], Loss: 3.4631, Accuracy: 30.43%, Grad Norm: 0.53860\n",
      "Epoch [59/10044], Batch [6/7], Loss: 3.4001, Accuracy: 31.00%, Grad Norm: 0.72814\n",
      "Epoch [59/10044], Batch [7/7], Loss: 3.0775, Accuracy: 36.17%, Grad Norm: 0.80576\n",
      "Epoch [59/10044], Loss: 3.0775\n",
      "Epoch [60/10044], Batch [1/7], Loss: 3.0342, Accuracy: 39.58%, Grad Norm: 0.52312\n",
      "Epoch [60/10044], Batch [2/7], Loss: 3.4293, Accuracy: 31.96%, Grad Norm: 0.79855\n",
      "Epoch [60/10044], Batch [3/7], Loss: 3.8258, Accuracy: 24.00%, Grad Norm: 0.86785\n",
      "Epoch [60/10044], Batch [4/7], Loss: 3.8220, Accuracy: 24.97%, Grad Norm: 0.68571\n",
      "Epoch [60/10044], Batch [5/7], Loss: 3.4368, Accuracy: 30.90%, Grad Norm: 0.61740\n",
      "Epoch [60/10044], Batch [6/7], Loss: 3.3748, Accuracy: 32.01%, Grad Norm: 0.89916\n",
      "Epoch [60/10044], Batch [7/7], Loss: 3.0327, Accuracy: 36.78%, Grad Norm: 0.76529\n",
      "Epoch [60/10044], Loss: 3.0327\n",
      "Epoch [61/10044], Batch [1/7], Loss: 3.0027, Accuracy: 39.47%, Grad Norm: 0.57477\n",
      "Epoch [61/10044], Batch [2/7], Loss: 3.4085, Accuracy: 31.93%, Grad Norm: 0.80883\n",
      "Epoch [61/10044], Batch [3/7], Loss: 3.8171, Accuracy: 23.57%, Grad Norm: 0.93181\n",
      "Epoch [61/10044], Batch [4/7], Loss: 3.7971, Accuracy: 24.81%, Grad Norm: 0.68686\n",
      "Epoch [61/10044], Batch [5/7], Loss: 3.4144, Accuracy: 30.89%, Grad Norm: 0.47384\n",
      "Epoch [61/10044], Batch [6/7], Loss: 3.3527, Accuracy: 32.41%, Grad Norm: 0.75138\n",
      "Epoch [61/10044], Batch [7/7], Loss: 3.0199, Accuracy: 37.70%, Grad Norm: 0.83213\n",
      "Epoch [61/10044], Loss: 3.0199\n",
      "Epoch [62/10044], Batch [1/7], Loss: 2.9870, Accuracy: 39.94%, Grad Norm: 0.62371\n",
      "Epoch [62/10044], Batch [2/7], Loss: 3.3724, Accuracy: 32.40%, Grad Norm: 0.56360\n",
      "Epoch [62/10044], Batch [3/7], Loss: 3.7664, Accuracy: 24.02%, Grad Norm: 0.65493\n",
      "Epoch [62/10044], Batch [4/7], Loss: 3.7707, Accuracy: 24.92%, Grad Norm: 0.73037\n",
      "Epoch [62/10044], Batch [5/7], Loss: 3.3931, Accuracy: 31.52%, Grad Norm: 0.58499\n",
      "Epoch [62/10044], Batch [6/7], Loss: 3.3261, Accuracy: 32.59%, Grad Norm: 0.81045\n",
      "Epoch [62/10044], Batch [7/7], Loss: 2.9742, Accuracy: 38.28%, Grad Norm: 0.61890\n",
      "Epoch [62/10044], Loss: 2.9742\n",
      "Epoch [63/10044], Batch [1/7], Loss: 2.9622, Accuracy: 40.09%, Grad Norm: 0.55782\n",
      "Epoch [63/10044], Batch [2/7], Loss: 3.3644, Accuracy: 32.32%, Grad Norm: 0.89488\n",
      "Epoch [63/10044], Batch [3/7], Loss: 3.7612, Accuracy: 24.17%, Grad Norm: 0.91763\n",
      "Epoch [63/10044], Batch [4/7], Loss: 3.7451, Accuracy: 24.75%, Grad Norm: 0.67255\n",
      "Epoch [63/10044], Batch [5/7], Loss: 3.3623, Accuracy: 31.99%, Grad Norm: 0.64330\n",
      "Epoch [63/10044], Batch [6/7], Loss: 3.2912, Accuracy: 33.38%, Grad Norm: 0.77299\n",
      "Epoch [63/10044], Batch [7/7], Loss: 2.9500, Accuracy: 39.10%, Grad Norm: 0.71277\n",
      "Epoch [63/10044], Loss: 2.9500\n",
      "Epoch [64/10044], Batch [1/7], Loss: 2.9402, Accuracy: 40.51%, Grad Norm: 0.57755\n",
      "Epoch [64/10044], Batch [2/7], Loss: 3.3385, Accuracy: 32.59%, Grad Norm: 0.95870\n",
      "Epoch [64/10044], Batch [3/7], Loss: 3.7150, Accuracy: 24.68%, Grad Norm: 0.89149\n",
      "Epoch [64/10044], Batch [4/7], Loss: 3.7081, Accuracy: 25.23%, Grad Norm: 0.61047\n",
      "Epoch [64/10044], Batch [5/7], Loss: 3.3416, Accuracy: 32.10%, Grad Norm: 0.69386\n",
      "Epoch [64/10044], Batch [6/7], Loss: 3.2833, Accuracy: 32.82%, Grad Norm: 0.99690\n",
      "Epoch [64/10044], Batch [7/7], Loss: 2.9212, Accuracy: 38.90%, Grad Norm: 0.82109\n",
      "Epoch [64/10044], Loss: 2.9212\n",
      "Epoch [65/10044], Batch [1/7], Loss: 2.9165, Accuracy: 40.96%, Grad Norm: 0.58951\n",
      "Epoch [65/10044], Batch [2/7], Loss: 3.3086, Accuracy: 33.72%, Grad Norm: 0.83120\n",
      "Epoch [65/10044], Batch [3/7], Loss: 3.6909, Accuracy: 25.79%, Grad Norm: 0.91398\n",
      "Epoch [65/10044], Batch [4/7], Loss: 3.6919, Accuracy: 26.03%, Grad Norm: 0.85685\n",
      "Epoch [65/10044], Batch [5/7], Loss: 3.3172, Accuracy: 32.77%, Grad Norm: 0.68763\n",
      "Epoch [65/10044], Batch [6/7], Loss: 3.2506, Accuracy: 32.61%, Grad Norm: 0.97304\n",
      "Epoch [65/10044], Batch [7/7], Loss: 2.8923, Accuracy: 38.88%, Grad Norm: 0.85997\n",
      "Epoch [65/10044], Loss: 2.8923\n",
      "Epoch [66/10044], Batch [1/7], Loss: 2.8951, Accuracy: 40.93%, Grad Norm: 0.67555\n",
      "Epoch [66/10044], Batch [2/7], Loss: 3.2873, Accuracy: 33.19%, Grad Norm: 0.80596\n",
      "Epoch [66/10044], Batch [3/7], Loss: 3.6725, Accuracy: 25.71%, Grad Norm: 0.95230\n",
      "Epoch [66/10044], Batch [4/7], Loss: 3.6710, Accuracy: 26.34%, Grad Norm: 0.99748\n",
      "Epoch [66/10044], Batch [5/7], Loss: 3.2924, Accuracy: 32.74%, Grad Norm: 0.60008\n",
      "Epoch [66/10044], Batch [6/7], Loss: 3.2212, Accuracy: 33.77%, Grad Norm: 0.88227\n",
      "Epoch [66/10044], Batch [7/7], Loss: 2.8737, Accuracy: 39.73%, Grad Norm: 0.84321\n",
      "Epoch [66/10044], Loss: 2.8737\n",
      "Epoch [67/10044], Batch [1/7], Loss: 2.8916, Accuracy: 41.29%, Grad Norm: 0.78775\n",
      "Epoch [67/10044], Batch [2/7], Loss: 3.2790, Accuracy: 33.14%, Grad Norm: 1.05971\n",
      "Epoch [67/10044], Batch [3/7], Loss: 3.6642, Accuracy: 24.82%, Grad Norm: 1.00481\n",
      "Epoch [67/10044], Batch [4/7], Loss: 3.6397, Accuracy: 25.92%, Grad Norm: 0.85663\n",
      "Epoch [67/10044], Batch [5/7], Loss: 3.2685, Accuracy: 32.99%, Grad Norm: 0.62498\n",
      "Epoch [67/10044], Batch [6/7], Loss: 3.2188, Accuracy: 33.65%, Grad Norm: 0.92463\n",
      "Epoch [67/10044], Batch [7/7], Loss: 2.8865, Accuracy: 39.48%, Grad Norm: 0.99278\n",
      "Epoch [67/10044], Loss: 2.8865\n",
      "Epoch [68/10044], Batch [1/7], Loss: 2.8840, Accuracy: 41.58%, Grad Norm: 0.83719\n",
      "Epoch [68/10044], Batch [2/7], Loss: 3.2486, Accuracy: 33.82%, Grad Norm: 0.98421\n",
      "Epoch [68/10044], Batch [3/7], Loss: 3.6157, Accuracy: 26.10%, Grad Norm: 0.84095\n",
      "Epoch [68/10044], Batch [4/7], Loss: 3.6204, Accuracy: 26.58%, Grad Norm: 0.80527\n",
      "Epoch [68/10044], Batch [5/7], Loss: 3.2651, Accuracy: 32.86%, Grad Norm: 0.81544\n",
      "Epoch [68/10044], Batch [6/7], Loss: 3.2142, Accuracy: 34.75%, Grad Norm: 1.15002\n",
      "Epoch [68/10044], Batch [7/7], Loss: 2.8274, Accuracy: 41.27%, Grad Norm: 0.88729\n",
      "Epoch [68/10044], Loss: 2.8274\n",
      "Epoch [69/10044], Batch [1/7], Loss: 2.8433, Accuracy: 41.96%, Grad Norm: 0.69349\n",
      "Epoch [69/10044], Batch [2/7], Loss: 3.2280, Accuracy: 34.01%, Grad Norm: 0.93702\n",
      "Epoch [69/10044], Batch [3/7], Loss: 3.6204, Accuracy: 25.67%, Grad Norm: 1.05081\n",
      "Epoch [69/10044], Batch [4/7], Loss: 3.6188, Accuracy: 26.02%, Grad Norm: 1.04849\n",
      "Epoch [69/10044], Batch [5/7], Loss: 3.2321, Accuracy: 32.97%, Grad Norm: 0.69471\n",
      "Epoch [69/10044], Batch [6/7], Loss: 3.1655, Accuracy: 34.27%, Grad Norm: 0.95622\n",
      "Epoch [69/10044], Batch [7/7], Loss: 2.8078, Accuracy: 41.58%, Grad Norm: 0.89106\n",
      "Epoch [69/10044], Loss: 2.8078\n",
      "Epoch [70/10044], Batch [1/7], Loss: 2.8628, Accuracy: 42.12%, Grad Norm: 1.03832\n",
      "Epoch [70/10044], Batch [2/7], Loss: 3.2087, Accuracy: 34.82%, Grad Norm: 1.01201\n",
      "Epoch [70/10044], Batch [3/7], Loss: 3.5672, Accuracy: 27.60%, Grad Norm: 0.87618\n",
      "Epoch [70/10044], Batch [4/7], Loss: 3.5696, Accuracy: 27.36%, Grad Norm: 0.78286\n",
      "Epoch [70/10044], Batch [5/7], Loss: 3.2148, Accuracy: 33.22%, Grad Norm: 0.75023\n",
      "Epoch [70/10044], Batch [6/7], Loss: 3.1533, Accuracy: 34.06%, Grad Norm: 0.91799\n",
      "Epoch [70/10044], Batch [7/7], Loss: 2.7778, Accuracy: 40.92%, Grad Norm: 0.98595\n",
      "Epoch [70/10044], Loss: 2.7778\n",
      "Epoch [71/10044], Batch [1/7], Loss: 2.8051, Accuracy: 42.43%, Grad Norm: 0.60495\n",
      "Epoch [71/10044], Batch [2/7], Loss: 3.1897, Accuracy: 35.45%, Grad Norm: 1.10458\n",
      "Epoch [71/10044], Batch [3/7], Loss: 3.5644, Accuracy: 27.77%, Grad Norm: 1.18458\n",
      "Epoch [71/10044], Batch [4/7], Loss: 3.5421, Accuracy: 28.16%, Grad Norm: 0.95568\n",
      "Epoch [71/10044], Batch [5/7], Loss: 3.1814, Accuracy: 33.95%, Grad Norm: 0.72622\n",
      "Epoch [71/10044], Batch [6/7], Loss: 3.1144, Accuracy: 34.61%, Grad Norm: 0.88930\n",
      "Epoch [71/10044], Batch [7/7], Loss: 2.7498, Accuracy: 41.30%, Grad Norm: 1.01239\n",
      "Epoch [71/10044], Loss: 2.7498\n",
      "Epoch [72/10044], Batch [1/7], Loss: 2.7899, Accuracy: 42.34%, Grad Norm: 0.88600\n",
      "Epoch [72/10044], Batch [2/7], Loss: 3.1606, Accuracy: 35.35%, Grad Norm: 1.08013\n",
      "Epoch [72/10044], Batch [3/7], Loss: 3.5308, Accuracy: 27.97%, Grad Norm: 1.07995\n",
      "Epoch [72/10044], Batch [4/7], Loss: 3.5311, Accuracy: 28.00%, Grad Norm: 1.36632\n",
      "Epoch [72/10044], Batch [5/7], Loss: 3.1619, Accuracy: 34.37%, Grad Norm: 0.95000\n",
      "Epoch [72/10044], Batch [6/7], Loss: 3.0885, Accuracy: 35.97%, Grad Norm: 0.93090\n",
      "Epoch [72/10044], Batch [7/7], Loss: 2.7153, Accuracy: 42.55%, Grad Norm: 0.84581\n",
      "Epoch [72/10044], Loss: 2.7153\n",
      "Epoch [73/10044], Batch [1/7], Loss: 2.7607, Accuracy: 43.21%, Grad Norm: 0.77860\n",
      "Epoch [73/10044], Batch [2/7], Loss: 3.1662, Accuracy: 35.67%, Grad Norm: 1.63261\n",
      "Epoch [73/10044], Batch [3/7], Loss: 3.5440, Accuracy: 27.48%, Grad Norm: 1.63735\n",
      "Epoch [73/10044], Batch [4/7], Loss: 3.4998, Accuracy: 28.92%, Grad Norm: 0.87607\n",
      "Epoch [73/10044], Batch [5/7], Loss: 3.1359, Accuracy: 35.14%, Grad Norm: 0.89698\n",
      "Epoch [73/10044], Batch [6/7], Loss: 3.0745, Accuracy: 35.95%, Grad Norm: 1.22223\n",
      "Epoch [73/10044], Batch [7/7], Loss: 2.7149, Accuracy: 42.40%, Grad Norm: 1.22374\n",
      "Epoch [73/10044], Loss: 2.7149\n",
      "Epoch [74/10044], Batch [1/7], Loss: 2.7797, Accuracy: 42.44%, Grad Norm: 1.08638\n",
      "Epoch [74/10044], Batch [2/7], Loss: 3.1434, Accuracy: 35.58%, Grad Norm: 1.50551\n",
      "Epoch [74/10044], Batch [3/7], Loss: 3.5369, Accuracy: 27.16%, Grad Norm: 1.80866\n",
      "Epoch [74/10044], Batch [4/7], Loss: 3.4908, Accuracy: 28.60%, Grad Norm: 1.28152\n",
      "Epoch [74/10044], Batch [5/7], Loss: 3.1103, Accuracy: 35.82%, Grad Norm: 0.78579\n",
      "Epoch [74/10044], Batch [6/7], Loss: 3.0745, Accuracy: 36.20%, Grad Norm: 1.22530\n",
      "Epoch [74/10044], Batch [7/7], Loss: 2.6981, Accuracy: 41.55%, Grad Norm: 1.36774\n",
      "Epoch [74/10044], Loss: 2.6981\n",
      "Epoch [75/10044], Batch [1/7], Loss: 2.7437, Accuracy: 42.14%, Grad Norm: 1.02605\n",
      "Epoch [75/10044], Batch [2/7], Loss: 3.1186, Accuracy: 35.05%, Grad Norm: 1.05351\n",
      "Epoch [75/10044], Batch [3/7], Loss: 3.4999, Accuracy: 27.66%, Grad Norm: 1.32495\n",
      "Epoch [75/10044], Batch [4/7], Loss: 3.4706, Accuracy: 29.00%, Grad Norm: 1.13482\n",
      "Epoch [75/10044], Batch [5/7], Loss: 3.1153, Accuracy: 35.61%, Grad Norm: 0.91445\n",
      "Epoch [75/10044], Batch [6/7], Loss: 3.0550, Accuracy: 37.21%, Grad Norm: 1.21451\n",
      "Epoch [75/10044], Batch [7/7], Loss: 2.6647, Accuracy: 43.83%, Grad Norm: 0.94691\n",
      "Epoch [75/10044], Loss: 2.6647\n",
      "Epoch [76/10044], Batch [1/7], Loss: 2.7565, Accuracy: 42.25%, Grad Norm: 0.98878\n",
      "Epoch [76/10044], Batch [2/7], Loss: 3.1296, Accuracy: 34.10%, Grad Norm: 1.42258\n",
      "Epoch [76/10044], Batch [3/7], Loss: 3.4914, Accuracy: 26.62%, Grad Norm: 1.36164\n",
      "Epoch [76/10044], Batch [4/7], Loss: 3.4533, Accuracy: 28.21%, Grad Norm: 1.13408\n",
      "Epoch [76/10044], Batch [5/7], Loss: 3.0828, Accuracy: 36.08%, Grad Norm: 0.87417\n",
      "Epoch [76/10044], Batch [6/7], Loss: 3.0412, Accuracy: 37.24%, Grad Norm: 1.28783\n",
      "Epoch [76/10044], Batch [7/7], Loss: 2.6615, Accuracy: 44.33%, Grad Norm: 1.21952\n",
      "Epoch [76/10044], Loss: 2.6615\n",
      "Epoch [77/10044], Batch [1/7], Loss: 2.7483, Accuracy: 43.64%, Grad Norm: 1.12263\n",
      "Epoch [77/10044], Batch [2/7], Loss: 3.0864, Accuracy: 35.72%, Grad Norm: 1.19026\n",
      "Epoch [77/10044], Batch [3/7], Loss: 3.4467, Accuracy: 27.41%, Grad Norm: 1.23317\n",
      "Epoch [77/10044], Batch [4/7], Loss: 3.4225, Accuracy: 28.24%, Grad Norm: 1.05829\n",
      "Epoch [77/10044], Batch [5/7], Loss: 3.0621, Accuracy: 35.91%, Grad Norm: 0.90296\n",
      "Epoch [77/10044], Batch [6/7], Loss: 3.0151, Accuracy: 37.22%, Grad Norm: 1.40254\n",
      "Epoch [77/10044], Batch [7/7], Loss: 2.6344, Accuracy: 45.03%, Grad Norm: 1.23671\n",
      "Epoch [77/10044], Loss: 2.6344\n",
      "Epoch [78/10044], Batch [1/7], Loss: 2.7068, Accuracy: 43.67%, Grad Norm: 0.95843\n",
      "Epoch [78/10044], Batch [2/7], Loss: 3.0780, Accuracy: 35.39%, Grad Norm: 1.43721\n",
      "Epoch [78/10044], Batch [3/7], Loss: 3.4301, Accuracy: 28.34%, Grad Norm: 1.34435\n",
      "Epoch [78/10044], Batch [4/7], Loss: 3.3936, Accuracy: 29.65%, Grad Norm: 1.14159\n",
      "Epoch [78/10044], Batch [5/7], Loss: 3.0326, Accuracy: 36.47%, Grad Norm: 0.92050\n",
      "Epoch [78/10044], Batch [6/7], Loss: 3.0187, Accuracy: 37.37%, Grad Norm: 1.66006\n",
      "Epoch [78/10044], Batch [7/7], Loss: 2.6213, Accuracy: 44.58%, Grad Norm: 1.35997\n",
      "Epoch [78/10044], Loss: 2.6213\n",
      "Epoch [79/10044], Batch [1/7], Loss: 2.6752, Accuracy: 44.03%, Grad Norm: 0.95719\n",
      "Epoch [79/10044], Batch [2/7], Loss: 3.0446, Accuracy: 35.94%, Grad Norm: 1.23413\n",
      "Epoch [79/10044], Batch [3/7], Loss: 3.4090, Accuracy: 27.99%, Grad Norm: 1.35664\n",
      "Epoch [79/10044], Batch [4/7], Loss: 3.3712, Accuracy: 29.26%, Grad Norm: 1.23427\n",
      "Epoch [79/10044], Batch [5/7], Loss: 2.9981, Accuracy: 36.86%, Grad Norm: 0.74296\n",
      "Epoch [79/10044], Batch [6/7], Loss: 2.9700, Accuracy: 38.37%, Grad Norm: 1.18741\n",
      "Epoch [79/10044], Batch [7/7], Loss: 2.6007, Accuracy: 46.23%, Grad Norm: 1.48454\n",
      "Epoch [79/10044], Loss: 2.6007\n",
      "Epoch [80/10044], Batch [1/7], Loss: 2.6463, Accuracy: 45.03%, Grad Norm: 0.89123\n",
      "Epoch [80/10044], Batch [2/7], Loss: 3.0007, Accuracy: 37.95%, Grad Norm: 0.86094\n",
      "Epoch [80/10044], Batch [3/7], Loss: 3.3652, Accuracy: 29.82%, Grad Norm: 0.91830\n",
      "Epoch [80/10044], Batch [4/7], Loss: 3.3393, Accuracy: 29.56%, Grad Norm: 1.19853\n",
      "Epoch [80/10044], Batch [5/7], Loss: 2.9903, Accuracy: 36.05%, Grad Norm: 0.99816\n",
      "Epoch [80/10044], Batch [6/7], Loss: 2.9691, Accuracy: 36.91%, Grad Norm: 1.38911\n",
      "Epoch [80/10044], Batch [7/7], Loss: 2.5553, Accuracy: 46.72%, Grad Norm: 0.98607\n",
      "Epoch [80/10044], Loss: 2.5553\n",
      "Epoch [81/10044], Batch [1/7], Loss: 2.6164, Accuracy: 44.69%, Grad Norm: 0.65952\n",
      "Epoch [81/10044], Batch [2/7], Loss: 3.0219, Accuracy: 38.23%, Grad Norm: 1.56460\n",
      "Epoch [81/10044], Batch [3/7], Loss: 3.3902, Accuracy: 30.68%, Grad Norm: 1.51755\n",
      "Epoch [81/10044], Batch [4/7], Loss: 3.3369, Accuracy: 31.22%, Grad Norm: 1.13616\n",
      "Epoch [81/10044], Batch [5/7], Loss: 2.9620, Accuracy: 37.39%, Grad Norm: 0.84716\n",
      "Epoch [81/10044], Batch [6/7], Loss: 2.9427, Accuracy: 36.37%, Grad Norm: 1.38563\n",
      "Epoch [81/10044], Batch [7/7], Loss: 2.5663, Accuracy: 44.43%, Grad Norm: 1.34978\n",
      "Epoch [81/10044], Loss: 2.5663\n",
      "Epoch [82/10044], Batch [1/7], Loss: 2.6566, Accuracy: 43.10%, Grad Norm: 1.23889\n",
      "Epoch [82/10044], Batch [2/7], Loss: 2.9844, Accuracy: 38.47%, Grad Norm: 1.17191\n",
      "Epoch [82/10044], Batch [3/7], Loss: 3.3542, Accuracy: 30.87%, Grad Norm: 1.38716\n",
      "Epoch [82/10044], Batch [4/7], Loss: 3.3231, Accuracy: 32.67%, Grad Norm: 1.31568\n",
      "Epoch [82/10044], Batch [5/7], Loss: 2.9448, Accuracy: 38.60%, Grad Norm: 0.91336\n",
      "Epoch [82/10044], Batch [6/7], Loss: 2.9061, Accuracy: 38.90%, Grad Norm: 0.96435\n",
      "Epoch [82/10044], Batch [7/7], Loss: 2.5155, Accuracy: 45.68%, Grad Norm: 0.85815\n",
      "Epoch [82/10044], Loss: 2.5155\n",
      "Epoch [83/10044], Batch [1/7], Loss: 2.6019, Accuracy: 43.87%, Grad Norm: 0.80006\n",
      "Epoch [83/10044], Batch [2/7], Loss: 2.9609, Accuracy: 36.84%, Grad Norm: 0.87697\n",
      "Epoch [83/10044], Batch [3/7], Loss: 3.3209, Accuracy: 30.07%, Grad Norm: 0.98711\n",
      "Epoch [83/10044], Batch [4/7], Loss: 3.2715, Accuracy: 32.58%, Grad Norm: 1.09266\n",
      "Epoch [83/10044], Batch [5/7], Loss: 2.8999, Accuracy: 39.27%, Grad Norm: 0.64731\n",
      "Epoch [83/10044], Batch [6/7], Loss: 2.8772, Accuracy: 40.59%, Grad Norm: 0.92376\n",
      "Epoch [83/10044], Batch [7/7], Loss: 2.4855, Accuracy: 47.98%, Grad Norm: 0.85037\n",
      "Epoch [83/10044], Loss: 2.4855\n",
      "Epoch [84/10044], Batch [1/7], Loss: 2.5819, Accuracy: 45.45%, Grad Norm: 0.79047\n",
      "Epoch [84/10044], Batch [2/7], Loss: 2.9220, Accuracy: 37.73%, Grad Norm: 0.91658\n",
      "Epoch [84/10044], Batch [3/7], Loss: 3.2780, Accuracy: 30.32%, Grad Norm: 0.86138\n",
      "Epoch [84/10044], Batch [4/7], Loss: 3.2262, Accuracy: 33.26%, Grad Norm: 0.85161\n",
      "Epoch [84/10044], Batch [5/7], Loss: 2.8767, Accuracy: 39.67%, Grad Norm: 0.73907\n",
      "Epoch [84/10044], Batch [6/7], Loss: 2.8640, Accuracy: 41.06%, Grad Norm: 1.18908\n",
      "Epoch [84/10044], Batch [7/7], Loss: 2.4625, Accuracy: 48.95%, Grad Norm: 0.85381\n",
      "Epoch [84/10044], Loss: 2.4625\n",
      "Epoch [85/10044], Batch [1/7], Loss: 2.5514, Accuracy: 46.03%, Grad Norm: 0.83406\n",
      "Epoch [85/10044], Batch [2/7], Loss: 2.9034, Accuracy: 38.07%, Grad Norm: 0.95192\n",
      "Epoch [85/10044], Batch [3/7], Loss: 3.2607, Accuracy: 30.57%, Grad Norm: 1.07626\n",
      "Epoch [85/10044], Batch [4/7], Loss: 3.2147, Accuracy: 32.92%, Grad Norm: 1.00227\n",
      "Epoch [85/10044], Batch [5/7], Loss: 2.8523, Accuracy: 40.13%, Grad Norm: 0.75165\n",
      "Epoch [85/10044], Batch [6/7], Loss: 2.8319, Accuracy: 42.21%, Grad Norm: 1.31581\n",
      "Epoch [85/10044], Batch [7/7], Loss: 2.4355, Accuracy: 50.85%, Grad Norm: 0.97482\n",
      "Epoch [85/10044], Loss: 2.4355\n",
      "Epoch [86/10044], Batch [1/7], Loss: 2.5339, Accuracy: 46.82%, Grad Norm: 0.81786\n",
      "Epoch [86/10044], Batch [2/7], Loss: 2.8902, Accuracy: 38.65%, Grad Norm: 1.23652\n",
      "Epoch [86/10044], Batch [3/7], Loss: 3.2419, Accuracy: 30.69%, Grad Norm: 1.50292\n",
      "Epoch [86/10044], Batch [4/7], Loss: 3.1972, Accuracy: 32.06%, Grad Norm: 1.20778\n",
      "Epoch [86/10044], Batch [5/7], Loss: 2.8294, Accuracy: 39.34%, Grad Norm: 0.71480\n",
      "Epoch [86/10044], Batch [6/7], Loss: 2.8269, Accuracy: 40.82%, Grad Norm: 1.40836\n",
      "Epoch [86/10044], Batch [7/7], Loss: 2.4339, Accuracy: 51.52%, Grad Norm: 1.51400\n",
      "Epoch [86/10044], Loss: 2.4339\n",
      "Epoch [87/10044], Batch [1/7], Loss: 2.5350, Accuracy: 47.07%, Grad Norm: 1.05496\n",
      "Epoch [87/10044], Batch [2/7], Loss: 2.8485, Accuracy: 41.06%, Grad Norm: 0.97156\n",
      "Epoch [87/10044], Batch [3/7], Loss: 3.1844, Accuracy: 33.12%, Grad Norm: 1.08769\n",
      "Epoch [87/10044], Batch [4/7], Loss: 3.1605, Accuracy: 33.41%, Grad Norm: 1.22000\n",
      "Epoch [87/10044], Batch [5/7], Loss: 2.8114, Accuracy: 39.57%, Grad Norm: 0.86000\n",
      "Epoch [87/10044], Batch [6/7], Loss: 2.7879, Accuracy: 40.48%, Grad Norm: 0.90682\n",
      "Epoch [87/10044], Batch [7/7], Loss: 2.3879, Accuracy: 49.72%, Grad Norm: 1.01112\n",
      "Epoch [87/10044], Loss: 2.3879\n",
      "Epoch [88/10044], Batch [1/7], Loss: 2.4858, Accuracy: 46.53%, Grad Norm: 0.78851\n",
      "Epoch [88/10044], Batch [2/7], Loss: 2.8359, Accuracy: 41.36%, Grad Norm: 1.33893\n",
      "Epoch [88/10044], Batch [3/7], Loss: 3.1758, Accuracy: 34.79%, Grad Norm: 1.15927\n",
      "Epoch [88/10044], Batch [4/7], Loss: 3.1386, Accuracy: 34.75%, Grad Norm: 1.14946\n",
      "Epoch [88/10044], Batch [5/7], Loss: 2.7967, Accuracy: 39.89%, Grad Norm: 1.10234\n",
      "Epoch [88/10044], Batch [6/7], Loss: 2.7731, Accuracy: 40.84%, Grad Norm: 1.32433\n",
      "Epoch [88/10044], Batch [7/7], Loss: 2.3642, Accuracy: 50.05%, Grad Norm: 0.98663\n",
      "Epoch [88/10044], Loss: 2.3642\n",
      "Epoch [89/10044], Batch [1/7], Loss: 2.4650, Accuracy: 47.10%, Grad Norm: 0.75557\n",
      "Epoch [89/10044], Batch [2/7], Loss: 2.8504, Accuracy: 41.47%, Grad Norm: 1.87719\n",
      "Epoch [89/10044], Batch [3/7], Loss: 3.2113, Accuracy: 33.45%, Grad Norm: 2.04460\n",
      "Epoch [89/10044], Batch [4/7], Loss: 3.1461, Accuracy: 35.30%, Grad Norm: 1.46472\n",
      "Epoch [89/10044], Batch [5/7], Loss: 2.7847, Accuracy: 41.67%, Grad Norm: 0.91606\n",
      "Epoch [89/10044], Batch [6/7], Loss: 2.7560, Accuracy: 41.79%, Grad Norm: 1.29037\n",
      "Epoch [89/10044], Batch [7/7], Loss: 2.3866, Accuracy: 47.83%, Grad Norm: 1.66520\n",
      "Epoch [89/10044], Loss: 2.3866\n",
      "Epoch [90/10044], Batch [1/7], Loss: 2.4932, Accuracy: 45.75%, Grad Norm: 1.35520\n",
      "Epoch [90/10044], Batch [2/7], Loss: 2.8047, Accuracy: 39.81%, Grad Norm: 1.05131\n",
      "Epoch [90/10044], Batch [3/7], Loss: 3.1504, Accuracy: 33.43%, Grad Norm: 1.49484\n",
      "Epoch [90/10044], Batch [4/7], Loss: 3.1121, Accuracy: 35.02%, Grad Norm: 1.39735\n",
      "Epoch [90/10044], Batch [5/7], Loss: 2.7527, Accuracy: 42.08%, Grad Norm: 0.93654\n",
      "Epoch [90/10044], Batch [6/7], Loss: 2.7427, Accuracy: 44.07%, Grad Norm: 1.14547\n",
      "Epoch [90/10044], Batch [7/7], Loss: 2.3419, Accuracy: 51.22%, Grad Norm: 1.22860\n",
      "Epoch [90/10044], Loss: 2.3419\n",
      "Epoch [91/10044], Batch [1/7], Loss: 2.4430, Accuracy: 47.62%, Grad Norm: 0.90417\n",
      "Epoch [91/10044], Batch [2/7], Loss: 2.8015, Accuracy: 39.70%, Grad Norm: 1.21655\n",
      "Epoch [91/10044], Batch [3/7], Loss: 3.1366, Accuracy: 32.47%, Grad Norm: 1.13850\n",
      "Epoch [91/10044], Batch [4/7], Loss: 3.0732, Accuracy: 34.88%, Grad Norm: 0.95295\n",
      "Epoch [91/10044], Batch [5/7], Loss: 2.7261, Accuracy: 42.64%, Grad Norm: 0.79062\n",
      "Epoch [91/10044], Batch [6/7], Loss: 2.7482, Accuracy: 44.25%, Grad Norm: 1.46582\n",
      "Epoch [91/10044], Batch [7/7], Loss: 2.3281, Accuracy: 54.02%, Grad Norm: 1.22508\n",
      "Epoch [91/10044], Loss: 2.3281\n",
      "Epoch [92/10044], Batch [1/7], Loss: 2.4536, Accuracy: 48.54%, Grad Norm: 1.10684\n",
      "Epoch [92/10044], Batch [2/7], Loss: 2.7460, Accuracy: 42.00%, Grad Norm: 0.91899\n",
      "Epoch [92/10044], Batch [3/7], Loss: 3.0954, Accuracy: 34.20%, Grad Norm: 0.99371\n",
      "Epoch [92/10044], Batch [4/7], Loss: 3.0679, Accuracy: 34.65%, Grad Norm: 1.07414\n",
      "Epoch [92/10044], Batch [5/7], Loss: 2.7239, Accuracy: 41.56%, Grad Norm: 0.92249\n",
      "Epoch [92/10044], Batch [6/7], Loss: 2.6889, Accuracy: 43.04%, Grad Norm: 1.13633\n",
      "Epoch [92/10044], Batch [7/7], Loss: 2.2819, Accuracy: 52.93%, Grad Norm: 0.84515\n",
      "Epoch [92/10044], Loss: 2.2819\n",
      "Epoch [93/10044], Batch [1/7], Loss: 2.4068, Accuracy: 48.92%, Grad Norm: 0.79080\n",
      "Epoch [93/10044], Batch [2/7], Loss: 2.7256, Accuracy: 43.23%, Grad Norm: 0.99287\n",
      "Epoch [93/10044], Batch [3/7], Loss: 3.0634, Accuracy: 36.06%, Grad Norm: 1.04685\n",
      "Epoch [93/10044], Batch [4/7], Loss: 3.0160, Accuracy: 36.87%, Grad Norm: 0.90124\n",
      "Epoch [93/10044], Batch [5/7], Loss: 2.6790, Accuracy: 42.90%, Grad Norm: 0.82064\n",
      "Epoch [93/10044], Batch [6/7], Loss: 2.6627, Accuracy: 43.55%, Grad Norm: 0.90216\n",
      "Epoch [93/10044], Batch [7/7], Loss: 2.2451, Accuracy: 52.78%, Grad Norm: 0.76848\n",
      "Epoch [93/10044], Loss: 2.2451\n",
      "Epoch [94/10044], Batch [1/7], Loss: 2.3779, Accuracy: 48.57%, Grad Norm: 0.65081\n",
      "Epoch [94/10044], Batch [2/7], Loss: 2.6951, Accuracy: 43.42%, Grad Norm: 0.88480\n",
      "Epoch [94/10044], Batch [3/7], Loss: 3.0297, Accuracy: 36.24%, Grad Norm: 1.06163\n",
      "Epoch [94/10044], Batch [4/7], Loss: 2.9823, Accuracy: 37.92%, Grad Norm: 0.86863\n",
      "Epoch [94/10044], Batch [5/7], Loss: 2.6497, Accuracy: 44.23%, Grad Norm: 0.74729\n",
      "Epoch [94/10044], Batch [6/7], Loss: 2.6358, Accuracy: 44.27%, Grad Norm: 0.94538\n",
      "Epoch [94/10044], Batch [7/7], Loss: 2.2368, Accuracy: 53.68%, Grad Norm: 0.94056\n",
      "Epoch [94/10044], Loss: 2.2368\n",
      "Epoch [95/10044], Batch [1/7], Loss: 2.3606, Accuracy: 49.62%, Grad Norm: 0.67524\n",
      "Epoch [95/10044], Batch [2/7], Loss: 2.6749, Accuracy: 43.37%, Grad Norm: 0.95371\n",
      "Epoch [95/10044], Batch [3/7], Loss: 3.0006, Accuracy: 36.87%, Grad Norm: 0.94131\n",
      "Epoch [95/10044], Batch [4/7], Loss: 2.9547, Accuracy: 38.81%, Grad Norm: 0.84036\n",
      "Epoch [95/10044], Batch [5/7], Loss: 2.6169, Accuracy: 44.78%, Grad Norm: 0.68042\n",
      "Epoch [95/10044], Batch [6/7], Loss: 2.6078, Accuracy: 45.48%, Grad Norm: 0.97452\n",
      "Epoch [95/10044], Batch [7/7], Loss: 2.2057, Accuracy: 54.97%, Grad Norm: 0.88591\n",
      "Epoch [95/10044], Loss: 2.2057\n",
      "Epoch [96/10044], Batch [1/7], Loss: 2.3314, Accuracy: 50.27%, Grad Norm: 0.64336\n",
      "Epoch [96/10044], Batch [2/7], Loss: 2.6443, Accuracy: 44.03%, Grad Norm: 0.97750\n",
      "Epoch [96/10044], Batch [3/7], Loss: 2.9716, Accuracy: 36.66%, Grad Norm: 1.07006\n",
      "Epoch [96/10044], Batch [4/7], Loss: 2.9204, Accuracy: 39.27%, Grad Norm: 0.85994\n",
      "Epoch [96/10044], Batch [5/7], Loss: 2.5914, Accuracy: 44.81%, Grad Norm: 0.70095\n",
      "Epoch [96/10044], Batch [6/7], Loss: 2.5846, Accuracy: 45.60%, Grad Norm: 0.97351\n",
      "Epoch [96/10044], Batch [7/7], Loss: 2.1767, Accuracy: 56.25%, Grad Norm: 1.04266\n",
      "Epoch [96/10044], Loss: 2.1767\n",
      "Epoch [97/10044], Batch [1/7], Loss: 2.3207, Accuracy: 50.78%, Grad Norm: 0.82190\n",
      "Epoch [97/10044], Batch [2/7], Loss: 2.6219, Accuracy: 44.45%, Grad Norm: 0.94469\n",
      "Epoch [97/10044], Batch [3/7], Loss: 2.9490, Accuracy: 37.67%, Grad Norm: 1.06738\n",
      "Epoch [97/10044], Batch [4/7], Loss: 2.8953, Accuracy: 39.16%, Grad Norm: 0.95363\n",
      "Epoch [97/10044], Batch [5/7], Loss: 2.5760, Accuracy: 45.07%, Grad Norm: 0.67884\n",
      "Epoch [97/10044], Batch [6/7], Loss: 2.5581, Accuracy: 46.19%, Grad Norm: 1.02815\n",
      "Epoch [97/10044], Batch [7/7], Loss: 2.1472, Accuracy: 55.93%, Grad Norm: 0.90665\n",
      "Epoch [97/10044], Loss: 2.1472\n",
      "Epoch [98/10044], Batch [1/7], Loss: 2.2994, Accuracy: 50.25%, Grad Norm: 0.78269\n",
      "Epoch [98/10044], Batch [2/7], Loss: 2.6023, Accuracy: 45.46%, Grad Norm: 1.12379\n",
      "Epoch [98/10044], Batch [3/7], Loss: 2.9313, Accuracy: 38.52%, Grad Norm: 1.12941\n",
      "Epoch [98/10044], Batch [4/7], Loss: 2.8730, Accuracy: 40.52%, Grad Norm: 0.97080\n",
      "Epoch [98/10044], Batch [5/7], Loss: 2.5573, Accuracy: 46.02%, Grad Norm: 0.79716\n",
      "Epoch [98/10044], Batch [6/7], Loss: 2.5331, Accuracy: 46.39%, Grad Norm: 1.07913\n",
      "Epoch [98/10044], Batch [7/7], Loss: 2.1262, Accuracy: 55.68%, Grad Norm: 0.92767\n",
      "Epoch [98/10044], Loss: 2.1262\n",
      "Epoch [99/10044], Batch [1/7], Loss: 2.2942, Accuracy: 50.38%, Grad Norm: 0.85169\n",
      "Epoch [99/10044], Batch [2/7], Loss: 2.5855, Accuracy: 45.42%, Grad Norm: 1.22500\n",
      "Epoch [99/10044], Batch [3/7], Loss: 2.8983, Accuracy: 39.00%, Grad Norm: 1.14890\n",
      "Epoch [99/10044], Batch [4/7], Loss: 2.8453, Accuracy: 41.24%, Grad Norm: 1.04851\n",
      "Epoch [99/10044], Batch [5/7], Loss: 2.5326, Accuracy: 46.14%, Grad Norm: 0.87679\n",
      "Epoch [99/10044], Batch [6/7], Loss: 2.5134, Accuracy: 47.27%, Grad Norm: 1.12270\n",
      "Epoch [99/10044], Batch [7/7], Loss: 2.1176, Accuracy: 56.95%, Grad Norm: 0.99193\n",
      "Epoch [99/10044], Loss: 2.1176\n",
      "Epoch [100/10044], Batch [1/7], Loss: 2.2740, Accuracy: 50.21%, Grad Norm: 0.89085\n",
      "Epoch [100/10044], Batch [2/7], Loss: 2.5733, Accuracy: 45.10%, Grad Norm: 1.33003\n",
      "Epoch [100/10044], Batch [3/7], Loss: 2.8957, Accuracy: 37.87%, Grad Norm: 1.42030\n",
      "Epoch [100/10044], Batch [4/7], Loss: 2.8288, Accuracy: 41.65%, Grad Norm: 1.08065\n",
      "Epoch [100/10044], Batch [5/7], Loss: 2.5151, Accuracy: 47.04%, Grad Norm: 0.94164\n",
      "Epoch [100/10044], Batch [6/7], Loss: 2.5164, Accuracy: 47.13%, Grad Norm: 1.44444\n",
      "Epoch [100/10044], Batch [7/7], Loss: 2.0974, Accuracy: 56.55%, Grad Norm: 1.37708\n",
      "Epoch [100/10044], Loss: 2.0974\n",
      "Epoch [101/10044], Batch [1/7], Loss: 2.2644, Accuracy: 51.99%, Grad Norm: 1.07438\n",
      "Epoch [101/10044], Batch [2/7], Loss: 2.5435, Accuracy: 46.31%, Grad Norm: 1.07817\n",
      "Epoch [101/10044], Batch [3/7], Loss: 2.8849, Accuracy: 39.02%, Grad Norm: 1.47945\n",
      "Epoch [101/10044], Batch [4/7], Loss: 2.8278, Accuracy: 40.62%, Grad Norm: 1.42972\n",
      "Epoch [101/10044], Batch [5/7], Loss: 2.5040, Accuracy: 47.26%, Grad Norm: 0.93330\n",
      "Epoch [101/10044], Batch [6/7], Loss: 2.4795, Accuracy: 47.92%, Grad Norm: 1.12874\n",
      "Epoch [101/10044], Batch [7/7], Loss: 2.0587, Accuracy: 57.10%, Grad Norm: 1.11233\n",
      "Epoch [101/10044], Loss: 2.0587\n",
      "Epoch [102/10044], Batch [1/7], Loss: 2.2342, Accuracy: 51.52%, Grad Norm: 0.89741\n",
      "Epoch [102/10044], Batch [2/7], Loss: 2.5389, Accuracy: 46.12%, Grad Norm: 1.19567\n",
      "Epoch [102/10044], Batch [3/7], Loss: 2.8569, Accuracy: 40.57%, Grad Norm: 1.33753\n",
      "Epoch [102/10044], Batch [4/7], Loss: 2.7814, Accuracy: 43.82%, Grad Norm: 1.09844\n",
      "Epoch [102/10044], Batch [5/7], Loss: 2.4714, Accuracy: 48.78%, Grad Norm: 0.85102\n",
      "Epoch [102/10044], Batch [6/7], Loss: 2.4651, Accuracy: 49.02%, Grad Norm: 1.24866\n",
      "Epoch [102/10044], Batch [7/7], Loss: 2.0538, Accuracy: 58.65%, Grad Norm: 0.97764\n",
      "Epoch [102/10044], Loss: 2.0538\n",
      "Epoch [103/10044], Batch [1/7], Loss: 2.2173, Accuracy: 51.26%, Grad Norm: 0.87425\n",
      "Epoch [103/10044], Batch [2/7], Loss: 2.5151, Accuracy: 44.97%, Grad Norm: 1.21047\n",
      "Epoch [103/10044], Batch [3/7], Loss: 2.8259, Accuracy: 38.87%, Grad Norm: 1.12186\n",
      "Epoch [103/10044], Batch [4/7], Loss: 2.7733, Accuracy: 42.26%, Grad Norm: 1.29464\n",
      "Epoch [103/10044], Batch [5/7], Loss: 2.4521, Accuracy: 49.47%, Grad Norm: 1.22472\n",
      "Epoch [103/10044], Batch [6/7], Loss: 2.4535, Accuracy: 51.16%, Grad Norm: 1.47905\n",
      "Epoch [103/10044], Batch [7/7], Loss: 2.0390, Accuracy: 60.42%, Grad Norm: 1.20531\n",
      "Epoch [103/10044], Loss: 2.0390\n",
      "Epoch [104/10044], Batch [1/7], Loss: 2.2100, Accuracy: 52.63%, Grad Norm: 0.90504\n",
      "Epoch [104/10044], Batch [2/7], Loss: 2.5121, Accuracy: 45.19%, Grad Norm: 1.65419\n",
      "Epoch [104/10044], Batch [3/7], Loss: 2.8324, Accuracy: 37.67%, Grad Norm: 1.72922\n",
      "Epoch [104/10044], Batch [4/7], Loss: 2.7621, Accuracy: 40.27%, Grad Norm: 1.41742\n",
      "Epoch [104/10044], Batch [5/7], Loss: 2.4349, Accuracy: 48.26%, Grad Norm: 1.03621\n",
      "Epoch [104/10044], Batch [6/7], Loss: 2.4588, Accuracy: 50.64%, Grad Norm: 1.98461\n",
      "Epoch [104/10044], Batch [7/7], Loss: 2.0321, Accuracy: 61.05%, Grad Norm: 1.59584\n",
      "Epoch [104/10044], Loss: 2.0321\n",
      "Epoch [105/10044], Batch [1/7], Loss: 2.2127, Accuracy: 53.86%, Grad Norm: 1.45004\n",
      "Epoch [105/10044], Batch [2/7], Loss: 2.4863, Accuracy: 47.28%, Grad Norm: 1.51023\n",
      "Epoch [105/10044], Batch [3/7], Loss: 2.8244, Accuracy: 38.47%, Grad Norm: 1.99184\n",
      "Epoch [105/10044], Batch [4/7], Loss: 2.7605, Accuracy: 39.70%, Grad Norm: 1.85449\n",
      "Epoch [105/10044], Batch [5/7], Loss: 2.4263, Accuracy: 47.14%, Grad Norm: 1.29941\n",
      "Epoch [105/10044], Batch [6/7], Loss: 2.3914, Accuracy: 50.14%, Grad Norm: 1.17475\n",
      "Epoch [105/10044], Batch [7/7], Loss: 2.0004, Accuracy: 60.83%, Grad Norm: 1.23408\n",
      "Epoch [105/10044], Loss: 2.0004\n",
      "Epoch [106/10044], Batch [1/7], Loss: 2.1839, Accuracy: 53.72%, Grad Norm: 1.19283\n",
      "Epoch [106/10044], Batch [2/7], Loss: 2.4686, Accuracy: 49.93%, Grad Norm: 1.53123\n",
      "Epoch [106/10044], Batch [3/7], Loss: 2.7564, Accuracy: 42.37%, Grad Norm: 1.19255\n",
      "Epoch [106/10044], Batch [4/7], Loss: 2.6882, Accuracy: 43.97%, Grad Norm: 0.97579\n",
      "Epoch [106/10044], Batch [5/7], Loss: 2.3938, Accuracy: 48.56%, Grad Norm: 1.18793\n",
      "Epoch [106/10044], Batch [6/7], Loss: 2.4115, Accuracy: 47.22%, Grad Norm: 1.68602\n",
      "Epoch [106/10044], Batch [7/7], Loss: 1.9817, Accuracy: 58.32%, Grad Norm: 1.26813\n",
      "Epoch [106/10044], Loss: 1.9817\n",
      "Epoch [107/10044], Batch [1/7], Loss: 2.1605, Accuracy: 52.85%, Grad Norm: 0.93455\n",
      "Epoch [107/10044], Batch [2/7], Loss: 2.4708, Accuracy: 49.02%, Grad Norm: 1.75797\n",
      "Epoch [107/10044], Batch [3/7], Loss: 2.7840, Accuracy: 42.37%, Grad Norm: 1.93631\n",
      "Epoch [107/10044], Batch [4/7], Loss: 2.7197, Accuracy: 45.24%, Grad Norm: 1.71261\n",
      "Epoch [107/10044], Batch [5/7], Loss: 2.3706, Accuracy: 51.04%, Grad Norm: 1.00527\n",
      "Epoch [107/10044], Batch [6/7], Loss: 2.3671, Accuracy: 50.00%, Grad Norm: 1.20288\n",
      "Epoch [107/10044], Batch [7/7], Loss: 1.9710, Accuracy: 59.18%, Grad Norm: 1.27421\n",
      "Epoch [107/10044], Loss: 1.9710\n",
      "Epoch [108/10044], Batch [1/7], Loss: 2.1478, Accuracy: 52.90%, Grad Norm: 1.16946\n",
      "Epoch [108/10044], Batch [2/7], Loss: 2.4342, Accuracy: 48.37%, Grad Norm: 1.19445\n",
      "Epoch [108/10044], Batch [3/7], Loss: 2.7358, Accuracy: 41.80%, Grad Norm: 1.24969\n",
      "Epoch [108/10044], Batch [4/7], Loss: 2.6680, Accuracy: 44.97%, Grad Norm: 1.28370\n",
      "Epoch [108/10044], Batch [5/7], Loss: 2.3655, Accuracy: 50.31%, Grad Norm: 1.07650\n",
      "Epoch [108/10044], Batch [6/7], Loss: 2.3872, Accuracy: 50.90%, Grad Norm: 1.61755\n",
      "Epoch [108/10044], Batch [7/7], Loss: 1.9452, Accuracy: 60.85%, Grad Norm: 1.13578\n",
      "Epoch [108/10044], Loss: 1.9452\n",
      "Epoch [109/10044], Batch [1/7], Loss: 2.1193, Accuracy: 54.84%, Grad Norm: 0.92162\n",
      "Epoch [109/10044], Batch [2/7], Loss: 2.4210, Accuracy: 48.43%, Grad Norm: 1.38095\n",
      "Epoch [109/10044], Batch [3/7], Loss: 2.7302, Accuracy: 41.97%, Grad Norm: 1.56837\n",
      "Epoch [109/10044], Batch [4/7], Loss: 2.6712, Accuracy: 44.33%, Grad Norm: 1.45177\n",
      "Epoch [109/10044], Batch [5/7], Loss: 2.3287, Accuracy: 50.77%, Grad Norm: 1.00385\n",
      "Epoch [109/10044], Batch [6/7], Loss: 2.3578, Accuracy: 51.19%, Grad Norm: 1.58384\n",
      "Epoch [109/10044], Batch [7/7], Loss: 1.9265, Accuracy: 62.28%, Grad Norm: 1.30828\n",
      "Epoch [109/10044], Loss: 1.9265\n",
      "Epoch [110/10044], Batch [1/7], Loss: 2.1253, Accuracy: 54.35%, Grad Norm: 1.06699\n",
      "Epoch [110/10044], Batch [2/7], Loss: 2.3859, Accuracy: 50.96%, Grad Norm: 1.18395\n",
      "Epoch [110/10044], Batch [3/7], Loss: 2.6928, Accuracy: 43.43%, Grad Norm: 1.27158\n",
      "Epoch [110/10044], Batch [4/7], Loss: 2.6147, Accuracy: 45.69%, Grad Norm: 1.15796\n",
      "Epoch [110/10044], Batch [5/7], Loss: 2.3001, Accuracy: 51.17%, Grad Norm: 0.96838\n",
      "Epoch [110/10044], Batch [6/7], Loss: 2.3152, Accuracy: 50.12%, Grad Norm: 1.26480\n",
      "Epoch [110/10044], Batch [7/7], Loss: 1.8855, Accuracy: 61.93%, Grad Norm: 0.90238\n",
      "Epoch [110/10044], Loss: 1.8855\n",
      "Epoch [111/10044], Batch [1/7], Loss: 2.0582, Accuracy: 55.83%, Grad Norm: 0.63626\n",
      "Epoch [111/10044], Batch [2/7], Loss: 2.3443, Accuracy: 52.10%, Grad Norm: 1.07229\n",
      "Epoch [111/10044], Batch [3/7], Loss: 2.6501, Accuracy: 45.53%, Grad Norm: 1.16097\n",
      "Epoch [111/10044], Batch [4/7], Loss: 2.5818, Accuracy: 48.35%, Grad Norm: 1.00692\n",
      "Epoch [111/10044], Batch [5/7], Loss: 2.2513, Accuracy: 53.85%, Grad Norm: 0.65469\n",
      "Epoch [111/10044], Batch [6/7], Loss: 2.2768, Accuracy: 52.47%, Grad Norm: 0.98342\n",
      "Epoch [111/10044], Batch [7/7], Loss: 1.8616, Accuracy: 62.83%, Grad Norm: 0.92066\n",
      "Epoch [111/10044], Loss: 1.8616\n",
      "Epoch [112/10044], Batch [1/7], Loss: 2.0475, Accuracy: 55.67%, Grad Norm: 0.73767\n",
      "Epoch [112/10044], Batch [2/7], Loss: 2.3154, Accuracy: 51.29%, Grad Norm: 0.84670\n",
      "Epoch [112/10044], Batch [3/7], Loss: 2.6228, Accuracy: 44.92%, Grad Norm: 1.01083\n",
      "Epoch [112/10044], Batch [4/7], Loss: 2.5494, Accuracy: 48.51%, Grad Norm: 1.02931\n",
      "Epoch [112/10044], Batch [5/7], Loss: 2.2397, Accuracy: 54.01%, Grad Norm: 0.77525\n",
      "Epoch [112/10044], Batch [6/7], Loss: 2.2563, Accuracy: 53.85%, Grad Norm: 1.07194\n",
      "Epoch [112/10044], Batch [7/7], Loss: 1.8329, Accuracy: 63.47%, Grad Norm: 0.90739\n",
      "Epoch [112/10044], Loss: 1.8329\n",
      "Epoch [113/10044], Batch [1/7], Loss: 2.0215, Accuracy: 56.42%, Grad Norm: 0.70391\n",
      "Epoch [113/10044], Batch [2/7], Loss: 2.3119, Accuracy: 51.23%, Grad Norm: 1.08704\n",
      "Epoch [113/10044], Batch [3/7], Loss: 2.6111, Accuracy: 44.94%, Grad Norm: 1.22049\n",
      "Epoch [113/10044], Batch [4/7], Loss: 2.5229, Accuracy: 48.76%, Grad Norm: 1.07720\n",
      "Epoch [113/10044], Batch [5/7], Loss: 2.2164, Accuracy: 54.30%, Grad Norm: 0.79243\n",
      "Epoch [113/10044], Batch [6/7], Loss: 2.2341, Accuracy: 54.11%, Grad Norm: 1.04724\n",
      "Epoch [113/10044], Batch [7/7], Loss: 1.8196, Accuracy: 64.73%, Grad Norm: 0.98985\n",
      "Epoch [113/10044], Loss: 1.8196\n",
      "Epoch [114/10044], Batch [1/7], Loss: 2.0211, Accuracy: 56.49%, Grad Norm: 0.84995\n",
      "Epoch [114/10044], Batch [2/7], Loss: 2.2921, Accuracy: 51.77%, Grad Norm: 1.34104\n",
      "Epoch [114/10044], Batch [3/7], Loss: 2.5858, Accuracy: 45.49%, Grad Norm: 1.33338\n",
      "Epoch [114/10044], Batch [4/7], Loss: 2.4971, Accuracy: 49.47%, Grad Norm: 0.94178\n",
      "Epoch [114/10044], Batch [5/7], Loss: 2.2025, Accuracy: 54.60%, Grad Norm: 1.00721\n",
      "Epoch [114/10044], Batch [6/7], Loss: 2.2227, Accuracy: 54.84%, Grad Norm: 1.30467\n",
      "Epoch [114/10044], Batch [7/7], Loss: 1.7930, Accuracy: 65.80%, Grad Norm: 1.03480\n",
      "Epoch [114/10044], Loss: 1.7930\n",
      "Epoch [115/10044], Batch [1/7], Loss: 2.0018, Accuracy: 57.46%, Grad Norm: 0.91131\n",
      "Epoch [115/10044], Batch [2/7], Loss: 2.2685, Accuracy: 51.72%, Grad Norm: 1.21560\n",
      "Epoch [115/10044], Batch [3/7], Loss: 2.5727, Accuracy: 44.58%, Grad Norm: 1.33073\n",
      "Epoch [115/10044], Batch [4/7], Loss: 2.4955, Accuracy: 48.39%, Grad Norm: 1.29137\n",
      "Epoch [115/10044], Batch [5/7], Loss: 2.1811, Accuracy: 55.16%, Grad Norm: 0.88742\n",
      "Epoch [115/10044], Batch [6/7], Loss: 2.2006, Accuracy: 55.26%, Grad Norm: 1.20629\n",
      "Epoch [115/10044], Batch [7/7], Loss: 1.7780, Accuracy: 65.50%, Grad Norm: 1.14126\n",
      "Epoch [115/10044], Loss: 1.7780\n",
      "Epoch [116/10044], Batch [1/7], Loss: 2.0081, Accuracy: 57.38%, Grad Norm: 1.10784\n",
      "Epoch [116/10044], Batch [2/7], Loss: 2.2538, Accuracy: 52.32%, Grad Norm: 1.27426\n",
      "Epoch [116/10044], Batch [3/7], Loss: 2.5538, Accuracy: 45.76%, Grad Norm: 1.49595\n",
      "Epoch [116/10044], Batch [4/7], Loss: 2.4635, Accuracy: 50.43%, Grad Norm: 1.16706\n",
      "Epoch [116/10044], Batch [5/7], Loss: 2.1600, Accuracy: 55.52%, Grad Norm: 1.02234\n",
      "Epoch [116/10044], Batch [6/7], Loss: 2.1955, Accuracy: 55.14%, Grad Norm: 1.34723\n",
      "Epoch [116/10044], Batch [7/7], Loss: 1.7626, Accuracy: 65.38%, Grad Norm: 1.14269\n",
      "Epoch [116/10044], Loss: 1.7626\n",
      "Epoch [117/10044], Batch [1/7], Loss: 1.9814, Accuracy: 57.28%, Grad Norm: 1.05706\n",
      "Epoch [117/10044], Batch [2/7], Loss: 2.2280, Accuracy: 53.24%, Grad Norm: 1.07109\n",
      "Epoch [117/10044], Batch [3/7], Loss: 2.5266, Accuracy: 46.88%, Grad Norm: 1.25864\n",
      "Epoch [117/10044], Batch [4/7], Loss: 2.4550, Accuracy: 49.82%, Grad Norm: 1.38288\n",
      "Epoch [117/10044], Batch [5/7], Loss: 2.1457, Accuracy: 55.91%, Grad Norm: 1.10050\n",
      "Epoch [117/10044], Batch [6/7], Loss: 2.1635, Accuracy: 55.44%, Grad Norm: 1.15255\n",
      "Epoch [117/10044], Batch [7/7], Loss: 1.7381, Accuracy: 66.37%, Grad Norm: 1.02876\n",
      "Epoch [117/10044], Loss: 1.7381\n",
      "Epoch [118/10044], Batch [1/7], Loss: 1.9625, Accuracy: 58.49%, Grad Norm: 0.96645\n",
      "Epoch [118/10044], Batch [2/7], Loss: 2.2182, Accuracy: 54.47%, Grad Norm: 1.28924\n",
      "Epoch [118/10044], Batch [3/7], Loss: 2.5121, Accuracy: 47.35%, Grad Norm: 1.39183\n",
      "Epoch [118/10044], Batch [4/7], Loss: 2.4153, Accuracy: 50.77%, Grad Norm: 1.18212\n",
      "Epoch [118/10044], Batch [5/7], Loss: 2.1190, Accuracy: 56.87%, Grad Norm: 0.98194\n",
      "Epoch [118/10044], Batch [6/7], Loss: 2.1751, Accuracy: 54.87%, Grad Norm: 1.53611\n",
      "Epoch [118/10044], Batch [7/7], Loss: 1.7368, Accuracy: 66.18%, Grad Norm: 1.24740\n",
      "Epoch [118/10044], Loss: 1.7368\n",
      "Epoch [119/10044], Batch [1/7], Loss: 1.9336, Accuracy: 58.43%, Grad Norm: 0.91335\n",
      "Epoch [119/10044], Batch [2/7], Loss: 2.1929, Accuracy: 54.11%, Grad Norm: 1.12972\n",
      "Epoch [119/10044], Batch [3/7], Loss: 2.4763, Accuracy: 48.36%, Grad Norm: 1.24795\n",
      "Epoch [119/10044], Batch [4/7], Loss: 2.4005, Accuracy: 52.19%, Grad Norm: 1.30976\n",
      "Epoch [119/10044], Batch [5/7], Loss: 2.0928, Accuracy: 57.97%, Grad Norm: 0.88616\n",
      "Epoch [119/10044], Batch [6/7], Loss: 2.1201, Accuracy: 56.83%, Grad Norm: 1.26145\n",
      "Epoch [119/10044], Batch [7/7], Loss: 1.7021, Accuracy: 67.22%, Grad Norm: 1.10060\n",
      "Epoch [119/10044], Loss: 1.7021\n",
      "Epoch [120/10044], Batch [1/7], Loss: 1.9099, Accuracy: 59.33%, Grad Norm: 0.72419\n",
      "Epoch [120/10044], Batch [2/7], Loss: 2.1672, Accuracy: 55.01%, Grad Norm: 1.15819\n",
      "Epoch [120/10044], Batch [3/7], Loss: 2.4477, Accuracy: 48.49%, Grad Norm: 1.21374\n",
      "Epoch [120/10044], Batch [4/7], Loss: 2.3576, Accuracy: 52.54%, Grad Norm: 1.00931\n",
      "Epoch [120/10044], Batch [5/7], Loss: 2.0634, Accuracy: 58.57%, Grad Norm: 0.81628\n",
      "Epoch [120/10044], Batch [6/7], Loss: 2.0917, Accuracy: 57.09%, Grad Norm: 1.19635\n",
      "Epoch [120/10044], Batch [7/7], Loss: 1.6816, Accuracy: 68.25%, Grad Norm: 1.24134\n",
      "Epoch [120/10044], Loss: 1.6816\n",
      "Epoch [121/10044], Batch [1/7], Loss: 1.8924, Accuracy: 60.32%, Grad Norm: 0.97362\n",
      "Epoch [121/10044], Batch [2/7], Loss: 2.1318, Accuracy: 54.96%, Grad Norm: 1.14236\n",
      "Epoch [121/10044], Batch [3/7], Loss: 2.4304, Accuracy: 48.82%, Grad Norm: 1.37337\n",
      "Epoch [121/10044], Batch [4/7], Loss: 2.3393, Accuracy: 52.65%, Grad Norm: 1.32665\n",
      "Epoch [121/10044], Batch [5/7], Loss: 2.0318, Accuracy: 59.43%, Grad Norm: 0.79178\n",
      "Epoch [121/10044], Batch [6/7], Loss: 2.0629, Accuracy: 59.12%, Grad Norm: 0.98271\n",
      "Epoch [121/10044], Batch [7/7], Loss: 1.6376, Accuracy: 69.50%, Grad Norm: 0.93849\n",
      "Epoch [121/10044], Loss: 1.6376\n",
      "Epoch [122/10044], Batch [1/7], Loss: 1.8566, Accuracy: 61.73%, Grad Norm: 0.77660\n",
      "Epoch [122/10044], Batch [2/7], Loss: 2.1270, Accuracy: 56.52%, Grad Norm: 1.19323\n",
      "Epoch [122/10044], Batch [3/7], Loss: 2.3888, Accuracy: 50.42%, Grad Norm: 1.11207\n",
      "Epoch [122/10044], Batch [4/7], Loss: 2.3093, Accuracy: 53.10%, Grad Norm: 1.10846\n",
      "Epoch [122/10044], Batch [5/7], Loss: 2.0150, Accuracy: 58.43%, Grad Norm: 0.88403\n",
      "Epoch [122/10044], Batch [6/7], Loss: 2.0571, Accuracy: 58.07%, Grad Norm: 1.21011\n",
      "Epoch [122/10044], Batch [7/7], Loss: 1.6282, Accuracy: 69.28%, Grad Norm: 0.99436\n",
      "Epoch [122/10044], Loss: 1.6282\n",
      "Epoch [123/10044], Batch [1/7], Loss: 1.8694, Accuracy: 61.43%, Grad Norm: 1.07343\n",
      "Epoch [123/10044], Batch [2/7], Loss: 2.1119, Accuracy: 57.70%, Grad Norm: 1.49360\n",
      "Epoch [123/10044], Batch [3/7], Loss: 2.3788, Accuracy: 51.15%, Grad Norm: 1.34357\n",
      "Epoch [123/10044], Batch [4/7], Loss: 2.2983, Accuracy: 52.77%, Grad Norm: 1.10741\n",
      "Epoch [123/10044], Batch [5/7], Loss: 2.0128, Accuracy: 58.16%, Grad Norm: 1.08367\n",
      "Epoch [123/10044], Batch [6/7], Loss: 2.0414, Accuracy: 56.36%, Grad Norm: 1.39003\n",
      "Epoch [123/10044], Batch [7/7], Loss: 1.6199, Accuracy: 68.80%, Grad Norm: 1.06625\n",
      "Epoch [123/10044], Loss: 1.6199\n",
      "Epoch [124/10044], Batch [1/7], Loss: 1.8391, Accuracy: 60.78%, Grad Norm: 0.86073\n",
      "Epoch [124/10044], Batch [2/7], Loss: 2.1028, Accuracy: 59.12%, Grad Norm: 1.60936\n",
      "Epoch [124/10044], Batch [3/7], Loss: 2.3801, Accuracy: 53.15%, Grad Norm: 1.77779\n",
      "Epoch [124/10044], Batch [4/7], Loss: 2.2887, Accuracy: 56.32%, Grad Norm: 1.51732\n",
      "Epoch [124/10044], Batch [5/7], Loss: 1.9844, Accuracy: 59.46%, Grad Norm: 1.00435\n",
      "Epoch [124/10044], Batch [6/7], Loss: 2.0230, Accuracy: 56.91%, Grad Norm: 1.48657\n",
      "Epoch [124/10044], Batch [7/7], Loss: 1.6003, Accuracy: 67.83%, Grad Norm: 1.36818\n",
      "Epoch [124/10044], Loss: 1.6003\n",
      "Epoch [125/10044], Batch [1/7], Loss: 1.8317, Accuracy: 60.06%, Grad Norm: 0.98040\n",
      "Epoch [125/10044], Batch [2/7], Loss: 2.0807, Accuracy: 58.15%, Grad Norm: 1.37407\n",
      "Epoch [125/10044], Batch [3/7], Loss: 2.3647, Accuracy: 52.83%, Grad Norm: 1.68338\n",
      "Epoch [125/10044], Batch [4/7], Loss: 2.2634, Accuracy: 57.27%, Grad Norm: 1.49101\n",
      "Epoch [125/10044], Batch [5/7], Loss: 1.9777, Accuracy: 60.84%, Grad Norm: 1.03370\n",
      "Epoch [125/10044], Batch [6/7], Loss: 2.0074, Accuracy: 59.25%, Grad Norm: 1.24376\n",
      "Epoch [125/10044], Batch [7/7], Loss: 1.5794, Accuracy: 69.13%, Grad Norm: 1.01019\n",
      "Epoch [125/10044], Loss: 1.5794\n",
      "Epoch [126/10044], Batch [1/7], Loss: 1.8008, Accuracy: 61.62%, Grad Norm: 0.84454\n",
      "Epoch [126/10044], Batch [2/7], Loss: 2.0698, Accuracy: 55.77%, Grad Norm: 1.37641\n",
      "Epoch [126/10044], Batch [3/7], Loss: 2.3405, Accuracy: 50.76%, Grad Norm: 1.50078\n",
      "Epoch [126/10044], Batch [4/7], Loss: 2.2344, Accuracy: 56.57%, Grad Norm: 1.25151\n",
      "Epoch [126/10044], Batch [5/7], Loss: 1.9353, Accuracy: 62.47%, Grad Norm: 0.96039\n",
      "Epoch [126/10044], Batch [6/7], Loss: 1.9793, Accuracy: 62.02%, Grad Norm: 1.35444\n",
      "Epoch [126/10044], Batch [7/7], Loss: 1.5603, Accuracy: 71.92%, Grad Norm: 1.14996\n",
      "Epoch [126/10044], Loss: 1.5603\n",
      "Epoch [127/10044], Batch [1/7], Loss: 1.8017, Accuracy: 62.27%, Grad Norm: 1.07746\n",
      "Epoch [127/10044], Batch [2/7], Loss: 2.0469, Accuracy: 55.80%, Grad Norm: 1.51092\n",
      "Epoch [127/10044], Batch [3/7], Loss: 2.3314, Accuracy: 49.56%, Grad Norm: 1.61003\n",
      "Epoch [127/10044], Batch [4/7], Loss: 2.2312, Accuracy: 54.97%, Grad Norm: 1.40503\n",
      "Epoch [127/10044], Batch [5/7], Loss: 1.9230, Accuracy: 62.07%, Grad Norm: 1.07516\n",
      "Epoch [127/10044], Batch [6/7], Loss: 1.9774, Accuracy: 62.47%, Grad Norm: 1.60067\n",
      "Epoch [127/10044], Batch [7/7], Loss: 1.5620, Accuracy: 73.12%, Grad Norm: 1.40007\n",
      "Epoch [127/10044], Loss: 1.5620\n",
      "Epoch [128/10044], Batch [1/7], Loss: 1.7935, Accuracy: 63.12%, Grad Norm: 1.27387\n",
      "Epoch [128/10044], Batch [2/7], Loss: 2.0135, Accuracy: 58.72%, Grad Norm: 1.11983\n",
      "Epoch [128/10044], Batch [3/7], Loss: 2.2943, Accuracy: 50.86%, Grad Norm: 1.33942\n",
      "Epoch [128/10044], Batch [4/7], Loss: 2.2072, Accuracy: 54.17%, Grad Norm: 1.54429\n",
      "Epoch [128/10044], Batch [5/7], Loss: 1.9146, Accuracy: 60.69%, Grad Norm: 1.20124\n",
      "Epoch [128/10044], Batch [6/7], Loss: 1.9381, Accuracy: 61.96%, Grad Norm: 1.09712\n",
      "Epoch [128/10044], Batch [7/7], Loss: 1.5075, Accuracy: 72.92%, Grad Norm: 0.99384\n",
      "Epoch [128/10044], Loss: 1.5075\n",
      "Epoch [129/10044], Batch [1/7], Loss: 1.7480, Accuracy: 64.27%, Grad Norm: 0.92829\n",
      "Epoch [129/10044], Batch [2/7], Loss: 1.9991, Accuracy: 58.87%, Grad Norm: 1.32424\n",
      "Epoch [129/10044], Batch [3/7], Loss: 2.2872, Accuracy: 52.76%, Grad Norm: 1.52464\n",
      "Epoch [129/10044], Batch [4/7], Loss: 2.1850, Accuracy: 56.10%, Grad Norm: 1.22640\n",
      "Epoch [129/10044], Batch [5/7], Loss: 1.8866, Accuracy: 62.24%, Grad Norm: 1.01037\n",
      "Epoch [129/10044], Batch [6/7], Loss: 1.9123, Accuracy: 61.70%, Grad Norm: 1.03093\n",
      "Epoch [129/10044], Batch [7/7], Loss: 1.5086, Accuracy: 72.13%, Grad Norm: 1.09530\n",
      "Epoch [129/10044], Loss: 1.5086\n",
      "Epoch [130/10044], Batch [1/7], Loss: 1.7433, Accuracy: 64.09%, Grad Norm: 0.96487\n",
      "Epoch [130/10044], Batch [2/7], Loss: 1.9901, Accuracy: 60.05%, Grad Norm: 1.39340\n",
      "Epoch [130/10044], Batch [3/7], Loss: 2.2757, Accuracy: 53.76%, Grad Norm: 1.56345\n",
      "Epoch [130/10044], Batch [4/7], Loss: 2.1576, Accuracy: 57.57%, Grad Norm: 1.28654\n",
      "Epoch [130/10044], Batch [5/7], Loss: 1.8567, Accuracy: 63.67%, Grad Norm: 0.93335\n",
      "Epoch [130/10044], Batch [6/7], Loss: 1.9036, Accuracy: 61.52%, Grad Norm: 1.38157\n",
      "Epoch [130/10044], Batch [7/7], Loss: 1.4876, Accuracy: 72.50%, Grad Norm: 1.11343\n",
      "Epoch [130/10044], Loss: 1.4876\n",
      "Epoch [131/10044], Batch [1/7], Loss: 1.7238, Accuracy: 63.44%, Grad Norm: 0.95195\n",
      "Epoch [131/10044], Batch [2/7], Loss: 1.9618, Accuracy: 61.14%, Grad Norm: 1.19609\n",
      "Epoch [131/10044], Batch [3/7], Loss: 2.2369, Accuracy: 54.87%, Grad Norm: 1.43545\n",
      "Epoch [131/10044], Batch [4/7], Loss: 2.1278, Accuracy: 58.67%, Grad Norm: 1.09507\n",
      "Epoch [131/10044], Batch [5/7], Loss: 1.8397, Accuracy: 63.18%, Grad Norm: 1.04599\n",
      "Epoch [131/10044], Batch [6/7], Loss: 1.8815, Accuracy: 62.17%, Grad Norm: 1.27708\n",
      "Epoch [131/10044], Batch [7/7], Loss: 1.4526, Accuracy: 73.03%, Grad Norm: 0.94872\n",
      "Epoch [131/10044], Loss: 1.4526\n",
      "Epoch [132/10044], Batch [1/7], Loss: 1.7046, Accuracy: 65.04%, Grad Norm: 0.82102\n",
      "Epoch [132/10044], Batch [2/7], Loss: 1.9541, Accuracy: 61.78%, Grad Norm: 1.31165\n",
      "Epoch [132/10044], Batch [3/7], Loss: 2.2385, Accuracy: 55.30%, Grad Norm: 1.60682\n",
      "Epoch [132/10044], Batch [4/7], Loss: 2.1148, Accuracy: 58.80%, Grad Norm: 1.22570\n",
      "Epoch [132/10044], Batch [5/7], Loss: 1.8154, Accuracy: 63.62%, Grad Norm: 0.90348\n",
      "Epoch [132/10044], Batch [6/7], Loss: 1.8559, Accuracy: 61.82%, Grad Norm: 1.26397\n",
      "Epoch [132/10044], Batch [7/7], Loss: 1.4394, Accuracy: 73.13%, Grad Norm: 0.98602\n",
      "Epoch [132/10044], Loss: 1.4394\n",
      "Epoch [133/10044], Batch [1/7], Loss: 1.7095, Accuracy: 63.78%, Grad Norm: 1.13670\n",
      "Epoch [133/10044], Batch [2/7], Loss: 1.9281, Accuracy: 62.54%, Grad Norm: 1.19241\n",
      "Epoch [133/10044], Batch [3/7], Loss: 2.1929, Accuracy: 57.22%, Grad Norm: 1.36309\n",
      "Epoch [133/10044], Batch [4/7], Loss: 2.0810, Accuracy: 61.73%, Grad Norm: 1.25134\n",
      "Epoch [133/10044], Batch [5/7], Loss: 1.7909, Accuracy: 65.70%, Grad Norm: 0.90454\n",
      "Epoch [133/10044], Batch [6/7], Loss: 1.8457, Accuracy: 62.88%, Grad Norm: 1.17957\n",
      "Epoch [133/10044], Batch [7/7], Loss: 1.4417, Accuracy: 72.48%, Grad Norm: 1.18803\n",
      "Epoch [133/10044], Loss: 1.4417\n",
      "Epoch [134/10044], Batch [1/7], Loss: 1.6683, Accuracy: 64.26%, Grad Norm: 0.89875\n",
      "Epoch [134/10044], Batch [2/7], Loss: 1.8996, Accuracy: 61.12%, Grad Norm: 1.27127\n",
      "Epoch [134/10044], Batch [3/7], Loss: 2.1786, Accuracy: 55.27%, Grad Norm: 1.30447\n",
      "Epoch [134/10044], Batch [4/7], Loss: 2.0595, Accuracy: 61.12%, Grad Norm: 1.17377\n",
      "Epoch [134/10044], Batch [5/7], Loss: 1.7769, Accuracy: 67.11%, Grad Norm: 1.09709\n",
      "Epoch [134/10044], Batch [6/7], Loss: 1.8257, Accuracy: 64.73%, Grad Norm: 1.33163\n",
      "Epoch [134/10044], Batch [7/7], Loss: 1.4230, Accuracy: 75.57%, Grad Norm: 1.24879\n",
      "Epoch [134/10044], Loss: 1.4230\n",
      "Epoch [135/10044], Batch [1/7], Loss: 1.6537, Accuracy: 65.20%, Grad Norm: 1.02790\n",
      "Epoch [135/10044], Batch [2/7], Loss: 1.8897, Accuracy: 60.40%, Grad Norm: 1.22990\n",
      "Epoch [135/10044], Batch [3/7], Loss: 2.1658, Accuracy: 53.94%, Grad Norm: 1.53342\n",
      "Epoch [135/10044], Batch [4/7], Loss: 2.0526, Accuracy: 60.32%, Grad Norm: 1.29965\n",
      "Epoch [135/10044], Batch [5/7], Loss: 1.7635, Accuracy: 66.38%, Grad Norm: 1.04758\n",
      "Epoch [135/10044], Batch [6/7], Loss: 1.8056, Accuracy: 66.62%, Grad Norm: 1.40482\n",
      "Epoch [135/10044], Batch [7/7], Loss: 1.4064, Accuracy: 75.92%, Grad Norm: 1.29844\n",
      "Epoch [135/10044], Loss: 1.4064\n",
      "Epoch [136/10044], Batch [1/7], Loss: 1.6435, Accuracy: 67.12%, Grad Norm: 1.02951\n",
      "Epoch [136/10044], Batch [2/7], Loss: 1.8625, Accuracy: 61.57%, Grad Norm: 1.13327\n",
      "Epoch [136/10044], Batch [3/7], Loss: 2.1472, Accuracy: 53.97%, Grad Norm: 1.45625\n",
      "Epoch [136/10044], Batch [4/7], Loss: 2.0412, Accuracy: 58.82%, Grad Norm: 1.50596\n",
      "Epoch [136/10044], Batch [5/7], Loss: 1.7519, Accuracy: 66.04%, Grad Norm: 1.14016\n",
      "Epoch [136/10044], Batch [6/7], Loss: 1.7793, Accuracy: 65.67%, Grad Norm: 1.13545\n",
      "Epoch [136/10044], Batch [7/7], Loss: 1.3642, Accuracy: 76.77%, Grad Norm: 0.91518\n",
      "Epoch [136/10044], Loss: 1.3642\n",
      "Epoch [137/10044], Batch [1/7], Loss: 1.6196, Accuracy: 67.61%, Grad Norm: 0.94922\n",
      "Epoch [137/10044], Batch [2/7], Loss: 1.8471, Accuracy: 63.27%, Grad Norm: 1.38572\n",
      "Epoch [137/10044], Batch [3/7], Loss: 2.1215, Accuracy: 56.34%, Grad Norm: 1.39513\n",
      "Epoch [137/10044], Batch [4/7], Loss: 1.9978, Accuracy: 61.16%, Grad Norm: 1.04964\n",
      "Epoch [137/10044], Batch [5/7], Loss: 1.7072, Accuracy: 67.64%, Grad Norm: 0.95103\n",
      "Epoch [137/10044], Batch [6/7], Loss: 1.7597, Accuracy: 65.89%, Grad Norm: 1.27497\n",
      "Epoch [137/10044], Batch [7/7], Loss: 1.3699, Accuracy: 75.68%, Grad Norm: 1.14844\n",
      "Epoch [137/10044], Loss: 1.3699\n",
      "Epoch [138/10044], Batch [1/7], Loss: 1.6133, Accuracy: 66.91%, Grad Norm: 1.01046\n",
      "Epoch [138/10044], Batch [2/7], Loss: 1.8345, Accuracy: 63.82%, Grad Norm: 1.23970\n",
      "Epoch [138/10044], Batch [3/7], Loss: 2.0998, Accuracy: 57.63%, Grad Norm: 1.47804\n",
      "Epoch [138/10044], Batch [4/7], Loss: 1.9750, Accuracy: 62.60%, Grad Norm: 1.41750\n",
      "Epoch [138/10044], Batch [5/7], Loss: 1.6867, Accuracy: 67.68%, Grad Norm: 0.87425\n",
      "Epoch [138/10044], Batch [6/7], Loss: 1.7369, Accuracy: 65.67%, Grad Norm: 1.05811\n",
      "Epoch [138/10044], Batch [7/7], Loss: 1.3425, Accuracy: 76.13%, Grad Norm: 1.03789\n",
      "Epoch [138/10044], Loss: 1.3425\n",
      "Epoch [139/10044], Batch [1/7], Loss: 1.5928, Accuracy: 67.31%, Grad Norm: 0.98524\n",
      "Epoch [139/10044], Batch [2/7], Loss: 1.7946, Accuracy: 65.16%, Grad Norm: 1.09497\n",
      "Epoch [139/10044], Batch [3/7], Loss: 2.0692, Accuracy: 58.45%, Grad Norm: 1.18807\n",
      "Epoch [139/10044], Batch [4/7], Loss: 1.9406, Accuracy: 63.60%, Grad Norm: 0.96885\n",
      "Epoch [139/10044], Batch [5/7], Loss: 1.6775, Accuracy: 68.72%, Grad Norm: 0.89778\n",
      "Epoch [139/10044], Batch [6/7], Loss: 1.7490, Accuracy: 65.32%, Grad Norm: 1.53896\n",
      "Epoch [139/10044], Batch [7/7], Loss: 1.3273, Accuracy: 75.93%, Grad Norm: 1.12029\n",
      "Epoch [139/10044], Loss: 1.3273\n",
      "Epoch [140/10044], Batch [1/7], Loss: 1.5714, Accuracy: 68.10%, Grad Norm: 0.96049\n",
      "Epoch [140/10044], Batch [2/7], Loss: 1.7694, Accuracy: 66.51%, Grad Norm: 1.05984\n",
      "Epoch [140/10044], Batch [3/7], Loss: 2.0329, Accuracy: 60.25%, Grad Norm: 1.22670\n",
      "Epoch [140/10044], Batch [4/7], Loss: 1.9460, Accuracy: 64.28%, Grad Norm: 1.42419\n",
      "Epoch [140/10044], Batch [5/7], Loss: 1.6677, Accuracy: 69.24%, Grad Norm: 1.09835\n",
      "Epoch [140/10044], Batch [6/7], Loss: 1.7080, Accuracy: 67.14%, Grad Norm: 1.30129\n",
      "Epoch [140/10044], Batch [7/7], Loss: 1.2983, Accuracy: 76.73%, Grad Norm: 0.98111\n",
      "Epoch [140/10044], Loss: 1.2983\n",
      "Epoch [141/10044], Batch [1/7], Loss: 1.5349, Accuracy: 68.07%, Grad Norm: 0.75923\n",
      "Epoch [141/10044], Batch [2/7], Loss: 1.7628, Accuracy: 65.22%, Grad Norm: 1.12542\n",
      "Epoch [141/10044], Batch [3/7], Loss: 2.0366, Accuracy: 59.17%, Grad Norm: 1.41788\n",
      "Epoch [141/10044], Batch [4/7], Loss: 1.9167, Accuracy: 64.48%, Grad Norm: 1.29518\n",
      "Epoch [141/10044], Batch [5/7], Loss: 1.6302, Accuracy: 70.25%, Grad Norm: 0.96505\n",
      "Epoch [141/10044], Batch [6/7], Loss: 1.6747, Accuracy: 68.34%, Grad Norm: 1.03713\n",
      "Epoch [141/10044], Batch [7/7], Loss: 1.2627, Accuracy: 78.02%, Grad Norm: 0.86633\n",
      "Epoch [141/10044], Loss: 1.2627\n",
      "Epoch [142/10044], Batch [1/7], Loss: 1.5357, Accuracy: 68.67%, Grad Norm: 0.84859\n",
      "Epoch [142/10044], Batch [2/7], Loss: 1.7348, Accuracy: 65.04%, Grad Norm: 1.15318\n",
      "Epoch [142/10044], Batch [3/7], Loss: 1.9982, Accuracy: 58.82%, Grad Norm: 1.33408\n",
      "Epoch [142/10044], Batch [4/7], Loss: 1.8801, Accuracy: 65.77%, Grad Norm: 1.17414\n",
      "Epoch [142/10044], Batch [5/7], Loss: 1.6070, Accuracy: 70.93%, Grad Norm: 0.92167\n",
      "Epoch [142/10044], Batch [6/7], Loss: 1.6611, Accuracy: 69.21%, Grad Norm: 1.15469\n",
      "Epoch [142/10044], Batch [7/7], Loss: 1.2538, Accuracy: 78.15%, Grad Norm: 0.99387\n",
      "Epoch [142/10044], Loss: 1.2538\n",
      "Epoch [143/10044], Batch [1/7], Loss: 1.5167, Accuracy: 70.21%, Grad Norm: 1.01753\n",
      "Epoch [143/10044], Batch [2/7], Loss: 1.7196, Accuracy: 65.75%, Grad Norm: 1.19485\n",
      "Epoch [143/10044], Batch [3/7], Loss: 1.9797, Accuracy: 59.39%, Grad Norm: 1.41271\n",
      "Epoch [143/10044], Batch [4/7], Loss: 1.8675, Accuracy: 63.99%, Grad Norm: 1.15956\n",
      "Epoch [143/10044], Batch [5/7], Loss: 1.5846, Accuracy: 70.45%, Grad Norm: 0.87839\n",
      "Epoch [143/10044], Batch [6/7], Loss: 1.6475, Accuracy: 69.68%, Grad Norm: 1.33104\n",
      "Epoch [143/10044], Batch [7/7], Loss: 1.2577, Accuracy: 79.25%, Grad Norm: 1.20470\n",
      "Epoch [143/10044], Loss: 1.2577\n",
      "Epoch [144/10044], Batch [1/7], Loss: 1.5152, Accuracy: 69.83%, Grad Norm: 1.14507\n",
      "Epoch [144/10044], Batch [2/7], Loss: 1.6917, Accuracy: 67.87%, Grad Norm: 1.21644\n",
      "Epoch [144/10044], Batch [3/7], Loss: 1.9666, Accuracy: 60.78%, Grad Norm: 1.52394\n",
      "Epoch [144/10044], Batch [4/7], Loss: 1.8418, Accuracy: 65.00%, Grad Norm: 1.32680\n",
      "Epoch [144/10044], Batch [5/7], Loss: 1.5876, Accuracy: 70.12%, Grad Norm: 1.17034\n",
      "Epoch [144/10044], Batch [6/7], Loss: 1.6168, Accuracy: 69.11%, Grad Norm: 1.16246\n",
      "Epoch [144/10044], Batch [7/7], Loss: 1.2352, Accuracy: 79.27%, Grad Norm: 1.11840\n",
      "Epoch [144/10044], Loss: 1.2352\n",
      "Epoch [145/10044], Batch [1/7], Loss: 1.4964, Accuracy: 69.32%, Grad Norm: 1.15286\n",
      "Epoch [145/10044], Batch [2/7], Loss: 1.7051, Accuracy: 67.49%, Grad Norm: 1.67086\n",
      "Epoch [145/10044], Batch [3/7], Loss: 1.9584, Accuracy: 61.72%, Grad Norm: 1.53967\n",
      "Epoch [145/10044], Batch [4/7], Loss: 1.8165, Accuracy: 67.37%, Grad Norm: 1.28538\n",
      "Epoch [145/10044], Batch [5/7], Loss: 1.5538, Accuracy: 71.45%, Grad Norm: 1.08559\n",
      "Epoch [145/10044], Batch [6/7], Loss: 1.6245, Accuracy: 67.94%, Grad Norm: 1.54361\n",
      "Epoch [145/10044], Batch [7/7], Loss: 1.2237, Accuracy: 78.23%, Grad Norm: 1.22134\n",
      "Epoch [145/10044], Loss: 1.2237\n",
      "Epoch [146/10044], Batch [1/7], Loss: 1.4803, Accuracy: 70.24%, Grad Norm: 1.09487\n",
      "Epoch [146/10044], Batch [2/7], Loss: 1.6864, Accuracy: 67.63%, Grad Norm: 1.61505\n",
      "Epoch [146/10044], Batch [3/7], Loss: 1.9452, Accuracy: 61.41%, Grad Norm: 1.74186\n",
      "Epoch [146/10044], Batch [4/7], Loss: 1.8087, Accuracy: 67.17%, Grad Norm: 1.40676\n",
      "Epoch [146/10044], Batch [5/7], Loss: 1.5268, Accuracy: 72.17%, Grad Norm: 1.01573\n",
      "Epoch [146/10044], Batch [6/7], Loss: 1.5975, Accuracy: 69.46%, Grad Norm: 1.33987\n",
      "Epoch [146/10044], Batch [7/7], Loss: 1.2032, Accuracy: 79.52%, Grad Norm: 1.09416\n",
      "Epoch [146/10044], Loss: 1.2032\n",
      "Epoch [147/10044], Batch [1/7], Loss: 1.4812, Accuracy: 70.33%, Grad Norm: 1.20387\n",
      "Epoch [147/10044], Batch [2/7], Loss: 1.6699, Accuracy: 67.68%, Grad Norm: 1.34047\n",
      "Epoch [147/10044], Batch [3/7], Loss: 1.9147, Accuracy: 62.27%, Grad Norm: 1.46930\n",
      "Epoch [147/10044], Batch [4/7], Loss: 1.7960, Accuracy: 67.09%, Grad Norm: 1.45424\n",
      "Epoch [147/10044], Batch [5/7], Loss: 1.5209, Accuracy: 72.77%, Grad Norm: 1.00380\n",
      "Epoch [147/10044], Batch [6/7], Loss: 1.5819, Accuracy: 70.08%, Grad Norm: 1.29118\n",
      "Epoch [147/10044], Batch [7/7], Loss: 1.1905, Accuracy: 79.53%, Grad Norm: 1.15776\n",
      "Epoch [147/10044], Loss: 1.1905\n",
      "Epoch [148/10044], Batch [1/7], Loss: 1.4552, Accuracy: 70.81%, Grad Norm: 1.06097\n",
      "Epoch [148/10044], Batch [2/7], Loss: 1.6513, Accuracy: 68.18%, Grad Norm: 1.21551\n",
      "Epoch [148/10044], Batch [3/7], Loss: 1.9050, Accuracy: 63.36%, Grad Norm: 1.43027\n",
      "Epoch [148/10044], Batch [4/7], Loss: 1.7731, Accuracy: 69.07%, Grad Norm: 1.37829\n",
      "Epoch [148/10044], Batch [5/7], Loss: 1.5045, Accuracy: 73.29%, Grad Norm: 1.04501\n",
      "Epoch [148/10044], Batch [6/7], Loss: 1.5570, Accuracy: 70.33%, Grad Norm: 1.15500\n",
      "Epoch [148/10044], Batch [7/7], Loss: 1.1607, Accuracy: 80.22%, Grad Norm: 0.93576\n",
      "Epoch [148/10044], Loss: 1.1607\n",
      "Epoch [149/10044], Batch [1/7], Loss: 1.4277, Accuracy: 70.33%, Grad Norm: 0.88437\n",
      "Epoch [149/10044], Batch [2/7], Loss: 1.6168, Accuracy: 68.49%, Grad Norm: 1.15838\n",
      "Epoch [149/10044], Batch [3/7], Loss: 1.8687, Accuracy: 63.41%, Grad Norm: 1.31370\n",
      "Epoch [149/10044], Batch [4/7], Loss: 1.7384, Accuracy: 69.01%, Grad Norm: 1.06619\n",
      "Epoch [149/10044], Batch [5/7], Loss: 1.4764, Accuracy: 74.19%, Grad Norm: 0.83593\n",
      "Epoch [149/10044], Batch [6/7], Loss: 1.5308, Accuracy: 72.27%, Grad Norm: 1.17285\n",
      "Epoch [149/10044], Batch [7/7], Loss: 1.1474, Accuracy: 81.23%, Grad Norm: 1.00252\n",
      "Epoch [149/10044], Loss: 1.1474\n",
      "Epoch [150/10044], Batch [1/7], Loss: 1.3999, Accuracy: 72.73%, Grad Norm: 0.78584\n",
      "Epoch [150/10044], Batch [2/7], Loss: 1.5851, Accuracy: 68.62%, Grad Norm: 0.97077\n",
      "Epoch [150/10044], Batch [3/7], Loss: 1.8384, Accuracy: 62.85%, Grad Norm: 1.10019\n",
      "Epoch [150/10044], Batch [4/7], Loss: 1.7069, Accuracy: 69.02%, Grad Norm: 0.99171\n",
      "Epoch [150/10044], Batch [5/7], Loss: 1.4352, Accuracy: 75.43%, Grad Norm: 0.79443\n",
      "Epoch [150/10044], Batch [6/7], Loss: 1.5043, Accuracy: 73.34%, Grad Norm: 1.03971\n",
      "Epoch [150/10044], Batch [7/7], Loss: 1.1234, Accuracy: 81.87%, Grad Norm: 0.86034\n",
      "Epoch [150/10044], Loss: 1.1234\n",
      "Epoch [151/10044], Batch [1/7], Loss: 1.3820, Accuracy: 73.61%, Grad Norm: 0.80611\n",
      "Epoch [151/10044], Batch [2/7], Loss: 1.5721, Accuracy: 69.51%, Grad Norm: 1.15150\n",
      "Epoch [151/10044], Batch [3/7], Loss: 1.8270, Accuracy: 62.91%, Grad Norm: 1.32418\n",
      "Epoch [151/10044], Batch [4/7], Loss: 1.6828, Accuracy: 69.21%, Grad Norm: 0.97543\n",
      "Epoch [151/10044], Batch [5/7], Loss: 1.4287, Accuracy: 75.15%, Grad Norm: 0.91094\n",
      "Epoch [151/10044], Batch [6/7], Loss: 1.4999, Accuracy: 73.09%, Grad Norm: 1.28681\n",
      "Epoch [151/10044], Batch [7/7], Loss: 1.1100, Accuracy: 82.92%, Grad Norm: 1.00650\n",
      "Epoch [151/10044], Loss: 1.1100\n",
      "Epoch [152/10044], Batch [1/7], Loss: 1.3675, Accuracy: 74.04%, Grad Norm: 0.94935\n",
      "Epoch [152/10044], Batch [2/7], Loss: 1.5456, Accuracy: 71.22%, Grad Norm: 1.14273\n",
      "Epoch [152/10044], Batch [3/7], Loss: 1.7908, Accuracy: 65.05%, Grad Norm: 1.23516\n",
      "Epoch [152/10044], Batch [4/7], Loss: 1.6722, Accuracy: 69.44%, Grad Norm: 1.21442\n",
      "Epoch [152/10044], Batch [5/7], Loss: 1.4217, Accuracy: 75.22%, Grad Norm: 0.92527\n",
      "Epoch [152/10044], Batch [6/7], Loss: 1.4720, Accuracy: 72.94%, Grad Norm: 1.22438\n",
      "Epoch [152/10044], Batch [7/7], Loss: 1.0888, Accuracy: 82.15%, Grad Norm: 0.95872\n",
      "Epoch [152/10044], Loss: 1.0888\n",
      "Epoch [153/10044], Batch [1/7], Loss: 1.3514, Accuracy: 74.59%, Grad Norm: 0.86629\n",
      "Epoch [153/10044], Batch [2/7], Loss: 1.5284, Accuracy: 71.86%, Grad Norm: 1.12530\n",
      "Epoch [153/10044], Batch [3/7], Loss: 1.7763, Accuracy: 65.87%, Grad Norm: 1.21696\n",
      "Epoch [153/10044], Batch [4/7], Loss: 1.6500, Accuracy: 69.74%, Grad Norm: 1.38109\n",
      "Epoch [153/10044], Batch [5/7], Loss: 1.4044, Accuracy: 73.87%, Grad Norm: 1.10879\n",
      "Epoch [153/10044], Batch [6/7], Loss: 1.4532, Accuracy: 72.41%, Grad Norm: 1.33157\n",
      "Epoch [153/10044], Batch [7/7], Loss: 1.0690, Accuracy: 82.65%, Grad Norm: 0.90620\n",
      "Epoch [153/10044], Loss: 1.0690\n",
      "Epoch [154/10044], Batch [1/7], Loss: 1.3457, Accuracy: 74.40%, Grad Norm: 1.12285\n",
      "Epoch [154/10044], Batch [2/7], Loss: 1.5346, Accuracy: 72.81%, Grad Norm: 1.55761\n",
      "Epoch [154/10044], Batch [3/7], Loss: 1.7774, Accuracy: 67.99%, Grad Norm: 1.68074\n",
      "Epoch [154/10044], Batch [4/7], Loss: 1.6450, Accuracy: 70.91%, Grad Norm: 1.47082\n",
      "Epoch [154/10044], Batch [5/7], Loss: 1.3930, Accuracy: 74.74%, Grad Norm: 1.17288\n",
      "Epoch [154/10044], Batch [6/7], Loss: 1.4550, Accuracy: 71.40%, Grad Norm: 1.39884\n",
      "Epoch [154/10044], Batch [7/7], Loss: 1.0798, Accuracy: 81.42%, Grad Norm: 1.25759\n",
      "Epoch [154/10044], Loss: 1.0798\n",
      "Epoch [155/10044], Batch [1/7], Loss: 1.3522, Accuracy: 72.54%, Grad Norm: 1.18691\n",
      "Epoch [155/10044], Batch [2/7], Loss: 1.5256, Accuracy: 73.10%, Grad Norm: 1.64736\n",
      "Epoch [155/10044], Batch [3/7], Loss: 1.7712, Accuracy: 68.37%, Grad Norm: 1.73640\n",
      "Epoch [155/10044], Batch [4/7], Loss: 1.6410, Accuracy: 73.74%, Grad Norm: 1.61346\n",
      "Epoch [155/10044], Batch [5/7], Loss: 1.3882, Accuracy: 76.67%, Grad Norm: 1.14202\n",
      "Epoch [155/10044], Batch [6/7], Loss: 1.4543, Accuracy: 71.90%, Grad Norm: 1.48702\n",
      "Epoch [155/10044], Batch [7/7], Loss: 1.0714, Accuracy: 81.15%, Grad Norm: 1.24267\n",
      "Epoch [155/10044], Loss: 1.0714\n",
      "Epoch [156/10044], Batch [1/7], Loss: 1.3256, Accuracy: 72.72%, Grad Norm: 1.12525\n",
      "Epoch [156/10044], Batch [2/7], Loss: 1.5007, Accuracy: 70.93%, Grad Norm: 1.23969\n",
      "Epoch [156/10044], Batch [3/7], Loss: 1.7283, Accuracy: 66.91%, Grad Norm: 1.47220\n",
      "Epoch [156/10044], Batch [4/7], Loss: 1.6065, Accuracy: 73.32%, Grad Norm: 1.37768\n",
      "Epoch [156/10044], Batch [5/7], Loss: 1.3566, Accuracy: 77.62%, Grad Norm: 1.13207\n",
      "Epoch [156/10044], Batch [6/7], Loss: 1.4183, Accuracy: 75.58%, Grad Norm: 1.25285\n",
      "Epoch [156/10044], Batch [7/7], Loss: 1.0343, Accuracy: 83.03%, Grad Norm: 0.95187\n",
      "Epoch [156/10044], Loss: 1.0343\n",
      "Epoch [157/10044], Batch [1/7], Loss: 1.2958, Accuracy: 75.07%, Grad Norm: 0.96435\n",
      "Epoch [157/10044], Batch [2/7], Loss: 1.4933, Accuracy: 70.04%, Grad Norm: 1.41360\n",
      "Epoch [157/10044], Batch [3/7], Loss: 1.7110, Accuracy: 65.50%, Grad Norm: 1.35007\n",
      "Epoch [157/10044], Batch [4/7], Loss: 1.5946, Accuracy: 71.12%, Grad Norm: 1.33080\n",
      "Epoch [157/10044], Batch [5/7], Loss: 1.3382, Accuracy: 77.72%, Grad Norm: 1.04750\n",
      "Epoch [157/10044], Batch [6/7], Loss: 1.4102, Accuracy: 76.94%, Grad Norm: 1.50592\n",
      "Epoch [157/10044], Batch [7/7], Loss: 1.0430, Accuracy: 84.82%, Grad Norm: 1.28206\n",
      "Epoch [157/10044], Loss: 1.0430\n",
      "Epoch [158/10044], Batch [1/7], Loss: 1.3042, Accuracy: 76.55%, Grad Norm: 1.14153\n",
      "Epoch [158/10044], Batch [2/7], Loss: 1.4584, Accuracy: 72.67%, Grad Norm: 1.30423\n",
      "Epoch [158/10044], Batch [3/7], Loss: 1.7129, Accuracy: 65.19%, Grad Norm: 1.72651\n",
      "Epoch [158/10044], Batch [4/7], Loss: 1.5955, Accuracy: 69.29%, Grad Norm: 1.53927\n",
      "Epoch [158/10044], Batch [5/7], Loss: 1.3333, Accuracy: 75.76%, Grad Norm: 1.18251\n",
      "Epoch [158/10044], Batch [6/7], Loss: 1.3768, Accuracy: 75.32%, Grad Norm: 1.06080\n",
      "Epoch [158/10044], Batch [7/7], Loss: 1.0101, Accuracy: 85.37%, Grad Norm: 1.04940\n",
      "Epoch [158/10044], Loss: 1.0101\n",
      "Epoch [159/10044], Batch [1/7], Loss: 1.2775, Accuracy: 75.91%, Grad Norm: 0.98284\n",
      "Epoch [159/10044], Batch [2/7], Loss: 1.4329, Accuracy: 74.57%, Grad Norm: 1.13068\n",
      "Epoch [159/10044], Batch [3/7], Loss: 1.6729, Accuracy: 68.53%, Grad Norm: 1.27612\n",
      "Epoch [159/10044], Batch [4/7], Loss: 1.5489, Accuracy: 72.57%, Grad Norm: 1.14491\n",
      "Epoch [159/10044], Batch [5/7], Loss: 1.3021, Accuracy: 77.12%, Grad Norm: 0.96255\n",
      "Epoch [159/10044], Batch [6/7], Loss: 1.3533, Accuracy: 75.57%, Grad Norm: 0.96218\n",
      "Epoch [159/10044], Batch [7/7], Loss: 0.9944, Accuracy: 83.95%, Grad Norm: 0.87558\n",
      "Epoch [159/10044], Loss: 0.9944\n",
      "Epoch [160/10044], Batch [1/7], Loss: 1.2578, Accuracy: 76.18%, Grad Norm: 0.88813\n",
      "Epoch [160/10044], Batch [2/7], Loss: 1.4156, Accuracy: 75.39%, Grad Norm: 1.29140\n",
      "Epoch [160/10044], Batch [3/7], Loss: 1.6374, Accuracy: 70.22%, Grad Norm: 1.23309\n",
      "Epoch [160/10044], Batch [4/7], Loss: 1.5142, Accuracy: 73.95%, Grad Norm: 0.97495\n",
      "Epoch [160/10044], Batch [5/7], Loss: 1.2743, Accuracy: 78.03%, Grad Norm: 0.96951\n",
      "Epoch [160/10044], Batch [6/7], Loss: 1.3384, Accuracy: 74.87%, Grad Norm: 1.22727\n",
      "Epoch [160/10044], Batch [7/7], Loss: 0.9721, Accuracy: 83.77%, Grad Norm: 0.95477\n",
      "Epoch [160/10044], Loss: 0.9721\n",
      "Epoch [161/10044], Batch [1/7], Loss: 1.2409, Accuracy: 76.72%, Grad Norm: 0.90780\n",
      "Epoch [161/10044], Batch [2/7], Loss: 1.4176, Accuracy: 76.03%, Grad Norm: 1.59246\n",
      "Epoch [161/10044], Batch [3/7], Loss: 1.6506, Accuracy: 71.39%, Grad Norm: 1.75925\n",
      "Epoch [161/10044], Batch [4/7], Loss: 1.5243, Accuracy: 74.54%, Grad Norm: 1.50441\n",
      "Epoch [161/10044], Batch [5/7], Loss: 1.2710, Accuracy: 78.52%, Grad Norm: 1.04586\n",
      "Epoch [161/10044], Batch [6/7], Loss: 1.3226, Accuracy: 75.44%, Grad Norm: 1.26597\n",
      "Epoch [161/10044], Batch [7/7], Loss: 0.9865, Accuracy: 83.68%, Grad Norm: 1.22483\n",
      "Epoch [161/10044], Loss: 0.9865\n",
      "Epoch [162/10044], Batch [1/7], Loss: 1.2508, Accuracy: 74.35%, Grad Norm: 1.25474\n",
      "Epoch [162/10044], Batch [2/7], Loss: 1.3959, Accuracy: 75.04%, Grad Norm: 1.14599\n",
      "Epoch [162/10044], Batch [3/7], Loss: 1.6132, Accuracy: 70.67%, Grad Norm: 1.25895\n",
      "Epoch [162/10044], Batch [4/7], Loss: 1.4868, Accuracy: 76.45%, Grad Norm: 1.35068\n",
      "Epoch [162/10044], Batch [5/7], Loss: 1.2563, Accuracy: 79.70%, Grad Norm: 1.11200\n",
      "Epoch [162/10044], Batch [6/7], Loss: 1.3259, Accuracy: 76.61%, Grad Norm: 1.39817\n",
      "Epoch [162/10044], Batch [7/7], Loss: 0.9668, Accuracy: 83.67%, Grad Norm: 1.06776\n",
      "Epoch [162/10044], Loss: 0.9668\n",
      "Epoch [163/10044], Batch [1/7], Loss: 1.2250, Accuracy: 76.73%, Grad Norm: 1.01542\n",
      "Epoch [163/10044], Batch [2/7], Loss: 1.3809, Accuracy: 74.12%, Grad Norm: 1.36486\n",
      "Epoch [163/10044], Batch [3/7], Loss: 1.6113, Accuracy: 69.18%, Grad Norm: 1.58629\n",
      "Epoch [163/10044], Batch [4/7], Loss: 1.4900, Accuracy: 75.30%, Grad Norm: 1.37504\n",
      "Epoch [163/10044], Batch [5/7], Loss: 1.2402, Accuracy: 80.16%, Grad Norm: 1.11053\n",
      "Epoch [163/10044], Batch [6/7], Loss: 1.3044, Accuracy: 78.47%, Grad Norm: 1.51715\n",
      "Epoch [163/10044], Batch [7/7], Loss: 0.9542, Accuracy: 86.02%, Grad Norm: 1.16830\n",
      "Epoch [163/10044], Loss: 0.9542\n",
      "Epoch [164/10044], Batch [1/7], Loss: 1.2213, Accuracy: 77.72%, Grad Norm: 1.22988\n",
      "Epoch [164/10044], Batch [2/7], Loss: 1.3605, Accuracy: 75.03%, Grad Norm: 1.22099\n",
      "Epoch [164/10044], Batch [3/7], Loss: 1.5861, Accuracy: 69.72%, Grad Norm: 1.53786\n",
      "Epoch [164/10044], Batch [4/7], Loss: 1.4811, Accuracy: 73.19%, Grad Norm: 1.58610\n",
      "Epoch [164/10044], Batch [5/7], Loss: 1.2287, Accuracy: 79.34%, Grad Norm: 1.13105\n",
      "Epoch [164/10044], Batch [6/7], Loss: 1.2766, Accuracy: 78.47%, Grad Norm: 1.23423\n",
      "Epoch [164/10044], Batch [7/7], Loss: 0.9187, Accuracy: 86.38%, Grad Norm: 0.89347\n",
      "Epoch [164/10044], Loss: 0.9187\n",
      "Epoch [165/10044], Batch [1/7], Loss: 1.1874, Accuracy: 78.52%, Grad Norm: 0.94919\n",
      "Epoch [165/10044], Batch [2/7], Loss: 1.3493, Accuracy: 75.76%, Grad Norm: 1.25617\n",
      "Epoch [165/10044], Batch [3/7], Loss: 1.5706, Accuracy: 71.11%, Grad Norm: 1.50647\n",
      "Epoch [165/10044], Batch [4/7], Loss: 1.4385, Accuracy: 74.95%, Grad Norm: 1.24525\n",
      "Epoch [165/10044], Batch [5/7], Loss: 1.1974, Accuracy: 80.55%, Grad Norm: 0.93818\n",
      "Epoch [165/10044], Batch [6/7], Loss: 1.2536, Accuracy: 78.38%, Grad Norm: 1.06787\n",
      "Epoch [165/10044], Batch [7/7], Loss: 0.9075, Accuracy: 87.08%, Grad Norm: 1.06633\n",
      "Epoch [165/10044], Loss: 0.9075\n",
      "Epoch [166/10044], Batch [1/7], Loss: 1.1708, Accuracy: 78.19%, Grad Norm: 0.86235\n",
      "Epoch [166/10044], Batch [2/7], Loss: 1.3193, Accuracy: 76.68%, Grad Norm: 1.15241\n",
      "Epoch [166/10044], Batch [3/7], Loss: 1.5324, Accuracy: 72.23%, Grad Norm: 1.29010\n",
      "Epoch [166/10044], Batch [4/7], Loss: 1.4039, Accuracy: 76.17%, Grad Norm: 1.06754\n",
      "Epoch [166/10044], Batch [5/7], Loss: 1.1688, Accuracy: 80.95%, Grad Norm: 0.82931\n",
      "Epoch [166/10044], Batch [6/7], Loss: 1.2403, Accuracy: 77.96%, Grad Norm: 1.16162\n",
      "Epoch [166/10044], Batch [7/7], Loss: 0.8943, Accuracy: 85.95%, Grad Norm: 0.99551\n",
      "Epoch [166/10044], Loss: 0.8943\n",
      "Epoch [167/10044], Batch [1/7], Loss: 1.1638, Accuracy: 78.73%, Grad Norm: 0.92500\n",
      "Epoch [167/10044], Batch [2/7], Loss: 1.2990, Accuracy: 78.35%, Grad Norm: 1.22331\n",
      "Epoch [167/10044], Batch [3/7], Loss: 1.5206, Accuracy: 73.48%, Grad Norm: 1.32968\n",
      "Epoch [167/10044], Batch [4/7], Loss: 1.3965, Accuracy: 77.52%, Grad Norm: 1.23077\n",
      "Epoch [167/10044], Batch [5/7], Loss: 1.1683, Accuracy: 80.87%, Grad Norm: 0.96453\n",
      "Epoch [167/10044], Batch [6/7], Loss: 1.2253, Accuracy: 78.61%, Grad Norm: 1.23988\n",
      "Epoch [167/10044], Batch [7/7], Loss: 0.8830, Accuracy: 86.03%, Grad Norm: 0.97479\n",
      "Epoch [167/10044], Loss: 0.8830\n",
      "Epoch [168/10044], Batch [1/7], Loss: 1.1401, Accuracy: 78.84%, Grad Norm: 0.93957\n",
      "Epoch [168/10044], Batch [2/7], Loss: 1.2951, Accuracy: 77.52%, Grad Norm: 1.30572\n",
      "Epoch [168/10044], Batch [3/7], Loss: 1.5216, Accuracy: 73.47%, Grad Norm: 1.51081\n",
      "Epoch [168/10044], Batch [4/7], Loss: 1.3933, Accuracy: 78.29%, Grad Norm: 1.39991\n",
      "Epoch [168/10044], Batch [5/7], Loss: 1.1587, Accuracy: 81.75%, Grad Norm: 1.00576\n",
      "Epoch [168/10044], Batch [6/7], Loss: 1.2105, Accuracy: 78.39%, Grad Norm: 1.32778\n",
      "Epoch [168/10044], Batch [7/7], Loss: 0.8741, Accuracy: 85.92%, Grad Norm: 0.98928\n",
      "Epoch [168/10044], Loss: 0.8741\n",
      "Epoch [169/10044], Batch [1/7], Loss: 1.1436, Accuracy: 78.24%, Grad Norm: 1.12615\n",
      "Epoch [169/10044], Batch [2/7], Loss: 1.2853, Accuracy: 76.91%, Grad Norm: 1.32972\n",
      "Epoch [169/10044], Batch [3/7], Loss: 1.5058, Accuracy: 72.48%, Grad Norm: 1.61790\n",
      "Epoch [169/10044], Batch [4/7], Loss: 1.3897, Accuracy: 77.42%, Grad Norm: 1.42779\n",
      "Epoch [169/10044], Batch [5/7], Loss: 1.1390, Accuracy: 82.35%, Grad Norm: 0.98676\n",
      "Epoch [169/10044], Batch [6/7], Loss: 1.1891, Accuracy: 80.90%, Grad Norm: 1.18226\n",
      "Epoch [169/10044], Batch [7/7], Loss: 0.8593, Accuracy: 87.28%, Grad Norm: 0.94235\n",
      "Epoch [169/10044], Loss: 0.8593\n",
      "Epoch [170/10044], Batch [1/7], Loss: 1.1212, Accuracy: 79.65%, Grad Norm: 0.94779\n",
      "Epoch [170/10044], Batch [2/7], Loss: 1.2721, Accuracy: 75.97%, Grad Norm: 1.38165\n",
      "Epoch [170/10044], Batch [3/7], Loss: 1.4986, Accuracy: 70.92%, Grad Norm: 1.51184\n",
      "Epoch [170/10044], Batch [4/7], Loss: 1.3443, Accuracy: 77.69%, Grad Norm: 1.03241\n",
      "Epoch [170/10044], Batch [5/7], Loss: 1.1250, Accuracy: 82.55%, Grad Norm: 0.90158\n",
      "Epoch [170/10044], Batch [6/7], Loss: 1.1901, Accuracy: 80.78%, Grad Norm: 1.24373\n",
      "Epoch [170/10044], Batch [7/7], Loss: 0.8395, Accuracy: 88.08%, Grad Norm: 0.91527\n",
      "Epoch [170/10044], Loss: 0.8395\n",
      "Epoch [171/10044], Batch [1/7], Loss: 1.1181, Accuracy: 80.37%, Grad Norm: 1.10524\n",
      "Epoch [171/10044], Batch [2/7], Loss: 1.2469, Accuracy: 77.62%, Grad Norm: 1.18775\n",
      "Epoch [171/10044], Batch [3/7], Loss: 1.4688, Accuracy: 72.83%, Grad Norm: 1.51105\n",
      "Epoch [171/10044], Batch [4/7], Loss: 1.3320, Accuracy: 77.82%, Grad Norm: 1.17452\n",
      "Epoch [171/10044], Batch [5/7], Loss: 1.1021, Accuracy: 82.33%, Grad Norm: 0.85951\n",
      "Epoch [171/10044], Batch [6/7], Loss: 1.1518, Accuracy: 81.25%, Grad Norm: 0.93163\n",
      "Epoch [171/10044], Batch [7/7], Loss: 0.8323, Accuracy: 88.15%, Grad Norm: 0.98634\n",
      "Epoch [171/10044], Loss: 0.8323\n",
      "Epoch [172/10044], Batch [1/7], Loss: 1.0950, Accuracy: 80.37%, Grad Norm: 0.88306\n",
      "Epoch [172/10044], Batch [2/7], Loss: 1.2138, Accuracy: 79.27%, Grad Norm: 0.96772\n",
      "Epoch [172/10044], Batch [3/7], Loss: 1.4242, Accuracy: 74.83%, Grad Norm: 1.10404\n",
      "Epoch [172/10044], Batch [4/7], Loss: 1.2946, Accuracy: 79.12%, Grad Norm: 0.91961\n",
      "Epoch [172/10044], Batch [5/7], Loss: 1.0840, Accuracy: 82.86%, Grad Norm: 0.78823\n",
      "Epoch [172/10044], Batch [6/7], Loss: 1.1373, Accuracy: 80.47%, Grad Norm: 1.07397\n",
      "Epoch [172/10044], Batch [7/7], Loss: 0.8082, Accuracy: 87.72%, Grad Norm: 0.81485\n",
      "Epoch [172/10044], Loss: 0.8082\n",
      "Epoch [173/10044], Batch [1/7], Loss: 1.0727, Accuracy: 80.63%, Grad Norm: 0.95103\n",
      "Epoch [173/10044], Batch [2/7], Loss: 1.2203, Accuracy: 80.06%, Grad Norm: 1.40534\n",
      "Epoch [173/10044], Batch [3/7], Loss: 1.4266, Accuracy: 75.90%, Grad Norm: 1.38985\n",
      "Epoch [173/10044], Batch [4/7], Loss: 1.2925, Accuracy: 80.61%, Grad Norm: 1.20466\n",
      "Epoch [173/10044], Batch [5/7], Loss: 1.0745, Accuracy: 83.23%, Grad Norm: 1.10521\n",
      "Epoch [173/10044], Batch [6/7], Loss: 1.1381, Accuracy: 79.80%, Grad Norm: 1.57605\n",
      "Epoch [173/10044], Batch [7/7], Loss: 0.8172, Accuracy: 86.33%, Grad Norm: 1.12448\n",
      "Epoch [173/10044], Loss: 0.8172\n",
      "Epoch [174/10044], Batch [1/7], Loss: 1.0766, Accuracy: 80.19%, Grad Norm: 1.08949\n",
      "Epoch [174/10044], Batch [2/7], Loss: 1.2175, Accuracy: 79.22%, Grad Norm: 1.49278\n",
      "Epoch [174/10044], Batch [3/7], Loss: 1.4408, Accuracy: 74.66%, Grad Norm: 1.97387\n",
      "Epoch [174/10044], Batch [4/7], Loss: 1.2940, Accuracy: 80.81%, Grad Norm: 1.54619\n",
      "Epoch [174/10044], Batch [5/7], Loss: 1.0599, Accuracy: 84.42%, Grad Norm: 1.05520\n",
      "Epoch [174/10044], Batch [6/7], Loss: 1.1069, Accuracy: 81.95%, Grad Norm: 1.02623\n",
      "Epoch [174/10044], Batch [7/7], Loss: 0.7951, Accuracy: 88.28%, Grad Norm: 0.94245\n",
      "Epoch [174/10044], Loss: 0.7951\n",
      "Epoch [175/10044], Batch [1/7], Loss: 1.0747, Accuracy: 80.00%, Grad Norm: 1.16084\n",
      "Epoch [175/10044], Batch [2/7], Loss: 1.2100, Accuracy: 77.92%, Grad Norm: 1.30387\n",
      "Epoch [175/10044], Batch [3/7], Loss: 1.4032, Accuracy: 74.78%, Grad Norm: 1.47903\n",
      "Epoch [175/10044], Batch [4/7], Loss: 1.2566, Accuracy: 80.37%, Grad Norm: 1.20582\n",
      "Epoch [175/10044], Batch [5/7], Loss: 1.0339, Accuracy: 84.12%, Grad Norm: 0.88325\n",
      "Epoch [175/10044], Batch [6/7], Loss: 1.0945, Accuracy: 82.14%, Grad Norm: 1.06146\n",
      "Epoch [175/10044], Batch [7/7], Loss: 0.7870, Accuracy: 87.63%, Grad Norm: 1.01808\n",
      "Epoch [175/10044], Loss: 0.7870\n",
      "Epoch [176/10044], Batch [1/7], Loss: 1.0496, Accuracy: 81.75%, Grad Norm: 0.92322\n",
      "Epoch [176/10044], Batch [2/7], Loss: 1.1697, Accuracy: 79.97%, Grad Norm: 1.07586\n",
      "Epoch [176/10044], Batch [3/7], Loss: 1.3731, Accuracy: 76.07%, Grad Norm: 1.20754\n",
      "Epoch [176/10044], Batch [4/7], Loss: 1.2371, Accuracy: 80.75%, Grad Norm: 0.99551\n",
      "Epoch [176/10044], Batch [5/7], Loss: 1.0188, Accuracy: 84.66%, Grad Norm: 0.77902\n",
      "Epoch [176/10044], Batch [6/7], Loss: 1.0744, Accuracy: 82.83%, Grad Norm: 1.05656\n",
      "Epoch [176/10044], Batch [7/7], Loss: 0.7601, Accuracy: 89.13%, Grad Norm: 0.83127\n",
      "Epoch [176/10044], Loss: 0.7601\n",
      "Epoch [177/10044], Batch [1/7], Loss: 1.0323, Accuracy: 81.62%, Grad Norm: 0.89582\n",
      "Epoch [177/10044], Batch [2/7], Loss: 1.1415, Accuracy: 80.82%, Grad Norm: 0.92904\n",
      "Epoch [177/10044], Batch [3/7], Loss: 1.3363, Accuracy: 76.97%, Grad Norm: 1.05544\n",
      "Epoch [177/10044], Batch [4/7], Loss: 1.2024, Accuracy: 82.21%, Grad Norm: 0.85603\n",
      "Epoch [177/10044], Batch [5/7], Loss: 1.0075, Accuracy: 85.19%, Grad Norm: 0.87092\n",
      "Epoch [177/10044], Batch [6/7], Loss: 1.0603, Accuracy: 82.72%, Grad Norm: 1.03270\n",
      "Epoch [177/10044], Batch [7/7], Loss: 0.7478, Accuracy: 89.28%, Grad Norm: 0.77628\n",
      "Epoch [177/10044], Loss: 0.7478\n",
      "Epoch [178/10044], Batch [1/7], Loss: 1.0056, Accuracy: 82.22%, Grad Norm: 0.85748\n",
      "Epoch [178/10044], Batch [2/7], Loss: 1.1344, Accuracy: 80.64%, Grad Norm: 1.22970\n",
      "Epoch [178/10044], Batch [3/7], Loss: 1.3338, Accuracy: 76.73%, Grad Norm: 1.34722\n",
      "Epoch [178/10044], Batch [4/7], Loss: 1.1788, Accuracy: 81.98%, Grad Norm: 1.03419\n",
      "Epoch [178/10044], Batch [5/7], Loss: 0.9785, Accuracy: 85.51%, Grad Norm: 0.85856\n",
      "Epoch [178/10044], Batch [6/7], Loss: 1.0511, Accuracy: 83.40%, Grad Norm: 1.30969\n",
      "Epoch [178/10044], Batch [7/7], Loss: 0.7510, Accuracy: 89.68%, Grad Norm: 0.98446\n",
      "Epoch [178/10044], Loss: 0.7510\n",
      "Epoch [179/10044], Batch [1/7], Loss: 1.0025, Accuracy: 82.28%, Grad Norm: 1.04799\n",
      "Epoch [179/10044], Batch [2/7], Loss: 1.1127, Accuracy: 81.84%, Grad Norm: 1.14636\n",
      "Epoch [179/10044], Batch [3/7], Loss: 1.2986, Accuracy: 78.12%, Grad Norm: 1.35424\n",
      "Epoch [179/10044], Batch [4/7], Loss: 1.1940, Accuracy: 81.10%, Grad Norm: 1.53592\n",
      "Epoch [179/10044], Batch [5/7], Loss: 0.9785, Accuracy: 85.22%, Grad Norm: 0.98628\n",
      "Epoch [179/10044], Batch [6/7], Loss: 1.0309, Accuracy: 82.86%, Grad Norm: 1.13523\n",
      "Epoch [179/10044], Batch [7/7], Loss: 0.7335, Accuracy: 89.53%, Grad Norm: 0.90235\n",
      "Epoch [179/10044], Loss: 0.7335\n",
      "Epoch [180/10044], Batch [1/7], Loss: 0.9846, Accuracy: 82.95%, Grad Norm: 0.85594\n",
      "Epoch [180/10044], Batch [2/7], Loss: 1.1052, Accuracy: 82.23%, Grad Norm: 1.26542\n",
      "Epoch [180/10044], Batch [3/7], Loss: 1.2932, Accuracy: 78.67%, Grad Norm: 1.35614\n",
      "Epoch [180/10044], Batch [4/7], Loss: 1.1548, Accuracy: 82.91%, Grad Norm: 1.16686\n",
      "Epoch [180/10044], Batch [5/7], Loss: 0.9626, Accuracy: 85.67%, Grad Norm: 0.96096\n",
      "Epoch [180/10044], Batch [6/7], Loss: 1.0236, Accuracy: 82.62%, Grad Norm: 1.35386\n",
      "Epoch [180/10044], Batch [7/7], Loss: 0.7253, Accuracy: 89.17%, Grad Norm: 1.03710\n",
      "Epoch [180/10044], Loss: 0.7253\n",
      "Epoch [181/10044], Batch [1/7], Loss: 0.9735, Accuracy: 82.62%, Grad Norm: 0.95960\n",
      "Epoch [181/10044], Batch [2/7], Loss: 1.0898, Accuracy: 82.91%, Grad Norm: 1.23165\n",
      "Epoch [181/10044], Batch [3/7], Loss: 1.2956, Accuracy: 79.33%, Grad Norm: 1.67214\n",
      "Epoch [181/10044], Batch [4/7], Loss: 1.1570, Accuracy: 83.77%, Grad Norm: 1.40762\n",
      "Epoch [181/10044], Batch [5/7], Loss: 0.9567, Accuracy: 86.38%, Grad Norm: 1.02638\n",
      "Epoch [181/10044], Batch [6/7], Loss: 1.0078, Accuracy: 83.98%, Grad Norm: 1.18611\n",
      "Epoch [181/10044], Batch [7/7], Loss: 0.7224, Accuracy: 88.45%, Grad Norm: 1.08859\n",
      "Epoch [181/10044], Loss: 0.7224\n",
      "Epoch [182/10044], Batch [1/7], Loss: 0.9668, Accuracy: 82.31%, Grad Norm: 1.07359\n",
      "Epoch [182/10044], Batch [2/7], Loss: 1.0999, Accuracy: 80.72%, Grad Norm: 1.36435\n",
      "Epoch [182/10044], Batch [3/7], Loss: 1.2648, Accuracy: 79.04%, Grad Norm: 1.39632\n",
      "Epoch [182/10044], Batch [4/7], Loss: 1.1403, Accuracy: 83.78%, Grad Norm: 1.30454\n",
      "Epoch [182/10044], Batch [5/7], Loss: 0.9529, Accuracy: 86.76%, Grad Norm: 1.08345\n",
      "Epoch [182/10044], Batch [6/7], Loss: 1.0039, Accuracy: 84.83%, Grad Norm: 1.28766\n",
      "Epoch [182/10044], Batch [7/7], Loss: 0.7204, Accuracy: 89.78%, Grad Norm: 1.14838\n",
      "Epoch [182/10044], Loss: 0.7204\n",
      "Epoch [183/10044], Batch [1/7], Loss: 0.9763, Accuracy: 83.12%, Grad Norm: 1.15762\n",
      "Epoch [183/10044], Batch [2/7], Loss: 1.0918, Accuracy: 80.37%, Grad Norm: 1.52751\n",
      "Epoch [183/10044], Batch [3/7], Loss: 1.2880, Accuracy: 75.59%, Grad Norm: 1.71604\n",
      "Epoch [183/10044], Batch [4/7], Loss: 1.1334, Accuracy: 81.83%, Grad Norm: 1.24643\n",
      "Epoch [183/10044], Batch [5/7], Loss: 0.9354, Accuracy: 85.96%, Grad Norm: 1.01784\n",
      "Epoch [183/10044], Batch [6/7], Loss: 0.9826, Accuracy: 85.52%, Grad Norm: 1.23581\n",
      "Epoch [183/10044], Batch [7/7], Loss: 0.7082, Accuracy: 90.20%, Grad Norm: 1.06197\n",
      "Epoch [183/10044], Loss: 0.7082\n",
      "Epoch [184/10044], Batch [1/7], Loss: 0.9655, Accuracy: 83.84%, Grad Norm: 1.30449\n",
      "Epoch [184/10044], Batch [2/7], Loss: 1.0659, Accuracy: 82.28%, Grad Norm: 1.32993\n",
      "Epoch [184/10044], Batch [3/7], Loss: 1.2461, Accuracy: 78.12%, Grad Norm: 1.55448\n",
      "Epoch [184/10044], Batch [4/7], Loss: 1.1010, Accuracy: 83.37%, Grad Norm: 1.18420\n",
      "Epoch [184/10044], Batch [5/7], Loss: 0.9049, Accuracy: 86.74%, Grad Norm: 0.91198\n",
      "Epoch [184/10044], Batch [6/7], Loss: 0.9702, Accuracy: 84.75%, Grad Norm: 1.14758\n",
      "Epoch [184/10044], Batch [7/7], Loss: 0.6872, Accuracy: 90.83%, Grad Norm: 0.99788\n",
      "Epoch [184/10044], Loss: 0.6872\n",
      "Epoch [185/10044], Batch [1/7], Loss: 0.9210, Accuracy: 84.53%, Grad Norm: 0.85978\n",
      "Epoch [185/10044], Batch [2/7], Loss: 1.0285, Accuracy: 84.34%, Grad Norm: 1.05634\n",
      "Epoch [185/10044], Batch [3/7], Loss: 1.2173, Accuracy: 80.88%, Grad Norm: 1.13721\n",
      "Epoch [185/10044], Batch [4/7], Loss: 1.0814, Accuracy: 84.91%, Grad Norm: 1.01412\n",
      "Epoch [185/10044], Batch [5/7], Loss: 0.8920, Accuracy: 87.55%, Grad Norm: 0.80521\n",
      "Epoch [185/10044], Batch [6/7], Loss: 0.9568, Accuracy: 84.36%, Grad Norm: 1.14466\n",
      "Epoch [185/10044], Batch [7/7], Loss: 0.6707, Accuracy: 90.80%, Grad Norm: 0.82295\n",
      "Epoch [185/10044], Loss: 0.6707\n",
      "Epoch [186/10044], Batch [1/7], Loss: 0.9174, Accuracy: 83.81%, Grad Norm: 0.88568\n",
      "Epoch [186/10044], Batch [2/7], Loss: 1.0201, Accuracy: 83.85%, Grad Norm: 0.92778\n",
      "Epoch [186/10044], Batch [3/7], Loss: 1.1900, Accuracy: 81.31%, Grad Norm: 1.19786\n",
      "Epoch [186/10044], Batch [4/7], Loss: 1.0652, Accuracy: 85.44%, Grad Norm: 1.04645\n",
      "Epoch [186/10044], Batch [5/7], Loss: 0.8754, Accuracy: 88.06%, Grad Norm: 0.84443\n",
      "Epoch [186/10044], Batch [6/7], Loss: 0.9264, Accuracy: 85.69%, Grad Norm: 0.94408\n",
      "Epoch [186/10044], Batch [7/7], Loss: 0.6528, Accuracy: 90.77%, Grad Norm: 0.80266\n",
      "Epoch [186/10044], Loss: 0.6528\n",
      "Epoch [187/10044], Batch [1/7], Loss: 0.9004, Accuracy: 85.24%, Grad Norm: 0.80748\n",
      "Epoch [187/10044], Batch [2/7], Loss: 1.0038, Accuracy: 83.92%, Grad Norm: 1.09754\n",
      "Epoch [187/10044], Batch [3/7], Loss: 1.1706, Accuracy: 81.17%, Grad Norm: 1.18199\n",
      "Epoch [187/10044], Batch [4/7], Loss: 1.0366, Accuracy: 85.54%, Grad Norm: 0.98310\n",
      "Epoch [187/10044], Batch [5/7], Loss: 0.8570, Accuracy: 87.84%, Grad Norm: 0.80382\n",
      "Epoch [187/10044], Batch [6/7], Loss: 0.9157, Accuracy: 86.27%, Grad Norm: 0.96535\n",
      "Epoch [187/10044], Batch [7/7], Loss: 0.6449, Accuracy: 91.27%, Grad Norm: 0.79675\n",
      "Epoch [187/10044], Loss: 0.6449\n",
      "Epoch [188/10044], Batch [1/7], Loss: 0.8934, Accuracy: 85.47%, Grad Norm: 0.92561\n",
      "Epoch [188/10044], Batch [2/7], Loss: 0.9754, Accuracy: 84.77%, Grad Norm: 0.92008\n",
      "Epoch [188/10044], Batch [3/7], Loss: 1.1471, Accuracy: 81.77%, Grad Norm: 1.15463\n",
      "Epoch [188/10044], Batch [4/7], Loss: 1.0167, Accuracy: 86.03%, Grad Norm: 0.90752\n",
      "Epoch [188/10044], Batch [5/7], Loss: 0.8540, Accuracy: 88.09%, Grad Norm: 0.82819\n",
      "Epoch [188/10044], Batch [6/7], Loss: 0.8992, Accuracy: 86.98%, Grad Norm: 0.95838\n",
      "Epoch [188/10044], Batch [7/7], Loss: 0.6421, Accuracy: 91.05%, Grad Norm: 0.81209\n",
      "Epoch [188/10044], Loss: 0.6421\n",
      "Epoch [189/10044], Batch [1/7], Loss: 0.8791, Accuracy: 85.05%, Grad Norm: 0.91865\n",
      "Epoch [189/10044], Batch [2/7], Loss: 0.9665, Accuracy: 85.15%, Grad Norm: 1.00507\n",
      "Epoch [189/10044], Batch [3/7], Loss: 1.1321, Accuracy: 81.96%, Grad Norm: 1.12995\n",
      "Epoch [189/10044], Batch [4/7], Loss: 1.0045, Accuracy: 86.32%, Grad Norm: 0.90197\n",
      "Epoch [189/10044], Batch [5/7], Loss: 0.8306, Accuracy: 88.82%, Grad Norm: 0.79693\n",
      "Epoch [189/10044], Batch [6/7], Loss: 0.8964, Accuracy: 86.97%, Grad Norm: 1.11675\n",
      "Epoch [189/10044], Batch [7/7], Loss: 0.6269, Accuracy: 91.68%, Grad Norm: 0.79539\n",
      "Epoch [189/10044], Loss: 0.6269\n",
      "Epoch [190/10044], Batch [1/7], Loss: 0.8622, Accuracy: 85.52%, Grad Norm: 0.73100\n",
      "Epoch [190/10044], Batch [2/7], Loss: 0.9565, Accuracy: 84.82%, Grad Norm: 1.13396\n",
      "Epoch [190/10044], Batch [3/7], Loss: 1.1185, Accuracy: 81.48%, Grad Norm: 1.25277\n",
      "Epoch [190/10044], Batch [4/7], Loss: 0.9971, Accuracy: 86.28%, Grad Norm: 0.99989\n",
      "Epoch [190/10044], Batch [5/7], Loss: 0.8195, Accuracy: 89.02%, Grad Norm: 0.76553\n",
      "Epoch [190/10044], Batch [6/7], Loss: 0.8797, Accuracy: 87.51%, Grad Norm: 1.03216\n",
      "Epoch [190/10044], Batch [7/7], Loss: 0.6282, Accuracy: 91.80%, Grad Norm: 0.96336\n",
      "Epoch [190/10044], Loss: 0.6282\n",
      "Epoch [191/10044], Batch [1/7], Loss: 0.8610, Accuracy: 85.82%, Grad Norm: 0.92542\n",
      "Epoch [191/10044], Batch [2/7], Loss: 0.9287, Accuracy: 85.37%, Grad Norm: 0.90109\n",
      "Epoch [191/10044], Batch [3/7], Loss: 1.0982, Accuracy: 82.13%, Grad Norm: 1.04303\n",
      "Epoch [191/10044], Batch [4/7], Loss: 0.9688, Accuracy: 86.91%, Grad Norm: 0.92888\n",
      "Epoch [191/10044], Batch [5/7], Loss: 0.8161, Accuracy: 88.78%, Grad Norm: 0.85094\n",
      "Epoch [191/10044], Batch [6/7], Loss: 0.8611, Accuracy: 87.65%, Grad Norm: 0.99991\n",
      "Epoch [191/10044], Batch [7/7], Loss: 0.6026, Accuracy: 92.35%, Grad Norm: 0.76359\n",
      "Epoch [191/10044], Loss: 0.6026\n",
      "Epoch [192/10044], Batch [1/7], Loss: 0.8386, Accuracy: 86.37%, Grad Norm: 0.84066\n",
      "Epoch [192/10044], Batch [2/7], Loss: 0.9135, Accuracy: 85.97%, Grad Norm: 0.95874\n",
      "Epoch [192/10044], Batch [3/7], Loss: 1.0773, Accuracy: 82.65%, Grad Norm: 1.14861\n",
      "Epoch [192/10044], Batch [4/7], Loss: 0.9519, Accuracy: 86.60%, Grad Norm: 0.95342\n",
      "Epoch [192/10044], Batch [5/7], Loss: 0.8025, Accuracy: 89.17%, Grad Norm: 0.88325\n",
      "Epoch [192/10044], Batch [6/7], Loss: 0.8515, Accuracy: 87.80%, Grad Norm: 1.05997\n",
      "Epoch [192/10044], Batch [7/7], Loss: 0.5838, Accuracy: 92.25%, Grad Norm: 0.71347\n",
      "Epoch [192/10044], Loss: 0.5838\n",
      "Epoch [193/10044], Batch [1/7], Loss: 0.8272, Accuracy: 86.77%, Grad Norm: 0.82418\n",
      "Epoch [193/10044], Batch [2/7], Loss: 0.9067, Accuracy: 87.36%, Grad Norm: 1.06098\n",
      "Epoch [193/10044], Batch [3/7], Loss: 1.0646, Accuracy: 84.51%, Grad Norm: 1.19846\n",
      "Epoch [193/10044], Batch [4/7], Loss: 0.9385, Accuracy: 87.59%, Grad Norm: 0.98127\n",
      "Epoch [193/10044], Batch [5/7], Loss: 0.7816, Accuracy: 88.92%, Grad Norm: 0.75815\n",
      "Epoch [193/10044], Batch [6/7], Loss: 0.8442, Accuracy: 86.40%, Grad Norm: 1.14054\n",
      "Epoch [193/10044], Batch [7/7], Loss: 0.5862, Accuracy: 92.23%, Grad Norm: 0.83399\n",
      "Epoch [193/10044], Loss: 0.5862\n",
      "Epoch [194/10044], Batch [1/7], Loss: 0.8254, Accuracy: 86.19%, Grad Norm: 0.88095\n",
      "Epoch [194/10044], Batch [2/7], Loss: 0.8926, Accuracy: 87.76%, Grad Norm: 1.11579\n",
      "Epoch [194/10044], Batch [3/7], Loss: 1.0291, Accuracy: 86.08%, Grad Norm: 1.14740\n",
      "Epoch [194/10044], Batch [4/7], Loss: 0.9238, Accuracy: 88.90%, Grad Norm: 1.03572\n",
      "Epoch [194/10044], Batch [5/7], Loss: 0.7766, Accuracy: 89.86%, Grad Norm: 0.85717\n",
      "Epoch [194/10044], Batch [6/7], Loss: 0.8228, Accuracy: 87.11%, Grad Norm: 1.20352\n",
      "Epoch [194/10044], Batch [7/7], Loss: 0.5773, Accuracy: 91.78%, Grad Norm: 0.78292\n",
      "Epoch [194/10044], Loss: 0.5773\n",
      "Epoch [195/10044], Batch [1/7], Loss: 0.8154, Accuracy: 86.11%, Grad Norm: 1.00349\n",
      "Epoch [195/10044], Batch [2/7], Loss: 0.8839, Accuracy: 86.77%, Grad Norm: 1.14688\n",
      "Epoch [195/10044], Batch [3/7], Loss: 1.0445, Accuracy: 84.42%, Grad Norm: 1.38688\n",
      "Epoch [195/10044], Batch [4/7], Loss: 0.9259, Accuracy: 88.54%, Grad Norm: 1.21166\n",
      "Epoch [195/10044], Batch [5/7], Loss: 0.7711, Accuracy: 90.02%, Grad Norm: 0.84371\n",
      "Epoch [195/10044], Batch [6/7], Loss: 0.8184, Accuracy: 88.73%, Grad Norm: 0.99738\n",
      "Epoch [195/10044], Batch [7/7], Loss: 0.5805, Accuracy: 91.85%, Grad Norm: 0.86839\n",
      "Epoch [195/10044], Loss: 0.5805\n",
      "Epoch [196/10044], Batch [1/7], Loss: 0.8171, Accuracy: 86.47%, Grad Norm: 1.05422\n",
      "Epoch [196/10044], Batch [2/7], Loss: 0.8817, Accuracy: 86.47%, Grad Norm: 1.19846\n",
      "Epoch [196/10044], Batch [3/7], Loss: 1.0260, Accuracy: 84.12%, Grad Norm: 1.36068\n",
      "Epoch [196/10044], Batch [4/7], Loss: 0.9062, Accuracy: 88.26%, Grad Norm: 1.24592\n",
      "Epoch [196/10044], Batch [5/7], Loss: 0.7567, Accuracy: 90.33%, Grad Norm: 1.00628\n",
      "Epoch [196/10044], Batch [6/7], Loss: 0.8076, Accuracy: 89.00%, Grad Norm: 1.18998\n",
      "Epoch [196/10044], Batch [7/7], Loss: 0.5708, Accuracy: 93.05%, Grad Norm: 0.85683\n",
      "Epoch [196/10044], Loss: 0.5708\n",
      "Epoch [197/10044], Batch [1/7], Loss: 0.8052, Accuracy: 86.33%, Grad Norm: 1.07506\n",
      "Epoch [197/10044], Batch [2/7], Loss: 0.8825, Accuracy: 85.75%, Grad Norm: 1.47364\n",
      "Epoch [197/10044], Batch [3/7], Loss: 1.0319, Accuracy: 83.87%, Grad Norm: 1.63329\n",
      "Epoch [197/10044], Batch [4/7], Loss: 0.8938, Accuracy: 88.16%, Grad Norm: 1.14848\n",
      "Epoch [197/10044], Batch [5/7], Loss: 0.7413, Accuracy: 90.55%, Grad Norm: 0.83299\n",
      "Epoch [197/10044], Batch [6/7], Loss: 0.8051, Accuracy: 89.23%, Grad Norm: 1.27198\n",
      "Epoch [197/10044], Batch [7/7], Loss: 0.5709, Accuracy: 92.82%, Grad Norm: 1.03434\n",
      "Epoch [197/10044], Loss: 0.5709\n",
      "Epoch [198/10044], Batch [1/7], Loss: 0.8096, Accuracy: 87.05%, Grad Norm: 1.20888\n",
      "Epoch [198/10044], Batch [2/7], Loss: 0.8602, Accuracy: 86.81%, Grad Norm: 1.14565\n",
      "Epoch [198/10044], Batch [3/7], Loss: 1.0144, Accuracy: 84.19%, Grad Norm: 1.53782\n",
      "Epoch [198/10044], Batch [4/7], Loss: 0.8856, Accuracy: 88.34%, Grad Norm: 1.43709\n",
      "Epoch [198/10044], Batch [5/7], Loss: 0.7434, Accuracy: 89.46%, Grad Norm: 0.98936\n",
      "Epoch [198/10044], Batch [6/7], Loss: 0.7834, Accuracy: 88.95%, Grad Norm: 1.05842\n",
      "Epoch [198/10044], Batch [7/7], Loss: 0.5596, Accuracy: 92.50%, Grad Norm: 0.88150\n",
      "Epoch [198/10044], Loss: 0.5596\n",
      "Epoch [199/10044], Batch [1/7], Loss: 0.7786, Accuracy: 88.31%, Grad Norm: 1.04431\n",
      "Epoch [199/10044], Batch [2/7], Loss: 0.8376, Accuracy: 88.42%, Grad Norm: 1.11550\n",
      "Epoch [199/10044], Batch [3/7], Loss: 0.9789, Accuracy: 85.60%, Grad Norm: 1.06201\n",
      "Epoch [199/10044], Batch [4/7], Loss: 0.8613, Accuracy: 88.39%, Grad Norm: 1.06375\n",
      "Epoch [199/10044], Batch [5/7], Loss: 0.7198, Accuracy: 90.59%, Grad Norm: 0.87950\n",
      "Epoch [199/10044], Batch [6/7], Loss: 0.7689, Accuracy: 88.21%, Grad Norm: 1.01323\n",
      "Epoch [199/10044], Batch [7/7], Loss: 0.5279, Accuracy: 92.77%, Grad Norm: 0.65415\n",
      "Epoch [199/10044], Loss: 0.5279\n",
      "Epoch [200/10044], Batch [1/7], Loss: 0.7644, Accuracy: 87.12%, Grad Norm: 0.87975\n",
      "Epoch [200/10044], Batch [2/7], Loss: 0.8254, Accuracy: 88.80%, Grad Norm: 1.20775\n",
      "Epoch [200/10044], Batch [3/7], Loss: 0.9754, Accuracy: 86.68%, Grad Norm: 1.44115\n",
      "Epoch [200/10044], Batch [4/7], Loss: 0.8532, Accuracy: 89.58%, Grad Norm: 1.02222\n",
      "Epoch [200/10044], Batch [5/7], Loss: 0.6968, Accuracy: 91.42%, Grad Norm: 0.74889\n",
      "Epoch [200/10044], Batch [6/7], Loss: 0.7421, Accuracy: 89.26%, Grad Norm: 0.98174\n",
      "Epoch [200/10044], Batch [7/7], Loss: 0.5381, Accuracy: 91.95%, Grad Norm: 0.97435\n",
      "Epoch [200/10044], Loss: 0.5381\n",
      "Epoch [201/10044], Batch [1/7], Loss: 0.7521, Accuracy: 87.42%, Grad Norm: 0.91896\n",
      "Epoch [201/10044], Batch [2/7], Loss: 0.8111, Accuracy: 88.24%, Grad Norm: 0.95256\n",
      "Epoch [201/10044], Batch [3/7], Loss: 0.9536, Accuracy: 86.82%, Grad Norm: 1.16271\n",
      "Epoch [201/10044], Batch [4/7], Loss: 0.8406, Accuracy: 90.29%, Grad Norm: 1.03698\n",
      "Epoch [201/10044], Batch [5/7], Loss: 0.7016, Accuracy: 91.59%, Grad Norm: 0.79921\n",
      "Epoch [201/10044], Batch [6/7], Loss: 0.7383, Accuracy: 90.02%, Grad Norm: 0.97115\n",
      "Epoch [201/10044], Batch [7/7], Loss: 0.5248, Accuracy: 93.28%, Grad Norm: 0.76281\n",
      "Epoch [201/10044], Loss: 0.5248\n",
      "Epoch [202/10044], Batch [1/7], Loss: 0.7477, Accuracy: 87.44%, Grad Norm: 1.01593\n",
      "Epoch [202/10044], Batch [2/7], Loss: 0.8099, Accuracy: 87.19%, Grad Norm: 1.28570\n",
      "Epoch [202/10044], Batch [3/7], Loss: 0.9449, Accuracy: 85.49%, Grad Norm: 1.20268\n",
      "Epoch [202/10044], Batch [4/7], Loss: 0.8116, Accuracy: 90.31%, Grad Norm: 0.89969\n",
      "Epoch [202/10044], Batch [5/7], Loss: 0.6826, Accuracy: 91.57%, Grad Norm: 0.76845\n",
      "Epoch [202/10044], Batch [6/7], Loss: 0.7358, Accuracy: 90.52%, Grad Norm: 1.06469\n",
      "Epoch [202/10044], Batch [7/7], Loss: 0.5196, Accuracy: 93.92%, Grad Norm: 0.90122\n",
      "Epoch [202/10044], Loss: 0.5196\n",
      "Epoch [203/10044], Batch [1/7], Loss: 0.7515, Accuracy: 88.23%, Grad Norm: 1.08081\n",
      "Epoch [203/10044], Batch [2/7], Loss: 0.7898, Accuracy: 88.19%, Grad Norm: 0.97098\n",
      "Epoch [203/10044], Batch [3/7], Loss: 0.9165, Accuracy: 85.78%, Grad Norm: 1.25682\n",
      "Epoch [203/10044], Batch [4/7], Loss: 0.8005, Accuracy: 89.60%, Grad Norm: 0.86823\n",
      "Epoch [203/10044], Batch [5/7], Loss: 0.6788, Accuracy: 91.26%, Grad Norm: 0.77754\n",
      "Epoch [203/10044], Batch [6/7], Loss: 0.7153, Accuracy: 90.87%, Grad Norm: 0.96947\n",
      "Epoch [203/10044], Batch [7/7], Loss: 0.5099, Accuracy: 93.63%, Grad Norm: 0.78380\n",
      "Epoch [203/10044], Loss: 0.5099\n",
      "Epoch [204/10044], Batch [1/7], Loss: 0.7197, Accuracy: 88.77%, Grad Norm: 0.91873\n",
      "Epoch [204/10044], Batch [2/7], Loss: 0.7798, Accuracy: 89.33%, Grad Norm: 0.95241\n",
      "Epoch [204/10044], Batch [3/7], Loss: 0.8942, Accuracy: 87.61%, Grad Norm: 0.98248\n",
      "Epoch [204/10044], Batch [4/7], Loss: 0.7773, Accuracy: 90.52%, Grad Norm: 0.87092\n",
      "Epoch [204/10044], Batch [5/7], Loss: 0.6583, Accuracy: 91.54%, Grad Norm: 0.72762\n",
      "Epoch [204/10044], Batch [6/7], Loss: 0.7058, Accuracy: 89.68%, Grad Norm: 0.97152\n",
      "Epoch [204/10044], Batch [7/7], Loss: 0.4950, Accuracy: 93.68%, Grad Norm: 0.73257\n",
      "Epoch [204/10044], Loss: 0.4950\n",
      "Epoch [205/10044], Batch [1/7], Loss: 0.7017, Accuracy: 89.27%, Grad Norm: 0.71563\n",
      "Epoch [205/10044], Batch [2/7], Loss: 0.7628, Accuracy: 89.91%, Grad Norm: 1.02831\n",
      "Epoch [205/10044], Batch [3/7], Loss: 0.8931, Accuracy: 88.60%, Grad Norm: 1.16410\n",
      "Epoch [205/10044], Batch [4/7], Loss: 0.7699, Accuracy: 91.02%, Grad Norm: 0.78283\n",
      "Epoch [205/10044], Batch [5/7], Loss: 0.6466, Accuracy: 91.77%, Grad Norm: 0.77668\n",
      "Epoch [205/10044], Batch [6/7], Loss: 0.6982, Accuracy: 89.28%, Grad Norm: 1.02076\n",
      "Epoch [205/10044], Batch [7/7], Loss: 0.4885, Accuracy: 93.17%, Grad Norm: 0.81899\n",
      "Epoch [205/10044], Loss: 0.4885\n",
      "Epoch [206/10044], Batch [1/7], Loss: 0.7109, Accuracy: 87.64%, Grad Norm: 0.95504\n",
      "Epoch [206/10044], Batch [2/7], Loss: 0.7470, Accuracy: 89.95%, Grad Norm: 0.98444\n",
      "Epoch [206/10044], Batch [3/7], Loss: 0.8785, Accuracy: 89.04%, Grad Norm: 1.31689\n",
      "Epoch [206/10044], Batch [4/7], Loss: 0.7698, Accuracy: 91.59%, Grad Norm: 1.10069\n",
      "Epoch [206/10044], Batch [5/7], Loss: 0.6511, Accuracy: 92.04%, Grad Norm: 0.89623\n",
      "Epoch [206/10044], Batch [6/7], Loss: 0.6888, Accuracy: 90.60%, Grad Norm: 1.03780\n",
      "Epoch [206/10044], Batch [7/7], Loss: 0.4814, Accuracy: 94.02%, Grad Norm: 0.79718\n",
      "Epoch [206/10044], Loss: 0.4814\n",
      "Epoch [207/10044], Batch [1/7], Loss: 0.6970, Accuracy: 88.55%, Grad Norm: 0.88620\n",
      "Epoch [207/10044], Batch [2/7], Loss: 0.7475, Accuracy: 88.47%, Grad Norm: 1.15161\n",
      "Epoch [207/10044], Batch [3/7], Loss: 0.8781, Accuracy: 86.67%, Grad Norm: 1.32341\n",
      "Epoch [207/10044], Batch [4/7], Loss: 0.7523, Accuracy: 91.11%, Grad Norm: 0.89161\n",
      "Epoch [207/10044], Batch [5/7], Loss: 0.6341, Accuracy: 92.56%, Grad Norm: 0.75516\n",
      "Epoch [207/10044], Batch [6/7], Loss: 0.6827, Accuracy: 91.59%, Grad Norm: 1.09434\n",
      "Epoch [207/10044], Batch [7/7], Loss: 0.4770, Accuracy: 94.53%, Grad Norm: 0.87824\n",
      "Epoch [207/10044], Loss: 0.4770\n",
      "Epoch [208/10044], Batch [1/7], Loss: 0.6819, Accuracy: 89.82%, Grad Norm: 0.90391\n",
      "Epoch [208/10044], Batch [2/7], Loss: 0.7239, Accuracy: 89.55%, Grad Norm: 1.04460\n",
      "Epoch [208/10044], Batch [3/7], Loss: 0.8499, Accuracy: 87.76%, Grad Norm: 1.14725\n",
      "Epoch [208/10044], Batch [4/7], Loss: 0.7330, Accuracy: 91.62%, Grad Norm: 0.90981\n",
      "Epoch [208/10044], Batch [5/7], Loss: 0.6254, Accuracy: 92.08%, Grad Norm: 0.83385\n",
      "Epoch [208/10044], Batch [6/7], Loss: 0.6666, Accuracy: 91.67%, Grad Norm: 1.08302\n",
      "Epoch [208/10044], Batch [7/7], Loss: 0.4653, Accuracy: 94.65%, Grad Norm: 0.75201\n",
      "Epoch [208/10044], Loss: 0.4653\n",
      "Epoch [209/10044], Batch [1/7], Loss: 0.6890, Accuracy: 89.63%, Grad Norm: 1.04932\n",
      "Epoch [209/10044], Batch [2/7], Loss: 0.7223, Accuracy: 89.69%, Grad Norm: 1.10290\n",
      "Epoch [209/10044], Batch [3/7], Loss: 0.8334, Accuracy: 88.42%, Grad Norm: 1.10522\n",
      "Epoch [209/10044], Batch [4/7], Loss: 0.7325, Accuracy: 90.82%, Grad Norm: 1.03121\n",
      "Epoch [209/10044], Batch [5/7], Loss: 0.6172, Accuracy: 91.84%, Grad Norm: 0.76775\n",
      "Epoch [209/10044], Batch [6/7], Loss: 0.6564, Accuracy: 90.83%, Grad Norm: 0.89312\n",
      "Epoch [209/10044], Batch [7/7], Loss: 0.4732, Accuracy: 94.27%, Grad Norm: 0.86888\n",
      "Epoch [209/10044], Loss: 0.4732\n",
      "Epoch [210/10044], Batch [1/7], Loss: 0.6583, Accuracy: 90.24%, Grad Norm: 0.79787\n",
      "Epoch [210/10044], Batch [2/7], Loss: 0.6958, Accuracy: 91.27%, Grad Norm: 0.91264\n",
      "Epoch [210/10044], Batch [3/7], Loss: 0.8120, Accuracy: 90.16%, Grad Norm: 1.00584\n",
      "Epoch [210/10044], Batch [4/7], Loss: 0.7105, Accuracy: 92.36%, Grad Norm: 0.79885\n",
      "Epoch [210/10044], Batch [5/7], Loss: 0.6127, Accuracy: 92.38%, Grad Norm: 0.83280\n",
      "Epoch [210/10044], Batch [6/7], Loss: 0.6469, Accuracy: 90.46%, Grad Norm: 0.97393\n",
      "Epoch [210/10044], Batch [7/7], Loss: 0.4503, Accuracy: 94.13%, Grad Norm: 0.74848\n",
      "Epoch [210/10044], Loss: 0.4503\n",
      "Epoch [211/10044], Batch [1/7], Loss: 0.6541, Accuracy: 89.33%, Grad Norm: 0.80485\n",
      "Epoch [211/10044], Batch [2/7], Loss: 0.7098, Accuracy: 90.50%, Grad Norm: 1.15907\n",
      "Epoch [211/10044], Batch [3/7], Loss: 0.8141, Accuracy: 89.68%, Grad Norm: 1.23738\n",
      "Epoch [211/10044], Batch [4/7], Loss: 0.7023, Accuracy: 92.67%, Grad Norm: 0.93048\n",
      "Epoch [211/10044], Batch [5/7], Loss: 0.5880, Accuracy: 92.81%, Grad Norm: 0.67084\n",
      "Epoch [211/10044], Batch [6/7], Loss: 0.6258, Accuracy: 91.81%, Grad Norm: 0.90022\n",
      "Epoch [211/10044], Batch [7/7], Loss: 0.4616, Accuracy: 93.42%, Grad Norm: 0.93553\n",
      "Epoch [211/10044], Loss: 0.4616\n",
      "Epoch [212/10044], Batch [1/7], Loss: 0.6438, Accuracy: 89.55%, Grad Norm: 0.98111\n",
      "Epoch [212/10044], Batch [2/7], Loss: 0.6849, Accuracy: 89.87%, Grad Norm: 1.02585\n",
      "Epoch [212/10044], Batch [3/7], Loss: 0.7819, Accuracy: 89.61%, Grad Norm: 0.96927\n",
      "Epoch [212/10044], Batch [4/7], Loss: 0.6945, Accuracy: 92.78%, Grad Norm: 0.98972\n",
      "Epoch [212/10044], Batch [5/7], Loss: 0.5880, Accuracy: 93.09%, Grad Norm: 0.84143\n",
      "Epoch [212/10044], Batch [6/7], Loss: 0.6292, Accuracy: 92.48%, Grad Norm: 1.00017\n",
      "Epoch [212/10044], Batch [7/7], Loss: 0.4489, Accuracy: 94.55%, Grad Norm: 0.78416\n",
      "Epoch [212/10044], Loss: 0.4489\n",
      "Epoch [213/10044], Batch [1/7], Loss: 0.6406, Accuracy: 90.82%, Grad Norm: 0.92015\n",
      "Epoch [213/10044], Batch [2/7], Loss: 0.6823, Accuracy: 89.76%, Grad Norm: 1.14583\n",
      "Epoch [213/10044], Batch [3/7], Loss: 0.7987, Accuracy: 88.16%, Grad Norm: 1.31945\n",
      "Epoch [213/10044], Batch [4/7], Loss: 0.6871, Accuracy: 91.93%, Grad Norm: 1.14052\n",
      "Epoch [213/10044], Batch [5/7], Loss: 0.5830, Accuracy: 92.67%, Grad Norm: 0.79388\n",
      "Epoch [213/10044], Batch [6/7], Loss: 0.6173, Accuracy: 92.61%, Grad Norm: 1.06594\n",
      "Epoch [213/10044], Batch [7/7], Loss: 0.4395, Accuracy: 95.23%, Grad Norm: 0.89920\n",
      "Epoch [213/10044], Loss: 0.4395\n",
      "Epoch [214/10044], Batch [1/7], Loss: 0.6433, Accuracy: 90.47%, Grad Norm: 0.99960\n",
      "Epoch [214/10044], Batch [2/7], Loss: 0.6714, Accuracy: 90.99%, Grad Norm: 1.13404\n",
      "Epoch [214/10044], Batch [3/7], Loss: 0.7708, Accuracy: 90.31%, Grad Norm: 1.30259\n",
      "Epoch [214/10044], Batch [4/7], Loss: 0.6675, Accuracy: 92.36%, Grad Norm: 0.91526\n",
      "Epoch [214/10044], Batch [5/7], Loss: 0.5702, Accuracy: 92.98%, Grad Norm: 0.71319\n",
      "Epoch [214/10044], Batch [6/7], Loss: 0.6073, Accuracy: 91.82%, Grad Norm: 0.88644\n",
      "Epoch [214/10044], Batch [7/7], Loss: 0.4244, Accuracy: 94.80%, Grad Norm: 0.68921\n",
      "Epoch [214/10044], Loss: 0.4244\n",
      "Epoch [215/10044], Batch [1/7], Loss: 0.6196, Accuracy: 90.62%, Grad Norm: 0.82956\n",
      "Epoch [215/10044], Batch [2/7], Loss: 0.6442, Accuracy: 92.02%, Grad Norm: 0.87264\n",
      "Epoch [215/10044], Batch [3/7], Loss: 0.7435, Accuracy: 90.88%, Grad Norm: 0.96026\n",
      "Epoch [215/10044], Batch [4/7], Loss: 0.6457, Accuracy: 93.32%, Grad Norm: 0.73168\n",
      "Epoch [215/10044], Batch [5/7], Loss: 0.5550, Accuracy: 93.47%, Grad Norm: 0.65035\n",
      "Epoch [215/10044], Batch [6/7], Loss: 0.5862, Accuracy: 92.19%, Grad Norm: 0.74516\n",
      "Epoch [215/10044], Batch [7/7], Loss: 0.4200, Accuracy: 94.45%, Grad Norm: 0.68861\n",
      "Epoch [215/10044], Loss: 0.4200\n",
      "Epoch [216/10044], Batch [1/7], Loss: 0.6048, Accuracy: 91.13%, Grad Norm: 0.73577\n",
      "Epoch [216/10044], Batch [2/7], Loss: 0.6386, Accuracy: 91.49%, Grad Norm: 0.91325\n",
      "Epoch [216/10044], Batch [3/7], Loss: 0.7233, Accuracy: 91.67%, Grad Norm: 0.91660\n",
      "Epoch [216/10044], Batch [4/7], Loss: 0.6310, Accuracy: 93.69%, Grad Norm: 0.73439\n",
      "Epoch [216/10044], Batch [5/7], Loss: 0.5441, Accuracy: 93.75%, Grad Norm: 0.69751\n",
      "Epoch [216/10044], Batch [6/7], Loss: 0.5743, Accuracy: 92.55%, Grad Norm: 0.85156\n",
      "Epoch [216/10044], Batch [7/7], Loss: 0.4109, Accuracy: 94.92%, Grad Norm: 0.67329\n",
      "Epoch [216/10044], Loss: 0.4109\n",
      "Epoch [217/10044], Batch [1/7], Loss: 0.6015, Accuracy: 90.60%, Grad Norm: 0.81454\n",
      "Epoch [217/10044], Batch [2/7], Loss: 0.6243, Accuracy: 91.52%, Grad Norm: 0.87151\n",
      "Epoch [217/10044], Batch [3/7], Loss: 0.7084, Accuracy: 91.55%, Grad Norm: 0.93316\n",
      "Epoch [217/10044], Batch [4/7], Loss: 0.6171, Accuracy: 94.04%, Grad Norm: 0.71584\n",
      "Epoch [217/10044], Batch [5/7], Loss: 0.5315, Accuracy: 94.22%, Grad Norm: 0.59372\n",
      "Epoch [217/10044], Batch [6/7], Loss: 0.5665, Accuracy: 93.13%, Grad Norm: 0.73758\n",
      "Epoch [217/10044], Batch [7/7], Loss: 0.3992, Accuracy: 95.28%, Grad Norm: 0.63660\n",
      "Epoch [217/10044], Loss: 0.3992\n",
      "Epoch [218/10044], Batch [1/7], Loss: 0.5827, Accuracy: 91.83%, Grad Norm: 0.81442\n",
      "Epoch [218/10044], Batch [2/7], Loss: 0.6122, Accuracy: 91.65%, Grad Norm: 0.82100\n",
      "Epoch [218/10044], Batch [3/7], Loss: 0.6997, Accuracy: 91.38%, Grad Norm: 0.90864\n",
      "Epoch [218/10044], Batch [4/7], Loss: 0.6096, Accuracy: 94.08%, Grad Norm: 0.70937\n",
      "Epoch [218/10044], Batch [5/7], Loss: 0.5240, Accuracy: 93.91%, Grad Norm: 0.63648\n",
      "Epoch [218/10044], Batch [6/7], Loss: 0.5560, Accuracy: 93.61%, Grad Norm: 0.82399\n",
      "Epoch [218/10044], Batch [7/7], Loss: 0.3927, Accuracy: 95.70%, Grad Norm: 0.65012\n",
      "Epoch [218/10044], Loss: 0.3927\n",
      "Epoch [219/10044], Batch [1/7], Loss: 0.5792, Accuracy: 91.86%, Grad Norm: 0.77928\n",
      "Epoch [219/10044], Batch [2/7], Loss: 0.5959, Accuracy: 92.37%, Grad Norm: 0.74578\n",
      "Epoch [219/10044], Batch [3/7], Loss: 0.6861, Accuracy: 91.58%, Grad Norm: 0.92007\n",
      "Epoch [219/10044], Batch [4/7], Loss: 0.6015, Accuracy: 93.37%, Grad Norm: 0.69308\n",
      "Epoch [219/10044], Batch [5/7], Loss: 0.5205, Accuracy: 93.90%, Grad Norm: 0.62717\n",
      "Epoch [219/10044], Batch [6/7], Loss: 0.5541, Accuracy: 93.12%, Grad Norm: 0.83168\n",
      "Epoch [219/10044], Batch [7/7], Loss: 0.3898, Accuracy: 95.45%, Grad Norm: 0.64334\n",
      "Epoch [219/10044], Loss: 0.3898\n",
      "Epoch [220/10044], Batch [1/7], Loss: 0.5662, Accuracy: 91.62%, Grad Norm: 0.68985\n",
      "Epoch [220/10044], Batch [2/7], Loss: 0.5852, Accuracy: 92.85%, Grad Norm: 0.74803\n",
      "Epoch [220/10044], Batch [3/7], Loss: 0.6727, Accuracy: 92.71%, Grad Norm: 0.90971\n",
      "Epoch [220/10044], Batch [4/7], Loss: 0.5900, Accuracy: 94.14%, Grad Norm: 0.76117\n",
      "Epoch [220/10044], Batch [5/7], Loss: 0.5107, Accuracy: 93.87%, Grad Norm: 0.55779\n",
      "Epoch [220/10044], Batch [6/7], Loss: 0.5400, Accuracy: 93.07%, Grad Norm: 0.70270\n",
      "Epoch [220/10044], Batch [7/7], Loss: 0.3867, Accuracy: 95.27%, Grad Norm: 0.58757\n",
      "Epoch [220/10044], Loss: 0.3867\n",
      "Epoch [221/10044], Batch [1/7], Loss: 0.5543, Accuracy: 91.71%, Grad Norm: 0.65764\n",
      "Epoch [221/10044], Batch [2/7], Loss: 0.5751, Accuracy: 92.60%, Grad Norm: 0.72610\n",
      "Epoch [221/10044], Batch [3/7], Loss: 0.6591, Accuracy: 92.77%, Grad Norm: 0.81291\n",
      "Epoch [221/10044], Batch [4/7], Loss: 0.5775, Accuracy: 94.42%, Grad Norm: 0.64255\n",
      "Epoch [221/10044], Batch [5/7], Loss: 0.5008, Accuracy: 94.37%, Grad Norm: 0.58242\n",
      "Epoch [221/10044], Batch [6/7], Loss: 0.5311, Accuracy: 93.44%, Grad Norm: 0.73015\n",
      "Epoch [221/10044], Batch [7/7], Loss: 0.3786, Accuracy: 95.63%, Grad Norm: 0.73014\n",
      "Epoch [221/10044], Loss: 0.3786\n",
      "Epoch [222/10044], Batch [1/7], Loss: 0.5442, Accuracy: 92.20%, Grad Norm: 0.67098\n",
      "Epoch [222/10044], Batch [2/7], Loss: 0.5667, Accuracy: 92.45%, Grad Norm: 0.75295\n",
      "Epoch [222/10044], Batch [3/7], Loss: 0.6518, Accuracy: 92.36%, Grad Norm: 0.81926\n",
      "Epoch [222/10044], Batch [4/7], Loss: 0.5701, Accuracy: 94.24%, Grad Norm: 0.67977\n",
      "Epoch [222/10044], Batch [5/7], Loss: 0.4891, Accuracy: 94.64%, Grad Norm: 0.55287\n",
      "Epoch [222/10044], Batch [6/7], Loss: 0.5167, Accuracy: 93.99%, Grad Norm: 0.66278\n",
      "Epoch [222/10044], Batch [7/7], Loss: 0.3761, Accuracy: 95.27%, Grad Norm: 0.64820\n",
      "Epoch [222/10044], Loss: 0.3761\n",
      "Epoch [223/10044], Batch [1/7], Loss: 0.5434, Accuracy: 92.57%, Grad Norm: 0.75107\n",
      "Epoch [223/10044], Batch [2/7], Loss: 0.5660, Accuracy: 92.79%, Grad Norm: 0.73763\n",
      "Epoch [223/10044], Batch [3/7], Loss: 0.6348, Accuracy: 92.99%, Grad Norm: 0.78078\n",
      "Epoch [223/10044], Batch [4/7], Loss: 0.5547, Accuracy: 94.67%, Grad Norm: 0.64081\n",
      "Epoch [223/10044], Batch [5/7], Loss: 0.4860, Accuracy: 94.66%, Grad Norm: 0.62800\n",
      "Epoch [223/10044], Batch [6/7], Loss: 0.5160, Accuracy: 93.62%, Grad Norm: 0.72700\n",
      "Epoch [223/10044], Batch [7/7], Loss: 0.3735, Accuracy: 95.58%, Grad Norm: 0.62035\n",
      "Epoch [223/10044], Loss: 0.3735\n",
      "Epoch [224/10044], Batch [1/7], Loss: 0.5291, Accuracy: 92.49%, Grad Norm: 0.68828\n",
      "Epoch [224/10044], Batch [2/7], Loss: 0.5432, Accuracy: 93.16%, Grad Norm: 0.72935\n",
      "Epoch [224/10044], Batch [3/7], Loss: 0.6218, Accuracy: 93.12%, Grad Norm: 0.90010\n",
      "Epoch [224/10044], Batch [4/7], Loss: 0.5443, Accuracy: 94.59%, Grad Norm: 0.67030\n",
      "Epoch [224/10044], Batch [5/7], Loss: 0.4757, Accuracy: 94.47%, Grad Norm: 0.55758\n",
      "Epoch [224/10044], Batch [6/7], Loss: 0.5103, Accuracy: 93.52%, Grad Norm: 0.79284\n",
      "Epoch [224/10044], Batch [7/7], Loss: 0.3598, Accuracy: 95.92%, Grad Norm: 0.59350\n",
      "Epoch [224/10044], Loss: 0.3598\n",
      "Epoch [225/10044], Batch [1/7], Loss: 0.5266, Accuracy: 92.43%, Grad Norm: 0.65067\n",
      "Epoch [225/10044], Batch [2/7], Loss: 0.5381, Accuracy: 93.45%, Grad Norm: 0.69870\n",
      "Epoch [225/10044], Batch [3/7], Loss: 0.6044, Accuracy: 93.64%, Grad Norm: 0.76826\n",
      "Epoch [225/10044], Batch [4/7], Loss: 0.5381, Accuracy: 94.70%, Grad Norm: 0.67794\n",
      "Epoch [225/10044], Batch [5/7], Loss: 0.4663, Accuracy: 94.50%, Grad Norm: 0.58410\n",
      "Epoch [225/10044], Batch [6/7], Loss: 0.4932, Accuracy: 93.77%, Grad Norm: 0.67030\n",
      "Epoch [225/10044], Batch [7/7], Loss: 0.3569, Accuracy: 95.48%, Grad Norm: 0.60396\n",
      "Epoch [225/10044], Loss: 0.3569\n",
      "Epoch [226/10044], Batch [1/7], Loss: 0.5210, Accuracy: 92.56%, Grad Norm: 0.75628\n",
      "Epoch [226/10044], Batch [2/7], Loss: 0.5228, Accuracy: 93.72%, Grad Norm: 0.71168\n",
      "Epoch [226/10044], Batch [3/7], Loss: 0.5975, Accuracy: 93.73%, Grad Norm: 0.85314\n",
      "Epoch [226/10044], Batch [4/7], Loss: 0.5307, Accuracy: 95.37%, Grad Norm: 0.75662\n",
      "Epoch [226/10044], Batch [5/7], Loss: 0.4668, Accuracy: 94.82%, Grad Norm: 0.59811\n",
      "Epoch [226/10044], Batch [6/7], Loss: 0.4909, Accuracy: 93.94%, Grad Norm: 0.75408\n",
      "Epoch [226/10044], Batch [7/7], Loss: 0.3528, Accuracy: 95.60%, Grad Norm: 0.62543\n",
      "Epoch [226/10044], Loss: 0.3528\n",
      "Epoch [227/10044], Batch [1/7], Loss: 0.5084, Accuracy: 92.65%, Grad Norm: 0.69739\n",
      "Epoch [227/10044], Batch [2/7], Loss: 0.5207, Accuracy: 93.34%, Grad Norm: 0.71920\n",
      "Epoch [227/10044], Batch [3/7], Loss: 0.5917, Accuracy: 93.25%, Grad Norm: 0.94561\n",
      "Epoch [227/10044], Batch [4/7], Loss: 0.5223, Accuracy: 95.07%, Grad Norm: 0.69188\n",
      "Epoch [227/10044], Batch [5/7], Loss: 0.4575, Accuracy: 95.23%, Grad Norm: 0.65062\n",
      "Epoch [227/10044], Batch [6/7], Loss: 0.4798, Accuracy: 94.37%, Grad Norm: 0.78981\n",
      "Epoch [227/10044], Batch [7/7], Loss: 0.3471, Accuracy: 96.27%, Grad Norm: 0.60656\n",
      "Epoch [227/10044], Loss: 0.3471\n",
      "Epoch [228/10044], Batch [1/7], Loss: 0.5045, Accuracy: 93.05%, Grad Norm: 0.68596\n",
      "Epoch [228/10044], Batch [2/7], Loss: 0.5180, Accuracy: 93.17%, Grad Norm: 0.83390\n",
      "Epoch [228/10044], Batch [3/7], Loss: 0.5777, Accuracy: 93.12%, Grad Norm: 0.85861\n",
      "Epoch [228/10044], Batch [4/7], Loss: 0.5105, Accuracy: 95.25%, Grad Norm: 0.73275\n",
      "Epoch [228/10044], Batch [5/7], Loss: 0.4520, Accuracy: 94.91%, Grad Norm: 0.56927\n",
      "Epoch [228/10044], Batch [6/7], Loss: 0.4756, Accuracy: 94.71%, Grad Norm: 0.78590\n",
      "Epoch [228/10044], Batch [7/7], Loss: 0.3450, Accuracy: 96.47%, Grad Norm: 0.69348\n",
      "Epoch [228/10044], Loss: 0.3450\n",
      "Epoch [229/10044], Batch [1/7], Loss: 0.5102, Accuracy: 93.20%, Grad Norm: 0.84916\n",
      "Epoch [229/10044], Batch [2/7], Loss: 0.5107, Accuracy: 93.60%, Grad Norm: 0.77553\n",
      "Epoch [229/10044], Batch [3/7], Loss: 0.5602, Accuracy: 93.64%, Grad Norm: 0.85756\n",
      "Epoch [229/10044], Batch [4/7], Loss: 0.5151, Accuracy: 94.76%, Grad Norm: 0.80276\n",
      "Epoch [229/10044], Batch [5/7], Loss: 0.4532, Accuracy: 94.64%, Grad Norm: 0.66846\n",
      "Epoch [229/10044], Batch [6/7], Loss: 0.4651, Accuracy: 94.27%, Grad Norm: 0.70401\n",
      "Epoch [229/10044], Batch [7/7], Loss: 0.3370, Accuracy: 96.38%, Grad Norm: 0.55888\n",
      "Epoch [229/10044], Loss: 0.3370\n",
      "Epoch [230/10044], Batch [1/7], Loss: 0.4992, Accuracy: 92.87%, Grad Norm: 0.73107\n",
      "Epoch [230/10044], Batch [2/7], Loss: 0.5085, Accuracy: 94.02%, Grad Norm: 0.95649\n",
      "Epoch [230/10044], Batch [3/7], Loss: 0.5646, Accuracy: 94.35%, Grad Norm: 0.93794\n",
      "Epoch [230/10044], Batch [4/7], Loss: 0.4930, Accuracy: 95.54%, Grad Norm: 0.64591\n",
      "Epoch [230/10044], Batch [5/7], Loss: 0.4374, Accuracy: 94.71%, Grad Norm: 0.65052\n",
      "Epoch [230/10044], Batch [6/7], Loss: 0.4671, Accuracy: 93.78%, Grad Norm: 0.84084\n",
      "Epoch [230/10044], Batch [7/7], Loss: 0.3334, Accuracy: 95.82%, Grad Norm: 0.63183\n",
      "Epoch [230/10044], Loss: 0.3334\n",
      "Epoch [231/10044], Batch [1/7], Loss: 0.4903, Accuracy: 92.83%, Grad Norm: 0.91182\n",
      "Epoch [231/10044], Batch [2/7], Loss: 0.4896, Accuracy: 93.96%, Grad Norm: 0.72362\n",
      "Epoch [231/10044], Batch [3/7], Loss: 0.5539, Accuracy: 94.53%, Grad Norm: 0.92297\n",
      "Epoch [231/10044], Batch [4/7], Loss: 0.4951, Accuracy: 95.62%, Grad Norm: 0.72724\n",
      "Epoch [231/10044], Batch [5/7], Loss: 0.4344, Accuracy: 95.53%, Grad Norm: 0.62134\n",
      "Epoch [231/10044], Batch [6/7], Loss: 0.4514, Accuracy: 95.08%, Grad Norm: 0.69194\n",
      "Epoch [231/10044], Batch [7/7], Loss: 0.3280, Accuracy: 96.27%, Grad Norm: 0.64957\n",
      "Epoch [231/10044], Loss: 0.3280\n",
      "Epoch [232/10044], Batch [1/7], Loss: 0.4855, Accuracy: 93.05%, Grad Norm: 0.79431\n",
      "Epoch [232/10044], Batch [2/7], Loss: 0.4910, Accuracy: 93.33%, Grad Norm: 0.82102\n",
      "Epoch [232/10044], Batch [3/7], Loss: 0.5299, Accuracy: 94.68%, Grad Norm: 0.76912\n",
      "Epoch [232/10044], Batch [4/7], Loss: 0.4841, Accuracy: 95.69%, Grad Norm: 0.70468\n",
      "Epoch [232/10044], Batch [5/7], Loss: 0.4285, Accuracy: 95.52%, Grad Norm: 0.62469\n",
      "Epoch [232/10044], Batch [6/7], Loss: 0.4509, Accuracy: 94.97%, Grad Norm: 0.83764\n",
      "Epoch [232/10044], Batch [7/7], Loss: 0.3281, Accuracy: 96.50%, Grad Norm: 0.71362\n",
      "Epoch [232/10044], Loss: 0.3281\n",
      "Epoch [233/10044], Batch [1/7], Loss: 0.4703, Accuracy: 93.71%, Grad Norm: 0.73513\n",
      "Epoch [233/10044], Batch [2/7], Loss: 0.4780, Accuracy: 93.82%, Grad Norm: 0.78890\n",
      "Epoch [233/10044], Batch [3/7], Loss: 0.5400, Accuracy: 93.91%, Grad Norm: 0.96958\n",
      "Epoch [233/10044], Batch [4/7], Loss: 0.4725, Accuracy: 95.54%, Grad Norm: 0.68110\n",
      "Epoch [233/10044], Batch [5/7], Loss: 0.4250, Accuracy: 95.22%, Grad Norm: 0.63633\n",
      "Epoch [233/10044], Batch [6/7], Loss: 0.4395, Accuracy: 94.99%, Grad Norm: 0.65221\n",
      "Epoch [233/10044], Batch [7/7], Loss: 0.3208, Accuracy: 96.35%, Grad Norm: 0.64437\n",
      "Epoch [233/10044], Loss: 0.3208\n",
      "Epoch [234/10044], Batch [1/7], Loss: 0.4736, Accuracy: 93.95%, Grad Norm: 0.81003\n",
      "Epoch [234/10044], Batch [2/7], Loss: 0.4604, Accuracy: 94.59%, Grad Norm: 0.75104\n",
      "Epoch [234/10044], Batch [3/7], Loss: 0.5207, Accuracy: 94.82%, Grad Norm: 0.77154\n",
      "Epoch [234/10044], Batch [4/7], Loss: 0.4585, Accuracy: 95.92%, Grad Norm: 0.59335\n",
      "Epoch [234/10044], Batch [5/7], Loss: 0.4161, Accuracy: 95.12%, Grad Norm: 0.58818\n",
      "Epoch [234/10044], Batch [6/7], Loss: 0.4433, Accuracy: 94.22%, Grad Norm: 0.80250\n",
      "Epoch [234/10044], Batch [7/7], Loss: 0.3251, Accuracy: 95.77%, Grad Norm: 0.73482\n",
      "Epoch [234/10044], Loss: 0.3251\n",
      "Epoch [235/10044], Batch [1/7], Loss: 0.4575, Accuracy: 93.72%, Grad Norm: 0.69014\n",
      "Epoch [235/10044], Batch [2/7], Loss: 0.4661, Accuracy: 94.64%, Grad Norm: 0.88763\n",
      "Epoch [235/10044], Batch [3/7], Loss: 0.5142, Accuracy: 95.12%, Grad Norm: 0.81374\n",
      "Epoch [235/10044], Batch [4/7], Loss: 0.4548, Accuracy: 96.11%, Grad Norm: 0.67104\n",
      "Epoch [235/10044], Batch [5/7], Loss: 0.4122, Accuracy: 95.75%, Grad Norm: 0.61111\n",
      "Epoch [235/10044], Batch [6/7], Loss: 0.4341, Accuracy: 94.47%, Grad Norm: 0.78780\n",
      "Epoch [235/10044], Batch [7/7], Loss: 0.3123, Accuracy: 96.48%, Grad Norm: 0.65199\n",
      "Epoch [235/10044], Loss: 0.3123\n",
      "Epoch [236/10044], Batch [1/7], Loss: 0.4616, Accuracy: 92.96%, Grad Norm: 0.93371\n",
      "Epoch [236/10044], Batch [2/7], Loss: 0.4566, Accuracy: 94.26%, Grad Norm: 0.70809\n",
      "Epoch [236/10044], Batch [3/7], Loss: 0.4994, Accuracy: 95.47%, Grad Norm: 0.73807\n",
      "Epoch [236/10044], Batch [4/7], Loss: 0.4621, Accuracy: 95.99%, Grad Norm: 0.77911\n",
      "Epoch [236/10044], Batch [5/7], Loss: 0.4123, Accuracy: 95.58%, Grad Norm: 0.71974\n",
      "Epoch [236/10044], Batch [6/7], Loss: 0.4245, Accuracy: 94.93%, Grad Norm: 0.73958\n",
      "Epoch [236/10044], Batch [7/7], Loss: 0.3116, Accuracy: 96.57%, Grad Norm: 0.64427\n",
      "Epoch [236/10044], Loss: 0.3116\n",
      "Epoch [237/10044], Batch [1/7], Loss: 0.4446, Accuracy: 94.09%, Grad Norm: 0.68567\n",
      "Epoch [237/10044], Batch [2/7], Loss: 0.4583, Accuracy: 93.63%, Grad Norm: 0.90112\n",
      "Epoch [237/10044], Batch [3/7], Loss: 0.4988, Accuracy: 94.31%, Grad Norm: 0.93655\n",
      "Epoch [237/10044], Batch [4/7], Loss: 0.4538, Accuracy: 95.64%, Grad Norm: 0.69317\n",
      "Epoch [237/10044], Batch [5/7], Loss: 0.4003, Accuracy: 95.62%, Grad Norm: 0.60058\n",
      "Epoch [237/10044], Batch [6/7], Loss: 0.4218, Accuracy: 95.21%, Grad Norm: 0.81592\n",
      "Epoch [237/10044], Batch [7/7], Loss: 0.3206, Accuracy: 96.05%, Grad Norm: 0.76617\n",
      "Epoch [237/10044], Loss: 0.3206\n",
      "Epoch [238/10044], Batch [1/7], Loss: 0.4539, Accuracy: 94.12%, Grad Norm: 0.88774\n",
      "Epoch [238/10044], Batch [2/7], Loss: 0.4399, Accuracy: 94.98%, Grad Norm: 0.69150\n",
      "Epoch [238/10044], Batch [3/7], Loss: 0.4820, Accuracy: 95.27%, Grad Norm: 0.81657\n",
      "Epoch [238/10044], Batch [4/7], Loss: 0.4408, Accuracy: 95.63%, Grad Norm: 0.69574\n",
      "Epoch [238/10044], Batch [5/7], Loss: 0.3995, Accuracy: 95.30%, Grad Norm: 0.66437\n",
      "Epoch [238/10044], Batch [6/7], Loss: 0.4156, Accuracy: 94.58%, Grad Norm: 0.75559\n",
      "Epoch [238/10044], Batch [7/7], Loss: 0.3028, Accuracy: 96.20%, Grad Norm: 0.56234\n",
      "Epoch [238/10044], Loss: 0.3028\n",
      "Epoch [239/10044], Batch [1/7], Loss: 0.4366, Accuracy: 94.14%, Grad Norm: 0.67502\n",
      "Epoch [239/10044], Batch [2/7], Loss: 0.4322, Accuracy: 95.31%, Grad Norm: 0.74919\n",
      "Epoch [239/10044], Batch [3/7], Loss: 0.4836, Accuracy: 95.57%, Grad Norm: 0.84776\n",
      "Epoch [239/10044], Batch [4/7], Loss: 0.4427, Accuracy: 96.08%, Grad Norm: 0.75834\n",
      "Epoch [239/10044], Batch [5/7], Loss: 0.3861, Accuracy: 95.87%, Grad Norm: 0.56831\n",
      "Epoch [239/10044], Batch [6/7], Loss: 0.4029, Accuracy: 95.07%, Grad Norm: 0.67641\n",
      "Epoch [239/10044], Batch [7/7], Loss: 0.2945, Accuracy: 96.58%, Grad Norm: 0.56089\n",
      "Epoch [239/10044], Loss: 0.2945\n",
      "Epoch [240/10044], Batch [1/7], Loss: 0.4347, Accuracy: 93.68%, Grad Norm: 0.84466\n",
      "Epoch [240/10044], Batch [2/7], Loss: 0.4360, Accuracy: 94.44%, Grad Norm: 0.76605\n",
      "Epoch [240/10044], Batch [3/7], Loss: 0.4667, Accuracy: 95.71%, Grad Norm: 0.79093\n",
      "Epoch [240/10044], Batch [4/7], Loss: 0.4291, Accuracy: 96.46%, Grad Norm: 0.68543\n",
      "Epoch [240/10044], Batch [5/7], Loss: 0.3824, Accuracy: 96.25%, Grad Norm: 0.63165\n",
      "Epoch [240/10044], Batch [6/7], Loss: 0.4009, Accuracy: 95.47%, Grad Norm: 0.78315\n",
      "Epoch [240/10044], Batch [7/7], Loss: 0.2894, Accuracy: 96.83%, Grad Norm: 0.54887\n",
      "Epoch [240/10044], Loss: 0.2894\n",
      "Epoch [241/10044], Batch [1/7], Loss: 0.4167, Accuracy: 94.54%, Grad Norm: 0.65359\n",
      "Epoch [241/10044], Batch [2/7], Loss: 0.4190, Accuracy: 94.74%, Grad Norm: 0.70775\n",
      "Epoch [241/10044], Batch [3/7], Loss: 0.4630, Accuracy: 95.20%, Grad Norm: 0.85493\n",
      "Epoch [241/10044], Batch [4/7], Loss: 0.4133, Accuracy: 96.40%, Grad Norm: 0.57482\n",
      "Epoch [241/10044], Batch [5/7], Loss: 0.3728, Accuracy: 95.98%, Grad Norm: 0.52153\n",
      "Epoch [241/10044], Batch [6/7], Loss: 0.4038, Accuracy: 95.33%, Grad Norm: 0.77029\n",
      "Epoch [241/10044], Batch [7/7], Loss: 0.2868, Accuracy: 96.75%, Grad Norm: 0.57262\n",
      "Epoch [241/10044], Loss: 0.2868\n",
      "Epoch [242/10044], Batch [1/7], Loss: 0.4216, Accuracy: 94.66%, Grad Norm: 0.79069\n",
      "Epoch [242/10044], Batch [2/7], Loss: 0.4119, Accuracy: 95.38%, Grad Norm: 0.57168\n",
      "Epoch [242/10044], Batch [3/7], Loss: 0.4540, Accuracy: 95.26%, Grad Norm: 0.72761\n",
      "Epoch [242/10044], Batch [4/7], Loss: 0.4106, Accuracy: 96.05%, Grad Norm: 0.63946\n",
      "Epoch [242/10044], Batch [5/7], Loss: 0.3663, Accuracy: 95.87%, Grad Norm: 0.51457\n",
      "Epoch [242/10044], Batch [6/7], Loss: 0.3806, Accuracy: 95.58%, Grad Norm: 0.54905\n",
      "Epoch [242/10044], Batch [7/7], Loss: 0.2799, Accuracy: 96.72%, Grad Norm: 0.51530\n",
      "Epoch [242/10044], Loss: 0.2799\n",
      "Epoch [243/10044], Batch [1/7], Loss: 0.3977, Accuracy: 95.35%, Grad Norm: 0.59945\n",
      "Epoch [243/10044], Batch [2/7], Loss: 0.4121, Accuracy: 95.32%, Grad Norm: 0.75384\n",
      "Epoch [243/10044], Batch [3/7], Loss: 0.4489, Accuracy: 96.14%, Grad Norm: 0.70389\n",
      "Epoch [243/10044], Batch [4/7], Loss: 0.3946, Accuracy: 96.84%, Grad Norm: 0.52665\n",
      "Epoch [243/10044], Batch [5/7], Loss: 0.3610, Accuracy: 96.09%, Grad Norm: 0.49414\n",
      "Epoch [243/10044], Batch [6/7], Loss: 0.3818, Accuracy: 95.19%, Grad Norm: 0.63051\n",
      "Epoch [243/10044], Batch [7/7], Loss: 0.2724, Accuracy: 96.78%, Grad Norm: 0.50537\n",
      "Epoch [243/10044], Loss: 0.2724\n",
      "Epoch [244/10044], Batch [1/7], Loss: 0.3996, Accuracy: 94.56%, Grad Norm: 0.61408\n",
      "Epoch [244/10044], Batch [2/7], Loss: 0.3950, Accuracy: 95.22%, Grad Norm: 0.57692\n",
      "Epoch [244/10044], Batch [3/7], Loss: 0.4355, Accuracy: 95.97%, Grad Norm: 0.67400\n",
      "Epoch [244/10044], Batch [4/7], Loss: 0.3884, Accuracy: 97.12%, Grad Norm: 0.52209\n",
      "Epoch [244/10044], Batch [5/7], Loss: 0.3523, Accuracy: 96.49%, Grad Norm: 0.46614\n",
      "Epoch [244/10044], Batch [6/7], Loss: 0.3712, Accuracy: 95.60%, Grad Norm: 0.56954\n",
      "Epoch [244/10044], Batch [7/7], Loss: 0.2711, Accuracy: 97.12%, Grad Norm: 0.48779\n",
      "Epoch [244/10044], Loss: 0.2711\n",
      "Epoch [245/10044], Batch [1/7], Loss: 0.3936, Accuracy: 95.01%, Grad Norm: 0.65081\n",
      "Epoch [245/10044], Batch [2/7], Loss: 0.3900, Accuracy: 95.13%, Grad Norm: 0.61591\n",
      "Epoch [245/10044], Batch [3/7], Loss: 0.4184, Accuracy: 96.14%, Grad Norm: 0.56861\n",
      "Epoch [245/10044], Batch [4/7], Loss: 0.3903, Accuracy: 96.67%, Grad Norm: 0.55710\n",
      "Epoch [245/10044], Batch [5/7], Loss: 0.3456, Accuracy: 96.45%, Grad Norm: 0.51800\n",
      "Epoch [245/10044], Batch [6/7], Loss: 0.3611, Accuracy: 95.77%, Grad Norm: 0.55021\n",
      "Epoch [245/10044], Batch [7/7], Loss: 0.2646, Accuracy: 97.23%, Grad Norm: 0.44698\n",
      "Epoch [245/10044], Loss: 0.2646\n",
      "Epoch [246/10044], Batch [1/7], Loss: 0.3794, Accuracy: 95.48%, Grad Norm: 0.49910\n",
      "Epoch [246/10044], Batch [2/7], Loss: 0.3814, Accuracy: 95.48%, Grad Norm: 0.53390\n",
      "Epoch [246/10044], Batch [3/7], Loss: 0.4187, Accuracy: 96.12%, Grad Norm: 0.70058\n",
      "Epoch [246/10044], Batch [4/7], Loss: 0.3781, Accuracy: 96.92%, Grad Norm: 0.52965\n",
      "Epoch [246/10044], Batch [5/7], Loss: 0.3478, Accuracy: 96.17%, Grad Norm: 0.49930\n",
      "Epoch [246/10044], Batch [6/7], Loss: 0.3520, Accuracy: 96.19%, Grad Norm: 0.44580\n",
      "Epoch [246/10044], Batch [7/7], Loss: 0.2635, Accuracy: 96.80%, Grad Norm: 0.47074\n",
      "Epoch [246/10044], Loss: 0.2635\n",
      "Epoch [247/10044], Batch [1/7], Loss: 0.3812, Accuracy: 95.62%, Grad Norm: 0.63048\n",
      "Epoch [247/10044], Batch [2/7], Loss: 0.3756, Accuracy: 95.72%, Grad Norm: 0.53170\n",
      "Epoch [247/10044], Batch [3/7], Loss: 0.4031, Accuracy: 96.55%, Grad Norm: 0.55524\n",
      "Epoch [247/10044], Batch [4/7], Loss: 0.3735, Accuracy: 96.79%, Grad Norm: 0.47754\n",
      "Epoch [247/10044], Batch [5/7], Loss: 0.3363, Accuracy: 96.44%, Grad Norm: 0.43754\n",
      "Epoch [247/10044], Batch [6/7], Loss: 0.3486, Accuracy: 95.79%, Grad Norm: 0.49290\n",
      "Epoch [247/10044], Batch [7/7], Loss: 0.2564, Accuracy: 96.95%, Grad Norm: 0.50317\n",
      "Epoch [247/10044], Loss: 0.2564\n",
      "Epoch [248/10044], Batch [1/7], Loss: 0.3826, Accuracy: 94.82%, Grad Norm: 0.53083\n",
      "Epoch [248/10044], Batch [2/7], Loss: 0.3724, Accuracy: 95.91%, Grad Norm: 0.50205\n",
      "Epoch [248/10044], Batch [3/7], Loss: 0.4010, Accuracy: 96.76%, Grad Norm: 0.59660\n",
      "Epoch [248/10044], Batch [4/7], Loss: 0.3689, Accuracy: 96.94%, Grad Norm: 0.49356\n",
      "Epoch [248/10044], Batch [5/7], Loss: 0.3372, Accuracy: 96.44%, Grad Norm: 0.47209\n",
      "Epoch [248/10044], Batch [6/7], Loss: 0.3471, Accuracy: 95.82%, Grad Norm: 0.47519\n",
      "Epoch [248/10044], Batch [7/7], Loss: 0.2578, Accuracy: 97.22%, Grad Norm: 0.51068\n",
      "Epoch [248/10044], Loss: 0.2578\n",
      "Epoch [249/10044], Batch [1/7], Loss: 0.3677, Accuracy: 95.13%, Grad Norm: 0.53886\n",
      "Epoch [249/10044], Batch [2/7], Loss: 0.3660, Accuracy: 95.55%, Grad Norm: 0.58764\n",
      "Epoch [249/10044], Batch [3/7], Loss: 0.3950, Accuracy: 96.37%, Grad Norm: 0.55361\n",
      "Epoch [249/10044], Batch [4/7], Loss: 0.3664, Accuracy: 97.01%, Grad Norm: 0.48550\n",
      "Epoch [249/10044], Batch [5/7], Loss: 0.3339, Accuracy: 96.51%, Grad Norm: 0.44418\n",
      "Epoch [249/10044], Batch [6/7], Loss: 0.3457, Accuracy: 96.04%, Grad Norm: 0.54057\n",
      "Epoch [249/10044], Batch [7/7], Loss: 0.2560, Accuracy: 97.20%, Grad Norm: 0.44668\n",
      "Epoch [249/10044], Loss: 0.2560\n",
      "Epoch [250/10044], Batch [1/7], Loss: 0.3660, Accuracy: 95.77%, Grad Norm: 0.57242\n",
      "Epoch [250/10044], Batch [2/7], Loss: 0.3544, Accuracy: 95.83%, Grad Norm: 0.52746\n",
      "Epoch [250/10044], Batch [3/7], Loss: 0.3866, Accuracy: 96.31%, Grad Norm: 0.65025\n",
      "Epoch [250/10044], Batch [4/7], Loss: 0.3586, Accuracy: 96.66%, Grad Norm: 0.47542\n",
      "Epoch [250/10044], Batch [5/7], Loss: 0.3237, Accuracy: 96.63%, Grad Norm: 0.43339\n",
      "Epoch [250/10044], Batch [6/7], Loss: 0.3385, Accuracy: 96.34%, Grad Norm: 0.50244\n",
      "Epoch [250/10044], Batch [7/7], Loss: 0.2487, Accuracy: 97.05%, Grad Norm: 0.53404\n",
      "Epoch [250/10044], Loss: 0.2487\n",
      "Epoch [251/10044], Batch [1/7], Loss: 0.3598, Accuracy: 95.43%, Grad Norm: 0.55054\n",
      "Epoch [251/10044], Batch [2/7], Loss: 0.3520, Accuracy: 96.02%, Grad Norm: 0.47503\n",
      "Epoch [251/10044], Batch [3/7], Loss: 0.3807, Accuracy: 96.71%, Grad Norm: 0.53188\n",
      "Epoch [251/10044], Batch [4/7], Loss: 0.3520, Accuracy: 97.17%, Grad Norm: 0.53408\n",
      "Epoch [251/10044], Batch [5/7], Loss: 0.3217, Accuracy: 96.54%, Grad Norm: 0.47008\n",
      "Epoch [251/10044], Batch [6/7], Loss: 0.3353, Accuracy: 95.98%, Grad Norm: 0.49657\n",
      "Epoch [251/10044], Batch [7/7], Loss: 0.2364, Accuracy: 97.43%, Grad Norm: 0.39970\n",
      "Epoch [251/10044], Loss: 0.2364\n",
      "Epoch [252/10044], Batch [1/7], Loss: 0.3491, Accuracy: 95.91%, Grad Norm: 0.49385\n",
      "Epoch [252/10044], Batch [2/7], Loss: 0.3482, Accuracy: 95.91%, Grad Norm: 0.52337\n",
      "Epoch [252/10044], Batch [3/7], Loss: 0.3734, Accuracy: 96.90%, Grad Norm: 0.52859\n",
      "Epoch [252/10044], Batch [4/7], Loss: 0.3425, Accuracy: 97.32%, Grad Norm: 0.44566\n",
      "Epoch [252/10044], Batch [5/7], Loss: 0.3135, Accuracy: 96.42%, Grad Norm: 0.42004\n",
      "Epoch [252/10044], Batch [6/7], Loss: 0.3293, Accuracy: 96.21%, Grad Norm: 0.47313\n",
      "Epoch [252/10044], Batch [7/7], Loss: 0.2368, Accuracy: 97.55%, Grad Norm: 0.44277\n",
      "Epoch [252/10044], Loss: 0.2368\n",
      "Epoch [253/10044], Batch [1/7], Loss: 0.3511, Accuracy: 95.75%, Grad Norm: 0.55409\n",
      "Epoch [253/10044], Batch [2/7], Loss: 0.3448, Accuracy: 95.89%, Grad Norm: 0.50098\n",
      "Epoch [253/10044], Batch [3/7], Loss: 0.3659, Accuracy: 96.96%, Grad Norm: 0.54205\n",
      "Epoch [253/10044], Batch [4/7], Loss: 0.3339, Accuracy: 97.45%, Grad Norm: 0.41753\n",
      "Epoch [253/10044], Batch [5/7], Loss: 0.3110, Accuracy: 96.78%, Grad Norm: 0.45553\n",
      "Epoch [253/10044], Batch [6/7], Loss: 0.3223, Accuracy: 96.04%, Grad Norm: 0.47971\n",
      "Epoch [253/10044], Batch [7/7], Loss: 0.2373, Accuracy: 97.33%, Grad Norm: 0.44422\n",
      "Epoch [253/10044], Loss: 0.2373\n",
      "Epoch [254/10044], Batch [1/7], Loss: 0.3403, Accuracy: 95.96%, Grad Norm: 0.51009\n",
      "Epoch [254/10044], Batch [2/7], Loss: 0.3399, Accuracy: 95.77%, Grad Norm: 0.51817\n",
      "Epoch [254/10044], Batch [3/7], Loss: 0.3595, Accuracy: 96.87%, Grad Norm: 0.50317\n",
      "Epoch [254/10044], Batch [4/7], Loss: 0.3340, Accuracy: 97.32%, Grad Norm: 0.47365\n",
      "Epoch [254/10044], Batch [5/7], Loss: 0.3030, Accuracy: 96.86%, Grad Norm: 0.43925\n",
      "Epoch [254/10044], Batch [6/7], Loss: 0.3139, Accuracy: 96.43%, Grad Norm: 0.43754\n",
      "Epoch [254/10044], Batch [7/7], Loss: 0.2303, Accuracy: 97.48%, Grad Norm: 0.41707\n",
      "Epoch [254/10044], Loss: 0.2303\n",
      "Epoch [255/10044], Batch [1/7], Loss: 0.3408, Accuracy: 96.14%, Grad Norm: 0.56992\n",
      "Epoch [255/10044], Batch [2/7], Loss: 0.3367, Accuracy: 95.79%, Grad Norm: 0.56505\n",
      "Epoch [255/10044], Batch [3/7], Loss: 0.3551, Accuracy: 96.93%, Grad Norm: 0.55268\n",
      "Epoch [255/10044], Batch [4/7], Loss: 0.3266, Accuracy: 97.39%, Grad Norm: 0.44878\n",
      "Epoch [255/10044], Batch [5/7], Loss: 0.3059, Accuracy: 96.45%, Grad Norm: 0.46222\n",
      "Epoch [255/10044], Batch [6/7], Loss: 0.3174, Accuracy: 96.44%, Grad Norm: 0.52733\n",
      "Epoch [255/10044], Batch [7/7], Loss: 0.2227, Accuracy: 97.70%, Grad Norm: 0.44008\n",
      "Epoch [255/10044], Loss: 0.2227\n",
      "Epoch [256/10044], Batch [1/7], Loss: 0.3340, Accuracy: 96.20%, Grad Norm: 0.48360\n",
      "Epoch [256/10044], Batch [2/7], Loss: 0.3264, Accuracy: 96.31%, Grad Norm: 0.52546\n",
      "Epoch [256/10044], Batch [3/7], Loss: 0.3464, Accuracy: 97.37%, Grad Norm: 0.50406\n",
      "Epoch [256/10044], Batch [4/7], Loss: 0.3295, Accuracy: 97.11%, Grad Norm: 0.49453\n",
      "Epoch [256/10044], Batch [5/7], Loss: 0.2995, Accuracy: 96.92%, Grad Norm: 0.49147\n",
      "Epoch [256/10044], Batch [6/7], Loss: 0.3061, Accuracy: 96.22%, Grad Norm: 0.42882\n",
      "Epoch [256/10044], Batch [7/7], Loss: 0.2330, Accuracy: 97.15%, Grad Norm: 0.43520\n",
      "Epoch [256/10044], Loss: 0.2330\n",
      "Epoch [257/10044], Batch [1/7], Loss: 0.3267, Accuracy: 95.82%, Grad Norm: 0.50299\n",
      "Epoch [257/10044], Batch [2/7], Loss: 0.3262, Accuracy: 96.03%, Grad Norm: 0.54475\n",
      "Epoch [257/10044], Batch [3/7], Loss: 0.3428, Accuracy: 97.27%, Grad Norm: 0.52345\n",
      "Epoch [257/10044], Batch [4/7], Loss: 0.3156, Accuracy: 97.74%, Grad Norm: 0.42880\n",
      "Epoch [257/10044], Batch [5/7], Loss: 0.2983, Accuracy: 96.87%, Grad Norm: 0.45403\n",
      "Epoch [257/10044], Batch [6/7], Loss: 0.3030, Accuracy: 96.49%, Grad Norm: 0.47794\n",
      "Epoch [257/10044], Batch [7/7], Loss: 0.2315, Accuracy: 97.42%, Grad Norm: 0.55499\n",
      "Epoch [257/10044], Loss: 0.2315\n",
      "Epoch [258/10044], Batch [1/7], Loss: 0.3221, Accuracy: 96.36%, Grad Norm: 0.49827\n",
      "Epoch [258/10044], Batch [2/7], Loss: 0.3181, Accuracy: 95.99%, Grad Norm: 0.52232\n",
      "Epoch [258/10044], Batch [3/7], Loss: 0.3392, Accuracy: 96.88%, Grad Norm: 0.52321\n",
      "Epoch [258/10044], Batch [4/7], Loss: 0.3123, Accuracy: 97.37%, Grad Norm: 0.41114\n",
      "Epoch [258/10044], Batch [5/7], Loss: 0.2917, Accuracy: 96.67%, Grad Norm: 0.43232\n",
      "Epoch [258/10044], Batch [6/7], Loss: 0.2961, Accuracy: 96.49%, Grad Norm: 0.44784\n",
      "Epoch [258/10044], Batch [7/7], Loss: 0.2210, Accuracy: 97.58%, Grad Norm: 0.43098\n",
      "Epoch [258/10044], Loss: 0.2210\n",
      "Epoch [259/10044], Batch [1/7], Loss: 0.3269, Accuracy: 96.55%, Grad Norm: 0.65767\n",
      "Epoch [259/10044], Batch [2/7], Loss: 0.3168, Accuracy: 96.37%, Grad Norm: 0.49200\n",
      "Epoch [259/10044], Batch [3/7], Loss: 0.3308, Accuracy: 97.47%, Grad Norm: 0.46908\n",
      "Epoch [259/10044], Batch [4/7], Loss: 0.3113, Accuracy: 97.32%, Grad Norm: 0.48708\n",
      "Epoch [259/10044], Batch [5/7], Loss: 0.2911, Accuracy: 96.74%, Grad Norm: 0.49704\n",
      "Epoch [259/10044], Batch [6/7], Loss: 0.2960, Accuracy: 96.49%, Grad Norm: 0.46195\n",
      "Epoch [259/10044], Batch [7/7], Loss: 0.2190, Accuracy: 97.62%, Grad Norm: 0.43510\n",
      "Epoch [259/10044], Loss: 0.2190\n",
      "Epoch [260/10044], Batch [1/7], Loss: 0.3161, Accuracy: 96.22%, Grad Norm: 0.44008\n",
      "Epoch [260/10044], Batch [2/7], Loss: 0.3140, Accuracy: 96.45%, Grad Norm: 0.50635\n",
      "Epoch [260/10044], Batch [3/7], Loss: 0.3373, Accuracy: 97.19%, Grad Norm: 0.56155\n",
      "Epoch [260/10044], Batch [4/7], Loss: 0.3029, Accuracy: 97.74%, Grad Norm: 0.42462\n",
      "Epoch [260/10044], Batch [5/7], Loss: 0.2790, Accuracy: 97.03%, Grad Norm: 0.39700\n",
      "Epoch [260/10044], Batch [6/7], Loss: 0.2959, Accuracy: 96.45%, Grad Norm: 0.40739\n",
      "Epoch [260/10044], Batch [7/7], Loss: 0.2180, Accuracy: 97.50%, Grad Norm: 0.41257\n",
      "Epoch [260/10044], Loss: 0.2180\n",
      "Epoch [261/10044], Batch [1/7], Loss: 0.3136, Accuracy: 95.97%, Grad Norm: 0.59438\n",
      "Epoch [261/10044], Batch [2/7], Loss: 0.3091, Accuracy: 96.10%, Grad Norm: 0.51434\n",
      "Epoch [261/10044], Batch [3/7], Loss: 0.3262, Accuracy: 97.18%, Grad Norm: 0.47294\n",
      "Epoch [261/10044], Batch [4/7], Loss: 0.3007, Accuracy: 97.54%, Grad Norm: 0.46990\n",
      "Epoch [261/10044], Batch [5/7], Loss: 0.2753, Accuracy: 97.22%, Grad Norm: 0.44473\n",
      "Epoch [261/10044], Batch [6/7], Loss: 0.2893, Accuracy: 96.77%, Grad Norm: 0.47650\n",
      "Epoch [261/10044], Batch [7/7], Loss: 0.2114, Accuracy: 97.55%, Grad Norm: 0.43723\n",
      "Epoch [261/10044], Loss: 0.2114\n",
      "Epoch [262/10044], Batch [1/7], Loss: 0.3085, Accuracy: 96.57%, Grad Norm: 0.47494\n",
      "Epoch [262/10044], Batch [2/7], Loss: 0.3063, Accuracy: 96.19%, Grad Norm: 0.50671\n",
      "Epoch [262/10044], Batch [3/7], Loss: 0.3218, Accuracy: 97.27%, Grad Norm: 0.55552\n",
      "Epoch [262/10044], Batch [4/7], Loss: 0.3015, Accuracy: 97.62%, Grad Norm: 0.47732\n",
      "Epoch [262/10044], Batch [5/7], Loss: 0.2774, Accuracy: 97.05%, Grad Norm: 0.38410\n",
      "Epoch [262/10044], Batch [6/7], Loss: 0.2836, Accuracy: 96.60%, Grad Norm: 0.45470\n",
      "Epoch [262/10044], Batch [7/7], Loss: 0.2068, Accuracy: 97.90%, Grad Norm: 0.40236\n",
      "Epoch [262/10044], Loss: 0.2068\n",
      "Epoch [263/10044], Batch [1/7], Loss: 0.3076, Accuracy: 96.68%, Grad Norm: 0.57966\n",
      "Epoch [263/10044], Batch [2/7], Loss: 0.3000, Accuracy: 96.34%, Grad Norm: 0.50922\n",
      "Epoch [263/10044], Batch [3/7], Loss: 0.3071, Accuracy: 97.68%, Grad Norm: 0.39672\n",
      "Epoch [263/10044], Batch [4/7], Loss: 0.2958, Accuracy: 97.54%, Grad Norm: 0.43824\n",
      "Epoch [263/10044], Batch [5/7], Loss: 0.2759, Accuracy: 97.06%, Grad Norm: 0.50690\n",
      "Epoch [263/10044], Batch [6/7], Loss: 0.2852, Accuracy: 96.65%, Grad Norm: 0.46429\n",
      "Epoch [263/10044], Batch [7/7], Loss: 0.2108, Accuracy: 97.53%, Grad Norm: 0.41262\n",
      "Epoch [263/10044], Loss: 0.2108\n",
      "Epoch [264/10044], Batch [1/7], Loss: 0.2998, Accuracy: 96.66%, Grad Norm: 0.53062\n",
      "Epoch [264/10044], Batch [2/7], Loss: 0.2939, Accuracy: 96.62%, Grad Norm: 0.46365\n",
      "Epoch [264/10044], Batch [3/7], Loss: 0.3122, Accuracy: 97.50%, Grad Norm: 0.51960\n",
      "Epoch [264/10044], Batch [4/7], Loss: 0.2904, Accuracy: 97.72%, Grad Norm: 0.50319\n",
      "Epoch [264/10044], Batch [5/7], Loss: 0.2676, Accuracy: 97.39%, Grad Norm: 0.39626\n",
      "Epoch [264/10044], Batch [6/7], Loss: 0.2801, Accuracy: 96.58%, Grad Norm: 0.41040\n",
      "Epoch [264/10044], Batch [7/7], Loss: 0.2046, Accuracy: 97.70%, Grad Norm: 0.39863\n",
      "Epoch [264/10044], Loss: 0.2046\n",
      "Epoch [265/10044], Batch [1/7], Loss: 0.2952, Accuracy: 96.42%, Grad Norm: 0.50692\n",
      "Epoch [265/10044], Batch [2/7], Loss: 0.2948, Accuracy: 96.20%, Grad Norm: 0.51261\n",
      "Epoch [265/10044], Batch [3/7], Loss: 0.3084, Accuracy: 97.34%, Grad Norm: 0.51089\n",
      "Epoch [265/10044], Batch [4/7], Loss: 0.2858, Accuracy: 97.90%, Grad Norm: 0.42649\n",
      "Epoch [265/10044], Batch [5/7], Loss: 0.2676, Accuracy: 97.20%, Grad Norm: 0.38654\n",
      "Epoch [265/10044], Batch [6/7], Loss: 0.2754, Accuracy: 96.86%, Grad Norm: 0.49195\n",
      "Epoch [265/10044], Batch [7/7], Loss: 0.2059, Accuracy: 97.50%, Grad Norm: 0.47872\n",
      "Epoch [265/10044], Loss: 0.2059\n",
      "Epoch [266/10044], Batch [1/7], Loss: 0.2897, Accuracy: 96.87%, Grad Norm: 0.55138\n",
      "Epoch [266/10044], Batch [2/7], Loss: 0.2860, Accuracy: 96.57%, Grad Norm: 0.43144\n",
      "Epoch [266/10044], Batch [3/7], Loss: 0.2972, Accuracy: 97.60%, Grad Norm: 0.49588\n",
      "Epoch [266/10044], Batch [4/7], Loss: 0.2820, Accuracy: 97.70%, Grad Norm: 0.48766\n",
      "Epoch [266/10044], Batch [5/7], Loss: 0.2634, Accuracy: 97.12%, Grad Norm: 0.43392\n",
      "Epoch [266/10044], Batch [6/7], Loss: 0.2738, Accuracy: 96.49%, Grad Norm: 0.40936\n",
      "Epoch [266/10044], Batch [7/7], Loss: 0.2010, Accuracy: 97.53%, Grad Norm: 0.37371\n",
      "Epoch [266/10044], Loss: 0.2010\n",
      "Epoch [267/10044], Batch [1/7], Loss: 0.2918, Accuracy: 96.87%, Grad Norm: 0.52939\n",
      "Epoch [267/10044], Batch [2/7], Loss: 0.2831, Accuracy: 96.83%, Grad Norm: 0.46432\n",
      "Epoch [267/10044], Batch [3/7], Loss: 0.2980, Accuracy: 97.58%, Grad Norm: 0.48379\n",
      "Epoch [267/10044], Batch [4/7], Loss: 0.2804, Accuracy: 97.86%, Grad Norm: 0.44113\n",
      "Epoch [267/10044], Batch [5/7], Loss: 0.2574, Accuracy: 97.50%, Grad Norm: 0.37270\n",
      "Epoch [267/10044], Batch [6/7], Loss: 0.2716, Accuracy: 96.74%, Grad Norm: 0.41859\n",
      "Epoch [267/10044], Batch [7/7], Loss: 0.1976, Accuracy: 97.92%, Grad Norm: 0.40309\n",
      "Epoch [267/10044], Loss: 0.1976\n",
      "Epoch [268/10044], Batch [1/7], Loss: 0.2813, Accuracy: 96.60%, Grad Norm: 0.45008\n",
      "Epoch [268/10044], Batch [2/7], Loss: 0.2769, Accuracy: 96.82%, Grad Norm: 0.42801\n",
      "Epoch [268/10044], Batch [3/7], Loss: 0.2914, Accuracy: 97.65%, Grad Norm: 0.46313\n",
      "Epoch [268/10044], Batch [4/7], Loss: 0.2724, Accuracy: 97.99%, Grad Norm: 0.38274\n",
      "Epoch [268/10044], Batch [5/7], Loss: 0.2547, Accuracy: 97.22%, Grad Norm: 0.35008\n",
      "Epoch [268/10044], Batch [6/7], Loss: 0.2588, Accuracy: 97.01%, Grad Norm: 0.36253\n",
      "Epoch [268/10044], Batch [7/7], Loss: 0.1964, Accuracy: 97.55%, Grad Norm: 0.39471\n",
      "Epoch [268/10044], Loss: 0.1964\n",
      "Epoch [269/10044], Batch [1/7], Loss: 0.2789, Accuracy: 97.12%, Grad Norm: 0.49189\n",
      "Epoch [269/10044], Batch [2/7], Loss: 0.2753, Accuracy: 96.82%, Grad Norm: 0.40380\n",
      "Epoch [269/10044], Batch [3/7], Loss: 0.2842, Accuracy: 97.58%, Grad Norm: 0.41797\n",
      "Epoch [269/10044], Batch [4/7], Loss: 0.2712, Accuracy: 97.68%, Grad Norm: 0.36299\n",
      "Epoch [269/10044], Batch [5/7], Loss: 0.2532, Accuracy: 97.22%, Grad Norm: 0.37802\n",
      "Epoch [269/10044], Batch [6/7], Loss: 0.2580, Accuracy: 97.04%, Grad Norm: 0.36818\n",
      "Epoch [269/10044], Batch [7/7], Loss: 0.1911, Accuracy: 97.82%, Grad Norm: 0.38003\n",
      "Epoch [269/10044], Loss: 0.1911\n",
      "Epoch [270/10044], Batch [1/7], Loss: 0.2732, Accuracy: 97.12%, Grad Norm: 0.43228\n",
      "Epoch [270/10044], Batch [2/7], Loss: 0.2691, Accuracy: 96.70%, Grad Norm: 0.38311\n",
      "Epoch [270/10044], Batch [3/7], Loss: 0.2811, Accuracy: 97.77%, Grad Norm: 0.39606\n",
      "Epoch [270/10044], Batch [4/7], Loss: 0.2666, Accuracy: 97.98%, Grad Norm: 0.39472\n",
      "Epoch [270/10044], Batch [5/7], Loss: 0.2463, Accuracy: 97.40%, Grad Norm: 0.35806\n",
      "Epoch [270/10044], Batch [6/7], Loss: 0.2524, Accuracy: 97.07%, Grad Norm: 0.35149\n",
      "Epoch [270/10044], Batch [7/7], Loss: 0.1879, Accuracy: 97.78%, Grad Norm: 0.35716\n",
      "Epoch [270/10044], Loss: 0.1879\n",
      "Epoch [271/10044], Batch [1/7], Loss: 0.2688, Accuracy: 97.09%, Grad Norm: 0.40315\n",
      "Epoch [271/10044], Batch [2/7], Loss: 0.2692, Accuracy: 96.61%, Grad Norm: 0.44838\n",
      "Epoch [271/10044], Batch [3/7], Loss: 0.2784, Accuracy: 97.84%, Grad Norm: 0.41850\n",
      "Epoch [271/10044], Batch [4/7], Loss: 0.2659, Accuracy: 97.72%, Grad Norm: 0.36757\n",
      "Epoch [271/10044], Batch [5/7], Loss: 0.2455, Accuracy: 97.42%, Grad Norm: 0.33541\n",
      "Epoch [271/10044], Batch [6/7], Loss: 0.2516, Accuracy: 97.01%, Grad Norm: 0.37794\n",
      "Epoch [271/10044], Batch [7/7], Loss: 0.1872, Accuracy: 98.00%, Grad Norm: 0.34468\n",
      "Epoch [271/10044], Loss: 0.1872\n",
      "Epoch [272/10044], Batch [1/7], Loss: 0.2659, Accuracy: 97.15%, Grad Norm: 0.47079\n",
      "Epoch [272/10044], Batch [2/7], Loss: 0.2638, Accuracy: 96.60%, Grad Norm: 0.42209\n",
      "Epoch [272/10044], Batch [3/7], Loss: 0.2742, Accuracy: 97.92%, Grad Norm: 0.38776\n",
      "Epoch [272/10044], Batch [4/7], Loss: 0.2571, Accuracy: 98.03%, Grad Norm: 0.38548\n",
      "Epoch [272/10044], Batch [5/7], Loss: 0.2424, Accuracy: 97.47%, Grad Norm: 0.38036\n",
      "Epoch [272/10044], Batch [6/7], Loss: 0.2468, Accuracy: 97.31%, Grad Norm: 0.36154\n",
      "Epoch [272/10044], Batch [7/7], Loss: 0.1872, Accuracy: 97.98%, Grad Norm: 0.36506\n",
      "Epoch [272/10044], Loss: 0.1872\n",
      "Epoch [273/10044], Batch [1/7], Loss: 0.2622, Accuracy: 97.22%, Grad Norm: 0.38970\n",
      "Epoch [273/10044], Batch [2/7], Loss: 0.2602, Accuracy: 96.86%, Grad Norm: 0.42957\n",
      "Epoch [273/10044], Batch [3/7], Loss: 0.2714, Accuracy: 97.67%, Grad Norm: 0.40160\n",
      "Epoch [273/10044], Batch [4/7], Loss: 0.2535, Accuracy: 97.98%, Grad Norm: 0.35882\n",
      "Epoch [273/10044], Batch [5/7], Loss: 0.2390, Accuracy: 97.33%, Grad Norm: 0.35632\n",
      "Epoch [273/10044], Batch [6/7], Loss: 0.2431, Accuracy: 97.17%, Grad Norm: 0.34056\n",
      "Epoch [273/10044], Batch [7/7], Loss: 0.1833, Accuracy: 97.92%, Grad Norm: 0.36367\n",
      "Epoch [273/10044], Loss: 0.1833\n",
      "Epoch [274/10044], Batch [1/7], Loss: 0.2593, Accuracy: 97.37%, Grad Norm: 0.42504\n",
      "Epoch [274/10044], Batch [2/7], Loss: 0.2596, Accuracy: 96.75%, Grad Norm: 0.44211\n",
      "Epoch [274/10044], Batch [3/7], Loss: 0.2631, Accuracy: 97.84%, Grad Norm: 0.40799\n",
      "Epoch [274/10044], Batch [4/7], Loss: 0.2525, Accuracy: 98.10%, Grad Norm: 0.34837\n",
      "Epoch [274/10044], Batch [5/7], Loss: 0.2360, Accuracy: 97.34%, Grad Norm: 0.35997\n",
      "Epoch [274/10044], Batch [6/7], Loss: 0.2467, Accuracy: 96.90%, Grad Norm: 0.39727\n",
      "Epoch [274/10044], Batch [7/7], Loss: 0.1804, Accuracy: 97.83%, Grad Norm: 0.39194\n",
      "Epoch [274/10044], Loss: 0.1804\n",
      "Epoch [275/10044], Batch [1/7], Loss: 0.2555, Accuracy: 97.40%, Grad Norm: 0.41006\n",
      "Epoch [275/10044], Batch [2/7], Loss: 0.2528, Accuracy: 96.85%, Grad Norm: 0.41917\n",
      "Epoch [275/10044], Batch [3/7], Loss: 0.2588, Accuracy: 98.03%, Grad Norm: 0.37474\n",
      "Epoch [275/10044], Batch [4/7], Loss: 0.2443, Accuracy: 98.16%, Grad Norm: 0.32975\n",
      "Epoch [275/10044], Batch [5/7], Loss: 0.2339, Accuracy: 97.47%, Grad Norm: 0.38593\n",
      "Epoch [275/10044], Batch [6/7], Loss: 0.2448, Accuracy: 96.96%, Grad Norm: 0.42669\n",
      "Epoch [275/10044], Batch [7/7], Loss: 0.1822, Accuracy: 98.12%, Grad Norm: 0.40752\n",
      "Epoch [275/10044], Loss: 0.1822\n",
      "Epoch [276/10044], Batch [1/7], Loss: 0.2506, Accuracy: 97.39%, Grad Norm: 0.41804\n",
      "Epoch [276/10044], Batch [2/7], Loss: 0.2515, Accuracy: 96.78%, Grad Norm: 0.38214\n",
      "Epoch [276/10044], Batch [3/7], Loss: 0.2588, Accuracy: 97.90%, Grad Norm: 0.39614\n",
      "Epoch [276/10044], Batch [4/7], Loss: 0.2487, Accuracy: 97.89%, Grad Norm: 0.38649\n",
      "Epoch [276/10044], Batch [5/7], Loss: 0.2312, Accuracy: 97.50%, Grad Norm: 0.34751\n",
      "Epoch [276/10044], Batch [6/7], Loss: 0.2374, Accuracy: 97.34%, Grad Norm: 0.35009\n",
      "Epoch [276/10044], Batch [7/7], Loss: 0.1785, Accuracy: 97.93%, Grad Norm: 0.34090\n",
      "Epoch [276/10044], Loss: 0.1785\n",
      "Epoch [277/10044], Batch [1/7], Loss: 0.2458, Accuracy: 97.50%, Grad Norm: 0.41752\n",
      "Epoch [277/10044], Batch [2/7], Loss: 0.2467, Accuracy: 96.97%, Grad Norm: 0.41625\n",
      "Epoch [277/10044], Batch [3/7], Loss: 0.2549, Accuracy: 97.89%, Grad Norm: 0.41715\n",
      "Epoch [277/10044], Batch [4/7], Loss: 0.2419, Accuracy: 97.97%, Grad Norm: 0.35004\n",
      "Epoch [277/10044], Batch [5/7], Loss: 0.2260, Accuracy: 97.52%, Grad Norm: 0.31493\n",
      "Epoch [277/10044], Batch [6/7], Loss: 0.2366, Accuracy: 97.07%, Grad Norm: 0.33945\n",
      "Epoch [277/10044], Batch [7/7], Loss: 0.1757, Accuracy: 98.15%, Grad Norm: 0.36618\n",
      "Epoch [277/10044], Loss: 0.1757\n",
      "Epoch [278/10044], Batch [1/7], Loss: 0.2478, Accuracy: 97.48%, Grad Norm: 0.44675\n",
      "Epoch [278/10044], Batch [2/7], Loss: 0.2460, Accuracy: 97.08%, Grad Norm: 0.39032\n",
      "Epoch [278/10044], Batch [3/7], Loss: 0.2502, Accuracy: 98.02%, Grad Norm: 0.34772\n",
      "Epoch [278/10044], Batch [4/7], Loss: 0.2368, Accuracy: 98.27%, Grad Norm: 0.33279\n",
      "Epoch [278/10044], Batch [5/7], Loss: 0.2309, Accuracy: 97.32%, Grad Norm: 0.35050\n",
      "Epoch [278/10044], Batch [6/7], Loss: 0.2276, Accuracy: 97.38%, Grad Norm: 0.33896\n",
      "Epoch [278/10044], Batch [7/7], Loss: 0.1748, Accuracy: 97.75%, Grad Norm: 0.34366\n",
      "Epoch [278/10044], Loss: 0.1748\n",
      "Epoch [279/10044], Batch [1/7], Loss: 0.2452, Accuracy: 97.31%, Grad Norm: 0.42165\n",
      "Epoch [279/10044], Batch [2/7], Loss: 0.2439, Accuracy: 97.06%, Grad Norm: 0.44014\n",
      "Epoch [279/10044], Batch [3/7], Loss: 0.2458, Accuracy: 98.07%, Grad Norm: 0.37262\n",
      "Epoch [279/10044], Batch [4/7], Loss: 0.2339, Accuracy: 98.23%, Grad Norm: 0.35160\n",
      "Epoch [279/10044], Batch [5/7], Loss: 0.2245, Accuracy: 97.59%, Grad Norm: 0.37744\n",
      "Epoch [279/10044], Batch [6/7], Loss: 0.2259, Accuracy: 97.39%, Grad Norm: 0.35708\n",
      "Epoch [279/10044], Batch [7/7], Loss: 0.1770, Accuracy: 97.75%, Grad Norm: 0.39552\n",
      "Epoch [279/10044], Loss: 0.1770\n",
      "Epoch [280/10044], Batch [1/7], Loss: 0.2428, Accuracy: 97.49%, Grad Norm: 0.39956\n",
      "Epoch [280/10044], Batch [2/7], Loss: 0.2380, Accuracy: 97.09%, Grad Norm: 0.38004\n",
      "Epoch [280/10044], Batch [3/7], Loss: 0.2416, Accuracy: 98.03%, Grad Norm: 0.39309\n",
      "Epoch [280/10044], Batch [4/7], Loss: 0.2327, Accuracy: 98.01%, Grad Norm: 0.38612\n",
      "Epoch [280/10044], Batch [5/7], Loss: 0.2199, Accuracy: 97.65%, Grad Norm: 0.38816\n",
      "Epoch [280/10044], Batch [6/7], Loss: 0.2266, Accuracy: 97.32%, Grad Norm: 0.35525\n",
      "Epoch [280/10044], Batch [7/7], Loss: 0.1675, Accuracy: 98.25%, Grad Norm: 0.38475\n",
      "Epoch [280/10044], Loss: 0.1675\n",
      "Epoch [281/10044], Batch [1/7], Loss: 0.2410, Accuracy: 97.62%, Grad Norm: 0.45008\n",
      "Epoch [281/10044], Batch [2/7], Loss: 0.2361, Accuracy: 97.15%, Grad Norm: 0.42925\n",
      "Epoch [281/10044], Batch [3/7], Loss: 0.2365, Accuracy: 98.29%, Grad Norm: 0.37135\n",
      "Epoch [281/10044], Batch [4/7], Loss: 0.2264, Accuracy: 98.31%, Grad Norm: 0.32207\n",
      "Epoch [281/10044], Batch [5/7], Loss: 0.2156, Accuracy: 97.74%, Grad Norm: 0.33018\n",
      "Epoch [281/10044], Batch [6/7], Loss: 0.2246, Accuracy: 97.27%, Grad Norm: 0.39293\n",
      "Epoch [281/10044], Batch [7/7], Loss: 0.1722, Accuracy: 97.87%, Grad Norm: 0.40847\n",
      "Epoch [281/10044], Loss: 0.1722\n",
      "Epoch [282/10044], Batch [1/7], Loss: 0.2346, Accuracy: 97.42%, Grad Norm: 0.43044\n",
      "Epoch [282/10044], Batch [2/7], Loss: 0.2342, Accuracy: 97.22%, Grad Norm: 0.38559\n",
      "Epoch [282/10044], Batch [3/7], Loss: 0.2362, Accuracy: 98.33%, Grad Norm: 0.38588\n",
      "Epoch [282/10044], Batch [4/7], Loss: 0.2294, Accuracy: 98.10%, Grad Norm: 0.40984\n",
      "Epoch [282/10044], Batch [5/7], Loss: 0.2167, Accuracy: 97.60%, Grad Norm: 0.38989\n",
      "Epoch [282/10044], Batch [6/7], Loss: 0.2183, Accuracy: 97.53%, Grad Norm: 0.35058\n",
      "Epoch [282/10044], Batch [7/7], Loss: 0.1659, Accuracy: 97.90%, Grad Norm: 0.35928\n",
      "Epoch [282/10044], Loss: 0.1659\n",
      "Epoch [283/10044], Batch [1/7], Loss: 0.2344, Accuracy: 97.65%, Grad Norm: 0.45129\n",
      "Epoch [283/10044], Batch [2/7], Loss: 0.2342, Accuracy: 97.03%, Grad Norm: 0.44983\n",
      "Epoch [283/10044], Batch [3/7], Loss: 0.2363, Accuracy: 98.09%, Grad Norm: 0.38201\n",
      "Epoch [283/10044], Batch [4/7], Loss: 0.2237, Accuracy: 98.26%, Grad Norm: 0.35391\n",
      "Epoch [283/10044], Batch [5/7], Loss: 0.2103, Accuracy: 97.80%, Grad Norm: 0.33741\n",
      "Epoch [283/10044], Batch [6/7], Loss: 0.2198, Accuracy: 97.27%, Grad Norm: 0.43694\n",
      "Epoch [283/10044], Batch [7/7], Loss: 0.1639, Accuracy: 98.13%, Grad Norm: 0.40831\n",
      "Epoch [283/10044], Loss: 0.1639\n",
      "Epoch [284/10044], Batch [1/7], Loss: 0.2302, Accuracy: 97.83%, Grad Norm: 0.44747\n",
      "Epoch [284/10044], Batch [2/7], Loss: 0.2264, Accuracy: 97.11%, Grad Norm: 0.38970\n",
      "Epoch [284/10044], Batch [3/7], Loss: 0.2323, Accuracy: 97.97%, Grad Norm: 0.41682\n",
      "Epoch [284/10044], Batch [4/7], Loss: 0.2266, Accuracy: 98.04%, Grad Norm: 0.39549\n",
      "Epoch [284/10044], Batch [5/7], Loss: 0.2118, Accuracy: 97.52%, Grad Norm: 0.40027\n",
      "Epoch [284/10044], Batch [6/7], Loss: 0.2183, Accuracy: 97.37%, Grad Norm: 0.33764\n",
      "Epoch [284/10044], Batch [7/7], Loss: 0.1579, Accuracy: 98.37%, Grad Norm: 0.32664\n",
      "Epoch [284/10044], Loss: 0.1579\n",
      "Epoch [285/10044], Batch [1/7], Loss: 0.2269, Accuracy: 97.84%, Grad Norm: 0.39929\n",
      "Epoch [285/10044], Batch [2/7], Loss: 0.2323, Accuracy: 97.08%, Grad Norm: 0.47052\n",
      "Epoch [285/10044], Batch [3/7], Loss: 0.2338, Accuracy: 97.98%, Grad Norm: 0.41065\n",
      "Epoch [285/10044], Batch [4/7], Loss: 0.2180, Accuracy: 98.37%, Grad Norm: 0.33161\n",
      "Epoch [285/10044], Batch [5/7], Loss: 0.2115, Accuracy: 97.47%, Grad Norm: 0.33538\n",
      "Epoch [285/10044], Batch [6/7], Loss: 0.2207, Accuracy: 97.24%, Grad Norm: 0.42929\n",
      "Epoch [285/10044], Batch [7/7], Loss: 0.1632, Accuracy: 97.98%, Grad Norm: 0.40123\n",
      "Epoch [285/10044], Loss: 0.1632\n",
      "Epoch [286/10044], Batch [1/7], Loss: 0.2243, Accuracy: 97.57%, Grad Norm: 0.40375\n",
      "Epoch [286/10044], Batch [2/7], Loss: 0.2294, Accuracy: 97.06%, Grad Norm: 0.39217\n",
      "Epoch [286/10044], Batch [3/7], Loss: 0.2270, Accuracy: 98.13%, Grad Norm: 0.38035\n",
      "Epoch [286/10044], Batch [4/7], Loss: 0.2177, Accuracy: 98.37%, Grad Norm: 0.40590\n",
      "Epoch [286/10044], Batch [5/7], Loss: 0.2080, Accuracy: 97.82%, Grad Norm: 0.38355\n",
      "Epoch [286/10044], Batch [6/7], Loss: 0.2138, Accuracy: 97.32%, Grad Norm: 0.37874\n",
      "Epoch [286/10044], Batch [7/7], Loss: 0.1538, Accuracy: 98.32%, Grad Norm: 0.34339\n",
      "Epoch [286/10044], Loss: 0.1538\n",
      "Epoch [287/10044], Batch [1/7], Loss: 0.2215, Accuracy: 97.64%, Grad Norm: 0.42004\n",
      "Epoch [287/10044], Batch [2/7], Loss: 0.2237, Accuracy: 97.05%, Grad Norm: 0.45391\n",
      "Epoch [287/10044], Batch [3/7], Loss: 0.2260, Accuracy: 98.17%, Grad Norm: 0.42114\n",
      "Epoch [287/10044], Batch [4/7], Loss: 0.2121, Accuracy: 98.32%, Grad Norm: 0.34929\n",
      "Epoch [287/10044], Batch [5/7], Loss: 0.1980, Accuracy: 98.05%, Grad Norm: 0.31498\n",
      "Epoch [287/10044], Batch [6/7], Loss: 0.2125, Accuracy: 97.36%, Grad Norm: 0.41296\n",
      "Epoch [287/10044], Batch [7/7], Loss: 0.1600, Accuracy: 98.12%, Grad Norm: 0.42241\n",
      "Epoch [287/10044], Loss: 0.1600\n",
      "Epoch [288/10044], Batch [1/7], Loss: 0.2256, Accuracy: 97.66%, Grad Norm: 0.52862\n",
      "Epoch [288/10044], Batch [2/7], Loss: 0.2181, Accuracy: 97.38%, Grad Norm: 0.35861\n",
      "Epoch [288/10044], Batch [3/7], Loss: 0.2212, Accuracy: 98.20%, Grad Norm: 0.38915\n",
      "Epoch [288/10044], Batch [4/7], Loss: 0.2158, Accuracy: 98.18%, Grad Norm: 0.40598\n",
      "Epoch [288/10044], Batch [5/7], Loss: 0.2062, Accuracy: 97.49%, Grad Norm: 0.41706\n",
      "Epoch [288/10044], Batch [6/7], Loss: 0.2074, Accuracy: 97.37%, Grad Norm: 0.37341\n",
      "Epoch [288/10044], Batch [7/7], Loss: 0.1554, Accuracy: 98.17%, Grad Norm: 0.36361\n",
      "Epoch [288/10044], Loss: 0.1554\n",
      "Epoch [289/10044], Batch [1/7], Loss: 0.2117, Accuracy: 98.07%, Grad Norm: 0.34857\n",
      "Epoch [289/10044], Batch [2/7], Loss: 0.2198, Accuracy: 97.42%, Grad Norm: 0.51351\n",
      "Epoch [289/10044], Batch [3/7], Loss: 0.2233, Accuracy: 98.33%, Grad Norm: 0.48184\n",
      "Epoch [289/10044], Batch [4/7], Loss: 0.2101, Accuracy: 98.37%, Grad Norm: 0.37095\n",
      "Epoch [289/10044], Batch [5/7], Loss: 0.1970, Accuracy: 97.94%, Grad Norm: 0.30400\n",
      "Epoch [289/10044], Batch [6/7], Loss: 0.2075, Accuracy: 97.17%, Grad Norm: 0.41279\n",
      "Epoch [289/10044], Batch [7/7], Loss: 0.1577, Accuracy: 98.07%, Grad Norm: 0.42880\n",
      "Epoch [289/10044], Loss: 0.1577\n",
      "Epoch [290/10044], Batch [1/7], Loss: 0.2196, Accuracy: 97.47%, Grad Norm: 0.45350\n",
      "Epoch [290/10044], Batch [2/7], Loss: 0.2151, Accuracy: 97.32%, Grad Norm: 0.41790\n",
      "Epoch [290/10044], Batch [3/7], Loss: 0.2158, Accuracy: 98.36%, Grad Norm: 0.35326\n",
      "Epoch [290/10044], Batch [4/7], Loss: 0.2078, Accuracy: 98.37%, Grad Norm: 0.35936\n",
      "Epoch [290/10044], Batch [5/7], Loss: 0.2006, Accuracy: 97.81%, Grad Norm: 0.44497\n",
      "Epoch [290/10044], Batch [6/7], Loss: 0.2060, Accuracy: 97.53%, Grad Norm: 0.47236\n",
      "Epoch [290/10044], Batch [7/7], Loss: 0.1555, Accuracy: 98.07%, Grad Norm: 0.35269\n",
      "Epoch [290/10044], Loss: 0.1555\n",
      "Epoch [291/10044], Batch [1/7], Loss: 0.2116, Accuracy: 97.94%, Grad Norm: 0.36265\n",
      "Epoch [291/10044], Batch [2/7], Loss: 0.2144, Accuracy: 97.17%, Grad Norm: 0.45112\n",
      "Epoch [291/10044], Batch [3/7], Loss: 0.2207, Accuracy: 98.07%, Grad Norm: 0.47899\n",
      "Epoch [291/10044], Batch [4/7], Loss: 0.2076, Accuracy: 98.22%, Grad Norm: 0.41846\n",
      "Epoch [291/10044], Batch [5/7], Loss: 0.1929, Accuracy: 97.80%, Grad Norm: 0.32977\n",
      "Epoch [291/10044], Batch [6/7], Loss: 0.1995, Accuracy: 97.59%, Grad Norm: 0.34166\n",
      "Epoch [291/10044], Batch [7/7], Loss: 0.1553, Accuracy: 98.13%, Grad Norm: 0.37522\n",
      "Epoch [291/10044], Loss: 0.1553\n",
      "Epoch [292/10044], Batch [1/7], Loss: 0.2154, Accuracy: 97.83%, Grad Norm: 0.49607\n",
      "Epoch [292/10044], Batch [2/7], Loss: 0.2147, Accuracy: 97.37%, Grad Norm: 0.47236\n",
      "Epoch [292/10044], Batch [3/7], Loss: 0.2069, Accuracy: 98.57%, Grad Norm: 0.32911\n",
      "Epoch [292/10044], Batch [4/7], Loss: 0.1975, Accuracy: 98.47%, Grad Norm: 0.31720\n",
      "Epoch [292/10044], Batch [5/7], Loss: 0.1927, Accuracy: 97.82%, Grad Norm: 0.38805\n",
      "Epoch [292/10044], Batch [6/7], Loss: 0.2036, Accuracy: 97.29%, Grad Norm: 0.45185\n",
      "Epoch [292/10044], Batch [7/7], Loss: 0.1554, Accuracy: 97.98%, Grad Norm: 0.44835\n",
      "Epoch [292/10044], Loss: 0.1554\n",
      "Epoch [293/10044], Batch [1/7], Loss: 0.2078, Accuracy: 97.77%, Grad Norm: 0.36864\n",
      "Epoch [293/10044], Batch [2/7], Loss: 0.2059, Accuracy: 97.52%, Grad Norm: 0.38606\n",
      "Epoch [293/10044], Batch [3/7], Loss: 0.2072, Accuracy: 98.57%, Grad Norm: 0.43067\n",
      "Epoch [293/10044], Batch [4/7], Loss: 0.2008, Accuracy: 98.47%, Grad Norm: 0.38225\n",
      "Epoch [293/10044], Batch [5/7], Loss: 0.1951, Accuracy: 97.79%, Grad Norm: 0.37623\n",
      "Epoch [293/10044], Batch [6/7], Loss: 0.1971, Accuracy: 97.54%, Grad Norm: 0.32048\n",
      "Epoch [293/10044], Batch [7/7], Loss: 0.1506, Accuracy: 98.22%, Grad Norm: 0.32163\n",
      "Epoch [293/10044], Loss: 0.1506\n",
      "Epoch [294/10044], Batch [1/7], Loss: 0.2088, Accuracy: 97.83%, Grad Norm: 0.46011\n",
      "Epoch [294/10044], Batch [2/7], Loss: 0.2135, Accuracy: 96.88%, Grad Norm: 0.51469\n",
      "Epoch [294/10044], Batch [3/7], Loss: 0.2052, Accuracy: 98.32%, Grad Norm: 0.37770\n",
      "Epoch [294/10044], Batch [4/7], Loss: 0.1953, Accuracy: 98.37%, Grad Norm: 0.31060\n",
      "Epoch [294/10044], Batch [5/7], Loss: 0.1917, Accuracy: 97.72%, Grad Norm: 0.34620\n",
      "Epoch [294/10044], Batch [6/7], Loss: 0.1980, Accuracy: 97.70%, Grad Norm: 0.42414\n",
      "Epoch [294/10044], Batch [7/7], Loss: 0.1541, Accuracy: 98.17%, Grad Norm: 0.47940\n",
      "Epoch [294/10044], Loss: 0.1541\n",
      "Epoch [295/10044], Batch [1/7], Loss: 0.2085, Accuracy: 97.98%, Grad Norm: 0.50276\n",
      "Epoch [295/10044], Batch [2/7], Loss: 0.2037, Accuracy: 97.47%, Grad Norm: 0.33410\n",
      "Epoch [295/10044], Batch [3/7], Loss: 0.2029, Accuracy: 98.17%, Grad Norm: 0.37136\n",
      "Epoch [295/10044], Batch [4/7], Loss: 0.1959, Accuracy: 98.34%, Grad Norm: 0.41855\n",
      "Epoch [295/10044], Batch [5/7], Loss: 0.1930, Accuracy: 97.62%, Grad Norm: 0.42599\n",
      "Epoch [295/10044], Batch [6/7], Loss: 0.1985, Accuracy: 97.23%, Grad Norm: 0.40983\n",
      "Epoch [295/10044], Batch [7/7], Loss: 0.1434, Accuracy: 98.28%, Grad Norm: 0.35440\n",
      "Epoch [295/10044], Loss: 0.1434\n",
      "Epoch [296/10044], Batch [1/7], Loss: 0.2065, Accuracy: 97.78%, Grad Norm: 0.41802\n",
      "Epoch [296/10044], Batch [2/7], Loss: 0.2000, Accuracy: 97.57%, Grad Norm: 0.42384\n",
      "Epoch [296/10044], Batch [3/7], Loss: 0.2060, Accuracy: 98.37%, Grad Norm: 0.43672\n",
      "Epoch [296/10044], Batch [4/7], Loss: 0.1966, Accuracy: 98.42%, Grad Norm: 0.37535\n",
      "Epoch [296/10044], Batch [5/7], Loss: 0.1847, Accuracy: 97.96%, Grad Norm: 0.31271\n",
      "Epoch [296/10044], Batch [6/7], Loss: 0.1898, Accuracy: 97.71%, Grad Norm: 0.29973\n",
      "Epoch [296/10044], Batch [7/7], Loss: 0.1440, Accuracy: 98.22%, Grad Norm: 0.34155\n",
      "Epoch [296/10044], Loss: 0.1440\n",
      "Epoch [297/10044], Batch [1/7], Loss: 0.2036, Accuracy: 97.87%, Grad Norm: 0.48296\n",
      "Epoch [297/10044], Batch [2/7], Loss: 0.2044, Accuracy: 97.21%, Grad Norm: 0.43100\n",
      "Epoch [297/10044], Batch [3/7], Loss: 0.1927, Accuracy: 98.42%, Grad Norm: 0.34798\n",
      "Epoch [297/10044], Batch [4/7], Loss: 0.1905, Accuracy: 98.49%, Grad Norm: 0.30785\n",
      "Epoch [297/10044], Batch [5/7], Loss: 0.1857, Accuracy: 97.92%, Grad Norm: 0.34953\n",
      "Epoch [297/10044], Batch [6/7], Loss: 0.1930, Accuracy: 97.62%, Grad Norm: 0.40571\n",
      "Epoch [297/10044], Batch [7/7], Loss: 0.1443, Accuracy: 98.27%, Grad Norm: 0.40047\n",
      "Epoch [297/10044], Loss: 0.1443\n",
      "Epoch [298/10044], Batch [1/7], Loss: 0.1965, Accuracy: 98.12%, Grad Norm: 0.42579\n",
      "Epoch [298/10044], Batch [2/7], Loss: 0.1930, Accuracy: 97.57%, Grad Norm: 0.32355\n",
      "Epoch [298/10044], Batch [3/7], Loss: 0.1941, Accuracy: 98.54%, Grad Norm: 0.35968\n",
      "Epoch [298/10044], Batch [4/7], Loss: 0.1892, Accuracy: 98.34%, Grad Norm: 0.36429\n",
      "Epoch [298/10044], Batch [5/7], Loss: 0.1846, Accuracy: 97.87%, Grad Norm: 0.33714\n",
      "Epoch [298/10044], Batch [6/7], Loss: 0.1885, Accuracy: 97.57%, Grad Norm: 0.33794\n",
      "Epoch [298/10044], Batch [7/7], Loss: 0.1435, Accuracy: 98.27%, Grad Norm: 0.33739\n",
      "Epoch [298/10044], Loss: 0.1435\n",
      "Epoch [299/10044], Batch [1/7], Loss: 0.1908, Accuracy: 98.19%, Grad Norm: 0.39549\n",
      "Epoch [299/10044], Batch [2/7], Loss: 0.1906, Accuracy: 97.82%, Grad Norm: 0.42161\n",
      "Epoch [299/10044], Batch [3/7], Loss: 0.1947, Accuracy: 98.35%, Grad Norm: 0.35563\n",
      "Epoch [299/10044], Batch [4/7], Loss: 0.1844, Accuracy: 98.49%, Grad Norm: 0.30533\n",
      "Epoch [299/10044], Batch [5/7], Loss: 0.1758, Accuracy: 98.15%, Grad Norm: 0.27912\n",
      "Epoch [299/10044], Batch [6/7], Loss: 0.1816, Accuracy: 97.77%, Grad Norm: 0.34796\n",
      "Epoch [299/10044], Batch [7/7], Loss: 0.1402, Accuracy: 98.20%, Grad Norm: 0.32166\n",
      "Epoch [299/10044], Loss: 0.1402\n",
      "Epoch [300/10044], Batch [1/7], Loss: 0.1887, Accuracy: 98.24%, Grad Norm: 0.34780\n",
      "Epoch [300/10044], Batch [2/7], Loss: 0.1919, Accuracy: 97.41%, Grad Norm: 0.33098\n",
      "Epoch [300/10044], Batch [3/7], Loss: 0.1855, Accuracy: 98.52%, Grad Norm: 0.29767\n",
      "Epoch [300/10044], Batch [4/7], Loss: 0.1810, Accuracy: 98.71%, Grad Norm: 0.30091\n",
      "Epoch [300/10044], Batch [5/7], Loss: 0.1805, Accuracy: 98.02%, Grad Norm: 0.37186\n",
      "Epoch [300/10044], Batch [6/7], Loss: 0.1833, Accuracy: 97.72%, Grad Norm: 0.36592\n",
      "Epoch [300/10044], Batch [7/7], Loss: 0.1390, Accuracy: 98.22%, Grad Norm: 0.33802\n",
      "Epoch [300/10044], Loss: 0.1390\n",
      "Epoch [301/10044], Batch [1/7], Loss: 0.1851, Accuracy: 98.10%, Grad Norm: 0.35065\n",
      "Epoch [301/10044], Batch [2/7], Loss: 0.1938, Accuracy: 97.44%, Grad Norm: 0.40239\n",
      "Epoch [301/10044], Batch [3/7], Loss: 0.1903, Accuracy: 98.31%, Grad Norm: 0.36895\n",
      "Epoch [301/10044], Batch [4/7], Loss: 0.1862, Accuracy: 98.46%, Grad Norm: 0.34523\n",
      "Epoch [301/10044], Batch [5/7], Loss: 0.1745, Accuracy: 97.96%, Grad Norm: 0.29818\n",
      "Epoch [301/10044], Batch [6/7], Loss: 0.1854, Accuracy: 97.65%, Grad Norm: 0.31314\n",
      "Epoch [301/10044], Batch [7/7], Loss: 0.1346, Accuracy: 98.47%, Grad Norm: 0.33961\n",
      "Epoch [301/10044], Loss: 0.1346\n",
      "Epoch [302/10044], Batch [1/7], Loss: 0.1871, Accuracy: 98.22%, Grad Norm: 0.41745\n",
      "Epoch [302/10044], Batch [2/7], Loss: 0.1867, Accuracy: 97.62%, Grad Norm: 0.40265\n",
      "Epoch [302/10044], Batch [3/7], Loss: 0.1893, Accuracy: 98.48%, Grad Norm: 0.32672\n",
      "Epoch [302/10044], Batch [4/7], Loss: 0.1787, Accuracy: 98.59%, Grad Norm: 0.28928\n",
      "Epoch [302/10044], Batch [5/7], Loss: 0.1719, Accuracy: 98.10%, Grad Norm: 0.31306\n",
      "Epoch [302/10044], Batch [6/7], Loss: 0.1780, Accuracy: 97.97%, Grad Norm: 0.33420\n",
      "Epoch [302/10044], Batch [7/7], Loss: 0.1344, Accuracy: 98.33%, Grad Norm: 0.36216\n",
      "Epoch [302/10044], Loss: 0.1344\n",
      "Epoch [303/10044], Batch [1/7], Loss: 0.1800, Accuracy: 98.16%, Grad Norm: 0.33046\n",
      "Epoch [303/10044], Batch [2/7], Loss: 0.1872, Accuracy: 97.50%, Grad Norm: 0.31204\n",
      "Epoch [303/10044], Batch [3/7], Loss: 0.1830, Accuracy: 98.44%, Grad Norm: 0.33997\n",
      "Epoch [303/10044], Batch [4/7], Loss: 0.1798, Accuracy: 98.53%, Grad Norm: 0.41208\n",
      "Epoch [303/10044], Batch [5/7], Loss: 0.1726, Accuracy: 98.03%, Grad Norm: 0.31034\n",
      "Epoch [303/10044], Batch [6/7], Loss: 0.1762, Accuracy: 97.88%, Grad Norm: 0.29036\n",
      "Epoch [303/10044], Batch [7/7], Loss: 0.1340, Accuracy: 98.30%, Grad Norm: 0.31477\n",
      "Epoch [303/10044], Loss: 0.1340\n",
      "Epoch [304/10044], Batch [1/7], Loss: 0.1829, Accuracy: 98.17%, Grad Norm: 0.35279\n",
      "Epoch [304/10044], Batch [2/7], Loss: 0.1860, Accuracy: 97.52%, Grad Norm: 0.39198\n",
      "Epoch [304/10044], Batch [3/7], Loss: 0.1791, Accuracy: 98.52%, Grad Norm: 0.29938\n",
      "Epoch [304/10044], Batch [4/7], Loss: 0.1721, Accuracy: 98.53%, Grad Norm: 0.26993\n",
      "Epoch [304/10044], Batch [5/7], Loss: 0.1655, Accuracy: 98.32%, Grad Norm: 0.27195\n",
      "Epoch [304/10044], Batch [6/7], Loss: 0.1766, Accuracy: 97.74%, Grad Norm: 0.33022\n",
      "Epoch [304/10044], Batch [7/7], Loss: 0.1340, Accuracy: 98.17%, Grad Norm: 0.32911\n",
      "Epoch [304/10044], Loss: 0.1340\n",
      "Epoch [305/10044], Batch [1/7], Loss: 0.1801, Accuracy: 98.29%, Grad Norm: 0.35378\n",
      "Epoch [305/10044], Batch [2/7], Loss: 0.1811, Accuracy: 97.62%, Grad Norm: 0.29796\n",
      "Epoch [305/10044], Batch [3/7], Loss: 0.1742, Accuracy: 98.67%, Grad Norm: 0.28237\n",
      "Epoch [305/10044], Batch [4/7], Loss: 0.1755, Accuracy: 98.47%, Grad Norm: 0.34604\n",
      "Epoch [305/10044], Batch [5/7], Loss: 0.1697, Accuracy: 98.13%, Grad Norm: 0.31783\n",
      "Epoch [305/10044], Batch [6/7], Loss: 0.1681, Accuracy: 97.89%, Grad Norm: 0.29209\n",
      "Epoch [305/10044], Batch [7/7], Loss: 0.1309, Accuracy: 98.45%, Grad Norm: 0.29117\n",
      "Epoch [305/10044], Loss: 0.1309\n",
      "Epoch [306/10044], Batch [1/7], Loss: 0.1778, Accuracy: 98.27%, Grad Norm: 0.31128\n",
      "Epoch [306/10044], Batch [2/7], Loss: 0.1768, Accuracy: 97.67%, Grad Norm: 0.33107\n",
      "Epoch [306/10044], Batch [3/7], Loss: 0.1769, Accuracy: 98.70%, Grad Norm: 0.30695\n",
      "Epoch [306/10044], Batch [4/7], Loss: 0.1692, Accuracy: 98.57%, Grad Norm: 0.28706\n",
      "Epoch [306/10044], Batch [5/7], Loss: 0.1611, Accuracy: 98.23%, Grad Norm: 0.25627\n",
      "Epoch [306/10044], Batch [6/7], Loss: 0.1728, Accuracy: 97.78%, Grad Norm: 0.30168\n",
      "Epoch [306/10044], Batch [7/7], Loss: 0.1283, Accuracy: 98.63%, Grad Norm: 0.32180\n",
      "Epoch [306/10044], Loss: 0.1283\n",
      "Epoch [307/10044], Batch [1/7], Loss: 0.1731, Accuracy: 98.25%, Grad Norm: 0.34766\n",
      "Epoch [307/10044], Batch [2/7], Loss: 0.1718, Accuracy: 97.83%, Grad Norm: 0.29254\n",
      "Epoch [307/10044], Batch [3/7], Loss: 0.1758, Accuracy: 98.55%, Grad Norm: 0.26227\n",
      "Epoch [307/10044], Batch [4/7], Loss: 0.1733, Accuracy: 98.62%, Grad Norm: 0.28680\n",
      "Epoch [307/10044], Batch [5/7], Loss: 0.1663, Accuracy: 97.96%, Grad Norm: 0.29815\n",
      "Epoch [307/10044], Batch [6/7], Loss: 0.1687, Accuracy: 97.82%, Grad Norm: 0.27912\n",
      "Epoch [307/10044], Batch [7/7], Loss: 0.1253, Accuracy: 98.45%, Grad Norm: 0.35396\n",
      "Epoch [307/10044], Loss: 0.1253\n",
      "Epoch [308/10044], Batch [1/7], Loss: 0.1720, Accuracy: 98.29%, Grad Norm: 0.28801\n",
      "Epoch [308/10044], Batch [2/7], Loss: 0.1693, Accuracy: 97.97%, Grad Norm: 0.31881\n",
      "Epoch [308/10044], Batch [3/7], Loss: 0.1741, Accuracy: 98.47%, Grad Norm: 0.29995\n",
      "Epoch [308/10044], Batch [4/7], Loss: 0.1647, Accuracy: 98.71%, Grad Norm: 0.25580\n",
      "Epoch [308/10044], Batch [5/7], Loss: 0.1655, Accuracy: 98.15%, Grad Norm: 0.31644\n",
      "Epoch [308/10044], Batch [6/7], Loss: 0.1660, Accuracy: 97.79%, Grad Norm: 0.30318\n",
      "Epoch [308/10044], Batch [7/7], Loss: 0.1251, Accuracy: 98.38%, Grad Norm: 0.29541\n",
      "Epoch [308/10044], Loss: 0.1251\n",
      "Epoch [309/10044], Batch [1/7], Loss: 0.1678, Accuracy: 98.43%, Grad Norm: 0.32271\n",
      "Epoch [309/10044], Batch [2/7], Loss: 0.1726, Accuracy: 97.59%, Grad Norm: 0.31786\n",
      "Epoch [309/10044], Batch [3/7], Loss: 0.1683, Accuracy: 98.65%, Grad Norm: 0.29194\n",
      "Epoch [309/10044], Batch [4/7], Loss: 0.1671, Accuracy: 98.67%, Grad Norm: 0.28338\n",
      "Epoch [309/10044], Batch [5/7], Loss: 0.1590, Accuracy: 98.19%, Grad Norm: 0.27735\n",
      "Epoch [309/10044], Batch [6/7], Loss: 0.1670, Accuracy: 97.81%, Grad Norm: 0.27659\n",
      "Epoch [309/10044], Batch [7/7], Loss: 0.1197, Accuracy: 98.65%, Grad Norm: 0.29995\n",
      "Epoch [309/10044], Loss: 0.1197\n",
      "Epoch [310/10044], Batch [1/7], Loss: 0.1710, Accuracy: 98.41%, Grad Norm: 0.34824\n",
      "Epoch [310/10044], Batch [2/7], Loss: 0.1666, Accuracy: 97.65%, Grad Norm: 0.32356\n",
      "Epoch [310/10044], Batch [3/7], Loss: 0.1720, Accuracy: 98.55%, Grad Norm: 0.29704\n",
      "Epoch [310/10044], Batch [4/7], Loss: 0.1634, Accuracy: 98.81%, Grad Norm: 0.27549\n",
      "Epoch [310/10044], Batch [5/7], Loss: 0.1564, Accuracy: 98.38%, Grad Norm: 0.27945\n",
      "Epoch [310/10044], Batch [6/7], Loss: 0.1658, Accuracy: 97.88%, Grad Norm: 0.29536\n",
      "Epoch [310/10044], Batch [7/7], Loss: 0.1234, Accuracy: 98.55%, Grad Norm: 0.29159\n",
      "Epoch [310/10044], Loss: 0.1234\n",
      "Epoch [311/10044], Batch [1/7], Loss: 0.1678, Accuracy: 98.32%, Grad Norm: 0.31816\n",
      "Epoch [311/10044], Batch [2/7], Loss: 0.1724, Accuracy: 97.67%, Grad Norm: 0.31663\n",
      "Epoch [311/10044], Batch [3/7], Loss: 0.1643, Accuracy: 98.82%, Grad Norm: 0.27361\n",
      "Epoch [311/10044], Batch [4/7], Loss: 0.1638, Accuracy: 98.67%, Grad Norm: 0.26772\n",
      "Epoch [311/10044], Batch [5/7], Loss: 0.1593, Accuracy: 98.21%, Grad Norm: 0.28954\n",
      "Epoch [311/10044], Batch [6/7], Loss: 0.1617, Accuracy: 97.88%, Grad Norm: 0.27101\n",
      "Epoch [311/10044], Batch [7/7], Loss: 0.1232, Accuracy: 98.53%, Grad Norm: 0.30393\n",
      "Epoch [311/10044], Loss: 0.1232\n",
      "Epoch [312/10044], Batch [1/7], Loss: 0.1668, Accuracy: 98.32%, Grad Norm: 0.30644\n",
      "Epoch [312/10044], Batch [2/7], Loss: 0.1640, Accuracy: 97.87%, Grad Norm: 0.29076\n",
      "Epoch [312/10044], Batch [3/7], Loss: 0.1656, Accuracy: 98.67%, Grad Norm: 0.29681\n",
      "Epoch [312/10044], Batch [4/7], Loss: 0.1577, Accuracy: 98.71%, Grad Norm: 0.27525\n",
      "Epoch [312/10044], Batch [5/7], Loss: 0.1566, Accuracy: 98.14%, Grad Norm: 0.28827\n",
      "Epoch [312/10044], Batch [6/7], Loss: 0.1626, Accuracy: 97.88%, Grad Norm: 0.28062\n",
      "Epoch [312/10044], Batch [7/7], Loss: 0.1204, Accuracy: 98.42%, Grad Norm: 0.29303\n",
      "Epoch [312/10044], Loss: 0.1204\n",
      "Epoch [313/10044], Batch [1/7], Loss: 0.1636, Accuracy: 98.46%, Grad Norm: 0.32686\n",
      "Epoch [313/10044], Batch [2/7], Loss: 0.1674, Accuracy: 97.87%, Grad Norm: 0.33989\n",
      "Epoch [313/10044], Batch [3/7], Loss: 0.1632, Accuracy: 98.66%, Grad Norm: 0.29016\n",
      "Epoch [313/10044], Batch [4/7], Loss: 0.1550, Accuracy: 98.85%, Grad Norm: 0.25241\n",
      "Epoch [313/10044], Batch [5/7], Loss: 0.1551, Accuracy: 98.24%, Grad Norm: 0.26470\n",
      "Epoch [313/10044], Batch [6/7], Loss: 0.1601, Accuracy: 97.94%, Grad Norm: 0.27065\n",
      "Epoch [313/10044], Batch [7/7], Loss: 0.1194, Accuracy: 98.38%, Grad Norm: 0.30124\n",
      "Epoch [313/10044], Loss: 0.1194\n",
      "Epoch [314/10044], Batch [1/7], Loss: 0.1638, Accuracy: 98.46%, Grad Norm: 0.31411\n",
      "Epoch [314/10044], Batch [2/7], Loss: 0.1633, Accuracy: 97.82%, Grad Norm: 0.33487\n",
      "Epoch [314/10044], Batch [3/7], Loss: 0.1583, Accuracy: 98.68%, Grad Norm: 0.26673\n",
      "Epoch [314/10044], Batch [4/7], Loss: 0.1579, Accuracy: 98.62%, Grad Norm: 0.23888\n",
      "Epoch [314/10044], Batch [5/7], Loss: 0.1512, Accuracy: 98.38%, Grad Norm: 0.27794\n",
      "Epoch [314/10044], Batch [6/7], Loss: 0.1584, Accuracy: 97.87%, Grad Norm: 0.28107\n",
      "Epoch [314/10044], Batch [7/7], Loss: 0.1181, Accuracy: 98.45%, Grad Norm: 0.33705\n",
      "Epoch [314/10044], Loss: 0.1181\n",
      "Epoch [315/10044], Batch [1/7], Loss: 0.1621, Accuracy: 98.30%, Grad Norm: 0.32126\n",
      "Epoch [315/10044], Batch [2/7], Loss: 0.1583, Accuracy: 98.02%, Grad Norm: 0.27294\n",
      "Epoch [315/10044], Batch [3/7], Loss: 0.1559, Accuracy: 98.97%, Grad Norm: 0.28254\n",
      "Epoch [315/10044], Batch [4/7], Loss: 0.1566, Accuracy: 98.67%, Grad Norm: 0.26719\n",
      "Epoch [315/10044], Batch [5/7], Loss: 0.1514, Accuracy: 98.31%, Grad Norm: 0.31651\n",
      "Epoch [315/10044], Batch [6/7], Loss: 0.1543, Accuracy: 97.91%, Grad Norm: 0.28887\n",
      "Epoch [315/10044], Batch [7/7], Loss: 0.1178, Accuracy: 98.57%, Grad Norm: 0.33071\n",
      "Epoch [315/10044], Loss: 0.1178\n",
      "Epoch [316/10044], Batch [1/7], Loss: 0.1572, Accuracy: 98.64%, Grad Norm: 0.30892\n",
      "Epoch [316/10044], Batch [2/7], Loss: 0.1583, Accuracy: 97.94%, Grad Norm: 0.32614\n",
      "Epoch [316/10044], Batch [3/7], Loss: 0.1597, Accuracy: 98.70%, Grad Norm: 0.30134\n",
      "Epoch [316/10044], Batch [4/7], Loss: 0.1530, Accuracy: 98.66%, Grad Norm: 0.25091\n",
      "Epoch [316/10044], Batch [5/7], Loss: 0.1493, Accuracy: 98.19%, Grad Norm: 0.29120\n",
      "Epoch [316/10044], Batch [6/7], Loss: 0.1571, Accuracy: 97.96%, Grad Norm: 0.28241\n",
      "Epoch [316/10044], Batch [7/7], Loss: 0.1164, Accuracy: 98.62%, Grad Norm: 0.30162\n",
      "Epoch [316/10044], Loss: 0.1164\n",
      "Epoch [317/10044], Batch [1/7], Loss: 0.1585, Accuracy: 98.40%, Grad Norm: 0.31313\n",
      "Epoch [317/10044], Batch [2/7], Loss: 0.1617, Accuracy: 97.82%, Grad Norm: 0.34840\n",
      "Epoch [317/10044], Batch [3/7], Loss: 0.1570, Accuracy: 98.69%, Grad Norm: 0.28186\n",
      "Epoch [317/10044], Batch [4/7], Loss: 0.1531, Accuracy: 98.75%, Grad Norm: 0.27077\n",
      "Epoch [317/10044], Batch [5/7], Loss: 0.1481, Accuracy: 98.37%, Grad Norm: 0.27797\n",
      "Epoch [317/10044], Batch [6/7], Loss: 0.1520, Accuracy: 97.94%, Grad Norm: 0.29258\n",
      "Epoch [317/10044], Batch [7/7], Loss: 0.1154, Accuracy: 98.62%, Grad Norm: 0.29820\n",
      "Epoch [317/10044], Loss: 0.1154\n",
      "Epoch [318/10044], Batch [1/7], Loss: 0.1577, Accuracy: 98.53%, Grad Norm: 0.32451\n",
      "Epoch [318/10044], Batch [2/7], Loss: 0.1603, Accuracy: 97.87%, Grad Norm: 0.34030\n",
      "Epoch [318/10044], Batch [3/7], Loss: 0.1527, Accuracy: 98.71%, Grad Norm: 0.27735\n",
      "Epoch [318/10044], Batch [4/7], Loss: 0.1504, Accuracy: 98.82%, Grad Norm: 0.24400\n",
      "Epoch [318/10044], Batch [5/7], Loss: 0.1498, Accuracy: 98.11%, Grad Norm: 0.27754\n",
      "Epoch [318/10044], Batch [6/7], Loss: 0.1551, Accuracy: 97.89%, Grad Norm: 0.29831\n",
      "Epoch [318/10044], Batch [7/7], Loss: 0.1150, Accuracy: 98.53%, Grad Norm: 0.34782\n",
      "Epoch [318/10044], Loss: 0.1150\n",
      "Epoch [319/10044], Batch [1/7], Loss: 0.1576, Accuracy: 98.56%, Grad Norm: 0.36444\n",
      "Epoch [319/10044], Batch [2/7], Loss: 0.1602, Accuracy: 97.66%, Grad Norm: 0.34651\n",
      "Epoch [319/10044], Batch [3/7], Loss: 0.1514, Accuracy: 98.87%, Grad Norm: 0.31066\n",
      "Epoch [319/10044], Batch [4/7], Loss: 0.1487, Accuracy: 98.73%, Grad Norm: 0.27296\n",
      "Epoch [319/10044], Batch [5/7], Loss: 0.1468, Accuracy: 98.27%, Grad Norm: 0.31344\n",
      "Epoch [319/10044], Batch [6/7], Loss: 0.1530, Accuracy: 97.80%, Grad Norm: 0.29264\n",
      "Epoch [319/10044], Batch [7/7], Loss: 0.1207, Accuracy: 98.53%, Grad Norm: 0.30617\n",
      "Epoch [319/10044], Loss: 0.1207\n",
      "Epoch [320/10044], Batch [1/7], Loss: 0.1567, Accuracy: 98.31%, Grad Norm: 0.37431\n",
      "Epoch [320/10044], Batch [2/7], Loss: 0.1574, Accuracy: 97.74%, Grad Norm: 0.32343\n",
      "Epoch [320/10044], Batch [3/7], Loss: 0.1514, Accuracy: 98.84%, Grad Norm: 0.28520\n",
      "Epoch [320/10044], Batch [4/7], Loss: 0.1477, Accuracy: 98.89%, Grad Norm: 0.27659\n",
      "Epoch [320/10044], Batch [5/7], Loss: 0.1479, Accuracy: 98.27%, Grad Norm: 0.34029\n",
      "Epoch [320/10044], Batch [6/7], Loss: 0.1502, Accuracy: 98.02%, Grad Norm: 0.33701\n",
      "Epoch [320/10044], Batch [7/7], Loss: 0.1127, Accuracy: 98.57%, Grad Norm: 0.32066\n",
      "Epoch [320/10044], Loss: 0.1127\n",
      "Epoch [321/10044], Batch [1/7], Loss: 0.1497, Accuracy: 98.57%, Grad Norm: 0.29540\n",
      "Epoch [321/10044], Batch [2/7], Loss: 0.1558, Accuracy: 97.92%, Grad Norm: 0.34970\n",
      "Epoch [321/10044], Batch [3/7], Loss: 0.1503, Accuracy: 98.65%, Grad Norm: 0.31577\n",
      "Epoch [321/10044], Batch [4/7], Loss: 0.1478, Accuracy: 98.52%, Grad Norm: 0.29225\n",
      "Epoch [321/10044], Batch [5/7], Loss: 0.1449, Accuracy: 98.34%, Grad Norm: 0.28595\n",
      "Epoch [321/10044], Batch [6/7], Loss: 0.1475, Accuracy: 98.27%, Grad Norm: 0.25146\n",
      "Epoch [321/10044], Batch [7/7], Loss: 0.1085, Accuracy: 98.68%, Grad Norm: 0.32009\n",
      "Epoch [321/10044], Loss: 0.1085\n",
      "Epoch [322/10044], Batch [1/7], Loss: 0.1549, Accuracy: 98.52%, Grad Norm: 0.33786\n",
      "Epoch [322/10044], Batch [2/7], Loss: 0.1496, Accuracy: 97.93%, Grad Norm: 0.33116\n",
      "Epoch [322/10044], Batch [3/7], Loss: 0.1457, Accuracy: 98.88%, Grad Norm: 0.27912\n",
      "Epoch [322/10044], Batch [4/7], Loss: 0.1472, Accuracy: 98.72%, Grad Norm: 0.28154\n",
      "Epoch [322/10044], Batch [5/7], Loss: 0.1431, Accuracy: 98.30%, Grad Norm: 0.32341\n",
      "Epoch [322/10044], Batch [6/7], Loss: 0.1451, Accuracy: 98.12%, Grad Norm: 0.30091\n",
      "Epoch [322/10044], Batch [7/7], Loss: 0.1108, Accuracy: 98.43%, Grad Norm: 0.31277\n",
      "Epoch [322/10044], Loss: 0.1108\n",
      "Epoch [323/10044], Batch [1/7], Loss: 0.1458, Accuracy: 98.73%, Grad Norm: 0.28416\n",
      "Epoch [323/10044], Batch [2/7], Loss: 0.1520, Accuracy: 97.87%, Grad Norm: 0.32124\n",
      "Epoch [323/10044], Batch [3/7], Loss: 0.1470, Accuracy: 98.72%, Grad Norm: 0.28747\n",
      "Epoch [323/10044], Batch [4/7], Loss: 0.1441, Accuracy: 98.72%, Grad Norm: 0.28074\n",
      "Epoch [323/10044], Batch [5/7], Loss: 0.1401, Accuracy: 98.33%, Grad Norm: 0.30987\n",
      "Epoch [323/10044], Batch [6/7], Loss: 0.1495, Accuracy: 97.88%, Grad Norm: 0.32651\n",
      "Epoch [323/10044], Batch [7/7], Loss: 0.1122, Accuracy: 98.63%, Grad Norm: 0.34444\n",
      "Epoch [323/10044], Loss: 0.1122\n",
      "Epoch [324/10044], Batch [1/7], Loss: 0.1492, Accuracy: 98.56%, Grad Norm: 0.35102\n",
      "Epoch [324/10044], Batch [2/7], Loss: 0.1539, Accuracy: 97.77%, Grad Norm: 0.39545\n",
      "Epoch [324/10044], Batch [3/7], Loss: 0.1470, Accuracy: 98.60%, Grad Norm: 0.33701\n",
      "Epoch [324/10044], Batch [4/7], Loss: 0.1437, Accuracy: 98.80%, Grad Norm: 0.30183\n",
      "Epoch [324/10044], Batch [5/7], Loss: 0.1409, Accuracy: 98.38%, Grad Norm: 0.28555\n",
      "Epoch [324/10044], Batch [6/7], Loss: 0.1424, Accuracy: 98.11%, Grad Norm: 0.28260\n",
      "Epoch [324/10044], Batch [7/7], Loss: 0.1078, Accuracy: 98.67%, Grad Norm: 0.32433\n",
      "Epoch [324/10044], Loss: 0.1078\n",
      "Epoch [325/10044], Batch [1/7], Loss: 0.1480, Accuracy: 98.58%, Grad Norm: 0.38798\n",
      "Epoch [325/10044], Batch [2/7], Loss: 0.1510, Accuracy: 97.97%, Grad Norm: 0.44475\n",
      "Epoch [325/10044], Batch [3/7], Loss: 0.1443, Accuracy: 98.85%, Grad Norm: 0.28325\n",
      "Epoch [325/10044], Batch [4/7], Loss: 0.1429, Accuracy: 98.80%, Grad Norm: 0.27196\n",
      "Epoch [325/10044], Batch [5/7], Loss: 0.1349, Accuracy: 98.39%, Grad Norm: 0.28964\n",
      "Epoch [325/10044], Batch [6/7], Loss: 0.1506, Accuracy: 97.79%, Grad Norm: 0.39516\n",
      "Epoch [325/10044], Batch [7/7], Loss: 0.1099, Accuracy: 98.63%, Grad Norm: 0.35674\n",
      "Epoch [325/10044], Loss: 0.1099\n",
      "Epoch [326/10044], Batch [1/7], Loss: 0.1493, Accuracy: 98.47%, Grad Norm: 0.32374\n",
      "Epoch [326/10044], Batch [2/7], Loss: 0.1469, Accuracy: 98.02%, Grad Norm: 0.31059\n",
      "Epoch [326/10044], Batch [3/7], Loss: 0.1442, Accuracy: 98.86%, Grad Norm: 0.28912\n",
      "Epoch [326/10044], Batch [4/7], Loss: 0.1378, Accuracy: 98.93%, Grad Norm: 0.29298\n",
      "Epoch [326/10044], Batch [5/7], Loss: 0.1402, Accuracy: 98.33%, Grad Norm: 0.36928\n",
      "Epoch [326/10044], Batch [6/7], Loss: 0.1427, Accuracy: 98.15%, Grad Norm: 0.32130\n",
      "Epoch [326/10044], Batch [7/7], Loss: 0.1057, Accuracy: 98.70%, Grad Norm: 0.28219\n",
      "Epoch [326/10044], Loss: 0.1057\n",
      "Epoch [327/10044], Batch [1/7], Loss: 0.1434, Accuracy: 98.56%, Grad Norm: 0.31456\n",
      "Epoch [327/10044], Batch [2/7], Loss: 0.1490, Accuracy: 97.92%, Grad Norm: 0.40123\n",
      "Epoch [327/10044], Batch [3/7], Loss: 0.1424, Accuracy: 98.80%, Grad Norm: 0.30215\n",
      "Epoch [327/10044], Batch [4/7], Loss: 0.1404, Accuracy: 98.68%, Grad Norm: 0.29882\n",
      "Epoch [327/10044], Batch [5/7], Loss: 0.1369, Accuracy: 98.40%, Grad Norm: 0.28411\n",
      "Epoch [327/10044], Batch [6/7], Loss: 0.1418, Accuracy: 98.17%, Grad Norm: 0.26949\n",
      "Epoch [327/10044], Batch [7/7], Loss: 0.1066, Accuracy: 98.60%, Grad Norm: 0.32905\n",
      "Epoch [327/10044], Loss: 0.1066\n",
      "Epoch [328/10044], Batch [1/7], Loss: 0.1445, Accuracy: 98.52%, Grad Norm: 0.37158\n",
      "Epoch [328/10044], Batch [2/7], Loss: 0.1486, Accuracy: 97.89%, Grad Norm: 0.37348\n",
      "Epoch [328/10044], Batch [3/7], Loss: 0.1426, Accuracy: 98.90%, Grad Norm: 0.28344\n",
      "Epoch [328/10044], Batch [4/7], Loss: 0.1350, Accuracy: 98.87%, Grad Norm: 0.24744\n",
      "Epoch [328/10044], Batch [5/7], Loss: 0.1353, Accuracy: 98.37%, Grad Norm: 0.29161\n",
      "Epoch [328/10044], Batch [6/7], Loss: 0.1422, Accuracy: 97.86%, Grad Norm: 0.36373\n",
      "Epoch [328/10044], Batch [7/7], Loss: 0.1112, Accuracy: 98.38%, Grad Norm: 0.38372\n",
      "Epoch [328/10044], Loss: 0.1112\n",
      "Epoch [329/10044], Batch [1/7], Loss: 0.1388, Accuracy: 98.65%, Grad Norm: 0.27827\n",
      "Epoch [329/10044], Batch [2/7], Loss: 0.1449, Accuracy: 97.99%, Grad Norm: 0.31484\n",
      "Epoch [329/10044], Batch [3/7], Loss: 0.1409, Accuracy: 98.81%, Grad Norm: 0.32605\n",
      "Epoch [329/10044], Batch [4/7], Loss: 0.1404, Accuracy: 98.84%, Grad Norm: 0.30344\n",
      "Epoch [329/10044], Batch [5/7], Loss: 0.1355, Accuracy: 98.42%, Grad Norm: 0.31206\n",
      "Epoch [329/10044], Batch [6/7], Loss: 0.1400, Accuracy: 98.12%, Grad Norm: 0.31478\n",
      "Epoch [329/10044], Batch [7/7], Loss: 0.1070, Accuracy: 98.50%, Grad Norm: 0.35497\n",
      "Epoch [329/10044], Loss: 0.1070\n",
      "Epoch [330/10044], Batch [1/7], Loss: 0.1403, Accuracy: 98.51%, Grad Norm: 0.35108\n",
      "Epoch [330/10044], Batch [2/7], Loss: 0.1437, Accuracy: 97.91%, Grad Norm: 0.35535\n",
      "Epoch [330/10044], Batch [3/7], Loss: 0.1386, Accuracy: 98.71%, Grad Norm: 0.33924\n",
      "Epoch [330/10044], Batch [4/7], Loss: 0.1355, Accuracy: 98.83%, Grad Norm: 0.26947\n",
      "Epoch [330/10044], Batch [5/7], Loss: 0.1302, Accuracy: 98.49%, Grad Norm: 0.24774\n",
      "Epoch [330/10044], Batch [6/7], Loss: 0.1363, Accuracy: 98.07%, Grad Norm: 0.27119\n",
      "Epoch [330/10044], Batch [7/7], Loss: 0.1044, Accuracy: 98.82%, Grad Norm: 0.30510\n",
      "Epoch [330/10044], Loss: 0.1044\n",
      "Epoch [331/10044], Batch [1/7], Loss: 0.1438, Accuracy: 98.53%, Grad Norm: 0.39758\n",
      "Epoch [331/10044], Batch [2/7], Loss: 0.1433, Accuracy: 98.00%, Grad Norm: 0.35893\n",
      "Epoch [331/10044], Batch [3/7], Loss: 0.1366, Accuracy: 98.86%, Grad Norm: 0.26894\n",
      "Epoch [331/10044], Batch [4/7], Loss: 0.1320, Accuracy: 98.82%, Grad Norm: 0.24389\n",
      "Epoch [331/10044], Batch [5/7], Loss: 0.1294, Accuracy: 98.33%, Grad Norm: 0.27821\n",
      "Epoch [331/10044], Batch [6/7], Loss: 0.1369, Accuracy: 98.11%, Grad Norm: 0.32538\n",
      "Epoch [331/10044], Batch [7/7], Loss: 0.1028, Accuracy: 98.65%, Grad Norm: 0.35889\n",
      "Epoch [331/10044], Loss: 0.1028\n",
      "Epoch [332/10044], Batch [1/7], Loss: 0.1402, Accuracy: 98.62%, Grad Norm: 0.31731\n",
      "Epoch [332/10044], Batch [2/7], Loss: 0.1436, Accuracy: 98.02%, Grad Norm: 0.30415\n",
      "Epoch [332/10044], Batch [3/7], Loss: 0.1363, Accuracy: 98.97%, Grad Norm: 0.29021\n",
      "Epoch [332/10044], Batch [4/7], Loss: 0.1361, Accuracy: 98.67%, Grad Norm: 0.36482\n",
      "Epoch [332/10044], Batch [5/7], Loss: 0.1345, Accuracy: 98.34%, Grad Norm: 0.37991\n",
      "Epoch [332/10044], Batch [6/7], Loss: 0.1381, Accuracy: 97.93%, Grad Norm: 0.33339\n",
      "Epoch [332/10044], Batch [7/7], Loss: 0.1002, Accuracy: 98.77%, Grad Norm: 0.36826\n",
      "Epoch [332/10044], Loss: 0.1002\n",
      "Epoch [333/10044], Batch [1/7], Loss: 0.1349, Accuracy: 98.62%, Grad Norm: 0.27392\n",
      "Epoch [333/10044], Batch [2/7], Loss: 0.1426, Accuracy: 97.77%, Grad Norm: 0.37036\n",
      "Epoch [333/10044], Batch [3/7], Loss: 0.1390, Accuracy: 98.76%, Grad Norm: 0.33114\n",
      "Epoch [333/10044], Batch [4/7], Loss: 0.1346, Accuracy: 98.82%, Grad Norm: 0.30755\n",
      "Epoch [333/10044], Batch [5/7], Loss: 0.1337, Accuracy: 98.32%, Grad Norm: 0.31695\n",
      "Epoch [333/10044], Batch [6/7], Loss: 0.1337, Accuracy: 98.26%, Grad Norm: 0.28960\n",
      "Epoch [333/10044], Batch [7/7], Loss: 0.1069, Accuracy: 98.30%, Grad Norm: 0.40898\n",
      "Epoch [333/10044], Loss: 0.1069\n",
      "Epoch [334/10044], Batch [1/7], Loss: 0.1369, Accuracy: 98.71%, Grad Norm: 0.37633\n",
      "Epoch [334/10044], Batch [2/7], Loss: 0.1373, Accuracy: 98.18%, Grad Norm: 0.36464\n",
      "Epoch [334/10044], Batch [3/7], Loss: 0.1306, Accuracy: 98.93%, Grad Norm: 0.27345\n",
      "Epoch [334/10044], Batch [4/7], Loss: 0.1310, Accuracy: 98.90%, Grad Norm: 0.28492\n",
      "Epoch [334/10044], Batch [5/7], Loss: 0.1290, Accuracy: 98.48%, Grad Norm: 0.29888\n",
      "Epoch [334/10044], Batch [6/7], Loss: 0.1363, Accuracy: 98.00%, Grad Norm: 0.29456\n",
      "Epoch [334/10044], Batch [7/7], Loss: 0.1029, Accuracy: 98.67%, Grad Norm: 0.30212\n",
      "Epoch [334/10044], Loss: 0.1029\n",
      "Epoch [335/10044], Batch [1/7], Loss: 0.1314, Accuracy: 98.82%, Grad Norm: 0.30070\n",
      "Epoch [335/10044], Batch [2/7], Loss: 0.1368, Accuracy: 98.03%, Grad Norm: 0.27027\n",
      "Epoch [335/10044], Batch [3/7], Loss: 0.1311, Accuracy: 98.97%, Grad Norm: 0.29261\n",
      "Epoch [335/10044], Batch [4/7], Loss: 0.1300, Accuracy: 98.87%, Grad Norm: 0.32869\n",
      "Epoch [335/10044], Batch [5/7], Loss: 0.1306, Accuracy: 98.48%, Grad Norm: 0.29074\n",
      "Epoch [335/10044], Batch [6/7], Loss: 0.1300, Accuracy: 98.23%, Grad Norm: 0.25859\n",
      "Epoch [335/10044], Batch [7/7], Loss: 0.0993, Accuracy: 98.75%, Grad Norm: 0.28562\n",
      "Epoch [335/10044], Loss: 0.0993\n",
      "Epoch [336/10044], Batch [1/7], Loss: 0.1336, Accuracy: 98.68%, Grad Norm: 0.32511\n",
      "Epoch [336/10044], Batch [2/7], Loss: 0.1352, Accuracy: 98.17%, Grad Norm: 0.29276\n",
      "Epoch [336/10044], Batch [3/7], Loss: 0.1290, Accuracy: 98.87%, Grad Norm: 0.28687\n",
      "Epoch [336/10044], Batch [4/7], Loss: 0.1250, Accuracy: 98.93%, Grad Norm: 0.25289\n",
      "Epoch [336/10044], Batch [5/7], Loss: 0.1282, Accuracy: 98.33%, Grad Norm: 0.28993\n",
      "Epoch [336/10044], Batch [6/7], Loss: 0.1297, Accuracy: 98.17%, Grad Norm: 0.27066\n",
      "Epoch [336/10044], Batch [7/7], Loss: 0.1018, Accuracy: 98.57%, Grad Norm: 0.33250\n",
      "Epoch [336/10044], Loss: 0.1018\n",
      "Epoch [337/10044], Batch [1/7], Loss: 0.1331, Accuracy: 98.66%, Grad Norm: 0.33392\n",
      "Epoch [337/10044], Batch [2/7], Loss: 0.1309, Accuracy: 98.18%, Grad Norm: 0.28809\n",
      "Epoch [337/10044], Batch [3/7], Loss: 0.1286, Accuracy: 98.95%, Grad Norm: 0.27204\n",
      "Epoch [337/10044], Batch [4/7], Loss: 0.1247, Accuracy: 98.93%, Grad Norm: 0.25081\n",
      "Epoch [337/10044], Batch [5/7], Loss: 0.1249, Accuracy: 98.39%, Grad Norm: 0.28392\n",
      "Epoch [337/10044], Batch [6/7], Loss: 0.1284, Accuracy: 98.05%, Grad Norm: 0.25498\n",
      "Epoch [337/10044], Batch [7/7], Loss: 0.0992, Accuracy: 98.65%, Grad Norm: 0.28521\n",
      "Epoch [337/10044], Loss: 0.0992\n",
      "Epoch [338/10044], Batch [1/7], Loss: 0.1315, Accuracy: 98.70%, Grad Norm: 0.29553\n",
      "Epoch [338/10044], Batch [2/7], Loss: 0.1338, Accuracy: 98.04%, Grad Norm: 0.29602\n",
      "Epoch [338/10044], Batch [3/7], Loss: 0.1292, Accuracy: 98.97%, Grad Norm: 0.28657\n",
      "Epoch [338/10044], Batch [4/7], Loss: 0.1237, Accuracy: 98.86%, Grad Norm: 0.22495\n",
      "Epoch [338/10044], Batch [5/7], Loss: 0.1237, Accuracy: 98.42%, Grad Norm: 0.25930\n",
      "Epoch [338/10044], Batch [6/7], Loss: 0.1307, Accuracy: 98.08%, Grad Norm: 0.25131\n",
      "Epoch [338/10044], Batch [7/7], Loss: 0.0954, Accuracy: 98.73%, Grad Norm: 0.26475\n",
      "Epoch [338/10044], Loss: 0.0954\n",
      "Epoch [339/10044], Batch [1/7], Loss: 0.1285, Accuracy: 98.74%, Grad Norm: 0.29832\n",
      "Epoch [339/10044], Batch [2/7], Loss: 0.1287, Accuracy: 98.31%, Grad Norm: 0.27132\n",
      "Epoch [339/10044], Batch [3/7], Loss: 0.1236, Accuracy: 99.00%, Grad Norm: 0.25573\n",
      "Epoch [339/10044], Batch [4/7], Loss: 0.1244, Accuracy: 98.82%, Grad Norm: 0.24788\n",
      "Epoch [339/10044], Batch [5/7], Loss: 0.1209, Accuracy: 98.61%, Grad Norm: 0.23268\n",
      "Epoch [339/10044], Batch [6/7], Loss: 0.1282, Accuracy: 98.15%, Grad Norm: 0.25285\n",
      "Epoch [339/10044], Batch [7/7], Loss: 0.0964, Accuracy: 98.77%, Grad Norm: 0.26233\n",
      "Epoch [339/10044], Loss: 0.0964\n",
      "Epoch [340/10044], Batch [1/7], Loss: 0.1286, Accuracy: 98.58%, Grad Norm: 0.29546\n",
      "Epoch [340/10044], Batch [2/7], Loss: 0.1284, Accuracy: 98.12%, Grad Norm: 0.28638\n",
      "Epoch [340/10044], Batch [3/7], Loss: 0.1243, Accuracy: 99.03%, Grad Norm: 0.27247\n",
      "Epoch [340/10044], Batch [4/7], Loss: 0.1212, Accuracy: 99.05%, Grad Norm: 0.23729\n",
      "Epoch [340/10044], Batch [5/7], Loss: 0.1199, Accuracy: 98.38%, Grad Norm: 0.23188\n",
      "Epoch [340/10044], Batch [6/7], Loss: 0.1262, Accuracy: 98.19%, Grad Norm: 0.25520\n",
      "Epoch [340/10044], Batch [7/7], Loss: 0.0913, Accuracy: 98.97%, Grad Norm: 0.27709\n",
      "Epoch [340/10044], Loss: 0.0913\n",
      "Epoch [341/10044], Batch [1/7], Loss: 0.1271, Accuracy: 98.62%, Grad Norm: 0.31469\n",
      "Epoch [341/10044], Batch [2/7], Loss: 0.1325, Accuracy: 98.02%, Grad Norm: 0.28905\n",
      "Epoch [341/10044], Batch [3/7], Loss: 0.1244, Accuracy: 98.91%, Grad Norm: 0.24438\n",
      "Epoch [341/10044], Batch [4/7], Loss: 0.1228, Accuracy: 98.87%, Grad Norm: 0.24687\n",
      "Epoch [341/10044], Batch [5/7], Loss: 0.1193, Accuracy: 98.57%, Grad Norm: 0.26708\n",
      "Epoch [341/10044], Batch [6/7], Loss: 0.1258, Accuracy: 98.19%, Grad Norm: 0.30372\n",
      "Epoch [341/10044], Batch [7/7], Loss: 0.0905, Accuracy: 98.77%, Grad Norm: 0.27818\n",
      "Epoch [341/10044], Loss: 0.0905\n",
      "Epoch [342/10044], Batch [1/7], Loss: 0.1264, Accuracy: 98.71%, Grad Norm: 0.27295\n",
      "Epoch [342/10044], Batch [2/7], Loss: 0.1274, Accuracy: 98.15%, Grad Norm: 0.29647\n",
      "Epoch [342/10044], Batch [3/7], Loss: 0.1206, Accuracy: 99.17%, Grad Norm: 0.26497\n",
      "Epoch [342/10044], Batch [4/7], Loss: 0.1212, Accuracy: 98.95%, Grad Norm: 0.25784\n",
      "Epoch [342/10044], Batch [5/7], Loss: 0.1181, Accuracy: 98.50%, Grad Norm: 0.26713\n",
      "Epoch [342/10044], Batch [6/7], Loss: 0.1237, Accuracy: 98.17%, Grad Norm: 0.25410\n",
      "Epoch [342/10044], Batch [7/7], Loss: 0.0892, Accuracy: 98.87%, Grad Norm: 0.25506\n",
      "Epoch [342/10044], Loss: 0.0892\n",
      "Epoch [343/10044], Batch [1/7], Loss: 0.1233, Accuracy: 98.76%, Grad Norm: 0.29320\n",
      "Epoch [343/10044], Batch [2/7], Loss: 0.1295, Accuracy: 98.09%, Grad Norm: 0.33013\n",
      "Epoch [343/10044], Batch [3/7], Loss: 0.1209, Accuracy: 98.95%, Grad Norm: 0.28680\n",
      "Epoch [343/10044], Batch [4/7], Loss: 0.1172, Accuracy: 98.96%, Grad Norm: 0.21635\n",
      "Epoch [343/10044], Batch [5/7], Loss: 0.1168, Accuracy: 98.62%, Grad Norm: 0.22912\n",
      "Epoch [343/10044], Batch [6/7], Loss: 0.1239, Accuracy: 98.16%, Grad Norm: 0.25824\n",
      "Epoch [343/10044], Batch [7/7], Loss: 0.0937, Accuracy: 98.60%, Grad Norm: 0.29947\n",
      "Epoch [343/10044], Loss: 0.0937\n",
      "Epoch [344/10044], Batch [1/7], Loss: 0.1237, Accuracy: 98.71%, Grad Norm: 0.27107\n",
      "Epoch [344/10044], Batch [2/7], Loss: 0.1253, Accuracy: 98.19%, Grad Norm: 0.27260\n",
      "Epoch [344/10044], Batch [3/7], Loss: 0.1177, Accuracy: 98.99%, Grad Norm: 0.24009\n",
      "Epoch [344/10044], Batch [4/7], Loss: 0.1195, Accuracy: 98.88%, Grad Norm: 0.21502\n",
      "Epoch [344/10044], Batch [5/7], Loss: 0.1172, Accuracy: 98.52%, Grad Norm: 0.27470\n",
      "Epoch [344/10044], Batch [6/7], Loss: 0.1193, Accuracy: 98.32%, Grad Norm: 0.26076\n",
      "Epoch [344/10044], Batch [7/7], Loss: 0.0910, Accuracy: 98.87%, Grad Norm: 0.29783\n",
      "Epoch [344/10044], Loss: 0.0910\n",
      "Epoch [345/10044], Batch [1/7], Loss: 0.1185, Accuracy: 98.83%, Grad Norm: 0.25091\n",
      "Epoch [345/10044], Batch [2/7], Loss: 0.1202, Accuracy: 98.24%, Grad Norm: 0.23974\n",
      "Epoch [345/10044], Batch [3/7], Loss: 0.1171, Accuracy: 98.97%, Grad Norm: 0.21642\n",
      "Epoch [345/10044], Batch [4/7], Loss: 0.1178, Accuracy: 98.82%, Grad Norm: 0.23026\n",
      "Epoch [345/10044], Batch [5/7], Loss: 0.1151, Accuracy: 98.57%, Grad Norm: 0.24727\n",
      "Epoch [345/10044], Batch [6/7], Loss: 0.1202, Accuracy: 98.21%, Grad Norm: 0.23611\n",
      "Epoch [345/10044], Batch [7/7], Loss: 0.0901, Accuracy: 98.75%, Grad Norm: 0.26829\n",
      "Epoch [345/10044], Loss: 0.0901\n",
      "Epoch [346/10044], Batch [1/7], Loss: 0.1191, Accuracy: 98.83%, Grad Norm: 0.28942\n",
      "Epoch [346/10044], Batch [2/7], Loss: 0.1267, Accuracy: 98.09%, Grad Norm: 0.32294\n",
      "Epoch [346/10044], Batch [3/7], Loss: 0.1162, Accuracy: 99.03%, Grad Norm: 0.21895\n",
      "Epoch [346/10044], Batch [4/7], Loss: 0.1146, Accuracy: 98.98%, Grad Norm: 0.22143\n",
      "Epoch [346/10044], Batch [5/7], Loss: 0.1134, Accuracy: 98.65%, Grad Norm: 0.22713\n",
      "Epoch [346/10044], Batch [6/7], Loss: 0.1203, Accuracy: 98.31%, Grad Norm: 0.24987\n",
      "Epoch [346/10044], Batch [7/7], Loss: 0.0877, Accuracy: 98.85%, Grad Norm: 0.27103\n",
      "Epoch [346/10044], Loss: 0.0877\n",
      "Epoch [347/10044], Batch [1/7], Loss: 0.1209, Accuracy: 98.77%, Grad Norm: 0.27335\n",
      "Epoch [347/10044], Batch [2/7], Loss: 0.1217, Accuracy: 98.14%, Grad Norm: 0.28305\n",
      "Epoch [347/10044], Batch [3/7], Loss: 0.1153, Accuracy: 99.03%, Grad Norm: 0.22631\n",
      "Epoch [347/10044], Batch [4/7], Loss: 0.1177, Accuracy: 98.84%, Grad Norm: 0.26149\n",
      "Epoch [347/10044], Batch [5/7], Loss: 0.1145, Accuracy: 98.67%, Grad Norm: 0.26499\n",
      "Epoch [347/10044], Batch [6/7], Loss: 0.1197, Accuracy: 98.22%, Grad Norm: 0.28217\n",
      "Epoch [347/10044], Batch [7/7], Loss: 0.0887, Accuracy: 98.77%, Grad Norm: 0.26841\n",
      "Epoch [347/10044], Loss: 0.0887\n",
      "Epoch [348/10044], Batch [1/7], Loss: 0.1164, Accuracy: 98.77%, Grad Norm: 0.25097\n",
      "Epoch [348/10044], Batch [2/7], Loss: 0.1227, Accuracy: 98.12%, Grad Norm: 0.27045\n",
      "Epoch [348/10044], Batch [3/7], Loss: 0.1137, Accuracy: 99.02%, Grad Norm: 0.24803\n",
      "Epoch [348/10044], Batch [4/7], Loss: 0.1178, Accuracy: 98.92%, Grad Norm: 0.23616\n",
      "Epoch [348/10044], Batch [5/7], Loss: 0.1136, Accuracy: 98.49%, Grad Norm: 0.25205\n",
      "Epoch [348/10044], Batch [6/7], Loss: 0.1194, Accuracy: 98.32%, Grad Norm: 0.26948\n",
      "Epoch [348/10044], Batch [7/7], Loss: 0.0866, Accuracy: 98.88%, Grad Norm: 0.29071\n",
      "Epoch [348/10044], Loss: 0.0866\n",
      "Epoch [349/10044], Batch [1/7], Loss: 0.1148, Accuracy: 98.87%, Grad Norm: 0.26107\n",
      "Epoch [349/10044], Batch [2/7], Loss: 0.1204, Accuracy: 98.31%, Grad Norm: 0.27156\n",
      "Epoch [349/10044], Batch [3/7], Loss: 0.1130, Accuracy: 99.06%, Grad Norm: 0.25695\n",
      "Epoch [349/10044], Batch [4/7], Loss: 0.1154, Accuracy: 98.86%, Grad Norm: 0.27829\n",
      "Epoch [349/10044], Batch [5/7], Loss: 0.1126, Accuracy: 98.62%, Grad Norm: 0.25227\n",
      "Epoch [349/10044], Batch [6/7], Loss: 0.1169, Accuracy: 98.33%, Grad Norm: 0.26213\n",
      "Epoch [349/10044], Batch [7/7], Loss: 0.0891, Accuracy: 98.70%, Grad Norm: 0.25676\n",
      "Epoch [349/10044], Loss: 0.0891\n",
      "Epoch [350/10044], Batch [1/7], Loss: 0.1150, Accuracy: 98.89%, Grad Norm: 0.26941\n",
      "Epoch [350/10044], Batch [2/7], Loss: 0.1197, Accuracy: 98.23%, Grad Norm: 0.27763\n",
      "Epoch [350/10044], Batch [3/7], Loss: 0.1138, Accuracy: 98.99%, Grad Norm: 0.26747\n",
      "Epoch [350/10044], Batch [4/7], Loss: 0.1109, Accuracy: 99.03%, Grad Norm: 0.26498\n",
      "Epoch [350/10044], Batch [5/7], Loss: 0.1130, Accuracy: 98.57%, Grad Norm: 0.25066\n",
      "Epoch [350/10044], Batch [6/7], Loss: 0.1133, Accuracy: 98.47%, Grad Norm: 0.23696\n",
      "Epoch [350/10044], Batch [7/7], Loss: 0.0906, Accuracy: 98.70%, Grad Norm: 0.29896\n",
      "Epoch [350/10044], Loss: 0.0906\n",
      "Epoch [351/10044], Batch [1/7], Loss: 0.1170, Accuracy: 98.65%, Grad Norm: 0.27417\n",
      "Epoch [351/10044], Batch [2/7], Loss: 0.1198, Accuracy: 98.19%, Grad Norm: 0.27990\n",
      "Epoch [351/10044], Batch [3/7], Loss: 0.1127, Accuracy: 98.99%, Grad Norm: 0.25724\n",
      "Epoch [351/10044], Batch [4/7], Loss: 0.1110, Accuracy: 99.05%, Grad Norm: 0.20917\n",
      "Epoch [351/10044], Batch [5/7], Loss: 0.1099, Accuracy: 98.68%, Grad Norm: 0.25484\n",
      "Epoch [351/10044], Batch [6/7], Loss: 0.1187, Accuracy: 98.17%, Grad Norm: 0.28480\n",
      "Epoch [351/10044], Batch [7/7], Loss: 0.0909, Accuracy: 98.67%, Grad Norm: 0.38435\n",
      "Epoch [351/10044], Loss: 0.0909\n",
      "Epoch [352/10044], Batch [1/7], Loss: 0.1120, Accuracy: 98.96%, Grad Norm: 0.26367\n",
      "Epoch [352/10044], Batch [2/7], Loss: 0.1168, Accuracy: 98.12%, Grad Norm: 0.24383\n",
      "Epoch [352/10044], Batch [3/7], Loss: 0.1092, Accuracy: 98.89%, Grad Norm: 0.24546\n",
      "Epoch [352/10044], Batch [4/7], Loss: 0.1139, Accuracy: 98.98%, Grad Norm: 0.26469\n",
      "Epoch [352/10044], Batch [5/7], Loss: 0.1125, Accuracy: 98.54%, Grad Norm: 0.27041\n",
      "Epoch [352/10044], Batch [6/7], Loss: 0.1142, Accuracy: 98.28%, Grad Norm: 0.26239\n",
      "Epoch [352/10044], Batch [7/7], Loss: 0.0857, Accuracy: 98.78%, Grad Norm: 0.27639\n",
      "Epoch [352/10044], Loss: 0.0857\n",
      "Epoch [353/10044], Batch [1/7], Loss: 0.1125, Accuracy: 98.93%, Grad Norm: 0.25467\n",
      "Epoch [353/10044], Batch [2/7], Loss: 0.1169, Accuracy: 98.40%, Grad Norm: 0.35829\n",
      "Epoch [353/10044], Batch [3/7], Loss: 0.1121, Accuracy: 98.95%, Grad Norm: 0.25781\n",
      "Epoch [353/10044], Batch [4/7], Loss: 0.1106, Accuracy: 98.96%, Grad Norm: 0.28076\n",
      "Epoch [353/10044], Batch [5/7], Loss: 0.1072, Accuracy: 98.60%, Grad Norm: 0.23257\n",
      "Epoch [353/10044], Batch [6/7], Loss: 0.1103, Accuracy: 98.47%, Grad Norm: 0.24354\n",
      "Epoch [353/10044], Batch [7/7], Loss: 0.0836, Accuracy: 98.93%, Grad Norm: 0.28575\n",
      "Epoch [353/10044], Loss: 0.0836\n",
      "Epoch [354/10044], Batch [1/7], Loss: 0.1101, Accuracy: 98.81%, Grad Norm: 0.26457\n",
      "Epoch [354/10044], Batch [2/7], Loss: 0.1155, Accuracy: 98.37%, Grad Norm: 0.29727\n",
      "Epoch [354/10044], Batch [3/7], Loss: 0.1084, Accuracy: 99.13%, Grad Norm: 0.22413\n",
      "Epoch [354/10044], Batch [4/7], Loss: 0.1076, Accuracy: 99.06%, Grad Norm: 0.21651\n",
      "Epoch [354/10044], Batch [5/7], Loss: 0.1073, Accuracy: 98.59%, Grad Norm: 0.24470\n",
      "Epoch [354/10044], Batch [6/7], Loss: 0.1146, Accuracy: 98.22%, Grad Norm: 0.27934\n",
      "Epoch [354/10044], Batch [7/7], Loss: 0.0851, Accuracy: 98.88%, Grad Norm: 0.30080\n",
      "Epoch [354/10044], Loss: 0.0851\n",
      "Epoch [355/10044], Batch [1/7], Loss: 0.1108, Accuracy: 98.90%, Grad Norm: 0.26581\n",
      "Epoch [355/10044], Batch [2/7], Loss: 0.1151, Accuracy: 98.22%, Grad Norm: 0.26180\n",
      "Epoch [355/10044], Batch [3/7], Loss: 0.1084, Accuracy: 98.94%, Grad Norm: 0.26245\n",
      "Epoch [355/10044], Batch [4/7], Loss: 0.1095, Accuracy: 98.97%, Grad Norm: 0.24173\n",
      "Epoch [355/10044], Batch [5/7], Loss: 0.1082, Accuracy: 98.61%, Grad Norm: 0.24672\n",
      "Epoch [355/10044], Batch [6/7], Loss: 0.1107, Accuracy: 98.33%, Grad Norm: 0.22549\n",
      "Epoch [355/10044], Batch [7/7], Loss: 0.0820, Accuracy: 98.92%, Grad Norm: 0.24520\n",
      "Epoch [355/10044], Loss: 0.0820\n",
      "Epoch [356/10044], Batch [1/7], Loss: 0.1095, Accuracy: 98.95%, Grad Norm: 0.27410\n",
      "Epoch [356/10044], Batch [2/7], Loss: 0.1178, Accuracy: 98.17%, Grad Norm: 0.28241\n",
      "Epoch [356/10044], Batch [3/7], Loss: 0.1089, Accuracy: 98.98%, Grad Norm: 0.27545\n",
      "Epoch [356/10044], Batch [4/7], Loss: 0.1071, Accuracy: 99.09%, Grad Norm: 0.21861\n",
      "Epoch [356/10044], Batch [5/7], Loss: 0.1070, Accuracy: 98.50%, Grad Norm: 0.23806\n",
      "Epoch [356/10044], Batch [6/7], Loss: 0.1100, Accuracy: 98.43%, Grad Norm: 0.24662\n",
      "Epoch [356/10044], Batch [7/7], Loss: 0.0828, Accuracy: 98.92%, Grad Norm: 0.28070\n",
      "Epoch [356/10044], Loss: 0.0828\n",
      "Epoch [357/10044], Batch [1/7], Loss: 0.1087, Accuracy: 98.89%, Grad Norm: 0.28966\n",
      "Epoch [357/10044], Batch [2/7], Loss: 0.1128, Accuracy: 98.30%, Grad Norm: 0.27311\n",
      "Epoch [357/10044], Batch [3/7], Loss: 0.1044, Accuracy: 99.07%, Grad Norm: 0.20382\n",
      "Epoch [357/10044], Batch [4/7], Loss: 0.1042, Accuracy: 99.09%, Grad Norm: 0.21112\n",
      "Epoch [357/10044], Batch [5/7], Loss: 0.1078, Accuracy: 98.67%, Grad Norm: 0.24501\n",
      "Epoch [357/10044], Batch [6/7], Loss: 0.1108, Accuracy: 98.19%, Grad Norm: 0.26495\n",
      "Epoch [357/10044], Batch [7/7], Loss: 0.0836, Accuracy: 98.90%, Grad Norm: 0.31849\n",
      "Epoch [357/10044], Loss: 0.0836\n",
      "Epoch [358/10044], Batch [1/7], Loss: 0.1090, Accuracy: 98.97%, Grad Norm: 0.30631\n",
      "Epoch [358/10044], Batch [2/7], Loss: 0.1069, Accuracy: 98.39%, Grad Norm: 0.24287\n",
      "Epoch [358/10044], Batch [3/7], Loss: 0.1065, Accuracy: 99.01%, Grad Norm: 0.25527\n",
      "Epoch [358/10044], Batch [4/7], Loss: 0.1070, Accuracy: 99.05%, Grad Norm: 0.25498\n",
      "Epoch [358/10044], Batch [5/7], Loss: 0.1047, Accuracy: 98.65%, Grad Norm: 0.25553\n",
      "Epoch [358/10044], Batch [6/7], Loss: 0.1097, Accuracy: 98.29%, Grad Norm: 0.25531\n",
      "Epoch [358/10044], Batch [7/7], Loss: 0.0754, Accuracy: 98.97%, Grad Norm: 0.23789\n",
      "Epoch [358/10044], Loss: 0.0754\n",
      "Epoch [359/10044], Batch [1/7], Loss: 0.1080, Accuracy: 98.92%, Grad Norm: 0.25742\n",
      "Epoch [359/10044], Batch [2/7], Loss: 0.1138, Accuracy: 98.16%, Grad Norm: 0.28150\n",
      "Epoch [359/10044], Batch [3/7], Loss: 0.1046, Accuracy: 99.13%, Grad Norm: 0.24563\n",
      "Epoch [359/10044], Batch [4/7], Loss: 0.1056, Accuracy: 98.98%, Grad Norm: 0.23228\n",
      "Epoch [359/10044], Batch [5/7], Loss: 0.1054, Accuracy: 98.57%, Grad Norm: 0.22361\n",
      "Epoch [359/10044], Batch [6/7], Loss: 0.1096, Accuracy: 98.30%, Grad Norm: 0.22516\n",
      "Epoch [359/10044], Batch [7/7], Loss: 0.0803, Accuracy: 98.98%, Grad Norm: 0.24291\n",
      "Epoch [359/10044], Loss: 0.0803\n",
      "Epoch [360/10044], Batch [1/7], Loss: 0.1070, Accuracy: 98.92%, Grad Norm: 0.24358\n",
      "Epoch [360/10044], Batch [2/7], Loss: 0.1125, Accuracy: 98.18%, Grad Norm: 0.28680\n",
      "Epoch [360/10044], Batch [3/7], Loss: 0.1019, Accuracy: 99.07%, Grad Norm: 0.22076\n",
      "Epoch [360/10044], Batch [4/7], Loss: 0.1033, Accuracy: 99.07%, Grad Norm: 0.22022\n",
      "Epoch [360/10044], Batch [5/7], Loss: 0.1026, Accuracy: 98.62%, Grad Norm: 0.23077\n",
      "Epoch [360/10044], Batch [6/7], Loss: 0.1057, Accuracy: 98.52%, Grad Norm: 0.23418\n",
      "Epoch [360/10044], Batch [7/7], Loss: 0.0786, Accuracy: 98.97%, Grad Norm: 0.26451\n",
      "Epoch [360/10044], Loss: 0.0786\n",
      "Epoch [361/10044], Batch [1/7], Loss: 0.1054, Accuracy: 98.96%, Grad Norm: 0.26602\n",
      "Epoch [361/10044], Batch [2/7], Loss: 0.1124, Accuracy: 98.32%, Grad Norm: 0.26643\n",
      "Epoch [361/10044], Batch [3/7], Loss: 0.0979, Accuracy: 99.20%, Grad Norm: 0.21392\n",
      "Epoch [361/10044], Batch [4/7], Loss: 0.1017, Accuracy: 99.05%, Grad Norm: 0.21742\n",
      "Epoch [361/10044], Batch [5/7], Loss: 0.1025, Accuracy: 98.57%, Grad Norm: 0.21865\n",
      "Epoch [361/10044], Batch [6/7], Loss: 0.1112, Accuracy: 98.22%, Grad Norm: 0.22410\n",
      "Epoch [361/10044], Batch [7/7], Loss: 0.0789, Accuracy: 98.95%, Grad Norm: 0.26942\n",
      "Epoch [361/10044], Loss: 0.0789\n",
      "Epoch [362/10044], Batch [1/7], Loss: 0.1024, Accuracy: 99.07%, Grad Norm: 0.23906\n",
      "Epoch [362/10044], Batch [2/7], Loss: 0.1081, Accuracy: 98.42%, Grad Norm: 0.23821\n",
      "Epoch [362/10044], Batch [3/7], Loss: 0.1026, Accuracy: 99.05%, Grad Norm: 0.24668\n",
      "Epoch [362/10044], Batch [4/7], Loss: 0.1011, Accuracy: 98.97%, Grad Norm: 0.21827\n",
      "Epoch [362/10044], Batch [5/7], Loss: 0.0983, Accuracy: 98.67%, Grad Norm: 0.22730\n",
      "Epoch [362/10044], Batch [6/7], Loss: 0.1078, Accuracy: 98.23%, Grad Norm: 0.25920\n",
      "Epoch [362/10044], Batch [7/7], Loss: 0.0816, Accuracy: 98.85%, Grad Norm: 0.29814\n",
      "Epoch [362/10044], Loss: 0.0816\n",
      "Epoch [363/10044], Batch [1/7], Loss: 0.1038, Accuracy: 98.97%, Grad Norm: 0.22787\n",
      "Epoch [363/10044], Batch [2/7], Loss: 0.1087, Accuracy: 98.24%, Grad Norm: 0.28201\n",
      "Epoch [363/10044], Batch [3/7], Loss: 0.1026, Accuracy: 99.07%, Grad Norm: 0.23895\n",
      "Epoch [363/10044], Batch [4/7], Loss: 0.0989, Accuracy: 99.07%, Grad Norm: 0.20176\n",
      "Epoch [363/10044], Batch [5/7], Loss: 0.1016, Accuracy: 98.63%, Grad Norm: 0.23854\n",
      "Epoch [363/10044], Batch [6/7], Loss: 0.1040, Accuracy: 98.37%, Grad Norm: 0.23179\n",
      "Epoch [363/10044], Batch [7/7], Loss: 0.0760, Accuracy: 99.00%, Grad Norm: 0.25739\n",
      "Epoch [363/10044], Loss: 0.0760\n",
      "Epoch [364/10044], Batch [1/7], Loss: 0.1006, Accuracy: 99.12%, Grad Norm: 0.22832\n",
      "Epoch [364/10044], Batch [2/7], Loss: 0.1063, Accuracy: 98.41%, Grad Norm: 0.30667\n",
      "Epoch [364/10044], Batch [3/7], Loss: 0.1012, Accuracy: 99.15%, Grad Norm: 0.22941\n",
      "Epoch [364/10044], Batch [4/7], Loss: 0.1002, Accuracy: 99.02%, Grad Norm: 0.21973\n",
      "Epoch [364/10044], Batch [5/7], Loss: 0.0985, Accuracy: 98.73%, Grad Norm: 0.21801\n",
      "Epoch [364/10044], Batch [6/7], Loss: 0.1044, Accuracy: 98.43%, Grad Norm: 0.24317\n",
      "Epoch [364/10044], Batch [7/7], Loss: 0.0793, Accuracy: 98.95%, Grad Norm: 0.27192\n",
      "Epoch [364/10044], Loss: 0.0793\n",
      "Epoch [365/10044], Batch [1/7], Loss: 0.1010, Accuracy: 98.90%, Grad Norm: 0.22875\n",
      "Epoch [365/10044], Batch [2/7], Loss: 0.1065, Accuracy: 98.44%, Grad Norm: 0.25846\n",
      "Epoch [365/10044], Batch [3/7], Loss: 0.0991, Accuracy: 99.08%, Grad Norm: 0.21398\n",
      "Epoch [365/10044], Batch [4/7], Loss: 0.0985, Accuracy: 99.18%, Grad Norm: 0.21179\n",
      "Epoch [365/10044], Batch [5/7], Loss: 0.0977, Accuracy: 98.64%, Grad Norm: 0.21273\n",
      "Epoch [365/10044], Batch [6/7], Loss: 0.1018, Accuracy: 98.63%, Grad Norm: 0.24869\n",
      "Epoch [365/10044], Batch [7/7], Loss: 0.0782, Accuracy: 98.98%, Grad Norm: 0.26488\n",
      "Epoch [365/10044], Loss: 0.0782\n",
      "Epoch [366/10044], Batch [1/7], Loss: 0.1008, Accuracy: 98.92%, Grad Norm: 0.23011\n",
      "Epoch [366/10044], Batch [2/7], Loss: 0.1080, Accuracy: 98.29%, Grad Norm: 0.25569\n",
      "Epoch [366/10044], Batch [3/7], Loss: 0.0973, Accuracy: 99.08%, Grad Norm: 0.23659\n",
      "Epoch [366/10044], Batch [4/7], Loss: 0.0969, Accuracy: 99.15%, Grad Norm: 0.22493\n",
      "Epoch [366/10044], Batch [5/7], Loss: 0.0947, Accuracy: 98.72%, Grad Norm: 0.21631\n",
      "Epoch [366/10044], Batch [6/7], Loss: 0.1012, Accuracy: 98.56%, Grad Norm: 0.23867\n",
      "Epoch [366/10044], Batch [7/7], Loss: 0.0760, Accuracy: 99.05%, Grad Norm: 0.24817\n",
      "Epoch [366/10044], Loss: 0.0760\n",
      "Epoch [367/10044], Batch [1/7], Loss: 0.1017, Accuracy: 98.92%, Grad Norm: 0.24422\n",
      "Epoch [367/10044], Batch [2/7], Loss: 0.1017, Accuracy: 98.49%, Grad Norm: 0.23461\n",
      "Epoch [367/10044], Batch [3/7], Loss: 0.0978, Accuracy: 99.13%, Grad Norm: 0.20368\n",
      "Epoch [367/10044], Batch [4/7], Loss: 0.0962, Accuracy: 99.07%, Grad Norm: 0.21886\n",
      "Epoch [367/10044], Batch [5/7], Loss: 0.0964, Accuracy: 98.81%, Grad Norm: 0.20370\n",
      "Epoch [367/10044], Batch [6/7], Loss: 0.0971, Accuracy: 98.72%, Grad Norm: 0.23397\n",
      "Epoch [367/10044], Batch [7/7], Loss: 0.0724, Accuracy: 98.90%, Grad Norm: 0.26741\n",
      "Epoch [367/10044], Loss: 0.0724\n",
      "Epoch [368/10044], Batch [1/7], Loss: 0.0990, Accuracy: 98.99%, Grad Norm: 0.21087\n",
      "Epoch [368/10044], Batch [2/7], Loss: 0.1013, Accuracy: 98.39%, Grad Norm: 0.24176\n",
      "Epoch [368/10044], Batch [3/7], Loss: 0.0982, Accuracy: 99.01%, Grad Norm: 0.22595\n",
      "Epoch [368/10044], Batch [4/7], Loss: 0.0970, Accuracy: 99.15%, Grad Norm: 0.21561\n",
      "Epoch [368/10044], Batch [5/7], Loss: 0.0963, Accuracy: 98.70%, Grad Norm: 0.22450\n",
      "Epoch [368/10044], Batch [6/7], Loss: 0.1005, Accuracy: 98.40%, Grad Norm: 0.22422\n",
      "Epoch [368/10044], Batch [7/7], Loss: 0.0778, Accuracy: 98.98%, Grad Norm: 0.26265\n",
      "Epoch [368/10044], Loss: 0.0778\n",
      "Epoch [369/10044], Batch [1/7], Loss: 0.0974, Accuracy: 99.09%, Grad Norm: 0.22524\n",
      "Epoch [369/10044], Batch [2/7], Loss: 0.1056, Accuracy: 98.41%, Grad Norm: 0.29031\n",
      "Epoch [369/10044], Batch [3/7], Loss: 0.0948, Accuracy: 99.18%, Grad Norm: 0.23630\n",
      "Epoch [369/10044], Batch [4/7], Loss: 0.0941, Accuracy: 99.12%, Grad Norm: 0.19665\n",
      "Epoch [369/10044], Batch [5/7], Loss: 0.0965, Accuracy: 98.75%, Grad Norm: 0.21589\n",
      "Epoch [369/10044], Batch [6/7], Loss: 0.0987, Accuracy: 98.59%, Grad Norm: 0.21243\n",
      "Epoch [369/10044], Batch [7/7], Loss: 0.0717, Accuracy: 99.12%, Grad Norm: 0.24297\n",
      "Epoch [369/10044], Loss: 0.0717\n",
      "Epoch [370/10044], Batch [1/7], Loss: 0.0967, Accuracy: 99.06%, Grad Norm: 0.24165\n",
      "Epoch [370/10044], Batch [2/7], Loss: 0.0999, Accuracy: 98.45%, Grad Norm: 0.24395\n",
      "Epoch [370/10044], Batch [3/7], Loss: 0.0937, Accuracy: 99.10%, Grad Norm: 0.22501\n",
      "Epoch [370/10044], Batch [4/7], Loss: 0.0937, Accuracy: 99.12%, Grad Norm: 0.18221\n",
      "Epoch [370/10044], Batch [5/7], Loss: 0.0934, Accuracy: 98.82%, Grad Norm: 0.22611\n",
      "Epoch [370/10044], Batch [6/7], Loss: 0.0985, Accuracy: 98.41%, Grad Norm: 0.23898\n",
      "Epoch [370/10044], Batch [7/7], Loss: 0.0728, Accuracy: 98.92%, Grad Norm: 0.24952\n",
      "Epoch [370/10044], Loss: 0.0728\n",
      "Epoch [371/10044], Batch [1/7], Loss: 0.0975, Accuracy: 98.93%, Grad Norm: 0.22979\n",
      "Epoch [371/10044], Batch [2/7], Loss: 0.0978, Accuracy: 98.52%, Grad Norm: 0.20995\n",
      "Epoch [371/10044], Batch [3/7], Loss: 0.0918, Accuracy: 99.29%, Grad Norm: 0.20420\n",
      "Epoch [371/10044], Batch [4/7], Loss: 0.0943, Accuracy: 99.10%, Grad Norm: 0.18601\n",
      "Epoch [371/10044], Batch [5/7], Loss: 0.0956, Accuracy: 98.69%, Grad Norm: 0.24585\n",
      "Epoch [371/10044], Batch [6/7], Loss: 0.0983, Accuracy: 98.52%, Grad Norm: 0.23942\n",
      "Epoch [371/10044], Batch [7/7], Loss: 0.0718, Accuracy: 99.15%, Grad Norm: 0.23208\n",
      "Epoch [371/10044], Loss: 0.0718\n",
      "Epoch [372/10044], Batch [1/7], Loss: 0.0948, Accuracy: 99.05%, Grad Norm: 0.21022\n",
      "Epoch [372/10044], Batch [2/7], Loss: 0.1038, Accuracy: 98.32%, Grad Norm: 0.25727\n",
      "Epoch [372/10044], Batch [3/7], Loss: 0.0912, Accuracy: 99.19%, Grad Norm: 0.20302\n",
      "Epoch [372/10044], Batch [4/7], Loss: 0.0931, Accuracy: 99.06%, Grad Norm: 0.20739\n",
      "Epoch [372/10044], Batch [5/7], Loss: 0.0941, Accuracy: 98.74%, Grad Norm: 0.23148\n",
      "Epoch [372/10044], Batch [6/7], Loss: 0.0963, Accuracy: 98.54%, Grad Norm: 0.23892\n",
      "Epoch [372/10044], Batch [7/7], Loss: 0.0726, Accuracy: 99.07%, Grad Norm: 0.23601\n",
      "Epoch [372/10044], Loss: 0.0726\n",
      "Epoch [373/10044], Batch [1/7], Loss: 0.0952, Accuracy: 99.02%, Grad Norm: 0.24155\n",
      "Epoch [373/10044], Batch [2/7], Loss: 0.1024, Accuracy: 98.27%, Grad Norm: 0.27485\n",
      "Epoch [373/10044], Batch [3/7], Loss: 0.0921, Accuracy: 99.18%, Grad Norm: 0.21625\n",
      "Epoch [373/10044], Batch [4/7], Loss: 0.0925, Accuracy: 99.08%, Grad Norm: 0.20242\n",
      "Epoch [373/10044], Batch [5/7], Loss: 0.0942, Accuracy: 98.65%, Grad Norm: 0.22740\n",
      "Epoch [373/10044], Batch [6/7], Loss: 0.0949, Accuracy: 98.59%, Grad Norm: 0.23169\n",
      "Epoch [373/10044], Batch [7/7], Loss: 0.0722, Accuracy: 98.95%, Grad Norm: 0.27611\n",
      "Epoch [373/10044], Loss: 0.0722\n",
      "Epoch [374/10044], Batch [1/7], Loss: 0.0958, Accuracy: 99.06%, Grad Norm: 0.23956\n",
      "Epoch [374/10044], Batch [2/7], Loss: 0.0986, Accuracy: 98.45%, Grad Norm: 0.24304\n",
      "Epoch [374/10044], Batch [3/7], Loss: 0.0921, Accuracy: 99.22%, Grad Norm: 0.24030\n",
      "Epoch [374/10044], Batch [4/7], Loss: 0.0933, Accuracy: 99.16%, Grad Norm: 0.21031\n",
      "Epoch [374/10044], Batch [5/7], Loss: 0.0944, Accuracy: 98.69%, Grad Norm: 0.30555\n",
      "Epoch [374/10044], Batch [6/7], Loss: 0.0954, Accuracy: 98.60%, Grad Norm: 0.23667\n",
      "Epoch [374/10044], Batch [7/7], Loss: 0.0722, Accuracy: 98.97%, Grad Norm: 0.25395\n",
      "Epoch [374/10044], Loss: 0.0722\n",
      "Epoch [375/10044], Batch [1/7], Loss: 0.0925, Accuracy: 99.15%, Grad Norm: 0.23004\n",
      "Epoch [375/10044], Batch [2/7], Loss: 0.1025, Accuracy: 98.36%, Grad Norm: 0.30042\n",
      "Epoch [375/10044], Batch [3/7], Loss: 0.0907, Accuracy: 99.11%, Grad Norm: 0.21139\n",
      "Epoch [375/10044], Batch [4/7], Loss: 0.0966, Accuracy: 98.97%, Grad Norm: 0.23190\n",
      "Epoch [375/10044], Batch [5/7], Loss: 0.0966, Accuracy: 98.62%, Grad Norm: 0.24701\n",
      "Epoch [375/10044], Batch [6/7], Loss: 0.0958, Accuracy: 98.54%, Grad Norm: 0.23209\n",
      "Epoch [375/10044], Batch [7/7], Loss: 0.0766, Accuracy: 98.83%, Grad Norm: 0.26813\n",
      "Epoch [375/10044], Loss: 0.0766\n",
      "Epoch [376/10044], Batch [1/7], Loss: 0.0932, Accuracy: 98.95%, Grad Norm: 0.28169\n",
      "Epoch [376/10044], Batch [2/7], Loss: 0.1000, Accuracy: 98.20%, Grad Norm: 0.31888\n",
      "Epoch [376/10044], Batch [3/7], Loss: 0.0878, Accuracy: 99.29%, Grad Norm: 0.18408\n",
      "Epoch [376/10044], Batch [4/7], Loss: 0.0901, Accuracy: 99.17%, Grad Norm: 0.20228\n",
      "Epoch [376/10044], Batch [5/7], Loss: 0.0912, Accuracy: 98.71%, Grad Norm: 0.21417\n",
      "Epoch [376/10044], Batch [6/7], Loss: 0.0971, Accuracy: 98.55%, Grad Norm: 0.23409\n",
      "Epoch [376/10044], Batch [7/7], Loss: 0.0712, Accuracy: 99.03%, Grad Norm: 0.24082\n",
      "Epoch [376/10044], Loss: 0.0712\n",
      "Epoch [377/10044], Batch [1/7], Loss: 0.0915, Accuracy: 99.15%, Grad Norm: 0.23577\n",
      "Epoch [377/10044], Batch [2/7], Loss: 0.0998, Accuracy: 98.40%, Grad Norm: 0.25717\n",
      "Epoch [377/10044], Batch [3/7], Loss: 0.0886, Accuracy: 99.27%, Grad Norm: 0.20563\n",
      "Epoch [377/10044], Batch [4/7], Loss: 0.0909, Accuracy: 99.04%, Grad Norm: 0.23346\n",
      "Epoch [377/10044], Batch [5/7], Loss: 0.0925, Accuracy: 98.69%, Grad Norm: 0.22082\n",
      "Epoch [377/10044], Batch [6/7], Loss: 0.0929, Accuracy: 98.64%, Grad Norm: 0.22786\n",
      "Epoch [377/10044], Batch [7/7], Loss: 0.0709, Accuracy: 98.98%, Grad Norm: 0.24857\n",
      "Epoch [377/10044], Loss: 0.0709\n",
      "Epoch [378/10044], Batch [1/7], Loss: 0.0887, Accuracy: 99.09%, Grad Norm: 0.19448\n",
      "Epoch [378/10044], Batch [2/7], Loss: 0.0964, Accuracy: 98.55%, Grad Norm: 0.24238\n",
      "Epoch [378/10044], Batch [3/7], Loss: 0.0897, Accuracy: 99.12%, Grad Norm: 0.21361\n",
      "Epoch [378/10044], Batch [4/7], Loss: 0.0886, Accuracy: 99.14%, Grad Norm: 0.20999\n",
      "Epoch [378/10044], Batch [5/7], Loss: 0.0896, Accuracy: 98.86%, Grad Norm: 0.20768\n",
      "Epoch [378/10044], Batch [6/7], Loss: 0.0944, Accuracy: 98.54%, Grad Norm: 0.22541\n",
      "Epoch [378/10044], Batch [7/7], Loss: 0.0728, Accuracy: 98.95%, Grad Norm: 0.24738\n",
      "Epoch [378/10044], Loss: 0.0728\n",
      "Epoch [379/10044], Batch [1/7], Loss: 0.0932, Accuracy: 99.00%, Grad Norm: 0.21735\n",
      "Epoch [379/10044], Batch [2/7], Loss: 0.0963, Accuracy: 98.45%, Grad Norm: 0.24611\n",
      "Epoch [379/10044], Batch [3/7], Loss: 0.0914, Accuracy: 99.03%, Grad Norm: 0.20754\n",
      "Epoch [379/10044], Batch [4/7], Loss: 0.0886, Accuracy: 99.06%, Grad Norm: 0.18922\n",
      "Epoch [379/10044], Batch [5/7], Loss: 0.0872, Accuracy: 98.81%, Grad Norm: 0.19928\n",
      "Epoch [379/10044], Batch [6/7], Loss: 0.0951, Accuracy: 98.52%, Grad Norm: 0.23768\n",
      "Epoch [379/10044], Batch [7/7], Loss: 0.0672, Accuracy: 99.13%, Grad Norm: 0.19755\n",
      "Epoch [379/10044], Loss: 0.0672\n",
      "Epoch [380/10044], Batch [1/7], Loss: 0.0905, Accuracy: 99.06%, Grad Norm: 0.21474\n",
      "Epoch [380/10044], Batch [2/7], Loss: 0.0928, Accuracy: 98.54%, Grad Norm: 0.22497\n",
      "Epoch [380/10044], Batch [3/7], Loss: 0.0825, Accuracy: 99.36%, Grad Norm: 0.17885\n",
      "Epoch [380/10044], Batch [4/7], Loss: 0.0878, Accuracy: 99.10%, Grad Norm: 0.18649\n",
      "Epoch [380/10044], Batch [5/7], Loss: 0.0874, Accuracy: 98.84%, Grad Norm: 0.20583\n",
      "Epoch [380/10044], Batch [6/7], Loss: 0.0924, Accuracy: 98.60%, Grad Norm: 0.21476\n",
      "Epoch [380/10044], Batch [7/7], Loss: 0.0654, Accuracy: 99.23%, Grad Norm: 0.21049\n",
      "Epoch [380/10044], Loss: 0.0654\n",
      "Epoch [381/10044], Batch [1/7], Loss: 0.0905, Accuracy: 99.03%, Grad Norm: 0.21517\n",
      "Epoch [381/10044], Batch [2/7], Loss: 0.0932, Accuracy: 98.51%, Grad Norm: 0.21674\n",
      "Epoch [381/10044], Batch [3/7], Loss: 0.0856, Accuracy: 99.27%, Grad Norm: 0.20578\n",
      "Epoch [381/10044], Batch [4/7], Loss: 0.0871, Accuracy: 99.18%, Grad Norm: 0.19021\n",
      "Epoch [381/10044], Batch [5/7], Loss: 0.0866, Accuracy: 98.88%, Grad Norm: 0.18488\n",
      "Epoch [381/10044], Batch [6/7], Loss: 0.0896, Accuracy: 98.67%, Grad Norm: 0.20519\n",
      "Epoch [381/10044], Batch [7/7], Loss: 0.0664, Accuracy: 98.93%, Grad Norm: 0.23403\n",
      "Epoch [381/10044], Loss: 0.0664\n",
      "Epoch [382/10044], Batch [1/7], Loss: 0.0874, Accuracy: 99.10%, Grad Norm: 0.21071\n",
      "Epoch [382/10044], Batch [2/7], Loss: 0.0940, Accuracy: 98.47%, Grad Norm: 0.23037\n",
      "Epoch [382/10044], Batch [3/7], Loss: 0.0841, Accuracy: 99.21%, Grad Norm: 0.19009\n",
      "Epoch [382/10044], Batch [4/7], Loss: 0.0859, Accuracy: 99.09%, Grad Norm: 0.18337\n",
      "Epoch [382/10044], Batch [5/7], Loss: 0.0888, Accuracy: 98.68%, Grad Norm: 0.22437\n",
      "Epoch [382/10044], Batch [6/7], Loss: 0.0918, Accuracy: 98.66%, Grad Norm: 0.22883\n",
      "Epoch [382/10044], Batch [7/7], Loss: 0.0661, Accuracy: 99.13%, Grad Norm: 0.24033\n",
      "Epoch [382/10044], Loss: 0.0661\n",
      "Epoch [383/10044], Batch [1/7], Loss: 0.0856, Accuracy: 99.07%, Grad Norm: 0.20540\n",
      "Epoch [383/10044], Batch [2/7], Loss: 0.0922, Accuracy: 98.52%, Grad Norm: 0.21742\n",
      "Epoch [383/10044], Batch [3/7], Loss: 0.0835, Accuracy: 99.22%, Grad Norm: 0.19934\n",
      "Epoch [383/10044], Batch [4/7], Loss: 0.0866, Accuracy: 99.08%, Grad Norm: 0.20187\n",
      "Epoch [383/10044], Batch [5/7], Loss: 0.0878, Accuracy: 98.79%, Grad Norm: 0.23092\n",
      "Epoch [383/10044], Batch [6/7], Loss: 0.0885, Accuracy: 98.60%, Grad Norm: 0.19240\n",
      "Epoch [383/10044], Batch [7/7], Loss: 0.0681, Accuracy: 99.00%, Grad Norm: 0.24926\n",
      "Epoch [383/10044], Loss: 0.0681\n",
      "Epoch [384/10044], Batch [1/7], Loss: 0.0870, Accuracy: 99.04%, Grad Norm: 0.21769\n",
      "Epoch [384/10044], Batch [2/7], Loss: 0.0917, Accuracy: 98.53%, Grad Norm: 0.24342\n",
      "Epoch [384/10044], Batch [3/7], Loss: 0.0870, Accuracy: 99.22%, Grad Norm: 0.21946\n",
      "Epoch [384/10044], Batch [4/7], Loss: 0.0853, Accuracy: 99.11%, Grad Norm: 0.19887\n",
      "Epoch [384/10044], Batch [5/7], Loss: 0.0869, Accuracy: 98.71%, Grad Norm: 0.21580\n",
      "Epoch [384/10044], Batch [6/7], Loss: 0.0909, Accuracy: 98.58%, Grad Norm: 0.21078\n",
      "Epoch [384/10044], Batch [7/7], Loss: 0.0689, Accuracy: 99.00%, Grad Norm: 0.25340\n",
      "Epoch [384/10044], Loss: 0.0689\n",
      "Epoch [385/10044], Batch [1/7], Loss: 0.0867, Accuracy: 99.06%, Grad Norm: 0.22430\n",
      "Epoch [385/10044], Batch [2/7], Loss: 0.0938, Accuracy: 98.38%, Grad Norm: 0.27761\n",
      "Epoch [385/10044], Batch [3/7], Loss: 0.0835, Accuracy: 99.25%, Grad Norm: 0.19806\n",
      "Epoch [385/10044], Batch [4/7], Loss: 0.0867, Accuracy: 99.12%, Grad Norm: 0.18912\n",
      "Epoch [385/10044], Batch [5/7], Loss: 0.0860, Accuracy: 98.81%, Grad Norm: 0.20621\n",
      "Epoch [385/10044], Batch [6/7], Loss: 0.0879, Accuracy: 98.77%, Grad Norm: 0.21016\n",
      "Epoch [385/10044], Batch [7/7], Loss: 0.0669, Accuracy: 98.90%, Grad Norm: 0.24012\n",
      "Epoch [385/10044], Loss: 0.0669\n",
      "Epoch [386/10044], Batch [1/7], Loss: 0.0857, Accuracy: 99.12%, Grad Norm: 0.22392\n",
      "Epoch [386/10044], Batch [2/7], Loss: 0.0944, Accuracy: 98.37%, Grad Norm: 0.29374\n",
      "Epoch [386/10044], Batch [3/7], Loss: 0.0844, Accuracy: 99.26%, Grad Norm: 0.21390\n",
      "Epoch [386/10044], Batch [4/7], Loss: 0.0850, Accuracy: 99.12%, Grad Norm: 0.20805\n",
      "Epoch [386/10044], Batch [5/7], Loss: 0.0844, Accuracy: 98.85%, Grad Norm: 0.21393\n",
      "Epoch [386/10044], Batch [6/7], Loss: 0.0937, Accuracy: 98.36%, Grad Norm: 0.26532\n",
      "Epoch [386/10044], Batch [7/7], Loss: 0.0661, Accuracy: 99.10%, Grad Norm: 0.23026\n",
      "Epoch [386/10044], Loss: 0.0661\n",
      "Epoch [387/10044], Batch [1/7], Loss: 0.0823, Accuracy: 99.16%, Grad Norm: 0.22382\n",
      "Epoch [387/10044], Batch [2/7], Loss: 0.0937, Accuracy: 98.44%, Grad Norm: 0.25696\n",
      "Epoch [387/10044], Batch [3/7], Loss: 0.0797, Accuracy: 99.29%, Grad Norm: 0.20130\n",
      "Epoch [387/10044], Batch [4/7], Loss: 0.0828, Accuracy: 99.17%, Grad Norm: 0.18676\n",
      "Epoch [387/10044], Batch [5/7], Loss: 0.0849, Accuracy: 98.97%, Grad Norm: 0.21065\n",
      "Epoch [387/10044], Batch [6/7], Loss: 0.0854, Accuracy: 98.81%, Grad Norm: 0.22598\n",
      "Epoch [387/10044], Batch [7/7], Loss: 0.0671, Accuracy: 99.08%, Grad Norm: 0.25901\n",
      "Epoch [387/10044], Loss: 0.0671\n",
      "Epoch [388/10044], Batch [1/7], Loss: 0.0845, Accuracy: 99.07%, Grad Norm: 0.21945\n",
      "Epoch [388/10044], Batch [2/7], Loss: 0.0890, Accuracy: 98.63%, Grad Norm: 0.23572\n",
      "Epoch [388/10044], Batch [3/7], Loss: 0.0815, Accuracy: 99.19%, Grad Norm: 0.21069\n",
      "Epoch [388/10044], Batch [4/7], Loss: 0.0834, Accuracy: 99.22%, Grad Norm: 0.18444\n",
      "Epoch [388/10044], Batch [5/7], Loss: 0.0815, Accuracy: 98.94%, Grad Norm: 0.19928\n",
      "Epoch [388/10044], Batch [6/7], Loss: 0.0869, Accuracy: 98.68%, Grad Norm: 0.19889\n",
      "Epoch [388/10044], Batch [7/7], Loss: 0.0674, Accuracy: 99.10%, Grad Norm: 0.26361\n",
      "Epoch [388/10044], Loss: 0.0674\n",
      "Epoch [389/10044], Batch [1/7], Loss: 0.0831, Accuracy: 99.22%, Grad Norm: 0.20460\n",
      "Epoch [389/10044], Batch [2/7], Loss: 0.0886, Accuracy: 98.57%, Grad Norm: 0.23746\n",
      "Epoch [389/10044], Batch [3/7], Loss: 0.0812, Accuracy: 99.21%, Grad Norm: 0.19744\n",
      "Epoch [389/10044], Batch [4/7], Loss: 0.0814, Accuracy: 99.22%, Grad Norm: 0.19131\n",
      "Epoch [389/10044], Batch [5/7], Loss: 0.0824, Accuracy: 98.99%, Grad Norm: 0.20433\n",
      "Epoch [389/10044], Batch [6/7], Loss: 0.0864, Accuracy: 98.69%, Grad Norm: 0.21515\n",
      "Epoch [389/10044], Batch [7/7], Loss: 0.0644, Accuracy: 99.05%, Grad Norm: 0.23438\n",
      "Epoch [389/10044], Loss: 0.0644\n",
      "Epoch [390/10044], Batch [1/7], Loss: 0.0812, Accuracy: 99.21%, Grad Norm: 0.20687\n",
      "Epoch [390/10044], Batch [2/7], Loss: 0.0862, Accuracy: 98.48%, Grad Norm: 0.22123\n",
      "Epoch [390/10044], Batch [3/7], Loss: 0.0795, Accuracy: 99.30%, Grad Norm: 0.18776\n",
      "Epoch [390/10044], Batch [4/7], Loss: 0.0818, Accuracy: 99.22%, Grad Norm: 0.19600\n",
      "Epoch [390/10044], Batch [5/7], Loss: 0.0830, Accuracy: 98.83%, Grad Norm: 0.21802\n",
      "Epoch [390/10044], Batch [6/7], Loss: 0.0839, Accuracy: 98.77%, Grad Norm: 0.21559\n",
      "Epoch [390/10044], Batch [7/7], Loss: 0.0654, Accuracy: 99.08%, Grad Norm: 0.23885\n",
      "Epoch [390/10044], Loss: 0.0654\n",
      "Epoch [391/10044], Batch [1/7], Loss: 0.0834, Accuracy: 99.02%, Grad Norm: 0.22262\n",
      "Epoch [391/10044], Batch [2/7], Loss: 0.0868, Accuracy: 98.58%, Grad Norm: 0.22750\n",
      "Epoch [391/10044], Batch [3/7], Loss: 0.0768, Accuracy: 99.34%, Grad Norm: 0.17934\n",
      "Epoch [391/10044], Batch [4/7], Loss: 0.0823, Accuracy: 99.18%, Grad Norm: 0.20374\n",
      "Epoch [391/10044], Batch [5/7], Loss: 0.0798, Accuracy: 98.93%, Grad Norm: 0.20568\n",
      "Epoch [391/10044], Batch [6/7], Loss: 0.0887, Accuracy: 98.52%, Grad Norm: 0.20893\n",
      "Epoch [391/10044], Batch [7/7], Loss: 0.0630, Accuracy: 99.07%, Grad Norm: 0.21744\n",
      "Epoch [391/10044], Loss: 0.0630\n",
      "Epoch [392/10044], Batch [1/7], Loss: 0.0816, Accuracy: 99.22%, Grad Norm: 0.21142\n",
      "Epoch [392/10044], Batch [2/7], Loss: 0.0882, Accuracy: 98.54%, Grad Norm: 0.23585\n",
      "Epoch [392/10044], Batch [3/7], Loss: 0.0790, Accuracy: 99.28%, Grad Norm: 0.19245\n",
      "Epoch [392/10044], Batch [4/7], Loss: 0.0829, Accuracy: 99.12%, Grad Norm: 0.22158\n",
      "Epoch [392/10044], Batch [5/7], Loss: 0.0832, Accuracy: 98.85%, Grad Norm: 0.22854\n",
      "Epoch [392/10044], Batch [6/7], Loss: 0.0851, Accuracy: 98.73%, Grad Norm: 0.20277\n",
      "Epoch [392/10044], Batch [7/7], Loss: 0.0656, Accuracy: 98.92%, Grad Norm: 0.24299\n",
      "Epoch [392/10044], Loss: 0.0656\n",
      "Epoch [393/10044], Batch [1/7], Loss: 0.0839, Accuracy: 99.05%, Grad Norm: 0.23221\n",
      "Epoch [393/10044], Batch [2/7], Loss: 0.0871, Accuracy: 98.58%, Grad Norm: 0.24113\n",
      "Epoch [393/10044], Batch [3/7], Loss: 0.0779, Accuracy: 99.30%, Grad Norm: 0.18614\n",
      "Epoch [393/10044], Batch [4/7], Loss: 0.0778, Accuracy: 99.24%, Grad Norm: 0.18063\n",
      "Epoch [393/10044], Batch [5/7], Loss: 0.0836, Accuracy: 98.72%, Grad Norm: 0.25510\n",
      "Epoch [393/10044], Batch [6/7], Loss: 0.0835, Accuracy: 98.67%, Grad Norm: 0.26388\n",
      "Epoch [393/10044], Batch [7/7], Loss: 0.0621, Accuracy: 99.32%, Grad Norm: 0.21794\n",
      "Epoch [393/10044], Loss: 0.0621\n",
      "Epoch [394/10044], Batch [1/7], Loss: 0.0811, Accuracy: 99.17%, Grad Norm: 0.23816\n",
      "Epoch [394/10044], Batch [2/7], Loss: 0.0894, Accuracy: 98.45%, Grad Norm: 0.24432\n",
      "Epoch [394/10044], Batch [3/7], Loss: 0.0775, Accuracy: 99.24%, Grad Norm: 0.19468\n",
      "Epoch [394/10044], Batch [4/7], Loss: 0.0811, Accuracy: 99.07%, Grad Norm: 0.20655\n",
      "Epoch [394/10044], Batch [5/7], Loss: 0.0813, Accuracy: 98.82%, Grad Norm: 0.26072\n",
      "Epoch [394/10044], Batch [6/7], Loss: 0.0863, Accuracy: 98.63%, Grad Norm: 0.24182\n",
      "Epoch [394/10044], Batch [7/7], Loss: 0.0656, Accuracy: 99.02%, Grad Norm: 0.27441\n",
      "Epoch [394/10044], Loss: 0.0656\n",
      "Epoch [395/10044], Batch [1/7], Loss: 0.0813, Accuracy: 99.14%, Grad Norm: 0.20960\n",
      "Epoch [395/10044], Batch [2/7], Loss: 0.0866, Accuracy: 98.69%, Grad Norm: 0.24763\n",
      "Epoch [395/10044], Batch [3/7], Loss: 0.0770, Accuracy: 99.34%, Grad Norm: 0.20095\n",
      "Epoch [395/10044], Batch [4/7], Loss: 0.0790, Accuracy: 99.22%, Grad Norm: 0.21070\n",
      "Epoch [395/10044], Batch [5/7], Loss: 0.0775, Accuracy: 98.94%, Grad Norm: 0.22583\n",
      "Epoch [395/10044], Batch [6/7], Loss: 0.0851, Accuracy: 98.66%, Grad Norm: 0.23596\n",
      "Epoch [395/10044], Batch [7/7], Loss: 0.0615, Accuracy: 99.12%, Grad Norm: 0.21480\n",
      "Epoch [395/10044], Loss: 0.0615\n",
      "Epoch [396/10044], Batch [1/7], Loss: 0.0827, Accuracy: 99.06%, Grad Norm: 0.23634\n",
      "Epoch [396/10044], Batch [2/7], Loss: 0.0888, Accuracy: 98.59%, Grad Norm: 0.24369\n",
      "Epoch [396/10044], Batch [3/7], Loss: 0.0779, Accuracy: 99.27%, Grad Norm: 0.20514\n",
      "Epoch [396/10044], Batch [4/7], Loss: 0.0804, Accuracy: 99.17%, Grad Norm: 0.19186\n",
      "Epoch [396/10044], Batch [5/7], Loss: 0.0789, Accuracy: 98.88%, Grad Norm: 0.20769\n",
      "Epoch [396/10044], Batch [6/7], Loss: 0.0851, Accuracy: 98.58%, Grad Norm: 0.22945\n",
      "Epoch [396/10044], Batch [7/7], Loss: 0.0607, Accuracy: 99.05%, Grad Norm: 0.23920\n",
      "Epoch [396/10044], Loss: 0.0607\n",
      "Epoch [397/10044], Batch [1/7], Loss: 0.0784, Accuracy: 99.14%, Grad Norm: 0.21066\n",
      "Epoch [397/10044], Batch [2/7], Loss: 0.0829, Accuracy: 98.73%, Grad Norm: 0.25019\n",
      "Epoch [397/10044], Batch [3/7], Loss: 0.0760, Accuracy: 99.32%, Grad Norm: 0.20457\n",
      "Epoch [397/10044], Batch [4/7], Loss: 0.0782, Accuracy: 99.17%, Grad Norm: 0.18667\n",
      "Epoch [397/10044], Batch [5/7], Loss: 0.0796, Accuracy: 98.82%, Grad Norm: 0.22257\n",
      "Epoch [397/10044], Batch [6/7], Loss: 0.0824, Accuracy: 98.61%, Grad Norm: 0.21624\n",
      "Epoch [397/10044], Batch [7/7], Loss: 0.0656, Accuracy: 99.00%, Grad Norm: 0.24938\n",
      "Epoch [397/10044], Loss: 0.0656\n",
      "Epoch [398/10044], Batch [1/7], Loss: 0.0798, Accuracy: 99.11%, Grad Norm: 0.22782\n",
      "Epoch [398/10044], Batch [2/7], Loss: 0.0857, Accuracy: 98.64%, Grad Norm: 0.26264\n",
      "Epoch [398/10044], Batch [3/7], Loss: 0.0757, Accuracy: 99.32%, Grad Norm: 0.22328\n",
      "Epoch [398/10044], Batch [4/7], Loss: 0.0787, Accuracy: 99.12%, Grad Norm: 0.18788\n",
      "Epoch [398/10044], Batch [5/7], Loss: 0.0774, Accuracy: 98.88%, Grad Norm: 0.23559\n",
      "Epoch [398/10044], Batch [6/7], Loss: 0.0822, Accuracy: 98.64%, Grad Norm: 0.28300\n",
      "Epoch [398/10044], Batch [7/7], Loss: 0.0609, Accuracy: 99.13%, Grad Norm: 0.23822\n",
      "Epoch [398/10044], Loss: 0.0609\n",
      "Epoch [399/10044], Batch [1/7], Loss: 0.0800, Accuracy: 99.03%, Grad Norm: 0.21291\n",
      "Epoch [399/10044], Batch [2/7], Loss: 0.0872, Accuracy: 98.56%, Grad Norm: 0.23736\n",
      "Epoch [399/10044], Batch [3/7], Loss: 0.0750, Accuracy: 99.34%, Grad Norm: 0.20619\n",
      "Epoch [399/10044], Batch [4/7], Loss: 0.0789, Accuracy: 99.13%, Grad Norm: 0.20748\n",
      "Epoch [399/10044], Batch [5/7], Loss: 0.0797, Accuracy: 98.77%, Grad Norm: 0.24361\n",
      "Epoch [399/10044], Batch [6/7], Loss: 0.0820, Accuracy: 98.62%, Grad Norm: 0.25642\n",
      "Epoch [399/10044], Batch [7/7], Loss: 0.0582, Accuracy: 99.22%, Grad Norm: 0.19866\n",
      "Epoch [399/10044], Loss: 0.0582\n",
      "Epoch [400/10044], Batch [1/7], Loss: 0.0794, Accuracy: 99.15%, Grad Norm: 0.20921\n",
      "Epoch [400/10044], Batch [2/7], Loss: 0.0866, Accuracy: 98.59%, Grad Norm: 0.32544\n",
      "Epoch [400/10044], Batch [3/7], Loss: 0.0772, Accuracy: 99.24%, Grad Norm: 0.23139\n",
      "Epoch [400/10044], Batch [4/7], Loss: 0.0788, Accuracy: 99.13%, Grad Norm: 0.21512\n",
      "Epoch [400/10044], Batch [5/7], Loss: 0.0756, Accuracy: 98.92%, Grad Norm: 0.18835\n",
      "Epoch [400/10044], Batch [6/7], Loss: 0.0823, Accuracy: 98.72%, Grad Norm: 0.23675\n",
      "Epoch [400/10044], Batch [7/7], Loss: 0.0598, Accuracy: 99.08%, Grad Norm: 0.21202\n",
      "Epoch [400/10044], Loss: 0.0598\n",
      "Epoch [401/10044], Batch [1/7], Loss: 0.0789, Accuracy: 99.09%, Grad Norm: 0.25904\n",
      "Epoch [401/10044], Batch [2/7], Loss: 0.0833, Accuracy: 98.50%, Grad Norm: 0.27651\n",
      "Epoch [401/10044], Batch [3/7], Loss: 0.0732, Accuracy: 99.27%, Grad Norm: 0.21260\n",
      "Epoch [401/10044], Batch [4/7], Loss: 0.0760, Accuracy: 99.12%, Grad Norm: 0.18621\n",
      "Epoch [401/10044], Batch [5/7], Loss: 0.0772, Accuracy: 98.94%, Grad Norm: 0.22457\n",
      "Epoch [401/10044], Batch [6/7], Loss: 0.0810, Accuracy: 98.72%, Grad Norm: 0.23776\n",
      "Epoch [401/10044], Batch [7/7], Loss: 0.0667, Accuracy: 98.85%, Grad Norm: 0.25751\n",
      "Epoch [401/10044], Loss: 0.0667\n",
      "Epoch [402/10044], Batch [1/7], Loss: 0.0781, Accuracy: 99.07%, Grad Norm: 0.24493\n",
      "Epoch [402/10044], Batch [2/7], Loss: 0.0858, Accuracy: 98.37%, Grad Norm: 0.25395\n",
      "Epoch [402/10044], Batch [3/7], Loss: 0.0721, Accuracy: 99.38%, Grad Norm: 0.19613\n",
      "Epoch [402/10044], Batch [4/7], Loss: 0.0778, Accuracy: 99.15%, Grad Norm: 0.21992\n",
      "Epoch [402/10044], Batch [5/7], Loss: 0.0791, Accuracy: 98.75%, Grad Norm: 0.23231\n",
      "Epoch [402/10044], Batch [6/7], Loss: 0.0841, Accuracy: 98.63%, Grad Norm: 0.24989\n",
      "Epoch [402/10044], Batch [7/7], Loss: 0.0596, Accuracy: 99.15%, Grad Norm: 0.22857\n",
      "Epoch [402/10044], Loss: 0.0596\n",
      "Epoch [403/10044], Batch [1/7], Loss: 0.0756, Accuracy: 99.17%, Grad Norm: 0.22362\n",
      "Epoch [403/10044], Batch [2/7], Loss: 0.0856, Accuracy: 98.51%, Grad Norm: 0.31396\n",
      "Epoch [403/10044], Batch [3/7], Loss: 0.0729, Accuracy: 99.36%, Grad Norm: 0.21641\n",
      "Epoch [403/10044], Batch [4/7], Loss: 0.0761, Accuracy: 99.11%, Grad Norm: 0.24547\n",
      "Epoch [403/10044], Batch [5/7], Loss: 0.0757, Accuracy: 98.92%, Grad Norm: 0.24279\n",
      "Epoch [403/10044], Batch [6/7], Loss: 0.0822, Accuracy: 98.65%, Grad Norm: 0.23621\n",
      "Epoch [403/10044], Batch [7/7], Loss: 0.0580, Accuracy: 99.00%, Grad Norm: 0.24526\n",
      "Epoch [403/10044], Loss: 0.0580\n",
      "Epoch [404/10044], Batch [1/7], Loss: 0.0769, Accuracy: 99.17%, Grad Norm: 0.26324\n",
      "Epoch [404/10044], Batch [2/7], Loss: 0.0828, Accuracy: 98.62%, Grad Norm: 0.29692\n",
      "Epoch [404/10044], Batch [3/7], Loss: 0.0728, Accuracy: 99.27%, Grad Norm: 0.21960\n",
      "Epoch [404/10044], Batch [4/7], Loss: 0.0764, Accuracy: 99.27%, Grad Norm: 0.22468\n",
      "Epoch [404/10044], Batch [5/7], Loss: 0.0752, Accuracy: 98.96%, Grad Norm: 0.20720\n",
      "Epoch [404/10044], Batch [6/7], Loss: 0.0814, Accuracy: 98.73%, Grad Norm: 0.24714\n",
      "Epoch [404/10044], Batch [7/7], Loss: 0.0617, Accuracy: 99.10%, Grad Norm: 0.31448\n",
      "Epoch [404/10044], Loss: 0.0617\n",
      "Epoch [405/10044], Batch [1/7], Loss: 0.0803, Accuracy: 98.88%, Grad Norm: 0.33787\n",
      "Epoch [405/10044], Batch [2/7], Loss: 0.0825, Accuracy: 98.62%, Grad Norm: 0.28288\n",
      "Epoch [405/10044], Batch [3/7], Loss: 0.0738, Accuracy: 99.25%, Grad Norm: 0.21097\n",
      "Epoch [405/10044], Batch [4/7], Loss: 0.0761, Accuracy: 99.12%, Grad Norm: 0.22254\n",
      "Epoch [405/10044], Batch [5/7], Loss: 0.0784, Accuracy: 98.80%, Grad Norm: 0.30511\n",
      "Epoch [405/10044], Batch [6/7], Loss: 0.0839, Accuracy: 98.51%, Grad Norm: 0.30381\n",
      "Epoch [405/10044], Batch [7/7], Loss: 0.0605, Accuracy: 99.05%, Grad Norm: 0.27184\n",
      "Epoch [405/10044], Loss: 0.0605\n",
      "Epoch [406/10044], Batch [1/7], Loss: 0.0786, Accuracy: 99.05%, Grad Norm: 0.25448\n",
      "Epoch [406/10044], Batch [2/7], Loss: 0.0861, Accuracy: 98.57%, Grad Norm: 0.29129\n",
      "Epoch [406/10044], Batch [3/7], Loss: 0.0751, Accuracy: 99.30%, Grad Norm: 0.25530\n",
      "Epoch [406/10044], Batch [4/7], Loss: 0.0778, Accuracy: 99.11%, Grad Norm: 0.26434\n",
      "Epoch [406/10044], Batch [5/7], Loss: 0.0791, Accuracy: 98.90%, Grad Norm: 0.29491\n",
      "Epoch [406/10044], Batch [6/7], Loss: 0.0823, Accuracy: 98.66%, Grad Norm: 0.28963\n",
      "Epoch [406/10044], Batch [7/7], Loss: 0.0611, Accuracy: 99.02%, Grad Norm: 0.28511\n",
      "Epoch [406/10044], Loss: 0.0611\n",
      "Epoch [407/10044], Batch [1/7], Loss: 0.0785, Accuracy: 99.10%, Grad Norm: 0.25106\n",
      "Epoch [407/10044], Batch [2/7], Loss: 0.0856, Accuracy: 98.50%, Grad Norm: 0.30888\n",
      "Epoch [407/10044], Batch [3/7], Loss: 0.0740, Accuracy: 99.31%, Grad Norm: 0.24088\n",
      "Epoch [407/10044], Batch [4/7], Loss: 0.0772, Accuracy: 99.13%, Grad Norm: 0.24665\n",
      "Epoch [407/10044], Batch [5/7], Loss: 0.0780, Accuracy: 98.98%, Grad Norm: 0.26947\n",
      "Epoch [407/10044], Batch [6/7], Loss: 0.0798, Accuracy: 98.82%, Grad Norm: 0.23109\n",
      "Epoch [407/10044], Batch [7/7], Loss: 0.0607, Accuracy: 99.08%, Grad Norm: 0.26592\n",
      "Epoch [407/10044], Loss: 0.0607\n",
      "Epoch [408/10044], Batch [1/7], Loss: 0.0742, Accuracy: 99.18%, Grad Norm: 0.24309\n",
      "Epoch [408/10044], Batch [2/7], Loss: 0.0858, Accuracy: 98.52%, Grad Norm: 0.30881\n",
      "Epoch [408/10044], Batch [3/7], Loss: 0.0722, Accuracy: 99.31%, Grad Norm: 0.21611\n",
      "Epoch [408/10044], Batch [4/7], Loss: 0.0757, Accuracy: 99.17%, Grad Norm: 0.22189\n",
      "Epoch [408/10044], Batch [5/7], Loss: 0.0713, Accuracy: 99.09%, Grad Norm: 0.24904\n",
      "Epoch [408/10044], Batch [6/7], Loss: 0.0780, Accuracy: 98.73%, Grad Norm: 0.22279\n",
      "Epoch [408/10044], Batch [7/7], Loss: 0.0584, Accuracy: 99.15%, Grad Norm: 0.21850\n",
      "Epoch [408/10044], Loss: 0.0584\n",
      "Epoch [409/10044], Batch [1/7], Loss: 0.0763, Accuracy: 99.10%, Grad Norm: 0.24040\n",
      "Epoch [409/10044], Batch [2/7], Loss: 0.0816, Accuracy: 98.52%, Grad Norm: 0.27010\n",
      "Epoch [409/10044], Batch [3/7], Loss: 0.0686, Accuracy: 99.54%, Grad Norm: 0.20223\n",
      "Epoch [409/10044], Batch [4/7], Loss: 0.0738, Accuracy: 99.27%, Grad Norm: 0.20055\n",
      "Epoch [409/10044], Batch [5/7], Loss: 0.0720, Accuracy: 99.00%, Grad Norm: 0.22565\n",
      "Epoch [409/10044], Batch [6/7], Loss: 0.0764, Accuracy: 98.75%, Grad Norm: 0.20443\n",
      "Epoch [409/10044], Batch [7/7], Loss: 0.0568, Accuracy: 99.13%, Grad Norm: 0.22496\n",
      "Epoch [409/10044], Loss: 0.0568\n",
      "Epoch [410/10044], Batch [1/7], Loss: 0.0748, Accuracy: 99.17%, Grad Norm: 0.22611\n",
      "Epoch [410/10044], Batch [2/7], Loss: 0.0788, Accuracy: 98.69%, Grad Norm: 0.24800\n",
      "Epoch [410/10044], Batch [3/7], Loss: 0.0704, Accuracy: 99.44%, Grad Norm: 0.19181\n",
      "Epoch [410/10044], Batch [4/7], Loss: 0.0712, Accuracy: 99.32%, Grad Norm: 0.19377\n",
      "Epoch [410/10044], Batch [5/7], Loss: 0.0726, Accuracy: 98.96%, Grad Norm: 0.20760\n",
      "Epoch [410/10044], Batch [6/7], Loss: 0.0746, Accuracy: 98.82%, Grad Norm: 0.20637\n",
      "Epoch [410/10044], Batch [7/7], Loss: 0.0580, Accuracy: 99.03%, Grad Norm: 0.23485\n",
      "Epoch [410/10044], Loss: 0.0580\n",
      "Epoch [411/10044], Batch [1/7], Loss: 0.0732, Accuracy: 99.22%, Grad Norm: 0.20980\n",
      "Epoch [411/10044], Batch [2/7], Loss: 0.0783, Accuracy: 98.63%, Grad Norm: 0.23909\n",
      "Epoch [411/10044], Batch [3/7], Loss: 0.0697, Accuracy: 99.37%, Grad Norm: 0.19031\n",
      "Epoch [411/10044], Batch [4/7], Loss: 0.0717, Accuracy: 99.22%, Grad Norm: 0.18903\n",
      "Epoch [411/10044], Batch [5/7], Loss: 0.0676, Accuracy: 99.02%, Grad Norm: 0.17682\n",
      "Epoch [411/10044], Batch [6/7], Loss: 0.0757, Accuracy: 98.70%, Grad Norm: 0.21811\n",
      "Epoch [411/10044], Batch [7/7], Loss: 0.0591, Accuracy: 99.03%, Grad Norm: 0.22565\n",
      "Epoch [411/10044], Loss: 0.0591\n",
      "Epoch [412/10044], Batch [1/7], Loss: 0.0714, Accuracy: 99.24%, Grad Norm: 0.19834\n",
      "Epoch [412/10044], Batch [2/7], Loss: 0.0787, Accuracy: 98.66%, Grad Norm: 0.24867\n",
      "Epoch [412/10044], Batch [3/7], Loss: 0.0671, Accuracy: 99.44%, Grad Norm: 0.17790\n",
      "Epoch [412/10044], Batch [4/7], Loss: 0.0710, Accuracy: 99.19%, Grad Norm: 0.17547\n",
      "Epoch [412/10044], Batch [5/7], Loss: 0.0687, Accuracy: 98.94%, Grad Norm: 0.18695\n",
      "Epoch [412/10044], Batch [6/7], Loss: 0.0739, Accuracy: 98.83%, Grad Norm: 0.18439\n",
      "Epoch [412/10044], Batch [7/7], Loss: 0.0567, Accuracy: 99.13%, Grad Norm: 0.22543\n",
      "Epoch [412/10044], Loss: 0.0567\n",
      "Epoch [413/10044], Batch [1/7], Loss: 0.0704, Accuracy: 99.20%, Grad Norm: 0.19678\n",
      "Epoch [413/10044], Batch [2/7], Loss: 0.0772, Accuracy: 98.70%, Grad Norm: 0.24772\n",
      "Epoch [413/10044], Batch [3/7], Loss: 0.0669, Accuracy: 99.34%, Grad Norm: 0.17796\n",
      "Epoch [413/10044], Batch [4/7], Loss: 0.0709, Accuracy: 99.21%, Grad Norm: 0.17776\n",
      "Epoch [413/10044], Batch [5/7], Loss: 0.0700, Accuracy: 99.02%, Grad Norm: 0.19163\n",
      "Epoch [413/10044], Batch [6/7], Loss: 0.0716, Accuracy: 98.85%, Grad Norm: 0.19557\n",
      "Epoch [413/10044], Batch [7/7], Loss: 0.0591, Accuracy: 99.17%, Grad Norm: 0.24412\n",
      "Epoch [413/10044], Loss: 0.0591\n",
      "Epoch [414/10044], Batch [1/7], Loss: 0.0712, Accuracy: 99.22%, Grad Norm: 0.22297\n",
      "Epoch [414/10044], Batch [2/7], Loss: 0.0803, Accuracy: 98.61%, Grad Norm: 0.24045\n",
      "Epoch [414/10044], Batch [3/7], Loss: 0.0702, Accuracy: 99.25%, Grad Norm: 0.18669\n",
      "Epoch [414/10044], Batch [4/7], Loss: 0.0705, Accuracy: 99.20%, Grad Norm: 0.17417\n",
      "Epoch [414/10044], Batch [5/7], Loss: 0.0686, Accuracy: 99.07%, Grad Norm: 0.22467\n",
      "Epoch [414/10044], Batch [6/7], Loss: 0.0742, Accuracy: 98.78%, Grad Norm: 0.25250\n",
      "Epoch [414/10044], Batch [7/7], Loss: 0.0546, Accuracy: 99.12%, Grad Norm: 0.23681\n",
      "Epoch [414/10044], Loss: 0.0546\n",
      "Epoch [415/10044], Batch [1/7], Loss: 0.0702, Accuracy: 99.25%, Grad Norm: 0.20358\n",
      "Epoch [415/10044], Batch [2/7], Loss: 0.0760, Accuracy: 98.73%, Grad Norm: 0.20774\n",
      "Epoch [415/10044], Batch [3/7], Loss: 0.0666, Accuracy: 99.28%, Grad Norm: 0.18435\n",
      "Epoch [415/10044], Batch [4/7], Loss: 0.0726, Accuracy: 99.22%, Grad Norm: 0.20514\n",
      "Epoch [415/10044], Batch [5/7], Loss: 0.0739, Accuracy: 98.95%, Grad Norm: 0.24684\n",
      "Epoch [415/10044], Batch [6/7], Loss: 0.0739, Accuracy: 98.79%, Grad Norm: 0.21880\n",
      "Epoch [415/10044], Batch [7/7], Loss: 0.0520, Accuracy: 99.27%, Grad Norm: 0.20107\n",
      "Epoch [415/10044], Loss: 0.0520\n",
      "Epoch [416/10044], Batch [1/7], Loss: 0.0719, Accuracy: 99.19%, Grad Norm: 0.20919\n",
      "Epoch [416/10044], Batch [2/7], Loss: 0.0771, Accuracy: 98.57%, Grad Norm: 0.25798\n",
      "Epoch [416/10044], Batch [3/7], Loss: 0.0684, Accuracy: 99.33%, Grad Norm: 0.20531\n",
      "Epoch [416/10044], Batch [4/7], Loss: 0.0706, Accuracy: 99.11%, Grad Norm: 0.19590\n",
      "Epoch [416/10044], Batch [5/7], Loss: 0.0702, Accuracy: 98.96%, Grad Norm: 0.19046\n",
      "Epoch [416/10044], Batch [6/7], Loss: 0.0741, Accuracy: 98.86%, Grad Norm: 0.19183\n",
      "Epoch [416/10044], Batch [7/7], Loss: 0.0546, Accuracy: 99.22%, Grad Norm: 0.21628\n",
      "Epoch [416/10044], Loss: 0.0546\n",
      "Epoch [417/10044], Batch [1/7], Loss: 0.0702, Accuracy: 99.24%, Grad Norm: 0.20393\n",
      "Epoch [417/10044], Batch [2/7], Loss: 0.0775, Accuracy: 98.61%, Grad Norm: 0.26371\n",
      "Epoch [417/10044], Batch [3/7], Loss: 0.0644, Accuracy: 99.38%, Grad Norm: 0.17755\n",
      "Epoch [417/10044], Batch [4/7], Loss: 0.0659, Accuracy: 99.38%, Grad Norm: 0.16930\n",
      "Epoch [417/10044], Batch [5/7], Loss: 0.0708, Accuracy: 98.95%, Grad Norm: 0.21473\n",
      "Epoch [417/10044], Batch [6/7], Loss: 0.0723, Accuracy: 98.83%, Grad Norm: 0.21847\n",
      "Epoch [417/10044], Batch [7/7], Loss: 0.0544, Accuracy: 99.02%, Grad Norm: 0.27762\n",
      "Epoch [417/10044], Loss: 0.0544\n",
      "Epoch [418/10044], Batch [1/7], Loss: 0.0677, Accuracy: 99.20%, Grad Norm: 0.17718\n",
      "Epoch [418/10044], Batch [2/7], Loss: 0.0740, Accuracy: 98.65%, Grad Norm: 0.22131\n",
      "Epoch [418/10044], Batch [3/7], Loss: 0.0646, Accuracy: 99.43%, Grad Norm: 0.16592\n",
      "Epoch [418/10044], Batch [4/7], Loss: 0.0688, Accuracy: 99.26%, Grad Norm: 0.17387\n",
      "Epoch [418/10044], Batch [5/7], Loss: 0.0699, Accuracy: 98.98%, Grad Norm: 0.22310\n",
      "Epoch [418/10044], Batch [6/7], Loss: 0.0734, Accuracy: 98.74%, Grad Norm: 0.23707\n",
      "Epoch [418/10044], Batch [7/7], Loss: 0.0525, Accuracy: 99.28%, Grad Norm: 0.26279\n",
      "Epoch [418/10044], Loss: 0.0525\n",
      "Epoch [419/10044], Batch [1/7], Loss: 0.0691, Accuracy: 99.27%, Grad Norm: 0.19478\n",
      "Epoch [419/10044], Batch [2/7], Loss: 0.0742, Accuracy: 98.62%, Grad Norm: 0.21154\n",
      "Epoch [419/10044], Batch [3/7], Loss: 0.0663, Accuracy: 99.35%, Grad Norm: 0.19044\n",
      "Epoch [419/10044], Batch [4/7], Loss: 0.0689, Accuracy: 99.24%, Grad Norm: 0.18211\n",
      "Epoch [419/10044], Batch [5/7], Loss: 0.0687, Accuracy: 98.96%, Grad Norm: 0.21727\n",
      "Epoch [419/10044], Batch [6/7], Loss: 0.0717, Accuracy: 98.87%, Grad Norm: 0.20763\n",
      "Epoch [419/10044], Batch [7/7], Loss: 0.0529, Accuracy: 99.18%, Grad Norm: 0.20401\n",
      "Epoch [419/10044], Loss: 0.0529\n",
      "Epoch [420/10044], Batch [1/7], Loss: 0.0681, Accuracy: 99.23%, Grad Norm: 0.20880\n",
      "Epoch [420/10044], Batch [2/7], Loss: 0.0743, Accuracy: 98.62%, Grad Norm: 0.22366\n",
      "Epoch [420/10044], Batch [3/7], Loss: 0.0634, Accuracy: 99.44%, Grad Norm: 0.17570\n",
      "Epoch [420/10044], Batch [4/7], Loss: 0.0673, Accuracy: 99.20%, Grad Norm: 0.18230\n",
      "Epoch [420/10044], Batch [5/7], Loss: 0.0670, Accuracy: 99.05%, Grad Norm: 0.18164\n",
      "Epoch [420/10044], Batch [6/7], Loss: 0.0691, Accuracy: 98.92%, Grad Norm: 0.20909\n",
      "Epoch [420/10044], Batch [7/7], Loss: 0.0555, Accuracy: 99.17%, Grad Norm: 0.22815\n",
      "Epoch [420/10044], Loss: 0.0555\n",
      "Epoch [421/10044], Batch [1/7], Loss: 0.0678, Accuracy: 99.28%, Grad Norm: 0.20203\n",
      "Epoch [421/10044], Batch [2/7], Loss: 0.0710, Accuracy: 98.92%, Grad Norm: 0.20316\n",
      "Epoch [421/10044], Batch [3/7], Loss: 0.0631, Accuracy: 99.42%, Grad Norm: 0.16039\n",
      "Epoch [421/10044], Batch [4/7], Loss: 0.0677, Accuracy: 99.17%, Grad Norm: 0.17942\n",
      "Epoch [421/10044], Batch [5/7], Loss: 0.0672, Accuracy: 99.00%, Grad Norm: 0.19702\n",
      "Epoch [421/10044], Batch [6/7], Loss: 0.0695, Accuracy: 98.92%, Grad Norm: 0.19319\n",
      "Epoch [421/10044], Batch [7/7], Loss: 0.0546, Accuracy: 99.13%, Grad Norm: 0.23701\n",
      "Epoch [421/10044], Loss: 0.0546\n",
      "Epoch [422/10044], Batch [1/7], Loss: 0.0681, Accuracy: 99.16%, Grad Norm: 0.18975\n",
      "Epoch [422/10044], Batch [2/7], Loss: 0.0715, Accuracy: 98.78%, Grad Norm: 0.20612\n",
      "Epoch [422/10044], Batch [3/7], Loss: 0.0639, Accuracy: 99.37%, Grad Norm: 0.18804\n",
      "Epoch [422/10044], Batch [4/7], Loss: 0.0652, Accuracy: 99.27%, Grad Norm: 0.17541\n",
      "Epoch [422/10044], Batch [5/7], Loss: 0.0687, Accuracy: 98.96%, Grad Norm: 0.20726\n",
      "Epoch [422/10044], Batch [6/7], Loss: 0.0660, Accuracy: 98.97%, Grad Norm: 0.17724\n",
      "Epoch [422/10044], Batch [7/7], Loss: 0.0511, Accuracy: 99.28%, Grad Norm: 0.21325\n",
      "Epoch [422/10044], Loss: 0.0511\n",
      "Epoch [423/10044], Batch [1/7], Loss: 0.0681, Accuracy: 99.18%, Grad Norm: 0.19218\n",
      "Epoch [423/10044], Batch [2/7], Loss: 0.0731, Accuracy: 98.67%, Grad Norm: 0.21675\n",
      "Epoch [423/10044], Batch [3/7], Loss: 0.0625, Accuracy: 99.48%, Grad Norm: 0.17718\n",
      "Epoch [423/10044], Batch [4/7], Loss: 0.0651, Accuracy: 99.31%, Grad Norm: 0.16684\n",
      "Epoch [423/10044], Batch [5/7], Loss: 0.0674, Accuracy: 98.95%, Grad Norm: 0.20762\n",
      "Epoch [423/10044], Batch [6/7], Loss: 0.0731, Accuracy: 98.81%, Grad Norm: 0.22108\n",
      "Epoch [423/10044], Batch [7/7], Loss: 0.0501, Accuracy: 99.28%, Grad Norm: 0.20696\n",
      "Epoch [423/10044], Loss: 0.0501\n",
      "Epoch [424/10044], Batch [1/7], Loss: 0.0673, Accuracy: 99.18%, Grad Norm: 0.22598\n",
      "Epoch [424/10044], Batch [2/7], Loss: 0.0714, Accuracy: 98.73%, Grad Norm: 0.20752\n",
      "Epoch [424/10044], Batch [3/7], Loss: 0.0635, Accuracy: 99.41%, Grad Norm: 0.18098\n",
      "Epoch [424/10044], Batch [4/7], Loss: 0.0665, Accuracy: 99.22%, Grad Norm: 0.17873\n",
      "Epoch [424/10044], Batch [5/7], Loss: 0.0689, Accuracy: 98.97%, Grad Norm: 0.21438\n",
      "Epoch [424/10044], Batch [6/7], Loss: 0.0708, Accuracy: 98.74%, Grad Norm: 0.22729\n",
      "Epoch [424/10044], Batch [7/7], Loss: 0.0511, Accuracy: 99.27%, Grad Norm: 0.20081\n",
      "Epoch [424/10044], Loss: 0.0511\n",
      "Epoch [425/10044], Batch [1/7], Loss: 0.0667, Accuracy: 99.22%, Grad Norm: 0.19198\n",
      "Epoch [425/10044], Batch [2/7], Loss: 0.0736, Accuracy: 98.59%, Grad Norm: 0.21995\n",
      "Epoch [425/10044], Batch [3/7], Loss: 0.0622, Accuracy: 99.47%, Grad Norm: 0.18645\n",
      "Epoch [425/10044], Batch [4/7], Loss: 0.0654, Accuracy: 99.17%, Grad Norm: 0.18525\n",
      "Epoch [425/10044], Batch [5/7], Loss: 0.0663, Accuracy: 99.10%, Grad Norm: 0.21580\n",
      "Epoch [425/10044], Batch [6/7], Loss: 0.0694, Accuracy: 98.84%, Grad Norm: 0.22292\n",
      "Epoch [425/10044], Batch [7/7], Loss: 0.0520, Accuracy: 99.20%, Grad Norm: 0.22993\n",
      "Epoch [425/10044], Loss: 0.0520\n",
      "Epoch [426/10044], Batch [1/7], Loss: 0.0645, Accuracy: 99.29%, Grad Norm: 0.17200\n",
      "Epoch [426/10044], Batch [2/7], Loss: 0.0737, Accuracy: 98.56%, Grad Norm: 0.24388\n",
      "Epoch [426/10044], Batch [3/7], Loss: 0.0652, Accuracy: 99.26%, Grad Norm: 0.22233\n",
      "Epoch [426/10044], Batch [4/7], Loss: 0.0657, Accuracy: 99.32%, Grad Norm: 0.17229\n",
      "Epoch [426/10044], Batch [5/7], Loss: 0.0658, Accuracy: 98.97%, Grad Norm: 0.20036\n",
      "Epoch [426/10044], Batch [6/7], Loss: 0.0674, Accuracy: 98.92%, Grad Norm: 0.18426\n",
      "Epoch [426/10044], Batch [7/7], Loss: 0.0491, Accuracy: 99.35%, Grad Norm: 0.19380\n",
      "Epoch [426/10044], Loss: 0.0491\n",
      "Epoch [427/10044], Batch [1/7], Loss: 0.0662, Accuracy: 99.26%, Grad Norm: 0.21227\n",
      "Epoch [427/10044], Batch [2/7], Loss: 0.0766, Accuracy: 98.58%, Grad Norm: 0.29153\n",
      "Epoch [427/10044], Batch [3/7], Loss: 0.0627, Accuracy: 99.36%, Grad Norm: 0.19442\n",
      "Epoch [427/10044], Batch [4/7], Loss: 0.0631, Accuracy: 99.27%, Grad Norm: 0.16422\n",
      "Epoch [427/10044], Batch [5/7], Loss: 0.0655, Accuracy: 99.03%, Grad Norm: 0.18298\n",
      "Epoch [427/10044], Batch [6/7], Loss: 0.0678, Accuracy: 98.85%, Grad Norm: 0.22693\n",
      "Epoch [427/10044], Batch [7/7], Loss: 0.0501, Accuracy: 99.37%, Grad Norm: 0.22592\n",
      "Epoch [427/10044], Loss: 0.0501\n",
      "Epoch [428/10044], Batch [1/7], Loss: 0.0679, Accuracy: 99.15%, Grad Norm: 0.23593\n",
      "Epoch [428/10044], Batch [2/7], Loss: 0.0743, Accuracy: 98.83%, Grad Norm: 0.28249\n",
      "Epoch [428/10044], Batch [3/7], Loss: 0.0627, Accuracy: 99.37%, Grad Norm: 0.18505\n",
      "Epoch [428/10044], Batch [4/7], Loss: 0.0656, Accuracy: 99.18%, Grad Norm: 0.20125\n",
      "Epoch [428/10044], Batch [5/7], Loss: 0.0692, Accuracy: 99.02%, Grad Norm: 0.25167\n",
      "Epoch [428/10044], Batch [6/7], Loss: 0.0690, Accuracy: 98.81%, Grad Norm: 0.24374\n",
      "Epoch [428/10044], Batch [7/7], Loss: 0.0537, Accuracy: 99.15%, Grad Norm: 0.25320\n",
      "Epoch [428/10044], Loss: 0.0537\n",
      "Epoch [429/10044], Batch [1/7], Loss: 0.0658, Accuracy: 99.12%, Grad Norm: 0.21633\n",
      "Epoch [429/10044], Batch [2/7], Loss: 0.0708, Accuracy: 98.71%, Grad Norm: 0.23327\n",
      "Epoch [429/10044], Batch [3/7], Loss: 0.0622, Accuracy: 99.36%, Grad Norm: 0.19724\n",
      "Epoch [429/10044], Batch [4/7], Loss: 0.0662, Accuracy: 99.26%, Grad Norm: 0.20521\n",
      "Epoch [429/10044], Batch [5/7], Loss: 0.0692, Accuracy: 98.86%, Grad Norm: 0.25073\n",
      "Epoch [429/10044], Batch [6/7], Loss: 0.0724, Accuracy: 98.64%, Grad Norm: 0.25364\n",
      "Epoch [429/10044], Batch [7/7], Loss: 0.0533, Accuracy: 99.17%, Grad Norm: 0.25108\n",
      "Epoch [429/10044], Loss: 0.0533\n",
      "Epoch [430/10044], Batch [1/7], Loss: 0.0634, Accuracy: 99.17%, Grad Norm: 0.20586\n",
      "Epoch [430/10044], Batch [2/7], Loss: 0.0703, Accuracy: 98.79%, Grad Norm: 0.23569\n",
      "Epoch [430/10044], Batch [3/7], Loss: 0.0603, Accuracy: 99.44%, Grad Norm: 0.18796\n",
      "Epoch [430/10044], Batch [4/7], Loss: 0.0645, Accuracy: 99.17%, Grad Norm: 0.18445\n",
      "Epoch [430/10044], Batch [5/7], Loss: 0.0645, Accuracy: 99.09%, Grad Norm: 0.22059\n",
      "Epoch [430/10044], Batch [6/7], Loss: 0.0687, Accuracy: 98.87%, Grad Norm: 0.22116\n",
      "Epoch [430/10044], Batch [7/7], Loss: 0.0488, Accuracy: 99.25%, Grad Norm: 0.24228\n",
      "Epoch [430/10044], Loss: 0.0488\n",
      "Epoch [431/10044], Batch [1/7], Loss: 0.0641, Accuracy: 99.17%, Grad Norm: 0.18440\n",
      "Epoch [431/10044], Batch [2/7], Loss: 0.0699, Accuracy: 98.74%, Grad Norm: 0.21921\n",
      "Epoch [431/10044], Batch [3/7], Loss: 0.0608, Accuracy: 99.37%, Grad Norm: 0.17819\n",
      "Epoch [431/10044], Batch [4/7], Loss: 0.0609, Accuracy: 99.32%, Grad Norm: 0.16237\n",
      "Epoch [431/10044], Batch [5/7], Loss: 0.0636, Accuracy: 99.16%, Grad Norm: 0.20854\n",
      "Epoch [431/10044], Batch [6/7], Loss: 0.0652, Accuracy: 98.87%, Grad Norm: 0.19907\n",
      "Epoch [431/10044], Batch [7/7], Loss: 0.0510, Accuracy: 99.12%, Grad Norm: 0.26734\n",
      "Epoch [431/10044], Loss: 0.0510\n",
      "Epoch [432/10044], Batch [1/7], Loss: 0.0628, Accuracy: 99.22%, Grad Norm: 0.20218\n",
      "Epoch [432/10044], Batch [2/7], Loss: 0.0699, Accuracy: 98.82%, Grad Norm: 0.23290\n",
      "Epoch [432/10044], Batch [3/7], Loss: 0.0583, Accuracy: 99.48%, Grad Norm: 0.17523\n",
      "Epoch [432/10044], Batch [4/7], Loss: 0.0627, Accuracy: 99.27%, Grad Norm: 0.16743\n",
      "Epoch [432/10044], Batch [5/7], Loss: 0.0640, Accuracy: 98.98%, Grad Norm: 0.22726\n",
      "Epoch [432/10044], Batch [6/7], Loss: 0.0677, Accuracy: 98.88%, Grad Norm: 0.22330\n",
      "Epoch [432/10044], Batch [7/7], Loss: 0.0508, Accuracy: 99.23%, Grad Norm: 0.22303\n",
      "Epoch [432/10044], Loss: 0.0508\n",
      "Epoch [433/10044], Batch [1/7], Loss: 0.0617, Accuracy: 99.19%, Grad Norm: 0.18478\n",
      "Epoch [433/10044], Batch [2/7], Loss: 0.0685, Accuracy: 98.80%, Grad Norm: 0.24442\n",
      "Epoch [433/10044], Batch [3/7], Loss: 0.0597, Accuracy: 99.45%, Grad Norm: 0.20707\n",
      "Epoch [433/10044], Batch [4/7], Loss: 0.0630, Accuracy: 99.28%, Grad Norm: 0.20133\n",
      "Epoch [433/10044], Batch [5/7], Loss: 0.0634, Accuracy: 99.03%, Grad Norm: 0.22640\n",
      "Epoch [433/10044], Batch [6/7], Loss: 0.0666, Accuracy: 98.90%, Grad Norm: 0.21388\n",
      "Epoch [433/10044], Batch [7/7], Loss: 0.0477, Accuracy: 99.32%, Grad Norm: 0.19582\n",
      "Epoch [433/10044], Loss: 0.0477\n",
      "Epoch [434/10044], Batch [1/7], Loss: 0.0623, Accuracy: 99.22%, Grad Norm: 0.18604\n",
      "Epoch [434/10044], Batch [2/7], Loss: 0.0705, Accuracy: 98.66%, Grad Norm: 0.26047\n",
      "Epoch [434/10044], Batch [3/7], Loss: 0.0602, Accuracy: 99.27%, Grad Norm: 0.19773\n",
      "Epoch [434/10044], Batch [4/7], Loss: 0.0634, Accuracy: 99.31%, Grad Norm: 0.18051\n",
      "Epoch [434/10044], Batch [5/7], Loss: 0.0627, Accuracy: 99.01%, Grad Norm: 0.19952\n",
      "Epoch [434/10044], Batch [6/7], Loss: 0.0678, Accuracy: 98.81%, Grad Norm: 0.20383\n",
      "Epoch [434/10044], Batch [7/7], Loss: 0.0468, Accuracy: 99.28%, Grad Norm: 0.22221\n",
      "Epoch [434/10044], Loss: 0.0468\n",
      "Epoch [435/10044], Batch [1/7], Loss: 0.0636, Accuracy: 99.26%, Grad Norm: 0.25384\n",
      "Epoch [435/10044], Batch [2/7], Loss: 0.0696, Accuracy: 98.72%, Grad Norm: 0.27505\n",
      "Epoch [435/10044], Batch [3/7], Loss: 0.0596, Accuracy: 99.45%, Grad Norm: 0.17612\n",
      "Epoch [435/10044], Batch [4/7], Loss: 0.0619, Accuracy: 99.26%, Grad Norm: 0.16768\n",
      "Epoch [435/10044], Batch [5/7], Loss: 0.0625, Accuracy: 98.98%, Grad Norm: 0.19209\n",
      "Epoch [435/10044], Batch [6/7], Loss: 0.0642, Accuracy: 98.89%, Grad Norm: 0.22134\n",
      "Epoch [435/10044], Batch [7/7], Loss: 0.0527, Accuracy: 99.13%, Grad Norm: 0.25349\n",
      "Epoch [435/10044], Loss: 0.0527\n",
      "Epoch [436/10044], Batch [1/7], Loss: 0.0642, Accuracy: 99.23%, Grad Norm: 0.20889\n",
      "Epoch [436/10044], Batch [2/7], Loss: 0.0679, Accuracy: 98.80%, Grad Norm: 0.22629\n",
      "Epoch [436/10044], Batch [3/7], Loss: 0.0592, Accuracy: 99.47%, Grad Norm: 0.16907\n",
      "Epoch [436/10044], Batch [4/7], Loss: 0.0610, Accuracy: 99.23%, Grad Norm: 0.20093\n",
      "Epoch [436/10044], Batch [5/7], Loss: 0.0643, Accuracy: 99.03%, Grad Norm: 0.25419\n",
      "Epoch [436/10044], Batch [6/7], Loss: 0.0648, Accuracy: 99.01%, Grad Norm: 0.25784\n",
      "Epoch [436/10044], Batch [7/7], Loss: 0.0502, Accuracy: 99.07%, Grad Norm: 0.27677\n",
      "Epoch [436/10044], Loss: 0.0502\n",
      "Epoch [437/10044], Batch [1/7], Loss: 0.0639, Accuracy: 99.19%, Grad Norm: 0.19363\n",
      "Epoch [437/10044], Batch [2/7], Loss: 0.0630, Accuracy: 98.86%, Grad Norm: 0.21583\n",
      "Epoch [437/10044], Batch [3/7], Loss: 0.0593, Accuracy: 99.40%, Grad Norm: 0.18445\n",
      "Epoch [437/10044], Batch [4/7], Loss: 0.0631, Accuracy: 99.27%, Grad Norm: 0.18604\n",
      "Epoch [437/10044], Batch [5/7], Loss: 0.0653, Accuracy: 98.92%, Grad Norm: 0.28581\n",
      "Epoch [437/10044], Batch [6/7], Loss: 0.0678, Accuracy: 98.78%, Grad Norm: 0.23986\n",
      "Epoch [437/10044], Batch [7/7], Loss: 0.0501, Accuracy: 99.13%, Grad Norm: 0.23822\n",
      "Epoch [437/10044], Loss: 0.0501\n",
      "Epoch [438/10044], Batch [1/7], Loss: 0.0620, Accuracy: 99.18%, Grad Norm: 0.22746\n",
      "Epoch [438/10044], Batch [2/7], Loss: 0.0680, Accuracy: 98.82%, Grad Norm: 0.31339\n",
      "Epoch [438/10044], Batch [3/7], Loss: 0.0594, Accuracy: 99.44%, Grad Norm: 0.19068\n",
      "Epoch [438/10044], Batch [4/7], Loss: 0.0614, Accuracy: 99.35%, Grad Norm: 0.18892\n",
      "Epoch [438/10044], Batch [5/7], Loss: 0.0628, Accuracy: 99.05%, Grad Norm: 0.23512\n",
      "Epoch [438/10044], Batch [6/7], Loss: 0.0643, Accuracy: 98.84%, Grad Norm: 0.22346\n",
      "Epoch [438/10044], Batch [7/7], Loss: 0.0507, Accuracy: 99.13%, Grad Norm: 0.26299\n",
      "Epoch [438/10044], Loss: 0.0507\n",
      "Epoch [439/10044], Batch [1/7], Loss: 0.0638, Accuracy: 99.18%, Grad Norm: 0.21487\n",
      "Epoch [439/10044], Batch [2/7], Loss: 0.0709, Accuracy: 98.77%, Grad Norm: 0.29616\n",
      "Epoch [439/10044], Batch [3/7], Loss: 0.0572, Accuracy: 99.41%, Grad Norm: 0.17585\n",
      "Epoch [439/10044], Batch [4/7], Loss: 0.0608, Accuracy: 99.20%, Grad Norm: 0.18684\n",
      "Epoch [439/10044], Batch [5/7], Loss: 0.0624, Accuracy: 99.11%, Grad Norm: 0.21986\n",
      "Epoch [439/10044], Batch [6/7], Loss: 0.0637, Accuracy: 98.94%, Grad Norm: 0.24191\n",
      "Epoch [439/10044], Batch [7/7], Loss: 0.0478, Accuracy: 99.27%, Grad Norm: 0.28490\n",
      "Epoch [439/10044], Loss: 0.0478\n",
      "Epoch [440/10044], Batch [1/7], Loss: 0.0617, Accuracy: 99.15%, Grad Norm: 0.21641\n",
      "Epoch [440/10044], Batch [2/7], Loss: 0.0685, Accuracy: 98.77%, Grad Norm: 0.24093\n",
      "Epoch [440/10044], Batch [3/7], Loss: 0.0584, Accuracy: 99.45%, Grad Norm: 0.19678\n",
      "Epoch [440/10044], Batch [4/7], Loss: 0.0604, Accuracy: 99.21%, Grad Norm: 0.19003\n",
      "Epoch [440/10044], Batch [5/7], Loss: 0.0614, Accuracy: 99.13%, Grad Norm: 0.22531\n",
      "Epoch [440/10044], Batch [6/7], Loss: 0.0663, Accuracy: 98.87%, Grad Norm: 0.22933\n",
      "Epoch [440/10044], Batch [7/7], Loss: 0.0518, Accuracy: 99.03%, Grad Norm: 0.26622\n",
      "Epoch [440/10044], Loss: 0.0518\n",
      "Epoch [441/10044], Batch [1/7], Loss: 0.0634, Accuracy: 99.21%, Grad Norm: 0.20460\n",
      "Epoch [441/10044], Batch [2/7], Loss: 0.0652, Accuracy: 98.87%, Grad Norm: 0.22655\n",
      "Epoch [441/10044], Batch [3/7], Loss: 0.0565, Accuracy: 99.43%, Grad Norm: 0.16828\n",
      "Epoch [441/10044], Batch [4/7], Loss: 0.0591, Accuracy: 99.29%, Grad Norm: 0.19735\n",
      "Epoch [441/10044], Batch [5/7], Loss: 0.0614, Accuracy: 99.01%, Grad Norm: 0.23225\n",
      "Epoch [441/10044], Batch [6/7], Loss: 0.0624, Accuracy: 98.86%, Grad Norm: 0.20898\n",
      "Epoch [441/10044], Batch [7/7], Loss: 0.0502, Accuracy: 99.18%, Grad Norm: 0.23718\n",
      "Epoch [441/10044], Loss: 0.0502\n",
      "Epoch [442/10044], Batch [1/7], Loss: 0.0582, Accuracy: 99.32%, Grad Norm: 0.17285\n",
      "Epoch [442/10044], Batch [2/7], Loss: 0.0651, Accuracy: 98.84%, Grad Norm: 0.22453\n",
      "Epoch [442/10044], Batch [3/7], Loss: 0.0561, Accuracy: 99.44%, Grad Norm: 0.16817\n",
      "Epoch [442/10044], Batch [4/7], Loss: 0.0604, Accuracy: 99.28%, Grad Norm: 0.17220\n",
      "Epoch [442/10044], Batch [5/7], Loss: 0.0568, Accuracy: 99.22%, Grad Norm: 0.18781\n",
      "Epoch [442/10044], Batch [6/7], Loss: 0.0637, Accuracy: 98.89%, Grad Norm: 0.21655\n",
      "Epoch [442/10044], Batch [7/7], Loss: 0.0425, Accuracy: 99.40%, Grad Norm: 0.17922\n",
      "Epoch [442/10044], Loss: 0.0425\n",
      "Epoch [443/10044], Batch [1/7], Loss: 0.0591, Accuracy: 99.30%, Grad Norm: 0.16773\n",
      "Epoch [443/10044], Batch [2/7], Loss: 0.0628, Accuracy: 98.86%, Grad Norm: 0.22412\n",
      "Epoch [443/10044], Batch [3/7], Loss: 0.0547, Accuracy: 99.47%, Grad Norm: 0.16589\n",
      "Epoch [443/10044], Batch [4/7], Loss: 0.0574, Accuracy: 99.36%, Grad Norm: 0.15164\n",
      "Epoch [443/10044], Batch [5/7], Loss: 0.0561, Accuracy: 99.20%, Grad Norm: 0.15724\n",
      "Epoch [443/10044], Batch [6/7], Loss: 0.0633, Accuracy: 98.90%, Grad Norm: 0.19451\n",
      "Epoch [443/10044], Batch [7/7], Loss: 0.0466, Accuracy: 99.27%, Grad Norm: 0.19688\n",
      "Epoch [443/10044], Loss: 0.0466\n",
      "Epoch [444/10044], Batch [1/7], Loss: 0.0556, Accuracy: 99.35%, Grad Norm: 0.15327\n",
      "Epoch [444/10044], Batch [2/7], Loss: 0.0632, Accuracy: 98.81%, Grad Norm: 0.20384\n",
      "Epoch [444/10044], Batch [3/7], Loss: 0.0566, Accuracy: 99.44%, Grad Norm: 0.17185\n",
      "Epoch [444/10044], Batch [4/7], Loss: 0.0584, Accuracy: 99.26%, Grad Norm: 0.16533\n",
      "Epoch [444/10044], Batch [5/7], Loss: 0.0571, Accuracy: 99.17%, Grad Norm: 0.15886\n",
      "Epoch [444/10044], Batch [6/7], Loss: 0.0622, Accuracy: 98.95%, Grad Norm: 0.17346\n",
      "Epoch [444/10044], Batch [7/7], Loss: 0.0469, Accuracy: 99.15%, Grad Norm: 0.20785\n",
      "Epoch [444/10044], Loss: 0.0469\n",
      "Epoch [445/10044], Batch [1/7], Loss: 0.0582, Accuracy: 99.27%, Grad Norm: 0.16588\n",
      "Epoch [445/10044], Batch [2/7], Loss: 0.0628, Accuracy: 98.86%, Grad Norm: 0.20744\n",
      "Epoch [445/10044], Batch [3/7], Loss: 0.0563, Accuracy: 99.46%, Grad Norm: 0.19803\n",
      "Epoch [445/10044], Batch [4/7], Loss: 0.0569, Accuracy: 99.27%, Grad Norm: 0.17026\n",
      "Epoch [445/10044], Batch [5/7], Loss: 0.0574, Accuracy: 99.17%, Grad Norm: 0.16837\n",
      "Epoch [445/10044], Batch [6/7], Loss: 0.0618, Accuracy: 98.89%, Grad Norm: 0.17132\n",
      "Epoch [445/10044], Batch [7/7], Loss: 0.0466, Accuracy: 99.33%, Grad Norm: 0.26932\n",
      "Epoch [445/10044], Loss: 0.0466\n",
      "Epoch [446/10044], Batch [1/7], Loss: 0.0576, Accuracy: 99.34%, Grad Norm: 0.17836\n",
      "Epoch [446/10044], Batch [2/7], Loss: 0.0654, Accuracy: 98.90%, Grad Norm: 0.24463\n",
      "Epoch [446/10044], Batch [3/7], Loss: 0.0530, Accuracy: 99.50%, Grad Norm: 0.14645\n",
      "Epoch [446/10044], Batch [4/7], Loss: 0.0557, Accuracy: 99.36%, Grad Norm: 0.15873\n",
      "Epoch [446/10044], Batch [5/7], Loss: 0.0574, Accuracy: 99.10%, Grad Norm: 0.18619\n",
      "Epoch [446/10044], Batch [6/7], Loss: 0.0610, Accuracy: 98.92%, Grad Norm: 0.20395\n",
      "Epoch [446/10044], Batch [7/7], Loss: 0.0483, Accuracy: 99.27%, Grad Norm: 0.24142\n",
      "Epoch [446/10044], Loss: 0.0483\n",
      "Epoch [447/10044], Batch [1/7], Loss: 0.0577, Accuracy: 99.25%, Grad Norm: 0.17922\n",
      "Epoch [447/10044], Batch [2/7], Loss: 0.0646, Accuracy: 98.76%, Grad Norm: 0.22150\n",
      "Epoch [447/10044], Batch [3/7], Loss: 0.0540, Accuracy: 99.50%, Grad Norm: 0.16186\n",
      "Epoch [447/10044], Batch [4/7], Loss: 0.0574, Accuracy: 99.23%, Grad Norm: 0.17624\n",
      "Epoch [447/10044], Batch [5/7], Loss: 0.0578, Accuracy: 99.08%, Grad Norm: 0.18926\n",
      "Epoch [447/10044], Batch [6/7], Loss: 0.0620, Accuracy: 98.90%, Grad Norm: 0.18285\n",
      "Epoch [447/10044], Batch [7/7], Loss: 0.0470, Accuracy: 99.30%, Grad Norm: 0.19549\n",
      "Epoch [447/10044], Loss: 0.0470\n",
      "Epoch [448/10044], Batch [1/7], Loss: 0.0576, Accuracy: 99.31%, Grad Norm: 0.17100\n",
      "Epoch [448/10044], Batch [2/7], Loss: 0.0617, Accuracy: 98.97%, Grad Norm: 0.23553\n",
      "Epoch [448/10044], Batch [3/7], Loss: 0.0522, Accuracy: 99.52%, Grad Norm: 0.15142\n",
      "Epoch [448/10044], Batch [4/7], Loss: 0.0576, Accuracy: 99.27%, Grad Norm: 0.16180\n",
      "Epoch [448/10044], Batch [5/7], Loss: 0.0587, Accuracy: 99.07%, Grad Norm: 0.17628\n",
      "Epoch [448/10044], Batch [6/7], Loss: 0.0610, Accuracy: 98.87%, Grad Norm: 0.18843\n",
      "Epoch [448/10044], Batch [7/7], Loss: 0.0460, Accuracy: 99.32%, Grad Norm: 0.19804\n",
      "Epoch [448/10044], Loss: 0.0460\n",
      "Epoch [449/10044], Batch [1/7], Loss: 0.0554, Accuracy: 99.28%, Grad Norm: 0.18050\n",
      "Epoch [449/10044], Batch [2/7], Loss: 0.0624, Accuracy: 98.74%, Grad Norm: 0.22637\n",
      "Epoch [449/10044], Batch [3/7], Loss: 0.0536, Accuracy: 99.42%, Grad Norm: 0.14918\n",
      "Epoch [449/10044], Batch [4/7], Loss: 0.0542, Accuracy: 99.35%, Grad Norm: 0.15494\n",
      "Epoch [449/10044], Batch [5/7], Loss: 0.0561, Accuracy: 99.15%, Grad Norm: 0.17843\n",
      "Epoch [449/10044], Batch [6/7], Loss: 0.0607, Accuracy: 98.91%, Grad Norm: 0.19675\n",
      "Epoch [449/10044], Batch [7/7], Loss: 0.0449, Accuracy: 99.27%, Grad Norm: 0.20810\n",
      "Epoch [449/10044], Loss: 0.0449\n",
      "Epoch [450/10044], Batch [1/7], Loss: 0.0571, Accuracy: 99.32%, Grad Norm: 0.17471\n",
      "Epoch [450/10044], Batch [2/7], Loss: 0.0639, Accuracy: 98.75%, Grad Norm: 0.23318\n",
      "Epoch [450/10044], Batch [3/7], Loss: 0.0530, Accuracy: 99.47%, Grad Norm: 0.16325\n",
      "Epoch [450/10044], Batch [4/7], Loss: 0.0571, Accuracy: 99.27%, Grad Norm: 0.15661\n",
      "Epoch [450/10044], Batch [5/7], Loss: 0.0567, Accuracy: 99.12%, Grad Norm: 0.20897\n",
      "Epoch [450/10044], Batch [6/7], Loss: 0.0593, Accuracy: 98.92%, Grad Norm: 0.23445\n",
      "Epoch [450/10044], Batch [7/7], Loss: 0.0448, Accuracy: 99.17%, Grad Norm: 0.23377\n",
      "Epoch [450/10044], Loss: 0.0448\n",
      "Epoch [451/10044], Batch [1/7], Loss: 0.0558, Accuracy: 99.39%, Grad Norm: 0.18736\n",
      "Epoch [451/10044], Batch [2/7], Loss: 0.0604, Accuracy: 98.87%, Grad Norm: 0.21777\n",
      "Epoch [451/10044], Batch [3/7], Loss: 0.0542, Accuracy: 99.40%, Grad Norm: 0.17582\n",
      "Epoch [451/10044], Batch [4/7], Loss: 0.0573, Accuracy: 99.29%, Grad Norm: 0.16986\n",
      "Epoch [451/10044], Batch [5/7], Loss: 0.0600, Accuracy: 99.07%, Grad Norm: 0.24349\n",
      "Epoch [451/10044], Batch [6/7], Loss: 0.0608, Accuracy: 98.94%, Grad Norm: 0.23266\n",
      "Epoch [451/10044], Batch [7/7], Loss: 0.0423, Accuracy: 99.37%, Grad Norm: 0.22275\n",
      "Epoch [451/10044], Loss: 0.0423\n",
      "Epoch [452/10044], Batch [1/7], Loss: 0.0565, Accuracy: 99.30%, Grad Norm: 0.19396\n",
      "Epoch [452/10044], Batch [2/7], Loss: 0.0629, Accuracy: 98.84%, Grad Norm: 0.28798\n",
      "Epoch [452/10044], Batch [3/7], Loss: 0.0542, Accuracy: 99.48%, Grad Norm: 0.17731\n",
      "Epoch [452/10044], Batch [4/7], Loss: 0.0552, Accuracy: 99.32%, Grad Norm: 0.17399\n",
      "Epoch [452/10044], Batch [5/7], Loss: 0.0582, Accuracy: 99.06%, Grad Norm: 0.22928\n",
      "Epoch [452/10044], Batch [6/7], Loss: 0.0609, Accuracy: 98.95%, Grad Norm: 0.23169\n",
      "Epoch [452/10044], Batch [7/7], Loss: 0.0481, Accuracy: 99.15%, Grad Norm: 0.25002\n",
      "Epoch [452/10044], Loss: 0.0481\n",
      "Epoch [453/10044], Batch [1/7], Loss: 0.0561, Accuracy: 99.23%, Grad Norm: 0.19636\n",
      "Epoch [453/10044], Batch [2/7], Loss: 0.0654, Accuracy: 98.79%, Grad Norm: 0.24784\n",
      "Epoch [453/10044], Batch [3/7], Loss: 0.0538, Accuracy: 99.42%, Grad Norm: 0.16551\n",
      "Epoch [453/10044], Batch [4/7], Loss: 0.0552, Accuracy: 99.35%, Grad Norm: 0.16282\n",
      "Epoch [453/10044], Batch [5/7], Loss: 0.0560, Accuracy: 99.12%, Grad Norm: 0.21021\n",
      "Epoch [453/10044], Batch [6/7], Loss: 0.0583, Accuracy: 98.94%, Grad Norm: 0.21406\n",
      "Epoch [453/10044], Batch [7/7], Loss: 0.0461, Accuracy: 99.33%, Grad Norm: 0.25377\n",
      "Epoch [453/10044], Loss: 0.0461\n",
      "Epoch [454/10044], Batch [1/7], Loss: 0.0563, Accuracy: 99.30%, Grad Norm: 0.21648\n",
      "Epoch [454/10044], Batch [2/7], Loss: 0.0596, Accuracy: 98.96%, Grad Norm: 0.20431\n",
      "Epoch [454/10044], Batch [3/7], Loss: 0.0533, Accuracy: 99.37%, Grad Norm: 0.19405\n",
      "Epoch [454/10044], Batch [4/7], Loss: 0.0559, Accuracy: 99.38%, Grad Norm: 0.17413\n",
      "Epoch [454/10044], Batch [5/7], Loss: 0.0576, Accuracy: 99.06%, Grad Norm: 0.19645\n",
      "Epoch [454/10044], Batch [6/7], Loss: 0.0592, Accuracy: 98.92%, Grad Norm: 0.18333\n",
      "Epoch [454/10044], Batch [7/7], Loss: 0.0448, Accuracy: 99.32%, Grad Norm: 0.22031\n",
      "Epoch [454/10044], Loss: 0.0448\n",
      "Epoch [455/10044], Batch [1/7], Loss: 0.0556, Accuracy: 99.29%, Grad Norm: 0.18740\n",
      "Epoch [455/10044], Batch [2/7], Loss: 0.0612, Accuracy: 98.98%, Grad Norm: 0.20625\n",
      "Epoch [455/10044], Batch [3/7], Loss: 0.0509, Accuracy: 99.51%, Grad Norm: 0.15996\n",
      "Epoch [455/10044], Batch [4/7], Loss: 0.0554, Accuracy: 99.21%, Grad Norm: 0.18703\n",
      "Epoch [455/10044], Batch [5/7], Loss: 0.0545, Accuracy: 99.12%, Grad Norm: 0.17964\n",
      "Epoch [455/10044], Batch [6/7], Loss: 0.0591, Accuracy: 98.99%, Grad Norm: 0.18962\n",
      "Epoch [455/10044], Batch [7/7], Loss: 0.0421, Accuracy: 99.22%, Grad Norm: 0.19550\n",
      "Epoch [455/10044], Loss: 0.0421\n",
      "Epoch [456/10044], Batch [1/7], Loss: 0.0557, Accuracy: 99.32%, Grad Norm: 0.18081\n",
      "Epoch [456/10044], Batch [2/7], Loss: 0.0605, Accuracy: 98.90%, Grad Norm: 0.21700\n",
      "Epoch [456/10044], Batch [3/7], Loss: 0.0530, Accuracy: 99.38%, Grad Norm: 0.16486\n",
      "Epoch [456/10044], Batch [4/7], Loss: 0.0553, Accuracy: 99.33%, Grad Norm: 0.16119\n",
      "Epoch [456/10044], Batch [5/7], Loss: 0.0545, Accuracy: 99.10%, Grad Norm: 0.18078\n",
      "Epoch [456/10044], Batch [6/7], Loss: 0.0586, Accuracy: 98.92%, Grad Norm: 0.20722\n",
      "Epoch [456/10044], Batch [7/7], Loss: 0.0444, Accuracy: 99.28%, Grad Norm: 0.23754\n",
      "Epoch [456/10044], Loss: 0.0444\n",
      "Epoch [457/10044], Batch [1/7], Loss: 0.0524, Accuracy: 99.41%, Grad Norm: 0.16426\n",
      "Epoch [457/10044], Batch [2/7], Loss: 0.0587, Accuracy: 98.88%, Grad Norm: 0.19352\n",
      "Epoch [457/10044], Batch [3/7], Loss: 0.0509, Accuracy: 99.42%, Grad Norm: 0.15383\n",
      "Epoch [457/10044], Batch [4/7], Loss: 0.0542, Accuracy: 99.32%, Grad Norm: 0.14954\n",
      "Epoch [457/10044], Batch [5/7], Loss: 0.0568, Accuracy: 99.05%, Grad Norm: 0.21460\n",
      "Epoch [457/10044], Batch [6/7], Loss: 0.0593, Accuracy: 98.96%, Grad Norm: 0.22038\n",
      "Epoch [457/10044], Batch [7/7], Loss: 0.0492, Accuracy: 99.10%, Grad Norm: 0.25728\n",
      "Epoch [457/10044], Loss: 0.0492\n",
      "Epoch [458/10044], Batch [1/7], Loss: 0.0520, Accuracy: 99.37%, Grad Norm: 0.17466\n",
      "Epoch [458/10044], Batch [2/7], Loss: 0.0608, Accuracy: 98.91%, Grad Norm: 0.21978\n",
      "Epoch [458/10044], Batch [3/7], Loss: 0.0511, Accuracy: 99.53%, Grad Norm: 0.17795\n",
      "Epoch [458/10044], Batch [4/7], Loss: 0.0559, Accuracy: 99.23%, Grad Norm: 0.18180\n",
      "Epoch [458/10044], Batch [5/7], Loss: 0.0550, Accuracy: 99.13%, Grad Norm: 0.18813\n",
      "Epoch [458/10044], Batch [6/7], Loss: 0.0581, Accuracy: 98.97%, Grad Norm: 0.20435\n",
      "Epoch [458/10044], Batch [7/7], Loss: 0.0445, Accuracy: 99.17%, Grad Norm: 0.19275\n",
      "Epoch [458/10044], Loss: 0.0445\n",
      "Epoch [459/10044], Batch [1/7], Loss: 0.0544, Accuracy: 99.36%, Grad Norm: 0.17518\n",
      "Epoch [459/10044], Batch [2/7], Loss: 0.0624, Accuracy: 98.79%, Grad Norm: 0.23666\n",
      "Epoch [459/10044], Batch [3/7], Loss: 0.0516, Accuracy: 99.35%, Grad Norm: 0.16622\n",
      "Epoch [459/10044], Batch [4/7], Loss: 0.0525, Accuracy: 99.35%, Grad Norm: 0.16071\n",
      "Epoch [459/10044], Batch [5/7], Loss: 0.0533, Accuracy: 99.19%, Grad Norm: 0.17304\n",
      "Epoch [459/10044], Batch [6/7], Loss: 0.0546, Accuracy: 99.10%, Grad Norm: 0.19074\n",
      "Epoch [459/10044], Batch [7/7], Loss: 0.0398, Accuracy: 99.48%, Grad Norm: 0.19109\n",
      "Epoch [459/10044], Loss: 0.0398\n",
      "Epoch [460/10044], Batch [1/7], Loss: 0.0551, Accuracy: 99.34%, Grad Norm: 0.20535\n",
      "Epoch [460/10044], Batch [2/7], Loss: 0.0564, Accuracy: 99.02%, Grad Norm: 0.20195\n",
      "Epoch [460/10044], Batch [3/7], Loss: 0.0502, Accuracy: 99.53%, Grad Norm: 0.15315\n",
      "Epoch [460/10044], Batch [4/7], Loss: 0.0527, Accuracy: 99.40%, Grad Norm: 0.15659\n",
      "Epoch [460/10044], Batch [5/7], Loss: 0.0546, Accuracy: 99.16%, Grad Norm: 0.20392\n",
      "Epoch [460/10044], Batch [6/7], Loss: 0.0578, Accuracy: 98.92%, Grad Norm: 0.22972\n",
      "Epoch [460/10044], Batch [7/7], Loss: 0.0412, Accuracy: 99.28%, Grad Norm: 0.19862\n",
      "Epoch [460/10044], Loss: 0.0412\n",
      "Epoch [461/10044], Batch [1/7], Loss: 0.0558, Accuracy: 99.35%, Grad Norm: 0.20446\n",
      "Epoch [461/10044], Batch [2/7], Loss: 0.0586, Accuracy: 98.88%, Grad Norm: 0.21805\n",
      "Epoch [461/10044], Batch [3/7], Loss: 0.0502, Accuracy: 99.43%, Grad Norm: 0.16706\n",
      "Epoch [461/10044], Batch [4/7], Loss: 0.0535, Accuracy: 99.36%, Grad Norm: 0.18948\n",
      "Epoch [461/10044], Batch [5/7], Loss: 0.0541, Accuracy: 99.11%, Grad Norm: 0.21568\n",
      "Epoch [461/10044], Batch [6/7], Loss: 0.0593, Accuracy: 99.02%, Grad Norm: 0.26945\n",
      "Epoch [461/10044], Batch [7/7], Loss: 0.0449, Accuracy: 99.37%, Grad Norm: 0.23113\n",
      "Epoch [461/10044], Loss: 0.0449\n",
      "Epoch [462/10044], Batch [1/7], Loss: 0.0519, Accuracy: 99.42%, Grad Norm: 0.17333\n",
      "Epoch [462/10044], Batch [2/7], Loss: 0.0581, Accuracy: 99.00%, Grad Norm: 0.21199\n",
      "Epoch [462/10044], Batch [3/7], Loss: 0.0514, Accuracy: 99.43%, Grad Norm: 0.18726\n",
      "Epoch [462/10044], Batch [4/7], Loss: 0.0547, Accuracy: 99.29%, Grad Norm: 0.20596\n",
      "Epoch [462/10044], Batch [5/7], Loss: 0.0529, Accuracy: 99.19%, Grad Norm: 0.21285\n",
      "Epoch [462/10044], Batch [6/7], Loss: 0.0547, Accuracy: 98.98%, Grad Norm: 0.19326\n",
      "Epoch [462/10044], Batch [7/7], Loss: 0.0400, Accuracy: 99.43%, Grad Norm: 0.18227\n",
      "Epoch [462/10044], Loss: 0.0400\n",
      "Epoch [463/10044], Batch [1/7], Loss: 0.0526, Accuracy: 99.37%, Grad Norm: 0.20174\n",
      "Epoch [463/10044], Batch [2/7], Loss: 0.0581, Accuracy: 98.92%, Grad Norm: 0.21065\n",
      "Epoch [463/10044], Batch [3/7], Loss: 0.0509, Accuracy: 99.47%, Grad Norm: 0.18034\n",
      "Epoch [463/10044], Batch [4/7], Loss: 0.0522, Accuracy: 99.36%, Grad Norm: 0.16691\n",
      "Epoch [463/10044], Batch [5/7], Loss: 0.0549, Accuracy: 99.08%, Grad Norm: 0.20460\n",
      "Epoch [463/10044], Batch [6/7], Loss: 0.0599, Accuracy: 98.90%, Grad Norm: 0.21168\n",
      "Epoch [463/10044], Batch [7/7], Loss: 0.0414, Accuracy: 99.23%, Grad Norm: 0.20761\n",
      "Epoch [463/10044], Loss: 0.0414\n",
      "Epoch [464/10044], Batch [1/7], Loss: 0.0536, Accuracy: 99.32%, Grad Norm: 0.19460\n",
      "Epoch [464/10044], Batch [2/7], Loss: 0.0573, Accuracy: 99.01%, Grad Norm: 0.20549\n",
      "Epoch [464/10044], Batch [3/7], Loss: 0.0487, Accuracy: 99.47%, Grad Norm: 0.15215\n",
      "Epoch [464/10044], Batch [4/7], Loss: 0.0539, Accuracy: 99.23%, Grad Norm: 0.16386\n",
      "Epoch [464/10044], Batch [5/7], Loss: 0.0542, Accuracy: 99.12%, Grad Norm: 0.22678\n",
      "Epoch [464/10044], Batch [6/7], Loss: 0.0553, Accuracy: 99.04%, Grad Norm: 0.24282\n",
      "Epoch [464/10044], Batch [7/7], Loss: 0.0394, Accuracy: 99.42%, Grad Norm: 0.17406\n",
      "Epoch [464/10044], Loss: 0.0394\n",
      "Epoch [465/10044], Batch [1/7], Loss: 0.0533, Accuracy: 99.27%, Grad Norm: 0.17240\n",
      "Epoch [465/10044], Batch [2/7], Loss: 0.0620, Accuracy: 98.85%, Grad Norm: 0.19522\n",
      "Epoch [465/10044], Batch [3/7], Loss: 0.0472, Accuracy: 99.58%, Grad Norm: 0.14931\n",
      "Epoch [465/10044], Batch [4/7], Loss: 0.0502, Accuracy: 99.37%, Grad Norm: 0.16511\n",
      "Epoch [465/10044], Batch [5/7], Loss: 0.0533, Accuracy: 99.13%, Grad Norm: 0.21826\n",
      "Epoch [465/10044], Batch [6/7], Loss: 0.0559, Accuracy: 98.97%, Grad Norm: 0.21329\n",
      "Epoch [465/10044], Batch [7/7], Loss: 0.0415, Accuracy: 99.23%, Grad Norm: 0.18951\n",
      "Epoch [465/10044], Loss: 0.0415\n",
      "Epoch [466/10044], Batch [1/7], Loss: 0.0506, Accuracy: 99.36%, Grad Norm: 0.17103\n",
      "Epoch [466/10044], Batch [2/7], Loss: 0.0558, Accuracy: 99.06%, Grad Norm: 0.18941\n",
      "Epoch [466/10044], Batch [3/7], Loss: 0.0480, Accuracy: 99.54%, Grad Norm: 0.15930\n",
      "Epoch [466/10044], Batch [4/7], Loss: 0.0506, Accuracy: 99.35%, Grad Norm: 0.15860\n",
      "Epoch [466/10044], Batch [5/7], Loss: 0.0519, Accuracy: 99.12%, Grad Norm: 0.18935\n",
      "Epoch [466/10044], Batch [6/7], Loss: 0.0529, Accuracy: 99.14%, Grad Norm: 0.18798\n",
      "Epoch [466/10044], Batch [7/7], Loss: 0.0398, Accuracy: 99.37%, Grad Norm: 0.17489\n",
      "Epoch [466/10044], Loss: 0.0398\n",
      "Epoch [467/10044], Batch [1/7], Loss: 0.0509, Accuracy: 99.37%, Grad Norm: 0.16412\n",
      "Epoch [467/10044], Batch [2/7], Loss: 0.0610, Accuracy: 98.72%, Grad Norm: 0.23791\n",
      "Epoch [467/10044], Batch [3/7], Loss: 0.0480, Accuracy: 99.49%, Grad Norm: 0.15517\n",
      "Epoch [467/10044], Batch [4/7], Loss: 0.0526, Accuracy: 99.24%, Grad Norm: 0.15837\n",
      "Epoch [467/10044], Batch [5/7], Loss: 0.0532, Accuracy: 99.07%, Grad Norm: 0.19338\n",
      "Epoch [467/10044], Batch [6/7], Loss: 0.0554, Accuracy: 98.92%, Grad Norm: 0.19580\n",
      "Epoch [467/10044], Batch [7/7], Loss: 0.0425, Accuracy: 99.25%, Grad Norm: 0.21784\n",
      "Epoch [467/10044], Loss: 0.0425\n",
      "Epoch [468/10044], Batch [1/7], Loss: 0.0520, Accuracy: 99.36%, Grad Norm: 0.17279\n",
      "Epoch [468/10044], Batch [2/7], Loss: 0.0562, Accuracy: 99.07%, Grad Norm: 0.19878\n",
      "Epoch [468/10044], Batch [3/7], Loss: 0.0467, Accuracy: 99.58%, Grad Norm: 0.15782\n",
      "Epoch [468/10044], Batch [4/7], Loss: 0.0491, Accuracy: 99.43%, Grad Norm: 0.14742\n",
      "Epoch [468/10044], Batch [5/7], Loss: 0.0532, Accuracy: 99.11%, Grad Norm: 0.18450\n",
      "Epoch [468/10044], Batch [6/7], Loss: 0.0566, Accuracy: 98.97%, Grad Norm: 0.19936\n",
      "Epoch [468/10044], Batch [7/7], Loss: 0.0414, Accuracy: 99.20%, Grad Norm: 0.20909\n",
      "Epoch [468/10044], Loss: 0.0414\n",
      "Epoch [469/10044], Batch [1/7], Loss: 0.0500, Accuracy: 99.37%, Grad Norm: 0.16739\n",
      "Epoch [469/10044], Batch [2/7], Loss: 0.0554, Accuracy: 98.99%, Grad Norm: 0.19329\n",
      "Epoch [469/10044], Batch [3/7], Loss: 0.0472, Accuracy: 99.52%, Grad Norm: 0.15538\n",
      "Epoch [469/10044], Batch [4/7], Loss: 0.0500, Accuracy: 99.41%, Grad Norm: 0.14877\n",
      "Epoch [469/10044], Batch [5/7], Loss: 0.0493, Accuracy: 99.37%, Grad Norm: 0.17431\n",
      "Epoch [469/10044], Batch [6/7], Loss: 0.0550, Accuracy: 99.02%, Grad Norm: 0.18898\n",
      "Epoch [469/10044], Batch [7/7], Loss: 0.0372, Accuracy: 99.43%, Grad Norm: 0.16992\n",
      "Epoch [469/10044], Loss: 0.0372\n",
      "Epoch [470/10044], Batch [1/7], Loss: 0.0504, Accuracy: 99.35%, Grad Norm: 0.16176\n",
      "Epoch [470/10044], Batch [2/7], Loss: 0.0535, Accuracy: 98.97%, Grad Norm: 0.19222\n",
      "Epoch [470/10044], Batch [3/7], Loss: 0.0466, Accuracy: 99.48%, Grad Norm: 0.15477\n",
      "Epoch [470/10044], Batch [4/7], Loss: 0.0503, Accuracy: 99.35%, Grad Norm: 0.14211\n",
      "Epoch [470/10044], Batch [5/7], Loss: 0.0484, Accuracy: 99.35%, Grad Norm: 0.16046\n",
      "Epoch [470/10044], Batch [6/7], Loss: 0.0529, Accuracy: 99.12%, Grad Norm: 0.18552\n",
      "Epoch [470/10044], Batch [7/7], Loss: 0.0399, Accuracy: 99.32%, Grad Norm: 0.20501\n",
      "Epoch [470/10044], Loss: 0.0399\n",
      "Epoch [471/10044], Batch [1/7], Loss: 0.0498, Accuracy: 99.35%, Grad Norm: 0.17170\n",
      "Epoch [471/10044], Batch [2/7], Loss: 0.0527, Accuracy: 99.06%, Grad Norm: 0.18669\n",
      "Epoch [471/10044], Batch [3/7], Loss: 0.0460, Accuracy: 99.46%, Grad Norm: 0.14091\n",
      "Epoch [471/10044], Batch [4/7], Loss: 0.0509, Accuracy: 99.35%, Grad Norm: 0.15301\n",
      "Epoch [471/10044], Batch [5/7], Loss: 0.0490, Accuracy: 99.24%, Grad Norm: 0.16172\n",
      "Epoch [471/10044], Batch [6/7], Loss: 0.0538, Accuracy: 99.05%, Grad Norm: 0.19288\n",
      "Epoch [471/10044], Batch [7/7], Loss: 0.0420, Accuracy: 99.18%, Grad Norm: 0.18847\n",
      "Epoch [471/10044], Loss: 0.0420\n",
      "Epoch [472/10044], Batch [1/7], Loss: 0.0495, Accuracy: 99.35%, Grad Norm: 0.15982\n",
      "Epoch [472/10044], Batch [2/7], Loss: 0.0525, Accuracy: 99.09%, Grad Norm: 0.19034\n",
      "Epoch [472/10044], Batch [3/7], Loss: 0.0451, Accuracy: 99.57%, Grad Norm: 0.14422\n",
      "Epoch [472/10044], Batch [4/7], Loss: 0.0487, Accuracy: 99.42%, Grad Norm: 0.15730\n",
      "Epoch [472/10044], Batch [5/7], Loss: 0.0494, Accuracy: 99.27%, Grad Norm: 0.19477\n",
      "Epoch [472/10044], Batch [6/7], Loss: 0.0529, Accuracy: 99.06%, Grad Norm: 0.19942\n",
      "Epoch [472/10044], Batch [7/7], Loss: 0.0370, Accuracy: 99.42%, Grad Norm: 0.17301\n",
      "Epoch [472/10044], Loss: 0.0370\n",
      "Epoch [473/10044], Batch [1/7], Loss: 0.0497, Accuracy: 99.30%, Grad Norm: 0.16042\n",
      "Epoch [473/10044], Batch [2/7], Loss: 0.0555, Accuracy: 98.95%, Grad Norm: 0.17888\n",
      "Epoch [473/10044], Batch [3/7], Loss: 0.0459, Accuracy: 99.52%, Grad Norm: 0.15315\n",
      "Epoch [473/10044], Batch [4/7], Loss: 0.0485, Accuracy: 99.37%, Grad Norm: 0.16805\n",
      "Epoch [473/10044], Batch [5/7], Loss: 0.0508, Accuracy: 99.20%, Grad Norm: 0.17996\n",
      "Epoch [473/10044], Batch [6/7], Loss: 0.0512, Accuracy: 98.98%, Grad Norm: 0.17599\n",
      "Epoch [473/10044], Batch [7/7], Loss: 0.0398, Accuracy: 99.28%, Grad Norm: 0.20973\n",
      "Epoch [473/10044], Loss: 0.0398\n",
      "Epoch [474/10044], Batch [1/7], Loss: 0.0510, Accuracy: 99.30%, Grad Norm: 0.22675\n",
      "Epoch [474/10044], Batch [2/7], Loss: 0.0531, Accuracy: 99.02%, Grad Norm: 0.22652\n",
      "Epoch [474/10044], Batch [3/7], Loss: 0.0483, Accuracy: 99.40%, Grad Norm: 0.16310\n",
      "Epoch [474/10044], Batch [4/7], Loss: 0.0490, Accuracy: 99.40%, Grad Norm: 0.14923\n",
      "Epoch [474/10044], Batch [5/7], Loss: 0.0497, Accuracy: 99.21%, Grad Norm: 0.17630\n",
      "Epoch [474/10044], Batch [6/7], Loss: 0.0534, Accuracy: 99.04%, Grad Norm: 0.20134\n",
      "Epoch [474/10044], Batch [7/7], Loss: 0.0390, Accuracy: 99.37%, Grad Norm: 0.22114\n",
      "Epoch [474/10044], Loss: 0.0390\n",
      "Epoch [475/10044], Batch [1/7], Loss: 0.0515, Accuracy: 99.32%, Grad Norm: 0.21734\n",
      "Epoch [475/10044], Batch [2/7], Loss: 0.0571, Accuracy: 98.85%, Grad Norm: 0.23563\n",
      "Epoch [475/10044], Batch [3/7], Loss: 0.0460, Accuracy: 99.53%, Grad Norm: 0.14993\n",
      "Epoch [475/10044], Batch [4/7], Loss: 0.0503, Accuracy: 99.27%, Grad Norm: 0.17207\n",
      "Epoch [475/10044], Batch [5/7], Loss: 0.0533, Accuracy: 99.17%, Grad Norm: 0.25849\n",
      "Epoch [475/10044], Batch [6/7], Loss: 0.0533, Accuracy: 98.96%, Grad Norm: 0.24262\n",
      "Epoch [475/10044], Batch [7/7], Loss: 0.0404, Accuracy: 99.28%, Grad Norm: 0.21211\n",
      "Epoch [475/10044], Loss: 0.0404\n",
      "Epoch [476/10044], Batch [1/7], Loss: 0.0461, Accuracy: 99.52%, Grad Norm: 0.16123\n",
      "Epoch [476/10044], Batch [2/7], Loss: 0.0555, Accuracy: 98.97%, Grad Norm: 0.20666\n",
      "Epoch [476/10044], Batch [3/7], Loss: 0.0471, Accuracy: 99.50%, Grad Norm: 0.19741\n",
      "Epoch [476/10044], Batch [4/7], Loss: 0.0522, Accuracy: 99.29%, Grad Norm: 0.20677\n",
      "Epoch [476/10044], Batch [5/7], Loss: 0.0498, Accuracy: 99.17%, Grad Norm: 0.23475\n",
      "Epoch [476/10044], Batch [6/7], Loss: 0.0544, Accuracy: 99.01%, Grad Norm: 0.21744\n",
      "Epoch [476/10044], Batch [7/7], Loss: 0.0392, Accuracy: 99.32%, Grad Norm: 0.22136\n",
      "Epoch [476/10044], Loss: 0.0392\n",
      "Epoch [477/10044], Batch [1/7], Loss: 0.0505, Accuracy: 99.27%, Grad Norm: 0.20024\n",
      "Epoch [477/10044], Batch [2/7], Loss: 0.0564, Accuracy: 98.93%, Grad Norm: 0.25300\n",
      "Epoch [477/10044], Batch [3/7], Loss: 0.0477, Accuracy: 99.47%, Grad Norm: 0.20076\n",
      "Epoch [477/10044], Batch [4/7], Loss: 0.0531, Accuracy: 99.19%, Grad Norm: 0.20507\n",
      "Epoch [477/10044], Batch [5/7], Loss: 0.0492, Accuracy: 99.10%, Grad Norm: 0.21473\n",
      "Epoch [477/10044], Batch [6/7], Loss: 0.0505, Accuracy: 99.19%, Grad Norm: 0.20495\n",
      "Epoch [477/10044], Batch [7/7], Loss: 0.0415, Accuracy: 99.33%, Grad Norm: 0.25264\n",
      "Epoch [477/10044], Loss: 0.0415\n",
      "Epoch [478/10044], Batch [1/7], Loss: 0.0513, Accuracy: 99.24%, Grad Norm: 0.24475\n",
      "Epoch [478/10044], Batch [2/7], Loss: 0.0556, Accuracy: 98.88%, Grad Norm: 0.27285\n",
      "Epoch [478/10044], Batch [3/7], Loss: 0.0473, Accuracy: 99.38%, Grad Norm: 0.17260\n",
      "Epoch [478/10044], Batch [4/7], Loss: 0.0493, Accuracy: 99.36%, Grad Norm: 0.17439\n",
      "Epoch [478/10044], Batch [5/7], Loss: 0.0477, Accuracy: 99.26%, Grad Norm: 0.19578\n",
      "Epoch [478/10044], Batch [6/7], Loss: 0.0530, Accuracy: 99.03%, Grad Norm: 0.23820\n",
      "Epoch [478/10044], Batch [7/7], Loss: 0.0391, Accuracy: 99.37%, Grad Norm: 0.19153\n",
      "Epoch [478/10044], Loss: 0.0391\n",
      "Epoch [479/10044], Batch [1/7], Loss: 0.0539, Accuracy: 99.29%, Grad Norm: 0.28504\n",
      "Epoch [479/10044], Batch [2/7], Loss: 0.0559, Accuracy: 98.94%, Grad Norm: 0.25725\n",
      "Epoch [479/10044], Batch [3/7], Loss: 0.0450, Accuracy: 99.52%, Grad Norm: 0.16255\n",
      "Epoch [479/10044], Batch [4/7], Loss: 0.0471, Accuracy: 99.43%, Grad Norm: 0.15612\n",
      "Epoch [479/10044], Batch [5/7], Loss: 0.0521, Accuracy: 99.18%, Grad Norm: 0.20306\n",
      "Epoch [479/10044], Batch [6/7], Loss: 0.0544, Accuracy: 98.96%, Grad Norm: 0.25052\n",
      "Epoch [479/10044], Batch [7/7], Loss: 0.0397, Accuracy: 99.20%, Grad Norm: 0.23748\n",
      "Epoch [479/10044], Loss: 0.0397\n",
      "Epoch [480/10044], Batch [1/7], Loss: 0.0508, Accuracy: 99.32%, Grad Norm: 0.18383\n",
      "Epoch [480/10044], Batch [2/7], Loss: 0.0542, Accuracy: 98.93%, Grad Norm: 0.21652\n",
      "Epoch [480/10044], Batch [3/7], Loss: 0.0462, Accuracy: 99.47%, Grad Norm: 0.16311\n",
      "Epoch [480/10044], Batch [4/7], Loss: 0.0498, Accuracy: 99.32%, Grad Norm: 0.17592\n",
      "Epoch [480/10044], Batch [5/7], Loss: 0.0514, Accuracy: 99.13%, Grad Norm: 0.24273\n",
      "Epoch [480/10044], Batch [6/7], Loss: 0.0536, Accuracy: 98.98%, Grad Norm: 0.23762\n",
      "Epoch [480/10044], Batch [7/7], Loss: 0.0371, Accuracy: 99.35%, Grad Norm: 0.23090\n",
      "Epoch [480/10044], Loss: 0.0371\n",
      "Epoch [481/10044], Batch [1/7], Loss: 0.0477, Accuracy: 99.40%, Grad Norm: 0.16398\n",
      "Epoch [481/10044], Batch [2/7], Loss: 0.0513, Accuracy: 99.03%, Grad Norm: 0.20648\n",
      "Epoch [481/10044], Batch [3/7], Loss: 0.0445, Accuracy: 99.49%, Grad Norm: 0.17265\n",
      "Epoch [481/10044], Batch [4/7], Loss: 0.0515, Accuracy: 99.28%, Grad Norm: 0.19908\n",
      "Epoch [481/10044], Batch [5/7], Loss: 0.0512, Accuracy: 99.13%, Grad Norm: 0.22303\n",
      "Epoch [481/10044], Batch [6/7], Loss: 0.0512, Accuracy: 99.00%, Grad Norm: 0.19636\n",
      "Epoch [481/10044], Batch [7/7], Loss: 0.0404, Accuracy: 99.18%, Grad Norm: 0.21660\n",
      "Epoch [481/10044], Loss: 0.0404\n",
      "Epoch [482/10044], Batch [1/7], Loss: 0.0473, Accuracy: 99.36%, Grad Norm: 0.17149\n",
      "Epoch [482/10044], Batch [2/7], Loss: 0.0552, Accuracy: 98.97%, Grad Norm: 0.28120\n",
      "Epoch [482/10044], Batch [3/7], Loss: 0.0456, Accuracy: 99.53%, Grad Norm: 0.19040\n",
      "Epoch [482/10044], Batch [4/7], Loss: 0.0471, Accuracy: 99.40%, Grad Norm: 0.16970\n",
      "Epoch [482/10044], Batch [5/7], Loss: 0.0477, Accuracy: 99.24%, Grad Norm: 0.17311\n",
      "Epoch [482/10044], Batch [6/7], Loss: 0.0511, Accuracy: 99.07%, Grad Norm: 0.19680\n",
      "Epoch [482/10044], Batch [7/7], Loss: 0.0378, Accuracy: 99.45%, Grad Norm: 0.19708\n",
      "Epoch [482/10044], Loss: 0.0378\n",
      "Epoch [483/10044], Batch [1/7], Loss: 0.0494, Accuracy: 99.38%, Grad Norm: 0.22181\n",
      "Epoch [483/10044], Batch [2/7], Loss: 0.0549, Accuracy: 99.01%, Grad Norm: 0.22323\n",
      "Epoch [483/10044], Batch [3/7], Loss: 0.0425, Accuracy: 99.60%, Grad Norm: 0.13861\n",
      "Epoch [483/10044], Batch [4/7], Loss: 0.0468, Accuracy: 99.42%, Grad Norm: 0.16140\n",
      "Epoch [483/10044], Batch [5/7], Loss: 0.0487, Accuracy: 99.14%, Grad Norm: 0.19227\n",
      "Epoch [483/10044], Batch [6/7], Loss: 0.0527, Accuracy: 98.97%, Grad Norm: 0.22719\n",
      "Epoch [483/10044], Batch [7/7], Loss: 0.0369, Accuracy: 99.48%, Grad Norm: 0.24218\n",
      "Epoch [483/10044], Loss: 0.0369\n",
      "Epoch [484/10044], Batch [1/7], Loss: 0.0465, Accuracy: 99.41%, Grad Norm: 0.19972\n",
      "Epoch [484/10044], Batch [2/7], Loss: 0.0527, Accuracy: 99.12%, Grad Norm: 0.18812\n",
      "Epoch [484/10044], Batch [3/7], Loss: 0.0450, Accuracy: 99.49%, Grad Norm: 0.16161\n",
      "Epoch [484/10044], Batch [4/7], Loss: 0.0476, Accuracy: 99.28%, Grad Norm: 0.16478\n",
      "Epoch [484/10044], Batch [5/7], Loss: 0.0495, Accuracy: 99.15%, Grad Norm: 0.20377\n",
      "Epoch [484/10044], Batch [6/7], Loss: 0.0511, Accuracy: 99.07%, Grad Norm: 0.20974\n",
      "Epoch [484/10044], Batch [7/7], Loss: 0.0399, Accuracy: 99.32%, Grad Norm: 0.24134\n",
      "Epoch [484/10044], Loss: 0.0399\n",
      "Epoch [485/10044], Batch [1/7], Loss: 0.0460, Accuracy: 99.39%, Grad Norm: 0.17891\n",
      "Epoch [485/10044], Batch [2/7], Loss: 0.0551, Accuracy: 98.86%, Grad Norm: 0.25645\n",
      "Epoch [485/10044], Batch [3/7], Loss: 0.0440, Accuracy: 99.55%, Grad Norm: 0.16438\n",
      "Epoch [485/10044], Batch [4/7], Loss: 0.0480, Accuracy: 99.27%, Grad Norm: 0.17327\n",
      "Epoch [485/10044], Batch [5/7], Loss: 0.0486, Accuracy: 99.26%, Grad Norm: 0.22748\n",
      "Epoch [485/10044], Batch [6/7], Loss: 0.0501, Accuracy: 99.13%, Grad Norm: 0.19232\n",
      "Epoch [485/10044], Batch [7/7], Loss: 0.0380, Accuracy: 99.30%, Grad Norm: 0.19437\n",
      "Epoch [485/10044], Loss: 0.0380\n",
      "Epoch [486/10044], Batch [1/7], Loss: 0.0467, Accuracy: 99.41%, Grad Norm: 0.18222\n",
      "Epoch [486/10044], Batch [2/7], Loss: 0.0568, Accuracy: 98.80%, Grad Norm: 0.27884\n",
      "Epoch [486/10044], Batch [3/7], Loss: 0.0422, Accuracy: 99.54%, Grad Norm: 0.14894\n",
      "Epoch [486/10044], Batch [4/7], Loss: 0.0464, Accuracy: 99.32%, Grad Norm: 0.14697\n",
      "Epoch [486/10044], Batch [5/7], Loss: 0.0495, Accuracy: 99.18%, Grad Norm: 0.20375\n",
      "Epoch [486/10044], Batch [6/7], Loss: 0.0510, Accuracy: 99.04%, Grad Norm: 0.18454\n",
      "Epoch [486/10044], Batch [7/7], Loss: 0.0369, Accuracy: 99.32%, Grad Norm: 0.22588\n",
      "Epoch [486/10044], Loss: 0.0369\n",
      "Epoch [487/10044], Batch [1/7], Loss: 0.0526, Accuracy: 99.10%, Grad Norm: 0.34496\n",
      "Epoch [487/10044], Batch [2/7], Loss: 0.0493, Accuracy: 99.18%, Grad Norm: 0.18420\n",
      "Epoch [487/10044], Batch [3/7], Loss: 0.0447, Accuracy: 99.48%, Grad Norm: 0.15645\n",
      "Epoch [487/10044], Batch [4/7], Loss: 0.0470, Accuracy: 99.42%, Grad Norm: 0.14526\n",
      "Epoch [487/10044], Batch [5/7], Loss: 0.0472, Accuracy: 99.25%, Grad Norm: 0.18130\n",
      "Epoch [487/10044], Batch [6/7], Loss: 0.0522, Accuracy: 99.00%, Grad Norm: 0.20447\n",
      "Epoch [487/10044], Batch [7/7], Loss: 0.0389, Accuracy: 99.33%, Grad Norm: 0.25322\n",
      "Epoch [487/10044], Loss: 0.0389\n",
      "Epoch [488/10044], Batch [1/7], Loss: 0.0529, Accuracy: 99.37%, Grad Norm: 0.20737\n",
      "Epoch [488/10044], Batch [2/7], Loss: 0.0536, Accuracy: 98.97%, Grad Norm: 0.21847\n",
      "Epoch [488/10044], Batch [3/7], Loss: 0.0423, Accuracy: 99.55%, Grad Norm: 0.15912\n",
      "Epoch [488/10044], Batch [4/7], Loss: 0.0500, Accuracy: 99.18%, Grad Norm: 0.22686\n",
      "Epoch [488/10044], Batch [5/7], Loss: 0.0506, Accuracy: 99.07%, Grad Norm: 0.25918\n",
      "Epoch [488/10044], Batch [6/7], Loss: 0.0507, Accuracy: 99.10%, Grad Norm: 0.22838\n",
      "Epoch [488/10044], Batch [7/7], Loss: 0.0391, Accuracy: 99.32%, Grad Norm: 0.21470\n",
      "Epoch [488/10044], Loss: 0.0391\n",
      "Epoch [489/10044], Batch [1/7], Loss: 0.0483, Accuracy: 99.29%, Grad Norm: 0.16577\n",
      "Epoch [489/10044], Batch [2/7], Loss: 0.0522, Accuracy: 98.94%, Grad Norm: 0.23633\n",
      "Epoch [489/10044], Batch [3/7], Loss: 0.0444, Accuracy: 99.50%, Grad Norm: 0.19242\n",
      "Epoch [489/10044], Batch [4/7], Loss: 0.0501, Accuracy: 99.26%, Grad Norm: 0.21061\n",
      "Epoch [489/10044], Batch [5/7], Loss: 0.0475, Accuracy: 99.27%, Grad Norm: 0.18064\n",
      "Epoch [489/10044], Batch [6/7], Loss: 0.0527, Accuracy: 98.92%, Grad Norm: 0.20203\n",
      "Epoch [489/10044], Batch [7/7], Loss: 0.0368, Accuracy: 99.28%, Grad Norm: 0.19820\n",
      "Epoch [489/10044], Loss: 0.0368\n",
      "Epoch [490/10044], Batch [1/7], Loss: 0.0493, Accuracy: 99.33%, Grad Norm: 0.21126\n",
      "Epoch [490/10044], Batch [2/7], Loss: 0.0533, Accuracy: 98.96%, Grad Norm: 0.28971\n",
      "Epoch [490/10044], Batch [3/7], Loss: 0.0435, Accuracy: 99.57%, Grad Norm: 0.15841\n",
      "Epoch [490/10044], Batch [4/7], Loss: 0.0472, Accuracy: 99.34%, Grad Norm: 0.17395\n",
      "Epoch [490/10044], Batch [5/7], Loss: 0.0442, Accuracy: 99.40%, Grad Norm: 0.18045\n",
      "Epoch [490/10044], Batch [6/7], Loss: 0.0505, Accuracy: 99.12%, Grad Norm: 0.20524\n",
      "Epoch [490/10044], Batch [7/7], Loss: 0.0421, Accuracy: 99.15%, Grad Norm: 0.25097\n",
      "Epoch [490/10044], Loss: 0.0421\n",
      "Epoch [491/10044], Batch [1/7], Loss: 0.0452, Accuracy: 99.41%, Grad Norm: 0.17980\n",
      "Epoch [491/10044], Batch [2/7], Loss: 0.0523, Accuracy: 98.95%, Grad Norm: 0.21031\n",
      "Epoch [491/10044], Batch [3/7], Loss: 0.0423, Accuracy: 99.55%, Grad Norm: 0.14392\n",
      "Epoch [491/10044], Batch [4/7], Loss: 0.0472, Accuracy: 99.34%, Grad Norm: 0.16242\n",
      "Epoch [491/10044], Batch [5/7], Loss: 0.0463, Accuracy: 99.31%, Grad Norm: 0.19178\n",
      "Epoch [491/10044], Batch [6/7], Loss: 0.0520, Accuracy: 99.09%, Grad Norm: 0.24369\n",
      "Epoch [491/10044], Batch [7/7], Loss: 0.0364, Accuracy: 99.52%, Grad Norm: 0.22778\n",
      "Epoch [491/10044], Loss: 0.0364\n",
      "Epoch [492/10044], Batch [1/7], Loss: 0.0478, Accuracy: 99.35%, Grad Norm: 0.17888\n",
      "Epoch [492/10044], Batch [2/7], Loss: 0.0497, Accuracy: 99.06%, Grad Norm: 0.18264\n",
      "Epoch [492/10044], Batch [3/7], Loss: 0.0429, Accuracy: 99.54%, Grad Norm: 0.17091\n",
      "Epoch [492/10044], Batch [4/7], Loss: 0.0467, Accuracy: 99.42%, Grad Norm: 0.17411\n",
      "Epoch [492/10044], Batch [5/7], Loss: 0.0474, Accuracy: 99.13%, Grad Norm: 0.20053\n",
      "Epoch [492/10044], Batch [6/7], Loss: 0.0482, Accuracy: 99.12%, Grad Norm: 0.17759\n",
      "Epoch [492/10044], Batch [7/7], Loss: 0.0355, Accuracy: 99.43%, Grad Norm: 0.18581\n",
      "Epoch [492/10044], Loss: 0.0355\n",
      "Epoch [493/10044], Batch [1/7], Loss: 0.0430, Accuracy: 99.46%, Grad Norm: 0.16197\n",
      "Epoch [493/10044], Batch [2/7], Loss: 0.0505, Accuracy: 99.07%, Grad Norm: 0.19324\n",
      "Epoch [493/10044], Batch [3/7], Loss: 0.0400, Accuracy: 99.61%, Grad Norm: 0.13873\n",
      "Epoch [493/10044], Batch [4/7], Loss: 0.0440, Accuracy: 99.37%, Grad Norm: 0.15367\n",
      "Epoch [493/10044], Batch [5/7], Loss: 0.0446, Accuracy: 99.28%, Grad Norm: 0.17826\n",
      "Epoch [493/10044], Batch [6/7], Loss: 0.0470, Accuracy: 99.13%, Grad Norm: 0.15749\n",
      "Epoch [493/10044], Batch [7/7], Loss: 0.0370, Accuracy: 99.25%, Grad Norm: 0.19688\n",
      "Epoch [493/10044], Loss: 0.0370\n",
      "Epoch [494/10044], Batch [1/7], Loss: 0.0450, Accuracy: 99.35%, Grad Norm: 0.16077\n",
      "Epoch [494/10044], Batch [2/7], Loss: 0.0506, Accuracy: 98.92%, Grad Norm: 0.18613\n",
      "Epoch [494/10044], Batch [3/7], Loss: 0.0408, Accuracy: 99.59%, Grad Norm: 0.13668\n",
      "Epoch [494/10044], Batch [4/7], Loss: 0.0441, Accuracy: 99.38%, Grad Norm: 0.13443\n",
      "Epoch [494/10044], Batch [5/7], Loss: 0.0434, Accuracy: 99.31%, Grad Norm: 0.16719\n",
      "Epoch [494/10044], Batch [6/7], Loss: 0.0477, Accuracy: 99.10%, Grad Norm: 0.18515\n",
      "Epoch [494/10044], Batch [7/7], Loss: 0.0365, Accuracy: 99.32%, Grad Norm: 0.21181\n",
      "Epoch [494/10044], Loss: 0.0365\n",
      "Epoch [495/10044], Batch [1/7], Loss: 0.0459, Accuracy: 99.34%, Grad Norm: 0.16749\n",
      "Epoch [495/10044], Batch [2/7], Loss: 0.0497, Accuracy: 98.97%, Grad Norm: 0.18752\n",
      "Epoch [495/10044], Batch [3/7], Loss: 0.0419, Accuracy: 99.50%, Grad Norm: 0.14127\n",
      "Epoch [495/10044], Batch [4/7], Loss: 0.0428, Accuracy: 99.43%, Grad Norm: 0.12996\n",
      "Epoch [495/10044], Batch [5/7], Loss: 0.0443, Accuracy: 99.19%, Grad Norm: 0.15985\n",
      "Epoch [495/10044], Batch [6/7], Loss: 0.0493, Accuracy: 99.03%, Grad Norm: 0.18596\n",
      "Epoch [495/10044], Batch [7/7], Loss: 0.0383, Accuracy: 99.23%, Grad Norm: 0.22452\n",
      "Epoch [495/10044], Loss: 0.0383\n",
      "Epoch [496/10044], Batch [1/7], Loss: 0.0436, Accuracy: 99.42%, Grad Norm: 0.14703\n",
      "Epoch [496/10044], Batch [2/7], Loss: 0.0483, Accuracy: 99.01%, Grad Norm: 0.19326\n",
      "Epoch [496/10044], Batch [3/7], Loss: 0.0393, Accuracy: 99.58%, Grad Norm: 0.12700\n",
      "Epoch [496/10044], Batch [4/7], Loss: 0.0429, Accuracy: 99.37%, Grad Norm: 0.13371\n",
      "Epoch [496/10044], Batch [5/7], Loss: 0.0446, Accuracy: 99.25%, Grad Norm: 0.18790\n",
      "Epoch [496/10044], Batch [6/7], Loss: 0.0464, Accuracy: 99.18%, Grad Norm: 0.17243\n",
      "Epoch [496/10044], Batch [7/7], Loss: 0.0341, Accuracy: 99.38%, Grad Norm: 0.18347\n",
      "Epoch [496/10044], Loss: 0.0341\n",
      "Epoch [497/10044], Batch [1/7], Loss: 0.0402, Accuracy: 99.48%, Grad Norm: 0.13482\n",
      "Epoch [497/10044], Batch [2/7], Loss: 0.0467, Accuracy: 99.17%, Grad Norm: 0.18477\n",
      "Epoch [497/10044], Batch [3/7], Loss: 0.0418, Accuracy: 99.56%, Grad Norm: 0.14840\n",
      "Epoch [497/10044], Batch [4/7], Loss: 0.0452, Accuracy: 99.36%, Grad Norm: 0.14442\n",
      "Epoch [497/10044], Batch [5/7], Loss: 0.0403, Accuracy: 99.43%, Grad Norm: 0.14825\n",
      "Epoch [497/10044], Batch [6/7], Loss: 0.0450, Accuracy: 99.19%, Grad Norm: 0.16029\n",
      "Epoch [497/10044], Batch [7/7], Loss: 0.0333, Accuracy: 99.50%, Grad Norm: 0.17951\n",
      "Epoch [497/10044], Loss: 0.0333\n",
      "Epoch [498/10044], Batch [1/7], Loss: 0.0432, Accuracy: 99.37%, Grad Norm: 0.16341\n",
      "Epoch [498/10044], Batch [2/7], Loss: 0.0478, Accuracy: 99.05%, Grad Norm: 0.19742\n",
      "Epoch [498/10044], Batch [3/7], Loss: 0.0399, Accuracy: 99.59%, Grad Norm: 0.13849\n",
      "Epoch [498/10044], Batch [4/7], Loss: 0.0424, Accuracy: 99.38%, Grad Norm: 0.13418\n",
      "Epoch [498/10044], Batch [5/7], Loss: 0.0422, Accuracy: 99.32%, Grad Norm: 0.14774\n",
      "Epoch [498/10044], Batch [6/7], Loss: 0.0464, Accuracy: 99.10%, Grad Norm: 0.16987\n",
      "Epoch [498/10044], Batch [7/7], Loss: 0.0354, Accuracy: 99.37%, Grad Norm: 0.17946\n",
      "Epoch [498/10044], Loss: 0.0354\n",
      "Epoch [499/10044], Batch [1/7], Loss: 0.0437, Accuracy: 99.35%, Grad Norm: 0.16069\n",
      "Epoch [499/10044], Batch [2/7], Loss: 0.0481, Accuracy: 98.98%, Grad Norm: 0.20909\n",
      "Epoch [499/10044], Batch [3/7], Loss: 0.0391, Accuracy: 99.61%, Grad Norm: 0.13296\n",
      "Epoch [499/10044], Batch [4/7], Loss: 0.0438, Accuracy: 99.32%, Grad Norm: 0.14138\n",
      "Epoch [499/10044], Batch [5/7], Loss: 0.0392, Accuracy: 99.52%, Grad Norm: 0.15353\n",
      "Epoch [499/10044], Batch [6/7], Loss: 0.0465, Accuracy: 99.12%, Grad Norm: 0.16844\n",
      "Epoch [499/10044], Batch [7/7], Loss: 0.0318, Accuracy: 99.57%, Grad Norm: 0.18808\n",
      "Epoch [499/10044], Loss: 0.0318\n",
      "Epoch [500/10044], Batch [1/7], Loss: 0.0426, Accuracy: 99.47%, Grad Norm: 0.14509\n",
      "Epoch [500/10044], Batch [2/7], Loss: 0.0477, Accuracy: 99.13%, Grad Norm: 0.18130\n",
      "Epoch [500/10044], Batch [3/7], Loss: 0.0391, Accuracy: 99.63%, Grad Norm: 0.12775\n",
      "Epoch [500/10044], Batch [4/7], Loss: 0.0422, Accuracy: 99.43%, Grad Norm: 0.13043\n",
      "Epoch [500/10044], Batch [5/7], Loss: 0.0435, Accuracy: 99.29%, Grad Norm: 0.16999\n",
      "Epoch [500/10044], Batch [6/7], Loss: 0.0435, Accuracy: 99.27%, Grad Norm: 0.15609\n",
      "Epoch [500/10044], Batch [7/7], Loss: 0.0328, Accuracy: 99.47%, Grad Norm: 0.17228\n",
      "Epoch [500/10044], Loss: 0.0328\n",
      "Epoch [501/10044], Batch [1/7], Loss: 0.0401, Accuracy: 99.51%, Grad Norm: 0.13284\n",
      "Epoch [501/10044], Batch [2/7], Loss: 0.0458, Accuracy: 99.15%, Grad Norm: 0.17438\n",
      "Epoch [501/10044], Batch [3/7], Loss: 0.0378, Accuracy: 99.61%, Grad Norm: 0.12782\n",
      "Epoch [501/10044], Batch [4/7], Loss: 0.0425, Accuracy: 99.42%, Grad Norm: 0.13766\n",
      "Epoch [501/10044], Batch [5/7], Loss: 0.0392, Accuracy: 99.43%, Grad Norm: 0.14306\n",
      "Epoch [501/10044], Batch [6/7], Loss: 0.0448, Accuracy: 99.20%, Grad Norm: 0.16229\n",
      "Epoch [501/10044], Batch [7/7], Loss: 0.0337, Accuracy: 99.50%, Grad Norm: 0.18514\n",
      "Epoch [501/10044], Loss: 0.0337\n",
      "Epoch [502/10044], Batch [1/7], Loss: 0.0427, Accuracy: 99.47%, Grad Norm: 0.16733\n",
      "Epoch [502/10044], Batch [2/7], Loss: 0.0452, Accuracy: 99.17%, Grad Norm: 0.18219\n",
      "Epoch [502/10044], Batch [3/7], Loss: 0.0404, Accuracy: 99.52%, Grad Norm: 0.13510\n",
      "Epoch [502/10044], Batch [4/7], Loss: 0.0425, Accuracy: 99.37%, Grad Norm: 0.12737\n",
      "Epoch [502/10044], Batch [5/7], Loss: 0.0413, Accuracy: 99.39%, Grad Norm: 0.15594\n",
      "Epoch [502/10044], Batch [6/7], Loss: 0.0457, Accuracy: 99.11%, Grad Norm: 0.17060\n",
      "Epoch [502/10044], Batch [7/7], Loss: 0.0341, Accuracy: 99.45%, Grad Norm: 0.17941\n",
      "Epoch [502/10044], Loss: 0.0341\n",
      "Epoch [503/10044], Batch [1/7], Loss: 0.0422, Accuracy: 99.39%, Grad Norm: 0.17794\n",
      "Epoch [503/10044], Batch [2/7], Loss: 0.0451, Accuracy: 99.13%, Grad Norm: 0.16933\n",
      "Epoch [503/10044], Batch [3/7], Loss: 0.0387, Accuracy: 99.53%, Grad Norm: 0.12728\n",
      "Epoch [503/10044], Batch [4/7], Loss: 0.0433, Accuracy: 99.42%, Grad Norm: 0.13617\n",
      "Epoch [503/10044], Batch [5/7], Loss: 0.0415, Accuracy: 99.32%, Grad Norm: 0.17741\n",
      "Epoch [503/10044], Batch [6/7], Loss: 0.0448, Accuracy: 99.22%, Grad Norm: 0.18262\n",
      "Epoch [503/10044], Batch [7/7], Loss: 0.0337, Accuracy: 99.40%, Grad Norm: 0.18766\n",
      "Epoch [503/10044], Loss: 0.0337\n",
      "Epoch [504/10044], Batch [1/7], Loss: 0.0421, Accuracy: 99.41%, Grad Norm: 0.14493\n",
      "Epoch [504/10044], Batch [2/7], Loss: 0.0464, Accuracy: 99.07%, Grad Norm: 0.19354\n",
      "Epoch [504/10044], Batch [3/7], Loss: 0.0396, Accuracy: 99.52%, Grad Norm: 0.12809\n",
      "Epoch [504/10044], Batch [4/7], Loss: 0.0424, Accuracy: 99.41%, Grad Norm: 0.14640\n",
      "Epoch [504/10044], Batch [5/7], Loss: 0.0428, Accuracy: 99.21%, Grad Norm: 0.18681\n",
      "Epoch [504/10044], Batch [6/7], Loss: 0.0437, Accuracy: 99.17%, Grad Norm: 0.16478\n",
      "Epoch [504/10044], Batch [7/7], Loss: 0.0347, Accuracy: 99.45%, Grad Norm: 0.18270\n",
      "Epoch [504/10044], Loss: 0.0347\n",
      "Epoch [505/10044], Batch [1/7], Loss: 0.0394, Accuracy: 99.47%, Grad Norm: 0.14142\n",
      "Epoch [505/10044], Batch [2/7], Loss: 0.0451, Accuracy: 99.17%, Grad Norm: 0.17390\n",
      "Epoch [505/10044], Batch [3/7], Loss: 0.0383, Accuracy: 99.67%, Grad Norm: 0.13064\n",
      "Epoch [505/10044], Batch [4/7], Loss: 0.0439, Accuracy: 99.28%, Grad Norm: 0.14480\n",
      "Epoch [505/10044], Batch [5/7], Loss: 0.0410, Accuracy: 99.33%, Grad Norm: 0.17603\n",
      "Epoch [505/10044], Batch [6/7], Loss: 0.0429, Accuracy: 99.33%, Grad Norm: 0.18109\n",
      "Epoch [505/10044], Batch [7/7], Loss: 0.0341, Accuracy: 99.30%, Grad Norm: 0.19622\n",
      "Epoch [505/10044], Loss: 0.0341\n",
      "Epoch [506/10044], Batch [1/7], Loss: 0.0378, Accuracy: 99.50%, Grad Norm: 0.14498\n",
      "Epoch [506/10044], Batch [2/7], Loss: 0.0463, Accuracy: 99.15%, Grad Norm: 0.18394\n",
      "Epoch [506/10044], Batch [3/7], Loss: 0.0379, Accuracy: 99.65%, Grad Norm: 0.13514\n",
      "Epoch [506/10044], Batch [4/7], Loss: 0.0428, Accuracy: 99.36%, Grad Norm: 0.13954\n",
      "Epoch [506/10044], Batch [5/7], Loss: 0.0413, Accuracy: 99.34%, Grad Norm: 0.19183\n",
      "Epoch [506/10044], Batch [6/7], Loss: 0.0426, Accuracy: 99.27%, Grad Norm: 0.18421\n",
      "Epoch [506/10044], Batch [7/7], Loss: 0.0344, Accuracy: 99.43%, Grad Norm: 0.20112\n",
      "Epoch [506/10044], Loss: 0.0344\n",
      "Epoch [507/10044], Batch [1/7], Loss: 0.0406, Accuracy: 99.44%, Grad Norm: 0.14311\n",
      "Epoch [507/10044], Batch [2/7], Loss: 0.0475, Accuracy: 99.04%, Grad Norm: 0.18031\n",
      "Epoch [507/10044], Batch [3/7], Loss: 0.0389, Accuracy: 99.53%, Grad Norm: 0.14325\n",
      "Epoch [507/10044], Batch [4/7], Loss: 0.0435, Accuracy: 99.40%, Grad Norm: 0.14465\n",
      "Epoch [507/10044], Batch [5/7], Loss: 0.0401, Accuracy: 99.44%, Grad Norm: 0.16621\n",
      "Epoch [507/10044], Batch [6/7], Loss: 0.0450, Accuracy: 99.12%, Grad Norm: 0.19496\n",
      "Epoch [507/10044], Batch [7/7], Loss: 0.0301, Accuracy: 99.45%, Grad Norm: 0.15303\n",
      "Epoch [507/10044], Loss: 0.0301\n",
      "Epoch [508/10044], Batch [1/7], Loss: 0.0440, Accuracy: 99.42%, Grad Norm: 0.14663\n",
      "Epoch [508/10044], Batch [2/7], Loss: 0.0451, Accuracy: 99.15%, Grad Norm: 0.19232\n",
      "Epoch [508/10044], Batch [3/7], Loss: 0.0382, Accuracy: 99.51%, Grad Norm: 0.13385\n",
      "Epoch [508/10044], Batch [4/7], Loss: 0.0401, Accuracy: 99.43%, Grad Norm: 0.11951\n",
      "Epoch [508/10044], Batch [5/7], Loss: 0.0410, Accuracy: 99.37%, Grad Norm: 0.16075\n",
      "Epoch [508/10044], Batch [6/7], Loss: 0.0445, Accuracy: 99.05%, Grad Norm: 0.19462\n",
      "Epoch [508/10044], Batch [7/7], Loss: 0.0328, Accuracy: 99.40%, Grad Norm: 0.17599\n",
      "Epoch [508/10044], Loss: 0.0328\n",
      "Epoch [509/10044], Batch [1/7], Loss: 0.0415, Accuracy: 99.37%, Grad Norm: 0.14772\n",
      "Epoch [509/10044], Batch [2/7], Loss: 0.0455, Accuracy: 99.12%, Grad Norm: 0.19935\n",
      "Epoch [509/10044], Batch [3/7], Loss: 0.0375, Accuracy: 99.58%, Grad Norm: 0.12452\n",
      "Epoch [509/10044], Batch [4/7], Loss: 0.0418, Accuracy: 99.39%, Grad Norm: 0.13977\n",
      "Epoch [509/10044], Batch [5/7], Loss: 0.0400, Accuracy: 99.30%, Grad Norm: 0.15920\n",
      "Epoch [509/10044], Batch [6/7], Loss: 0.0450, Accuracy: 99.14%, Grad Norm: 0.19615\n",
      "Epoch [509/10044], Batch [7/7], Loss: 0.0365, Accuracy: 99.38%, Grad Norm: 0.21950\n",
      "Epoch [509/10044], Loss: 0.0365\n",
      "Epoch [510/10044], Batch [1/7], Loss: 0.0411, Accuracy: 99.51%, Grad Norm: 0.15506\n",
      "Epoch [510/10044], Batch [2/7], Loss: 0.0422, Accuracy: 99.22%, Grad Norm: 0.17200\n",
      "Epoch [510/10044], Batch [3/7], Loss: 0.0367, Accuracy: 99.58%, Grad Norm: 0.12721\n",
      "Epoch [510/10044], Batch [4/7], Loss: 0.0409, Accuracy: 99.42%, Grad Norm: 0.13797\n",
      "Epoch [510/10044], Batch [5/7], Loss: 0.0421, Accuracy: 99.28%, Grad Norm: 0.19589\n",
      "Epoch [510/10044], Batch [6/7], Loss: 0.0430, Accuracy: 99.25%, Grad Norm: 0.18329\n",
      "Epoch [510/10044], Batch [7/7], Loss: 0.0318, Accuracy: 99.52%, Grad Norm: 0.17494\n",
      "Epoch [510/10044], Loss: 0.0318\n",
      "Epoch [511/10044], Batch [1/7], Loss: 0.0407, Accuracy: 99.55%, Grad Norm: 0.15743\n",
      "Epoch [511/10044], Batch [2/7], Loss: 0.0428, Accuracy: 99.23%, Grad Norm: 0.16496\n",
      "Epoch [511/10044], Batch [3/7], Loss: 0.0368, Accuracy: 99.58%, Grad Norm: 0.12394\n",
      "Epoch [511/10044], Batch [4/7], Loss: 0.0423, Accuracy: 99.36%, Grad Norm: 0.14363\n",
      "Epoch [511/10044], Batch [5/7], Loss: 0.0420, Accuracy: 99.31%, Grad Norm: 0.20039\n",
      "Epoch [511/10044], Batch [6/7], Loss: 0.0428, Accuracy: 99.20%, Grad Norm: 0.18230\n",
      "Epoch [511/10044], Batch [7/7], Loss: 0.0309, Accuracy: 99.48%, Grad Norm: 0.15706\n",
      "Epoch [511/10044], Loss: 0.0309\n",
      "Epoch [512/10044], Batch [1/7], Loss: 0.0395, Accuracy: 99.47%, Grad Norm: 0.14121\n",
      "Epoch [512/10044], Batch [2/7], Loss: 0.0429, Accuracy: 99.12%, Grad Norm: 0.19070\n",
      "Epoch [512/10044], Batch [3/7], Loss: 0.0368, Accuracy: 99.57%, Grad Norm: 0.13138\n",
      "Epoch [512/10044], Batch [4/7], Loss: 0.0418, Accuracy: 99.42%, Grad Norm: 0.14135\n",
      "Epoch [512/10044], Batch [5/7], Loss: 0.0406, Accuracy: 99.27%, Grad Norm: 0.17274\n",
      "Epoch [512/10044], Batch [6/7], Loss: 0.0426, Accuracy: 99.27%, Grad Norm: 0.17498\n",
      "Epoch [512/10044], Batch [7/7], Loss: 0.0314, Accuracy: 99.53%, Grad Norm: 0.16001\n",
      "Epoch [512/10044], Loss: 0.0314\n",
      "Epoch [513/10044], Batch [1/7], Loss: 0.0389, Accuracy: 99.52%, Grad Norm: 0.13745\n",
      "Epoch [513/10044], Batch [2/7], Loss: 0.0435, Accuracy: 99.22%, Grad Norm: 0.19094\n",
      "Epoch [513/10044], Batch [3/7], Loss: 0.0397, Accuracy: 99.54%, Grad Norm: 0.13491\n",
      "Epoch [513/10044], Batch [4/7], Loss: 0.0421, Accuracy: 99.42%, Grad Norm: 0.15243\n",
      "Epoch [513/10044], Batch [5/7], Loss: 0.0413, Accuracy: 99.29%, Grad Norm: 0.18184\n",
      "Epoch [513/10044], Batch [6/7], Loss: 0.0420, Accuracy: 99.27%, Grad Norm: 0.16911\n",
      "Epoch [513/10044], Batch [7/7], Loss: 0.0318, Accuracy: 99.48%, Grad Norm: 0.16838\n",
      "Epoch [513/10044], Loss: 0.0318\n",
      "Epoch [514/10044], Batch [1/7], Loss: 0.0410, Accuracy: 99.47%, Grad Norm: 0.16477\n",
      "Epoch [514/10044], Batch [2/7], Loss: 0.0465, Accuracy: 99.07%, Grad Norm: 0.21346\n",
      "Epoch [514/10044], Batch [3/7], Loss: 0.0367, Accuracy: 99.52%, Grad Norm: 0.15930\n",
      "Epoch [514/10044], Batch [4/7], Loss: 0.0419, Accuracy: 99.46%, Grad Norm: 0.13410\n",
      "Epoch [514/10044], Batch [5/7], Loss: 0.0375, Accuracy: 99.46%, Grad Norm: 0.15093\n",
      "Epoch [514/10044], Batch [6/7], Loss: 0.0429, Accuracy: 99.23%, Grad Norm: 0.17539\n",
      "Epoch [514/10044], Batch [7/7], Loss: 0.0311, Accuracy: 99.52%, Grad Norm: 0.17907\n",
      "Epoch [514/10044], Loss: 0.0311\n",
      "Epoch [515/10044], Batch [1/7], Loss: 0.0427, Accuracy: 99.36%, Grad Norm: 0.17304\n",
      "Epoch [515/10044], Batch [2/7], Loss: 0.0474, Accuracy: 98.98%, Grad Norm: 0.23773\n",
      "Epoch [515/10044], Batch [3/7], Loss: 0.0366, Accuracy: 99.56%, Grad Norm: 0.14424\n",
      "Epoch [515/10044], Batch [4/7], Loss: 0.0385, Accuracy: 99.49%, Grad Norm: 0.12691\n",
      "Epoch [515/10044], Batch [5/7], Loss: 0.0405, Accuracy: 99.32%, Grad Norm: 0.15527\n",
      "Epoch [515/10044], Batch [6/7], Loss: 0.0413, Accuracy: 99.27%, Grad Norm: 0.17994\n",
      "Epoch [515/10044], Batch [7/7], Loss: 0.0327, Accuracy: 99.43%, Grad Norm: 0.21735\n",
      "Epoch [515/10044], Loss: 0.0327\n",
      "Epoch [516/10044], Batch [1/7], Loss: 0.0397, Accuracy: 99.47%, Grad Norm: 0.15967\n",
      "Epoch [516/10044], Batch [2/7], Loss: 0.0481, Accuracy: 98.94%, Grad Norm: 0.23984\n",
      "Epoch [516/10044], Batch [3/7], Loss: 0.0365, Accuracy: 99.56%, Grad Norm: 0.13914\n",
      "Epoch [516/10044], Batch [4/7], Loss: 0.0401, Accuracy: 99.42%, Grad Norm: 0.12624\n",
      "Epoch [516/10044], Batch [5/7], Loss: 0.0400, Accuracy: 99.37%, Grad Norm: 0.17902\n",
      "Epoch [516/10044], Batch [6/7], Loss: 0.0439, Accuracy: 99.18%, Grad Norm: 0.18157\n",
      "Epoch [516/10044], Batch [7/7], Loss: 0.0324, Accuracy: 99.42%, Grad Norm: 0.20595\n",
      "Epoch [516/10044], Loss: 0.0324\n",
      "Epoch [517/10044], Batch [1/7], Loss: 0.0421, Accuracy: 99.34%, Grad Norm: 0.17978\n",
      "Epoch [517/10044], Batch [2/7], Loss: 0.0469, Accuracy: 99.12%, Grad Norm: 0.20292\n",
      "Epoch [517/10044], Batch [3/7], Loss: 0.0376, Accuracy: 99.55%, Grad Norm: 0.14807\n",
      "Epoch [517/10044], Batch [4/7], Loss: 0.0404, Accuracy: 99.37%, Grad Norm: 0.15802\n",
      "Epoch [517/10044], Batch [5/7], Loss: 0.0405, Accuracy: 99.30%, Grad Norm: 0.19327\n",
      "Epoch [517/10044], Batch [6/7], Loss: 0.0406, Accuracy: 99.33%, Grad Norm: 0.16576\n",
      "Epoch [517/10044], Batch [7/7], Loss: 0.0326, Accuracy: 99.32%, Grad Norm: 0.19681\n",
      "Epoch [517/10044], Loss: 0.0326\n",
      "Epoch [518/10044], Batch [1/7], Loss: 0.0395, Accuracy: 99.46%, Grad Norm: 0.14785\n",
      "Epoch [518/10044], Batch [2/7], Loss: 0.0438, Accuracy: 99.18%, Grad Norm: 0.22590\n",
      "Epoch [518/10044], Batch [3/7], Loss: 0.0359, Accuracy: 99.59%, Grad Norm: 0.14425\n",
      "Epoch [518/10044], Batch [4/7], Loss: 0.0413, Accuracy: 99.37%, Grad Norm: 0.15358\n",
      "Epoch [518/10044], Batch [5/7], Loss: 0.0387, Accuracy: 99.48%, Grad Norm: 0.17213\n",
      "Epoch [518/10044], Batch [6/7], Loss: 0.0419, Accuracy: 99.23%, Grad Norm: 0.15776\n",
      "Epoch [518/10044], Batch [7/7], Loss: 0.0332, Accuracy: 99.42%, Grad Norm: 0.20658\n",
      "Epoch [518/10044], Loss: 0.0332\n",
      "Epoch [519/10044], Batch [1/7], Loss: 0.0394, Accuracy: 99.38%, Grad Norm: 0.15120\n",
      "Epoch [519/10044], Batch [2/7], Loss: 0.0435, Accuracy: 99.19%, Grad Norm: 0.20402\n",
      "Epoch [519/10044], Batch [3/7], Loss: 0.0391, Accuracy: 99.48%, Grad Norm: 0.16732\n",
      "Epoch [519/10044], Batch [4/7], Loss: 0.0395, Accuracy: 99.42%, Grad Norm: 0.15364\n",
      "Epoch [519/10044], Batch [5/7], Loss: 0.0394, Accuracy: 99.32%, Grad Norm: 0.16015\n",
      "Epoch [519/10044], Batch [6/7], Loss: 0.0438, Accuracy: 99.12%, Grad Norm: 0.16343\n",
      "Epoch [519/10044], Batch [7/7], Loss: 0.0321, Accuracy: 99.40%, Grad Norm: 0.20244\n",
      "Epoch [519/10044], Loss: 0.0321\n",
      "Epoch [520/10044], Batch [1/7], Loss: 0.0383, Accuracy: 99.56%, Grad Norm: 0.14439\n",
      "Epoch [520/10044], Batch [2/7], Loss: 0.0435, Accuracy: 99.11%, Grad Norm: 0.21104\n",
      "Epoch [520/10044], Batch [3/7], Loss: 0.0371, Accuracy: 99.47%, Grad Norm: 0.16603\n",
      "Epoch [520/10044], Batch [4/7], Loss: 0.0398, Accuracy: 99.37%, Grad Norm: 0.13518\n",
      "Epoch [520/10044], Batch [5/7], Loss: 0.0373, Accuracy: 99.37%, Grad Norm: 0.14462\n",
      "Epoch [520/10044], Batch [6/7], Loss: 0.0420, Accuracy: 99.18%, Grad Norm: 0.17775\n",
      "Epoch [520/10044], Batch [7/7], Loss: 0.0334, Accuracy: 99.37%, Grad Norm: 0.20802\n",
      "Epoch [520/10044], Loss: 0.0334\n",
      "Epoch [521/10044], Batch [1/7], Loss: 0.0380, Accuracy: 99.50%, Grad Norm: 0.15911\n",
      "Epoch [521/10044], Batch [2/7], Loss: 0.0449, Accuracy: 99.06%, Grad Norm: 0.20169\n",
      "Epoch [521/10044], Batch [3/7], Loss: 0.0363, Accuracy: 99.56%, Grad Norm: 0.13638\n",
      "Epoch [521/10044], Batch [4/7], Loss: 0.0391, Accuracy: 99.44%, Grad Norm: 0.12501\n",
      "Epoch [521/10044], Batch [5/7], Loss: 0.0383, Accuracy: 99.38%, Grad Norm: 0.17274\n",
      "Epoch [521/10044], Batch [6/7], Loss: 0.0423, Accuracy: 99.22%, Grad Norm: 0.17930\n",
      "Epoch [521/10044], Batch [7/7], Loss: 0.0337, Accuracy: 99.30%, Grad Norm: 0.21238\n",
      "Epoch [521/10044], Loss: 0.0337\n",
      "Epoch [522/10044], Batch [1/7], Loss: 0.0382, Accuracy: 99.48%, Grad Norm: 0.15330\n",
      "Epoch [522/10044], Batch [2/7], Loss: 0.0410, Accuracy: 99.17%, Grad Norm: 0.16158\n",
      "Epoch [522/10044], Batch [3/7], Loss: 0.0344, Accuracy: 99.58%, Grad Norm: 0.11995\n",
      "Epoch [522/10044], Batch [4/7], Loss: 0.0399, Accuracy: 99.38%, Grad Norm: 0.13026\n",
      "Epoch [522/10044], Batch [5/7], Loss: 0.0374, Accuracy: 99.37%, Grad Norm: 0.17128\n",
      "Epoch [522/10044], Batch [6/7], Loss: 0.0406, Accuracy: 99.27%, Grad Norm: 0.17142\n",
      "Epoch [522/10044], Batch [7/7], Loss: 0.0341, Accuracy: 99.35%, Grad Norm: 0.19062\n",
      "Epoch [522/10044], Loss: 0.0341\n",
      "Epoch [523/10044], Batch [1/7], Loss: 0.0380, Accuracy: 99.45%, Grad Norm: 0.15087\n",
      "Epoch [523/10044], Batch [2/7], Loss: 0.0395, Accuracy: 99.29%, Grad Norm: 0.14189\n",
      "Epoch [523/10044], Batch [3/7], Loss: 0.0346, Accuracy: 99.59%, Grad Norm: 0.12486\n",
      "Epoch [523/10044], Batch [4/7], Loss: 0.0387, Accuracy: 99.42%, Grad Norm: 0.16628\n",
      "Epoch [523/10044], Batch [5/7], Loss: 0.0377, Accuracy: 99.42%, Grad Norm: 0.20262\n",
      "Epoch [523/10044], Batch [6/7], Loss: 0.0424, Accuracy: 99.11%, Grad Norm: 0.18114\n",
      "Epoch [523/10044], Batch [7/7], Loss: 0.0315, Accuracy: 99.43%, Grad Norm: 0.18107\n",
      "Epoch [523/10044], Loss: 0.0315\n",
      "Epoch [524/10044], Batch [1/7], Loss: 0.0385, Accuracy: 99.44%, Grad Norm: 0.15934\n",
      "Epoch [524/10044], Batch [2/7], Loss: 0.0429, Accuracy: 99.13%, Grad Norm: 0.18116\n",
      "Epoch [524/10044], Batch [3/7], Loss: 0.0359, Accuracy: 99.58%, Grad Norm: 0.14817\n",
      "Epoch [524/10044], Batch [4/7], Loss: 0.0397, Accuracy: 99.42%, Grad Norm: 0.15391\n",
      "Epoch [524/10044], Batch [5/7], Loss: 0.0399, Accuracy: 99.27%, Grad Norm: 0.17650\n",
      "Epoch [524/10044], Batch [6/7], Loss: 0.0424, Accuracy: 99.12%, Grad Norm: 0.19287\n",
      "Epoch [524/10044], Batch [7/7], Loss: 0.0309, Accuracy: 99.38%, Grad Norm: 0.19229\n",
      "Epoch [524/10044], Loss: 0.0309\n",
      "Epoch [525/10044], Batch [1/7], Loss: 0.0412, Accuracy: 99.29%, Grad Norm: 0.18683\n",
      "Epoch [525/10044], Batch [2/7], Loss: 0.0423, Accuracy: 99.15%, Grad Norm: 0.18485\n",
      "Epoch [525/10044], Batch [3/7], Loss: 0.0343, Accuracy: 99.66%, Grad Norm: 0.12992\n",
      "Epoch [525/10044], Batch [4/7], Loss: 0.0378, Accuracy: 99.43%, Grad Norm: 0.13005\n",
      "Epoch [525/10044], Batch [5/7], Loss: 0.0392, Accuracy: 99.32%, Grad Norm: 0.16216\n",
      "Epoch [525/10044], Batch [6/7], Loss: 0.0408, Accuracy: 99.17%, Grad Norm: 0.17036\n",
      "Epoch [525/10044], Batch [7/7], Loss: 0.0352, Accuracy: 99.32%, Grad Norm: 0.19920\n",
      "Epoch [525/10044], Loss: 0.0352\n",
      "Epoch [526/10044], Batch [1/7], Loss: 0.0389, Accuracy: 99.40%, Grad Norm: 0.18489\n",
      "Epoch [526/10044], Batch [2/7], Loss: 0.0421, Accuracy: 99.12%, Grad Norm: 0.18434\n",
      "Epoch [526/10044], Batch [3/7], Loss: 0.0344, Accuracy: 99.59%, Grad Norm: 0.13364\n",
      "Epoch [526/10044], Batch [4/7], Loss: 0.0401, Accuracy: 99.42%, Grad Norm: 0.16860\n",
      "Epoch [526/10044], Batch [5/7], Loss: 0.0377, Accuracy: 99.41%, Grad Norm: 0.17497\n",
      "Epoch [526/10044], Batch [6/7], Loss: 0.0399, Accuracy: 99.17%, Grad Norm: 0.15470\n",
      "Epoch [526/10044], Batch [7/7], Loss: 0.0317, Accuracy: 99.38%, Grad Norm: 0.17985\n",
      "Epoch [526/10044], Loss: 0.0317\n",
      "Epoch [527/10044], Batch [1/7], Loss: 0.0386, Accuracy: 99.35%, Grad Norm: 0.16269\n",
      "Epoch [527/10044], Batch [2/7], Loss: 0.0433, Accuracy: 99.11%, Grad Norm: 0.18335\n",
      "Epoch [527/10044], Batch [3/7], Loss: 0.0352, Accuracy: 99.53%, Grad Norm: 0.13473\n",
      "Epoch [527/10044], Batch [4/7], Loss: 0.0384, Accuracy: 99.43%, Grad Norm: 0.15727\n",
      "Epoch [527/10044], Batch [5/7], Loss: 0.0372, Accuracy: 99.37%, Grad Norm: 0.15583\n",
      "Epoch [527/10044], Batch [6/7], Loss: 0.0415, Accuracy: 99.21%, Grad Norm: 0.16547\n",
      "Epoch [527/10044], Batch [7/7], Loss: 0.0296, Accuracy: 99.55%, Grad Norm: 0.18479\n",
      "Epoch [527/10044], Loss: 0.0296\n",
      "Epoch [528/10044], Batch [1/7], Loss: 0.0378, Accuracy: 99.49%, Grad Norm: 0.16788\n",
      "Epoch [528/10044], Batch [2/7], Loss: 0.0400, Accuracy: 99.29%, Grad Norm: 0.18507\n",
      "Epoch [528/10044], Batch [3/7], Loss: 0.0338, Accuracy: 99.69%, Grad Norm: 0.13091\n",
      "Epoch [528/10044], Batch [4/7], Loss: 0.0365, Accuracy: 99.52%, Grad Norm: 0.11961\n",
      "Epoch [528/10044], Batch [5/7], Loss: 0.0390, Accuracy: 99.38%, Grad Norm: 0.16615\n",
      "Epoch [528/10044], Batch [6/7], Loss: 0.0417, Accuracy: 99.14%, Grad Norm: 0.18264\n",
      "Epoch [528/10044], Batch [7/7], Loss: 0.0302, Accuracy: 99.47%, Grad Norm: 0.18661\n",
      "Epoch [528/10044], Loss: 0.0302\n",
      "Epoch [529/10044], Batch [1/7], Loss: 0.0369, Accuracy: 99.37%, Grad Norm: 0.15894\n",
      "Epoch [529/10044], Batch [2/7], Loss: 0.0395, Accuracy: 99.29%, Grad Norm: 0.16384\n",
      "Epoch [529/10044], Batch [3/7], Loss: 0.0350, Accuracy: 99.52%, Grad Norm: 0.13286\n",
      "Epoch [529/10044], Batch [4/7], Loss: 0.0381, Accuracy: 99.41%, Grad Norm: 0.12983\n",
      "Epoch [529/10044], Batch [5/7], Loss: 0.0377, Accuracy: 99.42%, Grad Norm: 0.16888\n",
      "Epoch [529/10044], Batch [6/7], Loss: 0.0392, Accuracy: 99.26%, Grad Norm: 0.16744\n",
      "Epoch [529/10044], Batch [7/7], Loss: 0.0276, Accuracy: 99.55%, Grad Norm: 0.16027\n",
      "Epoch [529/10044], Loss: 0.0276\n",
      "Epoch [530/10044], Batch [1/7], Loss: 0.0357, Accuracy: 99.58%, Grad Norm: 0.15442\n",
      "Epoch [530/10044], Batch [2/7], Loss: 0.0423, Accuracy: 99.20%, Grad Norm: 0.18699\n",
      "Epoch [530/10044], Batch [3/7], Loss: 0.0344, Accuracy: 99.53%, Grad Norm: 0.13707\n",
      "Epoch [530/10044], Batch [4/7], Loss: 0.0376, Accuracy: 99.41%, Grad Norm: 0.13424\n",
      "Epoch [530/10044], Batch [5/7], Loss: 0.0372, Accuracy: 99.40%, Grad Norm: 0.15391\n",
      "Epoch [530/10044], Batch [6/7], Loss: 0.0362, Accuracy: 99.36%, Grad Norm: 0.15262\n",
      "Epoch [530/10044], Batch [7/7], Loss: 0.0317, Accuracy: 99.38%, Grad Norm: 0.18525\n",
      "Epoch [530/10044], Loss: 0.0317\n",
      "Epoch [531/10044], Batch [1/7], Loss: 0.0356, Accuracy: 99.46%, Grad Norm: 0.14739\n",
      "Epoch [531/10044], Batch [2/7], Loss: 0.0415, Accuracy: 99.17%, Grad Norm: 0.18031\n",
      "Epoch [531/10044], Batch [3/7], Loss: 0.0343, Accuracy: 99.57%, Grad Norm: 0.12177\n",
      "Epoch [531/10044], Batch [4/7], Loss: 0.0371, Accuracy: 99.42%, Grad Norm: 0.12579\n",
      "Epoch [531/10044], Batch [5/7], Loss: 0.0376, Accuracy: 99.37%, Grad Norm: 0.16534\n",
      "Epoch [531/10044], Batch [6/7], Loss: 0.0401, Accuracy: 99.27%, Grad Norm: 0.16298\n",
      "Epoch [531/10044], Batch [7/7], Loss: 0.0305, Accuracy: 99.35%, Grad Norm: 0.19697\n",
      "Epoch [531/10044], Loss: 0.0305\n",
      "Epoch [532/10044], Batch [1/7], Loss: 0.0372, Accuracy: 99.46%, Grad Norm: 0.15778\n",
      "Epoch [532/10044], Batch [2/7], Loss: 0.0413, Accuracy: 99.14%, Grad Norm: 0.20976\n",
      "Epoch [532/10044], Batch [3/7], Loss: 0.0330, Accuracy: 99.67%, Grad Norm: 0.13921\n",
      "Epoch [532/10044], Batch [4/7], Loss: 0.0370, Accuracy: 99.49%, Grad Norm: 0.13390\n",
      "Epoch [532/10044], Batch [5/7], Loss: 0.0371, Accuracy: 99.37%, Grad Norm: 0.16131\n",
      "Epoch [532/10044], Batch [6/7], Loss: 0.0385, Accuracy: 99.22%, Grad Norm: 0.16302\n",
      "Epoch [532/10044], Batch [7/7], Loss: 0.0292, Accuracy: 99.38%, Grad Norm: 0.16929\n",
      "Epoch [532/10044], Loss: 0.0292\n",
      "Epoch [533/10044], Batch [1/7], Loss: 0.0369, Accuracy: 99.47%, Grad Norm: 0.16445\n",
      "Epoch [533/10044], Batch [2/7], Loss: 0.0387, Accuracy: 99.29%, Grad Norm: 0.19737\n",
      "Epoch [533/10044], Batch [3/7], Loss: 0.0338, Accuracy: 99.57%, Grad Norm: 0.13611\n",
      "Epoch [533/10044], Batch [4/7], Loss: 0.0367, Accuracy: 99.46%, Grad Norm: 0.12706\n",
      "Epoch [533/10044], Batch [5/7], Loss: 0.0385, Accuracy: 99.39%, Grad Norm: 0.18196\n",
      "Epoch [533/10044], Batch [6/7], Loss: 0.0382, Accuracy: 99.33%, Grad Norm: 0.18022\n",
      "Epoch [533/10044], Batch [7/7], Loss: 0.0319, Accuracy: 99.32%, Grad Norm: 0.19469\n",
      "Epoch [533/10044], Loss: 0.0319\n",
      "Epoch [534/10044], Batch [1/7], Loss: 0.0360, Accuracy: 99.45%, Grad Norm: 0.15518\n",
      "Epoch [534/10044], Batch [2/7], Loss: 0.0414, Accuracy: 99.23%, Grad Norm: 0.19299\n",
      "Epoch [534/10044], Batch [3/7], Loss: 0.0335, Accuracy: 99.61%, Grad Norm: 0.12622\n",
      "Epoch [534/10044], Batch [4/7], Loss: 0.0368, Accuracy: 99.45%, Grad Norm: 0.14296\n",
      "Epoch [534/10044], Batch [5/7], Loss: 0.0392, Accuracy: 99.27%, Grad Norm: 0.19701\n",
      "Epoch [534/10044], Batch [6/7], Loss: 0.0394, Accuracy: 99.25%, Grad Norm: 0.18340\n",
      "Epoch [534/10044], Batch [7/7], Loss: 0.0309, Accuracy: 99.53%, Grad Norm: 0.18683\n",
      "Epoch [534/10044], Loss: 0.0309\n",
      "Epoch [535/10044], Batch [1/7], Loss: 0.0367, Accuracy: 99.48%, Grad Norm: 0.17955\n",
      "Epoch [535/10044], Batch [2/7], Loss: 0.0393, Accuracy: 99.18%, Grad Norm: 0.18636\n",
      "Epoch [535/10044], Batch [3/7], Loss: 0.0323, Accuracy: 99.67%, Grad Norm: 0.14036\n",
      "Epoch [535/10044], Batch [4/7], Loss: 0.0387, Accuracy: 99.43%, Grad Norm: 0.17530\n",
      "Epoch [535/10044], Batch [5/7], Loss: 0.0345, Accuracy: 99.52%, Grad Norm: 0.14038\n",
      "Epoch [535/10044], Batch [6/7], Loss: 0.0390, Accuracy: 99.22%, Grad Norm: 0.18381\n",
      "Epoch [535/10044], Batch [7/7], Loss: 0.0317, Accuracy: 99.33%, Grad Norm: 0.20383\n",
      "Epoch [535/10044], Loss: 0.0317\n",
      "Epoch [536/10044], Batch [1/7], Loss: 0.0369, Accuracy: 99.50%, Grad Norm: 0.15922\n",
      "Epoch [536/10044], Batch [2/7], Loss: 0.0424, Accuracy: 99.11%, Grad Norm: 0.22121\n",
      "Epoch [536/10044], Batch [3/7], Loss: 0.0331, Accuracy: 99.62%, Grad Norm: 0.14024\n",
      "Epoch [536/10044], Batch [4/7], Loss: 0.0356, Accuracy: 99.50%, Grad Norm: 0.14500\n",
      "Epoch [536/10044], Batch [5/7], Loss: 0.0360, Accuracy: 99.32%, Grad Norm: 0.16496\n",
      "Epoch [536/10044], Batch [6/7], Loss: 0.0403, Accuracy: 99.19%, Grad Norm: 0.18730\n",
      "Epoch [536/10044], Batch [7/7], Loss: 0.0302, Accuracy: 99.47%, Grad Norm: 0.22733\n",
      "Epoch [536/10044], Loss: 0.0302\n",
      "Epoch [537/10044], Batch [1/7], Loss: 0.0360, Accuracy: 99.42%, Grad Norm: 0.14897\n",
      "Epoch [537/10044], Batch [2/7], Loss: 0.0419, Accuracy: 99.17%, Grad Norm: 0.19968\n",
      "Epoch [537/10044], Batch [3/7], Loss: 0.0331, Accuracy: 99.53%, Grad Norm: 0.12330\n",
      "Epoch [537/10044], Batch [4/7], Loss: 0.0364, Accuracy: 99.47%, Grad Norm: 0.13671\n",
      "Epoch [537/10044], Batch [5/7], Loss: 0.0339, Accuracy: 99.47%, Grad Norm: 0.14881\n",
      "Epoch [537/10044], Batch [6/7], Loss: 0.0401, Accuracy: 99.17%, Grad Norm: 0.16390\n",
      "Epoch [537/10044], Batch [7/7], Loss: 0.0301, Accuracy: 99.45%, Grad Norm: 0.19294\n",
      "Epoch [537/10044], Loss: 0.0301\n",
      "Epoch [538/10044], Batch [1/7], Loss: 0.0342, Accuracy: 99.52%, Grad Norm: 0.13109\n",
      "Epoch [538/10044], Batch [2/7], Loss: 0.0391, Accuracy: 99.27%, Grad Norm: 0.17640\n",
      "Epoch [538/10044], Batch [3/7], Loss: 0.0327, Accuracy: 99.64%, Grad Norm: 0.11972\n",
      "Epoch [538/10044], Batch [4/7], Loss: 0.0381, Accuracy: 99.36%, Grad Norm: 0.15601\n",
      "Epoch [538/10044], Batch [5/7], Loss: 0.0356, Accuracy: 99.41%, Grad Norm: 0.16395\n",
      "Epoch [538/10044], Batch [6/7], Loss: 0.0374, Accuracy: 99.20%, Grad Norm: 0.15035\n",
      "Epoch [538/10044], Batch [7/7], Loss: 0.0322, Accuracy: 99.32%, Grad Norm: 0.19982\n",
      "Epoch [538/10044], Loss: 0.0322\n",
      "Epoch [539/10044], Batch [1/7], Loss: 0.0340, Accuracy: 99.50%, Grad Norm: 0.14106\n",
      "Epoch [539/10044], Batch [2/7], Loss: 0.0379, Accuracy: 99.30%, Grad Norm: 0.15844\n",
      "Epoch [539/10044], Batch [3/7], Loss: 0.0315, Accuracy: 99.67%, Grad Norm: 0.13192\n",
      "Epoch [539/10044], Batch [4/7], Loss: 0.0357, Accuracy: 99.47%, Grad Norm: 0.12684\n",
      "Epoch [539/10044], Batch [5/7], Loss: 0.0354, Accuracy: 99.47%, Grad Norm: 0.15270\n",
      "Epoch [539/10044], Batch [6/7], Loss: 0.0396, Accuracy: 99.20%, Grad Norm: 0.18823\n",
      "Epoch [539/10044], Batch [7/7], Loss: 0.0273, Accuracy: 99.50%, Grad Norm: 0.16746\n",
      "Epoch [539/10044], Loss: 0.0273\n",
      "Epoch [540/10044], Batch [1/7], Loss: 0.0320, Accuracy: 99.57%, Grad Norm: 0.13798\n",
      "Epoch [540/10044], Batch [2/7], Loss: 0.0377, Accuracy: 99.26%, Grad Norm: 0.16162\n",
      "Epoch [540/10044], Batch [3/7], Loss: 0.0319, Accuracy: 99.60%, Grad Norm: 0.12428\n",
      "Epoch [540/10044], Batch [4/7], Loss: 0.0353, Accuracy: 99.45%, Grad Norm: 0.12133\n",
      "Epoch [540/10044], Batch [5/7], Loss: 0.0357, Accuracy: 99.43%, Grad Norm: 0.15650\n",
      "Epoch [540/10044], Batch [6/7], Loss: 0.0357, Accuracy: 99.36%, Grad Norm: 0.15361\n",
      "Epoch [540/10044], Batch [7/7], Loss: 0.0272, Accuracy: 99.62%, Grad Norm: 0.16755\n",
      "Epoch [540/10044], Loss: 0.0272\n",
      "Epoch [541/10044], Batch [1/7], Loss: 0.0340, Accuracy: 99.47%, Grad Norm: 0.13719\n",
      "Epoch [541/10044], Batch [2/7], Loss: 0.0377, Accuracy: 99.25%, Grad Norm: 0.15311\n",
      "Epoch [541/10044], Batch [3/7], Loss: 0.0320, Accuracy: 99.64%, Grad Norm: 0.11639\n",
      "Epoch [541/10044], Batch [4/7], Loss: 0.0337, Accuracy: 99.62%, Grad Norm: 0.10945\n",
      "Epoch [541/10044], Batch [5/7], Loss: 0.0338, Accuracy: 99.47%, Grad Norm: 0.13813\n",
      "Epoch [541/10044], Batch [6/7], Loss: 0.0362, Accuracy: 99.30%, Grad Norm: 0.15138\n",
      "Epoch [541/10044], Batch [7/7], Loss: 0.0274, Accuracy: 99.53%, Grad Norm: 0.15485\n",
      "Epoch [541/10044], Loss: 0.0274\n",
      "Epoch [542/10044], Batch [1/7], Loss: 0.0356, Accuracy: 99.33%, Grad Norm: 0.15013\n",
      "Epoch [542/10044], Batch [2/7], Loss: 0.0373, Accuracy: 99.32%, Grad Norm: 0.16002\n",
      "Epoch [542/10044], Batch [3/7], Loss: 0.0314, Accuracy: 99.66%, Grad Norm: 0.11954\n",
      "Epoch [542/10044], Batch [4/7], Loss: 0.0360, Accuracy: 99.41%, Grad Norm: 0.13389\n",
      "Epoch [542/10044], Batch [5/7], Loss: 0.0320, Accuracy: 99.55%, Grad Norm: 0.14276\n",
      "Epoch [542/10044], Batch [6/7], Loss: 0.0357, Accuracy: 99.37%, Grad Norm: 0.13721\n",
      "Epoch [542/10044], Batch [7/7], Loss: 0.0280, Accuracy: 99.55%, Grad Norm: 0.15479\n",
      "Epoch [542/10044], Loss: 0.0280\n",
      "Epoch [543/10044], Batch [1/7], Loss: 0.0333, Accuracy: 99.48%, Grad Norm: 0.15258\n",
      "Epoch [543/10044], Batch [2/7], Loss: 0.0374, Accuracy: 99.27%, Grad Norm: 0.15097\n",
      "Epoch [543/10044], Batch [3/7], Loss: 0.0299, Accuracy: 99.65%, Grad Norm: 0.11258\n",
      "Epoch [543/10044], Batch [4/7], Loss: 0.0356, Accuracy: 99.45%, Grad Norm: 0.13689\n",
      "Epoch [543/10044], Batch [5/7], Loss: 0.0336, Accuracy: 99.47%, Grad Norm: 0.15718\n",
      "Epoch [543/10044], Batch [6/7], Loss: 0.0350, Accuracy: 99.41%, Grad Norm: 0.14661\n",
      "Epoch [543/10044], Batch [7/7], Loss: 0.0300, Accuracy: 99.38%, Grad Norm: 0.17598\n",
      "Epoch [543/10044], Loss: 0.0300\n",
      "Epoch [544/10044], Batch [1/7], Loss: 0.0350, Accuracy: 99.52%, Grad Norm: 0.14588\n",
      "Epoch [544/10044], Batch [2/7], Loss: 0.0395, Accuracy: 99.28%, Grad Norm: 0.19064\n",
      "Epoch [544/10044], Batch [3/7], Loss: 0.0339, Accuracy: 99.54%, Grad Norm: 0.14825\n",
      "Epoch [544/10044], Batch [4/7], Loss: 0.0370, Accuracy: 99.48%, Grad Norm: 0.17182\n",
      "Epoch [544/10044], Batch [5/7], Loss: 0.0348, Accuracy: 99.40%, Grad Norm: 0.15540\n",
      "Epoch [544/10044], Batch [6/7], Loss: 0.0380, Accuracy: 99.16%, Grad Norm: 0.16969\n",
      "Epoch [544/10044], Batch [7/7], Loss: 0.0277, Accuracy: 99.48%, Grad Norm: 0.17739\n",
      "Epoch [544/10044], Loss: 0.0277\n",
      "Epoch [545/10044], Batch [1/7], Loss: 0.0367, Accuracy: 99.42%, Grad Norm: 0.16187\n",
      "Epoch [545/10044], Batch [2/7], Loss: 0.0381, Accuracy: 99.26%, Grad Norm: 0.21170\n",
      "Epoch [545/10044], Batch [3/7], Loss: 0.0324, Accuracy: 99.53%, Grad Norm: 0.14453\n",
      "Epoch [545/10044], Batch [4/7], Loss: 0.0354, Accuracy: 99.46%, Grad Norm: 0.12711\n",
      "Epoch [545/10044], Batch [5/7], Loss: 0.0338, Accuracy: 99.37%, Grad Norm: 0.16907\n",
      "Epoch [545/10044], Batch [6/7], Loss: 0.0372, Accuracy: 99.24%, Grad Norm: 0.17418\n",
      "Epoch [545/10044], Batch [7/7], Loss: 0.0280, Accuracy: 99.52%, Grad Norm: 0.19966\n",
      "Epoch [545/10044], Loss: 0.0280\n",
      "Epoch [546/10044], Batch [1/7], Loss: 0.0348, Accuracy: 99.56%, Grad Norm: 0.18015\n",
      "Epoch [546/10044], Batch [2/7], Loss: 0.0413, Accuracy: 99.20%, Grad Norm: 0.21257\n",
      "Epoch [546/10044], Batch [3/7], Loss: 0.0315, Accuracy: 99.55%, Grad Norm: 0.11669\n",
      "Epoch [546/10044], Batch [4/7], Loss: 0.0327, Accuracy: 99.56%, Grad Norm: 0.11294\n",
      "Epoch [546/10044], Batch [5/7], Loss: 0.0354, Accuracy: 99.42%, Grad Norm: 0.17954\n",
      "Epoch [546/10044], Batch [6/7], Loss: 0.0397, Accuracy: 99.12%, Grad Norm: 0.20672\n",
      "Epoch [546/10044], Batch [7/7], Loss: 0.0273, Accuracy: 99.48%, Grad Norm: 0.16557\n",
      "Epoch [546/10044], Loss: 0.0273\n",
      "Epoch [547/10044], Batch [1/7], Loss: 0.0354, Accuracy: 99.52%, Grad Norm: 0.16357\n",
      "Epoch [547/10044], Batch [2/7], Loss: 0.0386, Accuracy: 99.20%, Grad Norm: 0.17488\n",
      "Epoch [547/10044], Batch [3/7], Loss: 0.0332, Accuracy: 99.61%, Grad Norm: 0.14060\n",
      "Epoch [547/10044], Batch [4/7], Loss: 0.0352, Accuracy: 99.45%, Grad Norm: 0.13559\n",
      "Epoch [547/10044], Batch [5/7], Loss: 0.0346, Accuracy: 99.37%, Grad Norm: 0.17961\n",
      "Epoch [547/10044], Batch [6/7], Loss: 0.0381, Accuracy: 99.28%, Grad Norm: 0.19555\n",
      "Epoch [547/10044], Batch [7/7], Loss: 0.0275, Accuracy: 99.50%, Grad Norm: 0.16036\n",
      "Epoch [547/10044], Loss: 0.0275\n",
      "Epoch [548/10044], Batch [1/7], Loss: 0.0344, Accuracy: 99.47%, Grad Norm: 0.13766\n",
      "Epoch [548/10044], Batch [2/7], Loss: 0.0391, Accuracy: 99.16%, Grad Norm: 0.19057\n",
      "Epoch [548/10044], Batch [3/7], Loss: 0.0320, Accuracy: 99.57%, Grad Norm: 0.14713\n",
      "Epoch [548/10044], Batch [4/7], Loss: 0.0346, Accuracy: 99.49%, Grad Norm: 0.12906\n",
      "Epoch [548/10044], Batch [5/7], Loss: 0.0357, Accuracy: 99.38%, Grad Norm: 0.16886\n",
      "Epoch [548/10044], Batch [6/7], Loss: 0.0379, Accuracy: 99.29%, Grad Norm: 0.17307\n",
      "Epoch [548/10044], Batch [7/7], Loss: 0.0272, Accuracy: 99.53%, Grad Norm: 0.15658\n",
      "Epoch [548/10044], Loss: 0.0272\n",
      "Epoch [549/10044], Batch [1/7], Loss: 0.0360, Accuracy: 99.53%, Grad Norm: 0.16254\n",
      "Epoch [549/10044], Batch [2/7], Loss: 0.0396, Accuracy: 99.21%, Grad Norm: 0.21046\n",
      "Epoch [549/10044], Batch [3/7], Loss: 0.0327, Accuracy: 99.60%, Grad Norm: 0.14232\n",
      "Epoch [549/10044], Batch [4/7], Loss: 0.0348, Accuracy: 99.47%, Grad Norm: 0.13524\n",
      "Epoch [549/10044], Batch [5/7], Loss: 0.0341, Accuracy: 99.40%, Grad Norm: 0.15384\n",
      "Epoch [549/10044], Batch [6/7], Loss: 0.0381, Accuracy: 99.27%, Grad Norm: 0.17131\n",
      "Epoch [549/10044], Batch [7/7], Loss: 0.0283, Accuracy: 99.50%, Grad Norm: 0.18860\n",
      "Epoch [549/10044], Loss: 0.0283\n",
      "Epoch [550/10044], Batch [1/7], Loss: 0.0345, Accuracy: 99.47%, Grad Norm: 0.16573\n",
      "Epoch [550/10044], Batch [2/7], Loss: 0.0389, Accuracy: 99.17%, Grad Norm: 0.19564\n",
      "Epoch [550/10044], Batch [3/7], Loss: 0.0304, Accuracy: 99.67%, Grad Norm: 0.12318\n",
      "Epoch [550/10044], Batch [4/7], Loss: 0.0334, Accuracy: 99.46%, Grad Norm: 0.10981\n",
      "Epoch [550/10044], Batch [5/7], Loss: 0.0337, Accuracy: 99.41%, Grad Norm: 0.15189\n",
      "Epoch [550/10044], Batch [6/7], Loss: 0.0381, Accuracy: 99.20%, Grad Norm: 0.17250\n",
      "Epoch [550/10044], Batch [7/7], Loss: 0.0288, Accuracy: 99.40%, Grad Norm: 0.18760\n",
      "Epoch [550/10044], Loss: 0.0288\n",
      "Epoch [551/10044], Batch [1/7], Loss: 0.0327, Accuracy: 99.46%, Grad Norm: 0.15044\n",
      "Epoch [551/10044], Batch [2/7], Loss: 0.0357, Accuracy: 99.38%, Grad Norm: 0.14510\n",
      "Epoch [551/10044], Batch [3/7], Loss: 0.0313, Accuracy: 99.55%, Grad Norm: 0.11734\n",
      "Epoch [551/10044], Batch [4/7], Loss: 0.0337, Accuracy: 99.49%, Grad Norm: 0.12754\n",
      "Epoch [551/10044], Batch [5/7], Loss: 0.0320, Accuracy: 99.53%, Grad Norm: 0.13665\n",
      "Epoch [551/10044], Batch [6/7], Loss: 0.0358, Accuracy: 99.37%, Grad Norm: 0.14765\n",
      "Epoch [551/10044], Batch [7/7], Loss: 0.0291, Accuracy: 99.47%, Grad Norm: 0.17561\n",
      "Epoch [551/10044], Loss: 0.0291\n",
      "Epoch [552/10044], Batch [1/7], Loss: 0.0328, Accuracy: 99.53%, Grad Norm: 0.13177\n",
      "Epoch [552/10044], Batch [2/7], Loss: 0.0360, Accuracy: 99.37%, Grad Norm: 0.15755\n",
      "Epoch [552/10044], Batch [3/7], Loss: 0.0296, Accuracy: 99.68%, Grad Norm: 0.11596\n",
      "Epoch [552/10044], Batch [4/7], Loss: 0.0337, Accuracy: 99.43%, Grad Norm: 0.14998\n",
      "Epoch [552/10044], Batch [5/7], Loss: 0.0327, Accuracy: 99.47%, Grad Norm: 0.15166\n",
      "Epoch [552/10044], Batch [6/7], Loss: 0.0333, Accuracy: 99.39%, Grad Norm: 0.14498\n",
      "Epoch [552/10044], Batch [7/7], Loss: 0.0271, Accuracy: 99.52%, Grad Norm: 0.15289\n",
      "Epoch [552/10044], Loss: 0.0271\n",
      "Epoch [553/10044], Batch [1/7], Loss: 0.0328, Accuracy: 99.52%, Grad Norm: 0.13091\n",
      "Epoch [553/10044], Batch [2/7], Loss: 0.0354, Accuracy: 99.37%, Grad Norm: 0.15918\n",
      "Epoch [553/10044], Batch [3/7], Loss: 0.0295, Accuracy: 99.67%, Grad Norm: 0.11158\n",
      "Epoch [553/10044], Batch [4/7], Loss: 0.0327, Accuracy: 99.51%, Grad Norm: 0.12257\n",
      "Epoch [553/10044], Batch [5/7], Loss: 0.0313, Accuracy: 99.47%, Grad Norm: 0.13976\n",
      "Epoch [553/10044], Batch [6/7], Loss: 0.0340, Accuracy: 99.36%, Grad Norm: 0.14171\n",
      "Epoch [553/10044], Batch [7/7], Loss: 0.0273, Accuracy: 99.62%, Grad Norm: 0.15689\n",
      "Epoch [553/10044], Loss: 0.0273\n",
      "Epoch [554/10044], Batch [1/7], Loss: 0.0328, Accuracy: 99.51%, Grad Norm: 0.13301\n",
      "Epoch [554/10044], Batch [2/7], Loss: 0.0337, Accuracy: 99.38%, Grad Norm: 0.15568\n",
      "Epoch [554/10044], Batch [3/7], Loss: 0.0299, Accuracy: 99.61%, Grad Norm: 0.11578\n",
      "Epoch [554/10044], Batch [4/7], Loss: 0.0320, Accuracy: 99.50%, Grad Norm: 0.10598\n",
      "Epoch [554/10044], Batch [5/7], Loss: 0.0325, Accuracy: 99.49%, Grad Norm: 0.14983\n",
      "Epoch [554/10044], Batch [6/7], Loss: 0.0350, Accuracy: 99.31%, Grad Norm: 0.15593\n",
      "Epoch [554/10044], Batch [7/7], Loss: 0.0266, Accuracy: 99.50%, Grad Norm: 0.17997\n",
      "Epoch [554/10044], Loss: 0.0266\n",
      "Epoch [555/10044], Batch [1/7], Loss: 0.0327, Accuracy: 99.49%, Grad Norm: 0.13709\n",
      "Epoch [555/10044], Batch [2/7], Loss: 0.0353, Accuracy: 99.27%, Grad Norm: 0.15973\n",
      "Epoch [555/10044], Batch [3/7], Loss: 0.0305, Accuracy: 99.61%, Grad Norm: 0.10958\n",
      "Epoch [555/10044], Batch [4/7], Loss: 0.0350, Accuracy: 99.37%, Grad Norm: 0.13159\n",
      "Epoch [555/10044], Batch [5/7], Loss: 0.0337, Accuracy: 99.42%, Grad Norm: 0.17456\n",
      "Epoch [555/10044], Batch [6/7], Loss: 0.0363, Accuracy: 99.29%, Grad Norm: 0.15872\n",
      "Epoch [555/10044], Batch [7/7], Loss: 0.0277, Accuracy: 99.53%, Grad Norm: 0.16716\n",
      "Epoch [555/10044], Loss: 0.0277\n",
      "Epoch [556/10044], Batch [1/7], Loss: 0.0330, Accuracy: 99.47%, Grad Norm: 0.14794\n",
      "Epoch [556/10044], Batch [2/7], Loss: 0.0347, Accuracy: 99.28%, Grad Norm: 0.17742\n",
      "Epoch [556/10044], Batch [3/7], Loss: 0.0306, Accuracy: 99.57%, Grad Norm: 0.13108\n",
      "Epoch [556/10044], Batch [4/7], Loss: 0.0313, Accuracy: 99.52%, Grad Norm: 0.10543\n",
      "Epoch [556/10044], Batch [5/7], Loss: 0.0306, Accuracy: 99.47%, Grad Norm: 0.14053\n",
      "Epoch [556/10044], Batch [6/7], Loss: 0.0362, Accuracy: 99.30%, Grad Norm: 0.17356\n",
      "Epoch [556/10044], Batch [7/7], Loss: 0.0248, Accuracy: 99.58%, Grad Norm: 0.16920\n",
      "Epoch [556/10044], Loss: 0.0248\n",
      "Epoch [557/10044], Batch [1/7], Loss: 0.0324, Accuracy: 99.52%, Grad Norm: 0.14844\n",
      "Epoch [557/10044], Batch [2/7], Loss: 0.0375, Accuracy: 99.17%, Grad Norm: 0.19841\n",
      "Epoch [557/10044], Batch [3/7], Loss: 0.0280, Accuracy: 99.69%, Grad Norm: 0.10514\n",
      "Epoch [557/10044], Batch [4/7], Loss: 0.0353, Accuracy: 99.29%, Grad Norm: 0.14200\n",
      "Epoch [557/10044], Batch [5/7], Loss: 0.0330, Accuracy: 99.43%, Grad Norm: 0.17365\n",
      "Epoch [557/10044], Batch [6/7], Loss: 0.0353, Accuracy: 99.39%, Grad Norm: 0.17713\n",
      "Epoch [557/10044], Batch [7/7], Loss: 0.0285, Accuracy: 99.45%, Grad Norm: 0.18633\n",
      "Epoch [557/10044], Loss: 0.0285\n",
      "Epoch [558/10044], Batch [1/7], Loss: 0.0337, Accuracy: 99.52%, Grad Norm: 0.22686\n",
      "Epoch [558/10044], Batch [2/7], Loss: 0.0350, Accuracy: 99.32%, Grad Norm: 0.17287\n",
      "Epoch [558/10044], Batch [3/7], Loss: 0.0312, Accuracy: 99.56%, Grad Norm: 0.13094\n",
      "Epoch [558/10044], Batch [4/7], Loss: 0.0334, Accuracy: 99.50%, Grad Norm: 0.12271\n",
      "Epoch [558/10044], Batch [5/7], Loss: 0.0334, Accuracy: 99.33%, Grad Norm: 0.16323\n",
      "Epoch [558/10044], Batch [6/7], Loss: 0.0356, Accuracy: 99.28%, Grad Norm: 0.17880\n",
      "Epoch [558/10044], Batch [7/7], Loss: 0.0274, Accuracy: 99.48%, Grad Norm: 0.18835\n",
      "Epoch [558/10044], Loss: 0.0274\n",
      "Epoch [559/10044], Batch [1/7], Loss: 0.0349, Accuracy: 99.54%, Grad Norm: 0.17078\n",
      "Epoch [559/10044], Batch [2/7], Loss: 0.0378, Accuracy: 99.29%, Grad Norm: 0.17029\n",
      "Epoch [559/10044], Batch [3/7], Loss: 0.0296, Accuracy: 99.61%, Grad Norm: 0.11375\n",
      "Epoch [559/10044], Batch [4/7], Loss: 0.0327, Accuracy: 99.48%, Grad Norm: 0.12731\n",
      "Epoch [559/10044], Batch [5/7], Loss: 0.0315, Accuracy: 99.49%, Grad Norm: 0.15059\n",
      "Epoch [559/10044], Batch [6/7], Loss: 0.0372, Accuracy: 99.26%, Grad Norm: 0.18578\n",
      "Epoch [559/10044], Batch [7/7], Loss: 0.0248, Accuracy: 99.50%, Grad Norm: 0.16894\n",
      "Epoch [559/10044], Loss: 0.0248\n",
      "Epoch [560/10044], Batch [1/7], Loss: 0.0312, Accuracy: 99.52%, Grad Norm: 0.13315\n",
      "Epoch [560/10044], Batch [2/7], Loss: 0.0351, Accuracy: 99.27%, Grad Norm: 0.15736\n",
      "Epoch [560/10044], Batch [3/7], Loss: 0.0289, Accuracy: 99.62%, Grad Norm: 0.12237\n",
      "Epoch [560/10044], Batch [4/7], Loss: 0.0340, Accuracy: 99.51%, Grad Norm: 0.14064\n",
      "Epoch [560/10044], Batch [5/7], Loss: 0.0345, Accuracy: 99.35%, Grad Norm: 0.17236\n",
      "Epoch [560/10044], Batch [6/7], Loss: 0.0335, Accuracy: 99.35%, Grad Norm: 0.14672\n",
      "Epoch [560/10044], Batch [7/7], Loss: 0.0239, Accuracy: 99.58%, Grad Norm: 0.15115\n",
      "Epoch [560/10044], Loss: 0.0239\n",
      "Epoch [561/10044], Batch [1/7], Loss: 0.0298, Accuracy: 99.63%, Grad Norm: 0.12604\n",
      "Epoch [561/10044], Batch [2/7], Loss: 0.0375, Accuracy: 99.22%, Grad Norm: 0.19023\n",
      "Epoch [561/10044], Batch [3/7], Loss: 0.0282, Accuracy: 99.71%, Grad Norm: 0.10881\n",
      "Epoch [561/10044], Batch [4/7], Loss: 0.0340, Accuracy: 99.39%, Grad Norm: 0.13291\n",
      "Epoch [561/10044], Batch [5/7], Loss: 0.0324, Accuracy: 99.44%, Grad Norm: 0.14429\n",
      "Epoch [561/10044], Batch [6/7], Loss: 0.0338, Accuracy: 99.37%, Grad Norm: 0.14102\n",
      "Epoch [561/10044], Batch [7/7], Loss: 0.0267, Accuracy: 99.53%, Grad Norm: 0.20838\n",
      "Epoch [561/10044], Loss: 0.0267\n",
      "Epoch [562/10044], Batch [1/7], Loss: 0.0308, Accuracy: 99.55%, Grad Norm: 0.13609\n",
      "Epoch [562/10044], Batch [2/7], Loss: 0.0349, Accuracy: 99.27%, Grad Norm: 0.17722\n",
      "Epoch [562/10044], Batch [3/7], Loss: 0.0291, Accuracy: 99.67%, Grad Norm: 0.11469\n",
      "Epoch [562/10044], Batch [4/7], Loss: 0.0323, Accuracy: 99.51%, Grad Norm: 0.11691\n",
      "Epoch [562/10044], Batch [5/7], Loss: 0.0318, Accuracy: 99.43%, Grad Norm: 0.14290\n",
      "Epoch [562/10044], Batch [6/7], Loss: 0.0363, Accuracy: 99.33%, Grad Norm: 0.17087\n",
      "Epoch [562/10044], Batch [7/7], Loss: 0.0317, Accuracy: 99.43%, Grad Norm: 0.22148\n",
      "Epoch [562/10044], Loss: 0.0317\n",
      "Epoch [563/10044], Batch [1/7], Loss: 0.0318, Accuracy: 99.56%, Grad Norm: 0.13670\n",
      "Epoch [563/10044], Batch [2/7], Loss: 0.0372, Accuracy: 99.22%, Grad Norm: 0.19582\n",
      "Epoch [563/10044], Batch [3/7], Loss: 0.0275, Accuracy: 99.67%, Grad Norm: 0.10508\n",
      "Epoch [563/10044], Batch [4/7], Loss: 0.0323, Accuracy: 99.46%, Grad Norm: 0.11683\n",
      "Epoch [563/10044], Batch [5/7], Loss: 0.0309, Accuracy: 99.55%, Grad Norm: 0.16433\n",
      "Epoch [563/10044], Batch [6/7], Loss: 0.0352, Accuracy: 99.30%, Grad Norm: 0.15404\n",
      "Epoch [563/10044], Batch [7/7], Loss: 0.0276, Accuracy: 99.48%, Grad Norm: 0.18023\n",
      "Epoch [563/10044], Loss: 0.0276\n",
      "Epoch [564/10044], Batch [1/7], Loss: 0.0307, Accuracy: 99.61%, Grad Norm: 0.13434\n",
      "Epoch [564/10044], Batch [2/7], Loss: 0.0356, Accuracy: 99.31%, Grad Norm: 0.17917\n",
      "Epoch [564/10044], Batch [3/7], Loss: 0.0289, Accuracy: 99.68%, Grad Norm: 0.11544\n",
      "Epoch [564/10044], Batch [4/7], Loss: 0.0320, Accuracy: 99.47%, Grad Norm: 0.12247\n",
      "Epoch [564/10044], Batch [5/7], Loss: 0.0322, Accuracy: 99.46%, Grad Norm: 0.16529\n",
      "Epoch [564/10044], Batch [6/7], Loss: 0.0319, Accuracy: 99.44%, Grad Norm: 0.13403\n",
      "Epoch [564/10044], Batch [7/7], Loss: 0.0260, Accuracy: 99.50%, Grad Norm: 0.17331\n",
      "Epoch [564/10044], Loss: 0.0260\n",
      "Epoch [565/10044], Batch [1/7], Loss: 0.0311, Accuracy: 99.52%, Grad Norm: 0.14963\n",
      "Epoch [565/10044], Batch [2/7], Loss: 0.0334, Accuracy: 99.36%, Grad Norm: 0.15889\n",
      "Epoch [565/10044], Batch [3/7], Loss: 0.0277, Accuracy: 99.64%, Grad Norm: 0.10506\n",
      "Epoch [565/10044], Batch [4/7], Loss: 0.0321, Accuracy: 99.49%, Grad Norm: 0.13799\n",
      "Epoch [565/10044], Batch [5/7], Loss: 0.0302, Accuracy: 99.52%, Grad Norm: 0.14153\n",
      "Epoch [565/10044], Batch [6/7], Loss: 0.0315, Accuracy: 99.45%, Grad Norm: 0.13912\n",
      "Epoch [565/10044], Batch [7/7], Loss: 0.0275, Accuracy: 99.45%, Grad Norm: 0.18900\n",
      "Epoch [565/10044], Loss: 0.0275\n",
      "Epoch [566/10044], Batch [1/7], Loss: 0.0325, Accuracy: 99.47%, Grad Norm: 0.14118\n",
      "Epoch [566/10044], Batch [2/7], Loss: 0.0350, Accuracy: 99.29%, Grad Norm: 0.15898\n",
      "Epoch [566/10044], Batch [3/7], Loss: 0.0292, Accuracy: 99.60%, Grad Norm: 0.13326\n",
      "Epoch [566/10044], Batch [4/7], Loss: 0.0314, Accuracy: 99.53%, Grad Norm: 0.11470\n",
      "Epoch [566/10044], Batch [5/7], Loss: 0.0321, Accuracy: 99.52%, Grad Norm: 0.18803\n",
      "Epoch [566/10044], Batch [6/7], Loss: 0.0338, Accuracy: 99.36%, Grad Norm: 0.19437\n",
      "Epoch [566/10044], Batch [7/7], Loss: 0.0269, Accuracy: 99.53%, Grad Norm: 0.18385\n",
      "Epoch [566/10044], Loss: 0.0269\n",
      "Epoch [567/10044], Batch [1/7], Loss: 0.0286, Accuracy: 99.65%, Grad Norm: 0.13559\n",
      "Epoch [567/10044], Batch [2/7], Loss: 0.0334, Accuracy: 99.37%, Grad Norm: 0.15315\n",
      "Epoch [567/10044], Batch [3/7], Loss: 0.0276, Accuracy: 99.70%, Grad Norm: 0.11826\n",
      "Epoch [567/10044], Batch [4/7], Loss: 0.0340, Accuracy: 99.50%, Grad Norm: 0.14359\n",
      "Epoch [567/10044], Batch [5/7], Loss: 0.0315, Accuracy: 99.47%, Grad Norm: 0.17625\n",
      "Epoch [567/10044], Batch [6/7], Loss: 0.0326, Accuracy: 99.37%, Grad Norm: 0.16295\n",
      "Epoch [567/10044], Batch [7/7], Loss: 0.0282, Accuracy: 99.48%, Grad Norm: 0.18077\n",
      "Epoch [567/10044], Loss: 0.0282\n",
      "Epoch [568/10044], Batch [1/7], Loss: 0.0309, Accuracy: 99.48%, Grad Norm: 0.14395\n",
      "Epoch [568/10044], Batch [2/7], Loss: 0.0372, Accuracy: 99.13%, Grad Norm: 0.19922\n",
      "Epoch [568/10044], Batch [3/7], Loss: 0.0303, Accuracy: 99.62%, Grad Norm: 0.15076\n",
      "Epoch [568/10044], Batch [4/7], Loss: 0.0347, Accuracy: 99.37%, Grad Norm: 0.19063\n",
      "Epoch [568/10044], Batch [5/7], Loss: 0.0300, Accuracy: 99.44%, Grad Norm: 0.13563\n",
      "Epoch [568/10044], Batch [6/7], Loss: 0.0315, Accuracy: 99.41%, Grad Norm: 0.13273\n",
      "Epoch [568/10044], Batch [7/7], Loss: 0.0264, Accuracy: 99.52%, Grad Norm: 0.16323\n",
      "Epoch [568/10044], Loss: 0.0264\n",
      "Epoch [569/10044], Batch [1/7], Loss: 0.0309, Accuracy: 99.51%, Grad Norm: 0.14881\n",
      "Epoch [569/10044], Batch [2/7], Loss: 0.0350, Accuracy: 99.30%, Grad Norm: 0.17553\n",
      "Epoch [569/10044], Batch [3/7], Loss: 0.0282, Accuracy: 99.57%, Grad Norm: 0.12691\n",
      "Epoch [569/10044], Batch [4/7], Loss: 0.0341, Accuracy: 99.40%, Grad Norm: 0.15961\n",
      "Epoch [569/10044], Batch [5/7], Loss: 0.0306, Accuracy: 99.51%, Grad Norm: 0.15175\n",
      "Epoch [569/10044], Batch [6/7], Loss: 0.0342, Accuracy: 99.27%, Grad Norm: 0.14804\n",
      "Epoch [569/10044], Batch [7/7], Loss: 0.0251, Accuracy: 99.47%, Grad Norm: 0.18147\n",
      "Epoch [569/10044], Loss: 0.0251\n",
      "Epoch [570/10044], Batch [1/7], Loss: 0.0338, Accuracy: 99.46%, Grad Norm: 0.18019\n",
      "Epoch [570/10044], Batch [2/7], Loss: 0.0358, Accuracy: 99.22%, Grad Norm: 0.20326\n",
      "Epoch [570/10044], Batch [3/7], Loss: 0.0279, Accuracy: 99.68%, Grad Norm: 0.11758\n",
      "Epoch [570/10044], Batch [4/7], Loss: 0.0315, Accuracy: 99.50%, Grad Norm: 0.12004\n",
      "Epoch [570/10044], Batch [5/7], Loss: 0.0313, Accuracy: 99.39%, Grad Norm: 0.14650\n",
      "Epoch [570/10044], Batch [6/7], Loss: 0.0337, Accuracy: 99.26%, Grad Norm: 0.19059\n",
      "Epoch [570/10044], Batch [7/7], Loss: 0.0301, Accuracy: 99.40%, Grad Norm: 0.23260\n",
      "Epoch [570/10044], Loss: 0.0301\n",
      "Epoch [571/10044], Batch [1/7], Loss: 0.0304, Accuracy: 99.53%, Grad Norm: 0.13966\n",
      "Epoch [571/10044], Batch [2/7], Loss: 0.0368, Accuracy: 99.12%, Grad Norm: 0.18428\n",
      "Epoch [571/10044], Batch [3/7], Loss: 0.0281, Accuracy: 99.70%, Grad Norm: 0.12010\n",
      "Epoch [571/10044], Batch [4/7], Loss: 0.0322, Accuracy: 99.41%, Grad Norm: 0.14603\n",
      "Epoch [571/10044], Batch [5/7], Loss: 0.0346, Accuracy: 99.32%, Grad Norm: 0.21737\n",
      "Epoch [571/10044], Batch [6/7], Loss: 0.0342, Accuracy: 99.38%, Grad Norm: 0.18360\n",
      "Epoch [571/10044], Batch [7/7], Loss: 0.0276, Accuracy: 99.42%, Grad Norm: 0.20188\n",
      "Epoch [571/10044], Loss: 0.0276\n",
      "Epoch [572/10044], Batch [1/7], Loss: 0.0301, Accuracy: 99.55%, Grad Norm: 0.13946\n",
      "Epoch [572/10044], Batch [2/7], Loss: 0.0344, Accuracy: 99.17%, Grad Norm: 0.16423\n",
      "Epoch [572/10044], Batch [3/7], Loss: 0.0287, Accuracy: 99.64%, Grad Norm: 0.13753\n",
      "Epoch [572/10044], Batch [4/7], Loss: 0.0319, Accuracy: 99.44%, Grad Norm: 0.15730\n",
      "Epoch [572/10044], Batch [5/7], Loss: 0.0305, Accuracy: 99.45%, Grad Norm: 0.16030\n",
      "Epoch [572/10044], Batch [6/7], Loss: 0.0325, Accuracy: 99.37%, Grad Norm: 0.16166\n",
      "Epoch [572/10044], Batch [7/7], Loss: 0.0244, Accuracy: 99.50%, Grad Norm: 0.16630\n",
      "Epoch [572/10044], Loss: 0.0244\n",
      "Epoch [573/10044], Batch [1/7], Loss: 0.0280, Accuracy: 99.54%, Grad Norm: 0.12169\n",
      "Epoch [573/10044], Batch [2/7], Loss: 0.0331, Accuracy: 99.32%, Grad Norm: 0.19539\n",
      "Epoch [573/10044], Batch [3/7], Loss: 0.0273, Accuracy: 99.72%, Grad Norm: 0.12818\n",
      "Epoch [573/10044], Batch [4/7], Loss: 0.0317, Accuracy: 99.44%, Grad Norm: 0.15557\n",
      "Epoch [573/10044], Batch [5/7], Loss: 0.0305, Accuracy: 99.43%, Grad Norm: 0.14873\n",
      "Epoch [573/10044], Batch [6/7], Loss: 0.0346, Accuracy: 99.24%, Grad Norm: 0.16766\n",
      "Epoch [573/10044], Batch [7/7], Loss: 0.0244, Accuracy: 99.55%, Grad Norm: 0.17694\n",
      "Epoch [573/10044], Loss: 0.0244\n",
      "Epoch [574/10044], Batch [1/7], Loss: 0.0294, Accuracy: 99.56%, Grad Norm: 0.13249\n",
      "Epoch [574/10044], Batch [2/7], Loss: 0.0342, Accuracy: 99.31%, Grad Norm: 0.16762\n",
      "Epoch [574/10044], Batch [3/7], Loss: 0.0276, Accuracy: 99.73%, Grad Norm: 0.11805\n",
      "Epoch [574/10044], Batch [4/7], Loss: 0.0321, Accuracy: 99.45%, Grad Norm: 0.13011\n",
      "Epoch [574/10044], Batch [5/7], Loss: 0.0313, Accuracy: 99.49%, Grad Norm: 0.16321\n",
      "Epoch [574/10044], Batch [6/7], Loss: 0.0316, Accuracy: 99.43%, Grad Norm: 0.16677\n",
      "Epoch [574/10044], Batch [7/7], Loss: 0.0267, Accuracy: 99.50%, Grad Norm: 0.21242\n",
      "Epoch [574/10044], Loss: 0.0267\n",
      "Epoch [575/10044], Batch [1/7], Loss: 0.0289, Accuracy: 99.65%, Grad Norm: 0.12602\n",
      "Epoch [575/10044], Batch [2/7], Loss: 0.0332, Accuracy: 99.37%, Grad Norm: 0.16990\n",
      "Epoch [575/10044], Batch [3/7], Loss: 0.0277, Accuracy: 99.69%, Grad Norm: 0.12340\n",
      "Epoch [575/10044], Batch [4/7], Loss: 0.0324, Accuracy: 99.48%, Grad Norm: 0.13380\n",
      "Epoch [575/10044], Batch [5/7], Loss: 0.0300, Accuracy: 99.50%, Grad Norm: 0.14394\n",
      "Epoch [575/10044], Batch [6/7], Loss: 0.0344, Accuracy: 99.17%, Grad Norm: 0.18096\n",
      "Epoch [575/10044], Batch [7/7], Loss: 0.0244, Accuracy: 99.52%, Grad Norm: 0.17592\n",
      "Epoch [575/10044], Loss: 0.0244\n",
      "Epoch [576/10044], Batch [1/7], Loss: 0.0284, Accuracy: 99.57%, Grad Norm: 0.12715\n",
      "Epoch [576/10044], Batch [2/7], Loss: 0.0327, Accuracy: 99.24%, Grad Norm: 0.17419\n",
      "Epoch [576/10044], Batch [3/7], Loss: 0.0290, Accuracy: 99.57%, Grad Norm: 0.12165\n",
      "Epoch [576/10044], Batch [4/7], Loss: 0.0312, Accuracy: 99.52%, Grad Norm: 0.13141\n",
      "Epoch [576/10044], Batch [5/7], Loss: 0.0297, Accuracy: 99.38%, Grad Norm: 0.16368\n",
      "Epoch [576/10044], Batch [6/7], Loss: 0.0350, Accuracy: 99.27%, Grad Norm: 0.15942\n",
      "Epoch [576/10044], Batch [7/7], Loss: 0.0237, Accuracy: 99.67%, Grad Norm: 0.15062\n",
      "Epoch [576/10044], Loss: 0.0237\n",
      "Epoch [577/10044], Batch [1/7], Loss: 0.0293, Accuracy: 99.61%, Grad Norm: 0.13457\n",
      "Epoch [577/10044], Batch [2/7], Loss: 0.0349, Accuracy: 99.26%, Grad Norm: 0.16553\n",
      "Epoch [577/10044], Batch [3/7], Loss: 0.0276, Accuracy: 99.67%, Grad Norm: 0.11948\n",
      "Epoch [577/10044], Batch [4/7], Loss: 0.0319, Accuracy: 99.49%, Grad Norm: 0.13941\n",
      "Epoch [577/10044], Batch [5/7], Loss: 0.0301, Accuracy: 99.54%, Grad Norm: 0.13318\n",
      "Epoch [577/10044], Batch [6/7], Loss: 0.0303, Accuracy: 99.49%, Grad Norm: 0.15342\n",
      "Epoch [577/10044], Batch [7/7], Loss: 0.0268, Accuracy: 99.45%, Grad Norm: 0.20159\n",
      "Epoch [577/10044], Loss: 0.0268\n",
      "Epoch [578/10044], Batch [1/7], Loss: 0.0290, Accuracy: 99.53%, Grad Norm: 0.14130\n",
      "Epoch [578/10044], Batch [2/7], Loss: 0.0336, Accuracy: 99.32%, Grad Norm: 0.17399\n",
      "Epoch [578/10044], Batch [3/7], Loss: 0.0275, Accuracy: 99.67%, Grad Norm: 0.11982\n",
      "Epoch [578/10044], Batch [4/7], Loss: 0.0293, Accuracy: 99.57%, Grad Norm: 0.11129\n",
      "Epoch [578/10044], Batch [5/7], Loss: 0.0313, Accuracy: 99.42%, Grad Norm: 0.16817\n",
      "Epoch [578/10044], Batch [6/7], Loss: 0.0328, Accuracy: 99.35%, Grad Norm: 0.15686\n",
      "Epoch [578/10044], Batch [7/7], Loss: 0.0253, Accuracy: 99.50%, Grad Norm: 0.16907\n",
      "Epoch [578/10044], Loss: 0.0253\n",
      "Epoch [579/10044], Batch [1/7], Loss: 0.0302, Accuracy: 99.49%, Grad Norm: 0.13657\n",
      "Epoch [579/10044], Batch [2/7], Loss: 0.0323, Accuracy: 99.37%, Grad Norm: 0.17133\n",
      "Epoch [579/10044], Batch [3/7], Loss: 0.0267, Accuracy: 99.61%, Grad Norm: 0.10872\n",
      "Epoch [579/10044], Batch [4/7], Loss: 0.0304, Accuracy: 99.47%, Grad Norm: 0.11055\n",
      "Epoch [579/10044], Batch [5/7], Loss: 0.0292, Accuracy: 99.52%, Grad Norm: 0.16012\n",
      "Epoch [579/10044], Batch [6/7], Loss: 0.0329, Accuracy: 99.36%, Grad Norm: 0.17471\n",
      "Epoch [579/10044], Batch [7/7], Loss: 0.0248, Accuracy: 99.48%, Grad Norm: 0.17337\n",
      "Epoch [579/10044], Loss: 0.0248\n",
      "Epoch [580/10044], Batch [1/7], Loss: 0.0269, Accuracy: 99.60%, Grad Norm: 0.11315\n",
      "Epoch [580/10044], Batch [2/7], Loss: 0.0352, Accuracy: 99.21%, Grad Norm: 0.17760\n",
      "Epoch [580/10044], Batch [3/7], Loss: 0.0275, Accuracy: 99.64%, Grad Norm: 0.10605\n",
      "Epoch [580/10044], Batch [4/7], Loss: 0.0295, Accuracy: 99.54%, Grad Norm: 0.10901\n",
      "Epoch [580/10044], Batch [5/7], Loss: 0.0319, Accuracy: 99.41%, Grad Norm: 0.17203\n",
      "Epoch [580/10044], Batch [6/7], Loss: 0.0352, Accuracy: 99.26%, Grad Norm: 0.18173\n",
      "Epoch [580/10044], Batch [7/7], Loss: 0.0240, Accuracy: 99.57%, Grad Norm: 0.17821\n",
      "Epoch [580/10044], Loss: 0.0240\n",
      "Epoch [581/10044], Batch [1/7], Loss: 0.0264, Accuracy: 99.56%, Grad Norm: 0.11948\n",
      "Epoch [581/10044], Batch [2/7], Loss: 0.0334, Accuracy: 99.35%, Grad Norm: 0.18855\n",
      "Epoch [581/10044], Batch [3/7], Loss: 0.0274, Accuracy: 99.67%, Grad Norm: 0.11339\n",
      "Epoch [581/10044], Batch [4/7], Loss: 0.0307, Accuracy: 99.46%, Grad Norm: 0.12279\n",
      "Epoch [581/10044], Batch [5/7], Loss: 0.0304, Accuracy: 99.48%, Grad Norm: 0.16770\n",
      "Epoch [581/10044], Batch [6/7], Loss: 0.0317, Accuracy: 99.37%, Grad Norm: 0.16189\n",
      "Epoch [581/10044], Batch [7/7], Loss: 0.0244, Accuracy: 99.52%, Grad Norm: 0.17298\n",
      "Epoch [581/10044], Loss: 0.0244\n",
      "Epoch [582/10044], Batch [1/7], Loss: 0.0258, Accuracy: 99.65%, Grad Norm: 0.11320\n",
      "Epoch [582/10044], Batch [2/7], Loss: 0.0305, Accuracy: 99.37%, Grad Norm: 0.15844\n",
      "Epoch [582/10044], Batch [3/7], Loss: 0.0268, Accuracy: 99.69%, Grad Norm: 0.10885\n",
      "Epoch [582/10044], Batch [4/7], Loss: 0.0313, Accuracy: 99.47%, Grad Norm: 0.13347\n",
      "Epoch [582/10044], Batch [5/7], Loss: 0.0306, Accuracy: 99.44%, Grad Norm: 0.16709\n",
      "Epoch [582/10044], Batch [6/7], Loss: 0.0321, Accuracy: 99.37%, Grad Norm: 0.17846\n",
      "Epoch [582/10044], Batch [7/7], Loss: 0.0255, Accuracy: 99.60%, Grad Norm: 0.19526\n",
      "Epoch [582/10044], Loss: 0.0255\n",
      "Epoch [583/10044], Batch [1/7], Loss: 0.0281, Accuracy: 99.59%, Grad Norm: 0.14245\n",
      "Epoch [583/10044], Batch [2/7], Loss: 0.0335, Accuracy: 99.31%, Grad Norm: 0.17949\n",
      "Epoch [583/10044], Batch [3/7], Loss: 0.0258, Accuracy: 99.70%, Grad Norm: 0.11297\n",
      "Epoch [583/10044], Batch [4/7], Loss: 0.0285, Accuracy: 99.48%, Grad Norm: 0.12559\n",
      "Epoch [583/10044], Batch [5/7], Loss: 0.0288, Accuracy: 99.59%, Grad Norm: 0.15748\n",
      "Epoch [583/10044], Batch [6/7], Loss: 0.0310, Accuracy: 99.45%, Grad Norm: 0.16513\n",
      "Epoch [583/10044], Batch [7/7], Loss: 0.0256, Accuracy: 99.50%, Grad Norm: 0.19788\n",
      "Epoch [583/10044], Loss: 0.0256\n",
      "Epoch [584/10044], Batch [1/7], Loss: 0.0286, Accuracy: 99.50%, Grad Norm: 0.13774\n",
      "Epoch [584/10044], Batch [2/7], Loss: 0.0325, Accuracy: 99.32%, Grad Norm: 0.16366\n",
      "Epoch [584/10044], Batch [3/7], Loss: 0.0277, Accuracy: 99.67%, Grad Norm: 0.12477\n",
      "Epoch [584/10044], Batch [4/7], Loss: 0.0317, Accuracy: 99.47%, Grad Norm: 0.14451\n",
      "Epoch [584/10044], Batch [5/7], Loss: 0.0309, Accuracy: 99.44%, Grad Norm: 0.16470\n",
      "Epoch [584/10044], Batch [6/7], Loss: 0.0329, Accuracy: 99.31%, Grad Norm: 0.21499\n",
      "Epoch [584/10044], Batch [7/7], Loss: 0.0246, Accuracy: 99.48%, Grad Norm: 0.18841\n",
      "Epoch [584/10044], Loss: 0.0246\n",
      "Epoch [585/10044], Batch [1/7], Loss: 0.0301, Accuracy: 99.50%, Grad Norm: 0.14311\n",
      "Epoch [585/10044], Batch [2/7], Loss: 0.0323, Accuracy: 99.32%, Grad Norm: 0.18167\n",
      "Epoch [585/10044], Batch [3/7], Loss: 0.0261, Accuracy: 99.69%, Grad Norm: 0.14361\n",
      "Epoch [585/10044], Batch [4/7], Loss: 0.0298, Accuracy: 99.52%, Grad Norm: 0.12443\n",
      "Epoch [585/10044], Batch [5/7], Loss: 0.0307, Accuracy: 99.38%, Grad Norm: 0.18120\n",
      "Epoch [585/10044], Batch [6/7], Loss: 0.0338, Accuracy: 99.22%, Grad Norm: 0.20656\n",
      "Epoch [585/10044], Batch [7/7], Loss: 0.0220, Accuracy: 99.63%, Grad Norm: 0.17578\n",
      "Epoch [585/10044], Loss: 0.0220\n",
      "Epoch [586/10044], Batch [1/7], Loss: 0.0283, Accuracy: 99.59%, Grad Norm: 0.13752\n",
      "Epoch [586/10044], Batch [2/7], Loss: 0.0329, Accuracy: 99.30%, Grad Norm: 0.17244\n",
      "Epoch [586/10044], Batch [3/7], Loss: 0.0259, Accuracy: 99.70%, Grad Norm: 0.12725\n",
      "Epoch [586/10044], Batch [4/7], Loss: 0.0301, Accuracy: 99.45%, Grad Norm: 0.13056\n",
      "Epoch [586/10044], Batch [5/7], Loss: 0.0286, Accuracy: 99.49%, Grad Norm: 0.16221\n",
      "Epoch [586/10044], Batch [6/7], Loss: 0.0310, Accuracy: 99.37%, Grad Norm: 0.16441\n",
      "Epoch [586/10044], Batch [7/7], Loss: 0.0276, Accuracy: 99.37%, Grad Norm: 0.19197\n",
      "Epoch [586/10044], Loss: 0.0276\n",
      "Epoch [587/10044], Batch [1/7], Loss: 0.0282, Accuracy: 99.55%, Grad Norm: 0.13467\n",
      "Epoch [587/10044], Batch [2/7], Loss: 0.0306, Accuracy: 99.41%, Grad Norm: 0.15267\n",
      "Epoch [587/10044], Batch [3/7], Loss: 0.0264, Accuracy: 99.68%, Grad Norm: 0.11840\n",
      "Epoch [587/10044], Batch [4/7], Loss: 0.0301, Accuracy: 99.52%, Grad Norm: 0.13097\n",
      "Epoch [587/10044], Batch [5/7], Loss: 0.0286, Accuracy: 99.52%, Grad Norm: 0.17745\n",
      "Epoch [587/10044], Batch [6/7], Loss: 0.0306, Accuracy: 99.41%, Grad Norm: 0.15695\n",
      "Epoch [587/10044], Batch [7/7], Loss: 0.0228, Accuracy: 99.60%, Grad Norm: 0.15650\n",
      "Epoch [587/10044], Loss: 0.0228\n",
      "Epoch [588/10044], Batch [1/7], Loss: 0.0265, Accuracy: 99.72%, Grad Norm: 0.11519\n",
      "Epoch [588/10044], Batch [2/7], Loss: 0.0313, Accuracy: 99.36%, Grad Norm: 0.15397\n",
      "Epoch [588/10044], Batch [3/7], Loss: 0.0244, Accuracy: 99.78%, Grad Norm: 0.09908\n",
      "Epoch [588/10044], Batch [4/7], Loss: 0.0302, Accuracy: 99.51%, Grad Norm: 0.15126\n",
      "Epoch [588/10044], Batch [5/7], Loss: 0.0275, Accuracy: 99.52%, Grad Norm: 0.15199\n",
      "Epoch [588/10044], Batch [6/7], Loss: 0.0303, Accuracy: 99.37%, Grad Norm: 0.14893\n",
      "Epoch [588/10044], Batch [7/7], Loss: 0.0220, Accuracy: 99.55%, Grad Norm: 0.15473\n",
      "Epoch [588/10044], Loss: 0.0220\n",
      "Epoch [589/10044], Batch [1/7], Loss: 0.0283, Accuracy: 99.62%, Grad Norm: 0.13004\n",
      "Epoch [589/10044], Batch [2/7], Loss: 0.0302, Accuracy: 99.41%, Grad Norm: 0.14961\n",
      "Epoch [589/10044], Batch [3/7], Loss: 0.0256, Accuracy: 99.66%, Grad Norm: 0.11439\n",
      "Epoch [589/10044], Batch [4/7], Loss: 0.0279, Accuracy: 99.54%, Grad Norm: 0.10881\n",
      "Epoch [589/10044], Batch [5/7], Loss: 0.0284, Accuracy: 99.47%, Grad Norm: 0.15143\n",
      "Epoch [589/10044], Batch [6/7], Loss: 0.0287, Accuracy: 99.49%, Grad Norm: 0.14179\n",
      "Epoch [589/10044], Batch [7/7], Loss: 0.0224, Accuracy: 99.58%, Grad Norm: 0.16010\n",
      "Epoch [589/10044], Loss: 0.0224\n",
      "Epoch [590/10044], Batch [1/7], Loss: 0.0273, Accuracy: 99.52%, Grad Norm: 0.12130\n",
      "Epoch [590/10044], Batch [2/7], Loss: 0.0332, Accuracy: 99.30%, Grad Norm: 0.18183\n",
      "Epoch [590/10044], Batch [3/7], Loss: 0.0248, Accuracy: 99.69%, Grad Norm: 0.10915\n",
      "Epoch [590/10044], Batch [4/7], Loss: 0.0280, Accuracy: 99.49%, Grad Norm: 0.10272\n",
      "Epoch [590/10044], Batch [5/7], Loss: 0.0282, Accuracy: 99.47%, Grad Norm: 0.14531\n",
      "Epoch [590/10044], Batch [6/7], Loss: 0.0305, Accuracy: 99.27%, Grad Norm: 0.15611\n",
      "Epoch [590/10044], Batch [7/7], Loss: 0.0217, Accuracy: 99.63%, Grad Norm: 0.16136\n",
      "Epoch [590/10044], Loss: 0.0217\n",
      "Epoch [591/10044], Batch [1/7], Loss: 0.0270, Accuracy: 99.57%, Grad Norm: 0.11084\n",
      "Epoch [591/10044], Batch [2/7], Loss: 0.0331, Accuracy: 99.32%, Grad Norm: 0.17946\n",
      "Epoch [591/10044], Batch [3/7], Loss: 0.0251, Accuracy: 99.68%, Grad Norm: 0.10227\n",
      "Epoch [591/10044], Batch [4/7], Loss: 0.0293, Accuracy: 99.52%, Grad Norm: 0.12466\n",
      "Epoch [591/10044], Batch [5/7], Loss: 0.0284, Accuracy: 99.50%, Grad Norm: 0.15646\n",
      "Epoch [591/10044], Batch [6/7], Loss: 0.0326, Accuracy: 99.22%, Grad Norm: 0.19156\n",
      "Epoch [591/10044], Batch [7/7], Loss: 0.0246, Accuracy: 99.50%, Grad Norm: 0.15827\n",
      "Epoch [591/10044], Loss: 0.0246\n",
      "Epoch [592/10044], Batch [1/7], Loss: 0.0260, Accuracy: 99.58%, Grad Norm: 0.12846\n",
      "Epoch [592/10044], Batch [2/7], Loss: 0.0320, Accuracy: 99.34%, Grad Norm: 0.19914\n",
      "Epoch [592/10044], Batch [3/7], Loss: 0.0253, Accuracy: 99.70%, Grad Norm: 0.11376\n",
      "Epoch [592/10044], Batch [4/7], Loss: 0.0288, Accuracy: 99.51%, Grad Norm: 0.14038\n",
      "Epoch [592/10044], Batch [5/7], Loss: 0.0267, Accuracy: 99.60%, Grad Norm: 0.13642\n",
      "Epoch [592/10044], Batch [6/7], Loss: 0.0295, Accuracy: 99.51%, Grad Norm: 0.14932\n",
      "Epoch [592/10044], Batch [7/7], Loss: 0.0223, Accuracy: 99.58%, Grad Norm: 0.16766\n",
      "Epoch [592/10044], Loss: 0.0223\n",
      "Epoch [593/10044], Batch [1/7], Loss: 0.0276, Accuracy: 99.52%, Grad Norm: 0.13747\n",
      "Epoch [593/10044], Batch [2/7], Loss: 0.0303, Accuracy: 99.34%, Grad Norm: 0.16521\n",
      "Epoch [593/10044], Batch [3/7], Loss: 0.0264, Accuracy: 99.62%, Grad Norm: 0.11616\n",
      "Epoch [593/10044], Batch [4/7], Loss: 0.0287, Accuracy: 99.60%, Grad Norm: 0.13436\n",
      "Epoch [593/10044], Batch [5/7], Loss: 0.0276, Accuracy: 99.52%, Grad Norm: 0.14525\n",
      "Epoch [593/10044], Batch [6/7], Loss: 0.0300, Accuracy: 99.35%, Grad Norm: 0.14656\n",
      "Epoch [593/10044], Batch [7/7], Loss: 0.0227, Accuracy: 99.52%, Grad Norm: 0.18557\n",
      "Epoch [593/10044], Loss: 0.0227\n",
      "Epoch [594/10044], Batch [1/7], Loss: 0.0299, Accuracy: 99.47%, Grad Norm: 0.19475\n",
      "Epoch [594/10044], Batch [2/7], Loss: 0.0302, Accuracy: 99.40%, Grad Norm: 0.17753\n",
      "Epoch [594/10044], Batch [3/7], Loss: 0.0249, Accuracy: 99.66%, Grad Norm: 0.10803\n",
      "Epoch [594/10044], Batch [4/7], Loss: 0.0294, Accuracy: 99.51%, Grad Norm: 0.11847\n",
      "Epoch [594/10044], Batch [5/7], Loss: 0.0281, Accuracy: 99.50%, Grad Norm: 0.14776\n",
      "Epoch [594/10044], Batch [6/7], Loss: 0.0317, Accuracy: 99.37%, Grad Norm: 0.15029\n",
      "Epoch [594/10044], Batch [7/7], Loss: 0.0238, Accuracy: 99.57%, Grad Norm: 0.17647\n",
      "Epoch [594/10044], Loss: 0.0238\n",
      "Epoch [595/10044], Batch [1/7], Loss: 0.0306, Accuracy: 99.44%, Grad Norm: 0.15521\n",
      "Epoch [595/10044], Batch [2/7], Loss: 0.0323, Accuracy: 99.29%, Grad Norm: 0.16567\n",
      "Epoch [595/10044], Batch [3/7], Loss: 0.0259, Accuracy: 99.71%, Grad Norm: 0.11465\n",
      "Epoch [595/10044], Batch [4/7], Loss: 0.0293, Accuracy: 99.52%, Grad Norm: 0.12618\n",
      "Epoch [595/10044], Batch [5/7], Loss: 0.0294, Accuracy: 99.43%, Grad Norm: 0.18541\n",
      "Epoch [595/10044], Batch [6/7], Loss: 0.0315, Accuracy: 99.37%, Grad Norm: 0.18072\n",
      "Epoch [595/10044], Batch [7/7], Loss: 0.0220, Accuracy: 99.60%, Grad Norm: 0.16144\n",
      "Epoch [595/10044], Loss: 0.0220\n",
      "Epoch [596/10044], Batch [1/7], Loss: 0.0268, Accuracy: 99.54%, Grad Norm: 0.13288\n",
      "Epoch [596/10044], Batch [2/7], Loss: 0.0297, Accuracy: 99.45%, Grad Norm: 0.17410\n",
      "Epoch [596/10044], Batch [3/7], Loss: 0.0233, Accuracy: 99.78%, Grad Norm: 0.09127\n",
      "Epoch [596/10044], Batch [4/7], Loss: 0.0266, Accuracy: 99.61%, Grad Norm: 0.10711\n",
      "Epoch [596/10044], Batch [5/7], Loss: 0.0284, Accuracy: 99.45%, Grad Norm: 0.16128\n",
      "Epoch [596/10044], Batch [6/7], Loss: 0.0316, Accuracy: 99.35%, Grad Norm: 0.18036\n",
      "Epoch [596/10044], Batch [7/7], Loss: 0.0254, Accuracy: 99.50%, Grad Norm: 0.19931\n",
      "Epoch [596/10044], Loss: 0.0254\n",
      "Epoch [597/10044], Batch [1/7], Loss: 0.0270, Accuracy: 99.62%, Grad Norm: 0.13191\n",
      "Epoch [597/10044], Batch [2/7], Loss: 0.0301, Accuracy: 99.41%, Grad Norm: 0.17770\n",
      "Epoch [597/10044], Batch [3/7], Loss: 0.0252, Accuracy: 99.68%, Grad Norm: 0.11813\n",
      "Epoch [597/10044], Batch [4/7], Loss: 0.0278, Accuracy: 99.57%, Grad Norm: 0.12505\n",
      "Epoch [597/10044], Batch [5/7], Loss: 0.0272, Accuracy: 99.48%, Grad Norm: 0.17103\n",
      "Epoch [597/10044], Batch [6/7], Loss: 0.0309, Accuracy: 99.31%, Grad Norm: 0.18459\n",
      "Epoch [597/10044], Batch [7/7], Loss: 0.0205, Accuracy: 99.57%, Grad Norm: 0.14280\n",
      "Epoch [597/10044], Loss: 0.0205\n",
      "Epoch [598/10044], Batch [1/7], Loss: 0.0289, Accuracy: 99.47%, Grad Norm: 0.13728\n",
      "Epoch [598/10044], Batch [2/7], Loss: 0.0305, Accuracy: 99.37%, Grad Norm: 0.17920\n",
      "Epoch [598/10044], Batch [3/7], Loss: 0.0255, Accuracy: 99.66%, Grad Norm: 0.13259\n",
      "Epoch [598/10044], Batch [4/7], Loss: 0.0277, Accuracy: 99.56%, Grad Norm: 0.12561\n",
      "Epoch [598/10044], Batch [5/7], Loss: 0.0273, Accuracy: 99.53%, Grad Norm: 0.15385\n",
      "Epoch [598/10044], Batch [6/7], Loss: 0.0288, Accuracy: 99.46%, Grad Norm: 0.14588\n",
      "Epoch [598/10044], Batch [7/7], Loss: 0.0205, Accuracy: 99.65%, Grad Norm: 0.13402\n",
      "Epoch [598/10044], Loss: 0.0205\n",
      "Epoch [599/10044], Batch [1/7], Loss: 0.0276, Accuracy: 99.54%, Grad Norm: 0.13538\n",
      "Epoch [599/10044], Batch [2/7], Loss: 0.0295, Accuracy: 99.33%, Grad Norm: 0.19480\n",
      "Epoch [599/10044], Batch [3/7], Loss: 0.0235, Accuracy: 99.72%, Grad Norm: 0.11252\n",
      "Epoch [599/10044], Batch [4/7], Loss: 0.0294, Accuracy: 99.51%, Grad Norm: 0.13094\n",
      "Epoch [599/10044], Batch [5/7], Loss: 0.0256, Accuracy: 99.59%, Grad Norm: 0.13178\n",
      "Epoch [599/10044], Batch [6/7], Loss: 0.0275, Accuracy: 99.47%, Grad Norm: 0.13829\n",
      "Epoch [599/10044], Batch [7/7], Loss: 0.0214, Accuracy: 99.60%, Grad Norm: 0.14716\n",
      "Epoch [599/10044], Loss: 0.0214\n",
      "Epoch [600/10044], Batch [1/7], Loss: 0.0258, Accuracy: 99.60%, Grad Norm: 0.14673\n",
      "Epoch [600/10044], Batch [2/7], Loss: 0.0321, Accuracy: 99.31%, Grad Norm: 0.18315\n",
      "Epoch [600/10044], Batch [3/7], Loss: 0.0237, Accuracy: 99.75%, Grad Norm: 0.10294\n",
      "Epoch [600/10044], Batch [4/7], Loss: 0.0252, Accuracy: 99.55%, Grad Norm: 0.09819\n",
      "Epoch [600/10044], Batch [5/7], Loss: 0.0282, Accuracy: 99.48%, Grad Norm: 0.16029\n",
      "Epoch [600/10044], Batch [6/7], Loss: 0.0294, Accuracy: 99.40%, Grad Norm: 0.17039\n",
      "Epoch [600/10044], Batch [7/7], Loss: 0.0226, Accuracy: 99.65%, Grad Norm: 0.16473\n",
      "Epoch [600/10044], Loss: 0.0226\n",
      "Epoch [601/10044], Batch [1/7], Loss: 0.0275, Accuracy: 99.49%, Grad Norm: 0.17305\n",
      "Epoch [601/10044], Batch [2/7], Loss: 0.0285, Accuracy: 99.45%, Grad Norm: 0.16559\n",
      "Epoch [601/10044], Batch [3/7], Loss: 0.0250, Accuracy: 99.70%, Grad Norm: 0.12922\n",
      "Epoch [601/10044], Batch [4/7], Loss: 0.0291, Accuracy: 99.46%, Grad Norm: 0.14275\n",
      "Epoch [601/10044], Batch [5/7], Loss: 0.0279, Accuracy: 99.41%, Grad Norm: 0.16036\n",
      "Epoch [601/10044], Batch [6/7], Loss: 0.0288, Accuracy: 99.37%, Grad Norm: 0.14158\n",
      "Epoch [601/10044], Batch [7/7], Loss: 0.0216, Accuracy: 99.48%, Grad Norm: 0.17509\n",
      "Epoch [601/10044], Loss: 0.0216\n",
      "Epoch [602/10044], Batch [1/7], Loss: 0.0267, Accuracy: 99.52%, Grad Norm: 0.13578\n",
      "Epoch [602/10044], Batch [2/7], Loss: 0.0299, Accuracy: 99.42%, Grad Norm: 0.17587\n",
      "Epoch [602/10044], Batch [3/7], Loss: 0.0265, Accuracy: 99.59%, Grad Norm: 0.16223\n",
      "Epoch [602/10044], Batch [4/7], Loss: 0.0290, Accuracy: 99.53%, Grad Norm: 0.13909\n",
      "Epoch [602/10044], Batch [5/7], Loss: 0.0263, Accuracy: 99.51%, Grad Norm: 0.13769\n",
      "Epoch [602/10044], Batch [6/7], Loss: 0.0281, Accuracy: 99.51%, Grad Norm: 0.14350\n",
      "Epoch [602/10044], Batch [7/7], Loss: 0.0225, Accuracy: 99.55%, Grad Norm: 0.15577\n",
      "Epoch [602/10044], Loss: 0.0225\n",
      "Epoch [603/10044], Batch [1/7], Loss: 0.0259, Accuracy: 99.63%, Grad Norm: 0.13960\n",
      "Epoch [603/10044], Batch [2/7], Loss: 0.0301, Accuracy: 99.32%, Grad Norm: 0.16998\n",
      "Epoch [603/10044], Batch [3/7], Loss: 0.0257, Accuracy: 99.65%, Grad Norm: 0.16203\n",
      "Epoch [603/10044], Batch [4/7], Loss: 0.0299, Accuracy: 99.47%, Grad Norm: 0.13746\n",
      "Epoch [603/10044], Batch [5/7], Loss: 0.0259, Accuracy: 99.54%, Grad Norm: 0.14256\n",
      "Epoch [603/10044], Batch [6/7], Loss: 0.0298, Accuracy: 99.32%, Grad Norm: 0.16601\n",
      "Epoch [603/10044], Batch [7/7], Loss: 0.0208, Accuracy: 99.63%, Grad Norm: 0.15230\n",
      "Epoch [603/10044], Loss: 0.0208\n",
      "Epoch [604/10044], Batch [1/7], Loss: 0.0264, Accuracy: 99.61%, Grad Norm: 0.16395\n",
      "Epoch [604/10044], Batch [2/7], Loss: 0.0319, Accuracy: 99.32%, Grad Norm: 0.19331\n",
      "Epoch [604/10044], Batch [3/7], Loss: 0.0237, Accuracy: 99.68%, Grad Norm: 0.11951\n",
      "Epoch [604/10044], Batch [4/7], Loss: 0.0289, Accuracy: 99.50%, Grad Norm: 0.13438\n",
      "Epoch [604/10044], Batch [5/7], Loss: 0.0254, Accuracy: 99.52%, Grad Norm: 0.13393\n",
      "Epoch [604/10044], Batch [6/7], Loss: 0.0309, Accuracy: 99.34%, Grad Norm: 0.17236\n",
      "Epoch [604/10044], Batch [7/7], Loss: 0.0231, Accuracy: 99.58%, Grad Norm: 0.15970\n",
      "Epoch [604/10044], Loss: 0.0231\n",
      "Epoch [605/10044], Batch [1/7], Loss: 0.0270, Accuracy: 99.61%, Grad Norm: 0.14952\n",
      "Epoch [605/10044], Batch [2/7], Loss: 0.0301, Accuracy: 99.32%, Grad Norm: 0.15222\n",
      "Epoch [605/10044], Batch [3/7], Loss: 0.0236, Accuracy: 99.72%, Grad Norm: 0.11152\n",
      "Epoch [605/10044], Batch [4/7], Loss: 0.0266, Accuracy: 99.60%, Grad Norm: 0.11960\n",
      "Epoch [605/10044], Batch [5/7], Loss: 0.0282, Accuracy: 99.37%, Grad Norm: 0.16651\n",
      "Epoch [605/10044], Batch [6/7], Loss: 0.0291, Accuracy: 99.41%, Grad Norm: 0.15359\n",
      "Epoch [605/10044], Batch [7/7], Loss: 0.0212, Accuracy: 99.65%, Grad Norm: 0.15469\n",
      "Epoch [605/10044], Loss: 0.0212\n",
      "Epoch [606/10044], Batch [1/7], Loss: 0.0259, Accuracy: 99.61%, Grad Norm: 0.14496\n",
      "Epoch [606/10044], Batch [2/7], Loss: 0.0291, Accuracy: 99.46%, Grad Norm: 0.16688\n",
      "Epoch [606/10044], Batch [3/7], Loss: 0.0244, Accuracy: 99.65%, Grad Norm: 0.11544\n",
      "Epoch [606/10044], Batch [4/7], Loss: 0.0259, Accuracy: 99.60%, Grad Norm: 0.10451\n",
      "Epoch [606/10044], Batch [5/7], Loss: 0.0291, Accuracy: 99.40%, Grad Norm: 0.15992\n",
      "Epoch [606/10044], Batch [6/7], Loss: 0.0299, Accuracy: 99.37%, Grad Norm: 0.16146\n",
      "Epoch [606/10044], Batch [7/7], Loss: 0.0229, Accuracy: 99.53%, Grad Norm: 0.18225\n",
      "Epoch [606/10044], Loss: 0.0229\n",
      "Epoch [607/10044], Batch [1/7], Loss: 0.0291, Accuracy: 99.48%, Grad Norm: 0.16756\n",
      "Epoch [607/10044], Batch [2/7], Loss: 0.0299, Accuracy: 99.35%, Grad Norm: 0.20163\n",
      "Epoch [607/10044], Batch [3/7], Loss: 0.0232, Accuracy: 99.72%, Grad Norm: 0.10605\n",
      "Epoch [607/10044], Batch [4/7], Loss: 0.0287, Accuracy: 99.49%, Grad Norm: 0.13167\n",
      "Epoch [607/10044], Batch [5/7], Loss: 0.0253, Accuracy: 99.58%, Grad Norm: 0.13711\n",
      "Epoch [607/10044], Batch [6/7], Loss: 0.0273, Accuracy: 99.51%, Grad Norm: 0.13566\n",
      "Epoch [607/10044], Batch [7/7], Loss: 0.0231, Accuracy: 99.57%, Grad Norm: 0.19700\n",
      "Epoch [607/10044], Loss: 0.0231\n",
      "Epoch [608/10044], Batch [1/7], Loss: 0.0266, Accuracy: 99.62%, Grad Norm: 0.15891\n",
      "Epoch [608/10044], Batch [2/7], Loss: 0.0327, Accuracy: 99.27%, Grad Norm: 0.20087\n",
      "Epoch [608/10044], Batch [3/7], Loss: 0.0228, Accuracy: 99.77%, Grad Norm: 0.11079\n",
      "Epoch [608/10044], Batch [4/7], Loss: 0.0281, Accuracy: 99.51%, Grad Norm: 0.12126\n",
      "Epoch [608/10044], Batch [5/7], Loss: 0.0288, Accuracy: 99.51%, Grad Norm: 0.16486\n",
      "Epoch [608/10044], Batch [6/7], Loss: 0.0302, Accuracy: 99.40%, Grad Norm: 0.15527\n",
      "Epoch [608/10044], Batch [7/7], Loss: 0.0219, Accuracy: 99.50%, Grad Norm: 0.22369\n",
      "Epoch [608/10044], Loss: 0.0219\n",
      "Epoch [609/10044], Batch [1/7], Loss: 0.0266, Accuracy: 99.62%, Grad Norm: 0.13449\n",
      "Epoch [609/10044], Batch [2/7], Loss: 0.0322, Accuracy: 99.28%, Grad Norm: 0.18590\n",
      "Epoch [609/10044], Batch [3/7], Loss: 0.0243, Accuracy: 99.66%, Grad Norm: 0.14758\n",
      "Epoch [609/10044], Batch [4/7], Loss: 0.0277, Accuracy: 99.50%, Grad Norm: 0.14029\n",
      "Epoch [609/10044], Batch [5/7], Loss: 0.0263, Accuracy: 99.55%, Grad Norm: 0.15581\n",
      "Epoch [609/10044], Batch [6/7], Loss: 0.0316, Accuracy: 99.24%, Grad Norm: 0.18533\n",
      "Epoch [609/10044], Batch [7/7], Loss: 0.0221, Accuracy: 99.60%, Grad Norm: 0.19763\n",
      "Epoch [609/10044], Loss: 0.0221\n",
      "Epoch [610/10044], Batch [1/7], Loss: 0.0259, Accuracy: 99.56%, Grad Norm: 0.13411\n",
      "Epoch [610/10044], Batch [2/7], Loss: 0.0318, Accuracy: 99.22%, Grad Norm: 0.21823\n",
      "Epoch [610/10044], Batch [3/7], Loss: 0.0271, Accuracy: 99.55%, Grad Norm: 0.16610\n",
      "Epoch [610/10044], Batch [4/7], Loss: 0.0290, Accuracy: 99.53%, Grad Norm: 0.14574\n",
      "Epoch [610/10044], Batch [5/7], Loss: 0.0266, Accuracy: 99.51%, Grad Norm: 0.15257\n",
      "Epoch [610/10044], Batch [6/7], Loss: 0.0282, Accuracy: 99.44%, Grad Norm: 0.15512\n",
      "Epoch [610/10044], Batch [7/7], Loss: 0.0244, Accuracy: 99.40%, Grad Norm: 0.19445\n",
      "Epoch [610/10044], Loss: 0.0244\n",
      "Epoch [611/10044], Batch [1/7], Loss: 0.0253, Accuracy: 99.62%, Grad Norm: 0.12986\n",
      "Epoch [611/10044], Batch [2/7], Loss: 0.0310, Accuracy: 99.30%, Grad Norm: 0.19180\n",
      "Epoch [611/10044], Batch [3/7], Loss: 0.0232, Accuracy: 99.72%, Grad Norm: 0.12817\n",
      "Epoch [611/10044], Batch [4/7], Loss: 0.0268, Accuracy: 99.53%, Grad Norm: 0.13782\n",
      "Epoch [611/10044], Batch [5/7], Loss: 0.0261, Accuracy: 99.50%, Grad Norm: 0.15833\n",
      "Epoch [611/10044], Batch [6/7], Loss: 0.0275, Accuracy: 99.41%, Grad Norm: 0.15104\n",
      "Epoch [611/10044], Batch [7/7], Loss: 0.0207, Accuracy: 99.63%, Grad Norm: 0.15732\n",
      "Epoch [611/10044], Loss: 0.0207\n",
      "Epoch [612/10044], Batch [1/7], Loss: 0.0247, Accuracy: 99.60%, Grad Norm: 0.12539\n",
      "Epoch [612/10044], Batch [2/7], Loss: 0.0286, Accuracy: 99.42%, Grad Norm: 0.16025\n",
      "Epoch [612/10044], Batch [3/7], Loss: 0.0232, Accuracy: 99.71%, Grad Norm: 0.10126\n",
      "Epoch [612/10044], Batch [4/7], Loss: 0.0268, Accuracy: 99.51%, Grad Norm: 0.12165\n",
      "Epoch [612/10044], Batch [5/7], Loss: 0.0268, Accuracy: 99.49%, Grad Norm: 0.15285\n",
      "Epoch [612/10044], Batch [6/7], Loss: 0.0286, Accuracy: 99.40%, Grad Norm: 0.16674\n",
      "Epoch [612/10044], Batch [7/7], Loss: 0.0233, Accuracy: 99.53%, Grad Norm: 0.18405\n",
      "Epoch [612/10044], Loss: 0.0233\n",
      "Epoch [613/10044], Batch [1/7], Loss: 0.0249, Accuracy: 99.61%, Grad Norm: 0.14714\n",
      "Epoch [613/10044], Batch [2/7], Loss: 0.0276, Accuracy: 99.47%, Grad Norm: 0.14845\n",
      "Epoch [613/10044], Batch [3/7], Loss: 0.0237, Accuracy: 99.68%, Grad Norm: 0.11517\n",
      "Epoch [613/10044], Batch [4/7], Loss: 0.0254, Accuracy: 99.63%, Grad Norm: 0.11731\n",
      "Epoch [613/10044], Batch [5/7], Loss: 0.0253, Accuracy: 99.59%, Grad Norm: 0.14623\n",
      "Epoch [613/10044], Batch [6/7], Loss: 0.0303, Accuracy: 99.32%, Grad Norm: 0.17791\n",
      "Epoch [613/10044], Batch [7/7], Loss: 0.0217, Accuracy: 99.55%, Grad Norm: 0.19633\n",
      "Epoch [613/10044], Loss: 0.0217\n",
      "Epoch [614/10044], Batch [1/7], Loss: 0.0241, Accuracy: 99.60%, Grad Norm: 0.12294\n",
      "Epoch [614/10044], Batch [2/7], Loss: 0.0265, Accuracy: 99.47%, Grad Norm: 0.14445\n",
      "Epoch [614/10044], Batch [3/7], Loss: 0.0237, Accuracy: 99.65%, Grad Norm: 0.12072\n",
      "Epoch [614/10044], Batch [4/7], Loss: 0.0269, Accuracy: 99.56%, Grad Norm: 0.11892\n",
      "Epoch [614/10044], Batch [5/7], Loss: 0.0265, Accuracy: 99.54%, Grad Norm: 0.17219\n",
      "Epoch [614/10044], Batch [6/7], Loss: 0.0296, Accuracy: 99.36%, Grad Norm: 0.16422\n",
      "Epoch [614/10044], Batch [7/7], Loss: 0.0220, Accuracy: 99.63%, Grad Norm: 0.17095\n",
      "Epoch [614/10044], Loss: 0.0220\n",
      "Epoch [615/10044], Batch [1/7], Loss: 0.0251, Accuracy: 99.67%, Grad Norm: 0.13411\n",
      "Epoch [615/10044], Batch [2/7], Loss: 0.0300, Accuracy: 99.41%, Grad Norm: 0.16926\n",
      "Epoch [615/10044], Batch [3/7], Loss: 0.0231, Accuracy: 99.66%, Grad Norm: 0.10540\n",
      "Epoch [615/10044], Batch [4/7], Loss: 0.0266, Accuracy: 99.53%, Grad Norm: 0.12693\n",
      "Epoch [615/10044], Batch [5/7], Loss: 0.0257, Accuracy: 99.57%, Grad Norm: 0.15882\n",
      "Epoch [615/10044], Batch [6/7], Loss: 0.0294, Accuracy: 99.29%, Grad Norm: 0.17246\n",
      "Epoch [615/10044], Batch [7/7], Loss: 0.0216, Accuracy: 99.52%, Grad Norm: 0.16953\n",
      "Epoch [615/10044], Loss: 0.0216\n",
      "Epoch [616/10044], Batch [1/7], Loss: 0.0272, Accuracy: 99.56%, Grad Norm: 0.15247\n",
      "Epoch [616/10044], Batch [2/7], Loss: 0.0297, Accuracy: 99.38%, Grad Norm: 0.16481\n",
      "Epoch [616/10044], Batch [3/7], Loss: 0.0236, Accuracy: 99.71%, Grad Norm: 0.11624\n",
      "Epoch [616/10044], Batch [4/7], Loss: 0.0268, Accuracy: 99.53%, Grad Norm: 0.12127\n",
      "Epoch [616/10044], Batch [5/7], Loss: 0.0261, Accuracy: 99.50%, Grad Norm: 0.17783\n",
      "Epoch [616/10044], Batch [6/7], Loss: 0.0259, Accuracy: 99.47%, Grad Norm: 0.14178\n",
      "Epoch [616/10044], Batch [7/7], Loss: 0.0240, Accuracy: 99.60%, Grad Norm: 0.21583\n",
      "Epoch [616/10044], Loss: 0.0240\n",
      "Epoch [617/10044], Batch [1/7], Loss: 0.0238, Accuracy: 99.61%, Grad Norm: 0.13448\n",
      "Epoch [617/10044], Batch [2/7], Loss: 0.0286, Accuracy: 99.44%, Grad Norm: 0.16298\n",
      "Epoch [617/10044], Batch [3/7], Loss: 0.0237, Accuracy: 99.67%, Grad Norm: 0.11679\n",
      "Epoch [617/10044], Batch [4/7], Loss: 0.0261, Accuracy: 99.52%, Grad Norm: 0.11285\n",
      "Epoch [617/10044], Batch [5/7], Loss: 0.0242, Accuracy: 99.65%, Grad Norm: 0.13900\n",
      "Epoch [617/10044], Batch [6/7], Loss: 0.0259, Accuracy: 99.52%, Grad Norm: 0.14255\n",
      "Epoch [617/10044], Batch [7/7], Loss: 0.0252, Accuracy: 99.43%, Grad Norm: 0.20404\n",
      "Epoch [617/10044], Loss: 0.0252\n",
      "Epoch [618/10044], Batch [1/7], Loss: 0.0262, Accuracy: 99.59%, Grad Norm: 0.14327\n",
      "Epoch [618/10044], Batch [2/7], Loss: 0.0277, Accuracy: 99.46%, Grad Norm: 0.17741\n",
      "Epoch [618/10044], Batch [3/7], Loss: 0.0220, Accuracy: 99.72%, Grad Norm: 0.10937\n",
      "Epoch [618/10044], Batch [4/7], Loss: 0.0262, Accuracy: 99.64%, Grad Norm: 0.11987\n",
      "Epoch [618/10044], Batch [5/7], Loss: 0.0243, Accuracy: 99.62%, Grad Norm: 0.13099\n",
      "Epoch [618/10044], Batch [6/7], Loss: 0.0266, Accuracy: 99.42%, Grad Norm: 0.14827\n",
      "Epoch [618/10044], Batch [7/7], Loss: 0.0211, Accuracy: 99.58%, Grad Norm: 0.18438\n",
      "Epoch [618/10044], Loss: 0.0211\n",
      "Epoch [619/10044], Batch [1/7], Loss: 0.0257, Accuracy: 99.54%, Grad Norm: 0.14796\n",
      "Epoch [619/10044], Batch [2/7], Loss: 0.0299, Accuracy: 99.28%, Grad Norm: 0.17269\n",
      "Epoch [619/10044], Batch [3/7], Loss: 0.0221, Accuracy: 99.74%, Grad Norm: 0.11420\n",
      "Epoch [619/10044], Batch [4/7], Loss: 0.0266, Accuracy: 99.61%, Grad Norm: 0.13333\n",
      "Epoch [619/10044], Batch [5/7], Loss: 0.0263, Accuracy: 99.49%, Grad Norm: 0.16022\n",
      "Epoch [619/10044], Batch [6/7], Loss: 0.0261, Accuracy: 99.46%, Grad Norm: 0.13651\n",
      "Epoch [619/10044], Batch [7/7], Loss: 0.0212, Accuracy: 99.65%, Grad Norm: 0.15900\n",
      "Epoch [619/10044], Loss: 0.0212\n",
      "Epoch [620/10044], Batch [1/7], Loss: 0.0252, Accuracy: 99.62%, Grad Norm: 0.13253\n",
      "Epoch [620/10044], Batch [2/7], Loss: 0.0291, Accuracy: 99.35%, Grad Norm: 0.16877\n",
      "Epoch [620/10044], Batch [3/7], Loss: 0.0223, Accuracy: 99.71%, Grad Norm: 0.10212\n",
      "Epoch [620/10044], Batch [4/7], Loss: 0.0265, Accuracy: 99.52%, Grad Norm: 0.13498\n",
      "Epoch [620/10044], Batch [5/7], Loss: 0.0245, Accuracy: 99.52%, Grad Norm: 0.13622\n",
      "Epoch [620/10044], Batch [6/7], Loss: 0.0270, Accuracy: 99.39%, Grad Norm: 0.15241\n",
      "Epoch [620/10044], Batch [7/7], Loss: 0.0198, Accuracy: 99.63%, Grad Norm: 0.15870\n",
      "Epoch [620/10044], Loss: 0.0198\n",
      "Epoch [621/10044], Batch [1/7], Loss: 0.0252, Accuracy: 99.57%, Grad Norm: 0.13447\n",
      "Epoch [621/10044], Batch [2/7], Loss: 0.0263, Accuracy: 99.45%, Grad Norm: 0.13812\n",
      "Epoch [621/10044], Batch [3/7], Loss: 0.0233, Accuracy: 99.70%, Grad Norm: 0.10722\n",
      "Epoch [621/10044], Batch [4/7], Loss: 0.0261, Accuracy: 99.51%, Grad Norm: 0.11625\n",
      "Epoch [621/10044], Batch [5/7], Loss: 0.0236, Accuracy: 99.58%, Grad Norm: 0.12885\n",
      "Epoch [621/10044], Batch [6/7], Loss: 0.0270, Accuracy: 99.42%, Grad Norm: 0.14206\n",
      "Epoch [621/10044], Batch [7/7], Loss: 0.0223, Accuracy: 99.43%, Grad Norm: 0.20300\n",
      "Epoch [621/10044], Loss: 0.0223\n",
      "Epoch [622/10044], Batch [1/7], Loss: 0.0239, Accuracy: 99.61%, Grad Norm: 0.12980\n",
      "Epoch [622/10044], Batch [2/7], Loss: 0.0274, Accuracy: 99.49%, Grad Norm: 0.16817\n",
      "Epoch [622/10044], Batch [3/7], Loss: 0.0214, Accuracy: 99.73%, Grad Norm: 0.10009\n",
      "Epoch [622/10044], Batch [4/7], Loss: 0.0264, Accuracy: 99.53%, Grad Norm: 0.10811\n",
      "Epoch [622/10044], Batch [5/7], Loss: 0.0244, Accuracy: 99.59%, Grad Norm: 0.13958\n",
      "Epoch [622/10044], Batch [6/7], Loss: 0.0270, Accuracy: 99.48%, Grad Norm: 0.14724\n",
      "Epoch [622/10044], Batch [7/7], Loss: 0.0234, Accuracy: 99.45%, Grad Norm: 0.17654\n",
      "Epoch [622/10044], Loss: 0.0234\n",
      "Epoch [623/10044], Batch [1/7], Loss: 0.0236, Accuracy: 99.67%, Grad Norm: 0.11659\n",
      "Epoch [623/10044], Batch [2/7], Loss: 0.0270, Accuracy: 99.45%, Grad Norm: 0.15443\n",
      "Epoch [623/10044], Batch [3/7], Loss: 0.0235, Accuracy: 99.65%, Grad Norm: 0.10941\n",
      "Epoch [623/10044], Batch [4/7], Loss: 0.0253, Accuracy: 99.56%, Grad Norm: 0.12666\n",
      "Epoch [623/10044], Batch [5/7], Loss: 0.0236, Accuracy: 99.67%, Grad Norm: 0.12066\n",
      "Epoch [623/10044], Batch [6/7], Loss: 0.0267, Accuracy: 99.48%, Grad Norm: 0.14817\n",
      "Epoch [623/10044], Batch [7/7], Loss: 0.0201, Accuracy: 99.62%, Grad Norm: 0.14230\n",
      "Epoch [623/10044], Loss: 0.0201\n",
      "Epoch [624/10044], Batch [1/7], Loss: 0.0270, Accuracy: 99.55%, Grad Norm: 0.18207\n",
      "Epoch [624/10044], Batch [2/7], Loss: 0.0288, Accuracy: 99.39%, Grad Norm: 0.15893\n",
      "Epoch [624/10044], Batch [3/7], Loss: 0.0215, Accuracy: 99.71%, Grad Norm: 0.11211\n",
      "Epoch [624/10044], Batch [4/7], Loss: 0.0256, Accuracy: 99.52%, Grad Norm: 0.12052\n",
      "Epoch [624/10044], Batch [5/7], Loss: 0.0229, Accuracy: 99.63%, Grad Norm: 0.12527\n",
      "Epoch [624/10044], Batch [6/7], Loss: 0.0262, Accuracy: 99.43%, Grad Norm: 0.13256\n",
      "Epoch [624/10044], Batch [7/7], Loss: 0.0186, Accuracy: 99.65%, Grad Norm: 0.13935\n",
      "Epoch [624/10044], Loss: 0.0186\n",
      "Epoch [625/10044], Batch [1/7], Loss: 0.0260, Accuracy: 99.61%, Grad Norm: 0.16180\n",
      "Epoch [625/10044], Batch [2/7], Loss: 0.0256, Accuracy: 99.44%, Grad Norm: 0.12705\n",
      "Epoch [625/10044], Batch [3/7], Loss: 0.0222, Accuracy: 99.71%, Grad Norm: 0.12597\n",
      "Epoch [625/10044], Batch [4/7], Loss: 0.0252, Accuracy: 99.59%, Grad Norm: 0.11939\n",
      "Epoch [625/10044], Batch [5/7], Loss: 0.0238, Accuracy: 99.58%, Grad Norm: 0.14310\n",
      "Epoch [625/10044], Batch [6/7], Loss: 0.0257, Accuracy: 99.52%, Grad Norm: 0.12969\n",
      "Epoch [625/10044], Batch [7/7], Loss: 0.0227, Accuracy: 99.47%, Grad Norm: 0.16881\n",
      "Epoch [625/10044], Loss: 0.0227\n",
      "Epoch [626/10044], Batch [1/7], Loss: 0.0245, Accuracy: 99.52%, Grad Norm: 0.13584\n",
      "Epoch [626/10044], Batch [2/7], Loss: 0.0263, Accuracy: 99.46%, Grad Norm: 0.14272\n",
      "Epoch [626/10044], Batch [3/7], Loss: 0.0223, Accuracy: 99.70%, Grad Norm: 0.10508\n",
      "Epoch [626/10044], Batch [4/7], Loss: 0.0257, Accuracy: 99.51%, Grad Norm: 0.13566\n",
      "Epoch [626/10044], Batch [5/7], Loss: 0.0239, Accuracy: 99.54%, Grad Norm: 0.14189\n",
      "Epoch [626/10044], Batch [6/7], Loss: 0.0267, Accuracy: 99.39%, Grad Norm: 0.16150\n",
      "Epoch [626/10044], Batch [7/7], Loss: 0.0193, Accuracy: 99.68%, Grad Norm: 0.16428\n",
      "Epoch [626/10044], Loss: 0.0193\n",
      "Epoch [627/10044], Batch [1/7], Loss: 0.0239, Accuracy: 99.56%, Grad Norm: 0.17189\n",
      "Epoch [627/10044], Batch [2/7], Loss: 0.0284, Accuracy: 99.42%, Grad Norm: 0.16727\n",
      "Epoch [627/10044], Batch [3/7], Loss: 0.0214, Accuracy: 99.69%, Grad Norm: 0.10603\n",
      "Epoch [627/10044], Batch [4/7], Loss: 0.0250, Accuracy: 99.59%, Grad Norm: 0.10609\n",
      "Epoch [627/10044], Batch [5/7], Loss: 0.0264, Accuracy: 99.53%, Grad Norm: 0.17321\n",
      "Epoch [627/10044], Batch [6/7], Loss: 0.0288, Accuracy: 99.40%, Grad Norm: 0.18352\n",
      "Epoch [627/10044], Batch [7/7], Loss: 0.0212, Accuracy: 99.63%, Grad Norm: 0.21261\n",
      "Epoch [627/10044], Loss: 0.0212\n",
      "Epoch [628/10044], Batch [1/7], Loss: 0.0243, Accuracy: 99.72%, Grad Norm: 0.13552\n",
      "Epoch [628/10044], Batch [2/7], Loss: 0.0276, Accuracy: 99.33%, Grad Norm: 0.16245\n",
      "Epoch [628/10044], Batch [3/7], Loss: 0.0222, Accuracy: 99.76%, Grad Norm: 0.10853\n",
      "Epoch [628/10044], Batch [4/7], Loss: 0.0247, Accuracy: 99.51%, Grad Norm: 0.11431\n",
      "Epoch [628/10044], Batch [5/7], Loss: 0.0259, Accuracy: 99.60%, Grad Norm: 0.16028\n",
      "Epoch [628/10044], Batch [6/7], Loss: 0.0278, Accuracy: 99.35%, Grad Norm: 0.18101\n",
      "Epoch [628/10044], Batch [7/7], Loss: 0.0219, Accuracy: 99.60%, Grad Norm: 0.18152\n",
      "Epoch [628/10044], Loss: 0.0219\n",
      "Epoch [629/10044], Batch [1/7], Loss: 0.0241, Accuracy: 99.68%, Grad Norm: 0.12359\n",
      "Epoch [629/10044], Batch [2/7], Loss: 0.0270, Accuracy: 99.47%, Grad Norm: 0.17514\n",
      "Epoch [629/10044], Batch [3/7], Loss: 0.0230, Accuracy: 99.75%, Grad Norm: 0.11350\n",
      "Epoch [629/10044], Batch [4/7], Loss: 0.0260, Accuracy: 99.53%, Grad Norm: 0.12101\n",
      "Epoch [629/10044], Batch [5/7], Loss: 0.0233, Accuracy: 99.63%, Grad Norm: 0.12320\n",
      "Epoch [629/10044], Batch [6/7], Loss: 0.0244, Accuracy: 99.62%, Grad Norm: 0.12891\n",
      "Epoch [629/10044], Batch [7/7], Loss: 0.0210, Accuracy: 99.55%, Grad Norm: 0.20233\n",
      "Epoch [629/10044], Loss: 0.0210\n",
      "Epoch [630/10044], Batch [1/7], Loss: 0.0266, Accuracy: 99.46%, Grad Norm: 0.17526\n",
      "Epoch [630/10044], Batch [2/7], Loss: 0.0279, Accuracy: 99.32%, Grad Norm: 0.17695\n",
      "Epoch [630/10044], Batch [3/7], Loss: 0.0219, Accuracy: 99.67%, Grad Norm: 0.10595\n",
      "Epoch [630/10044], Batch [4/7], Loss: 0.0245, Accuracy: 99.58%, Grad Norm: 0.10369\n",
      "Epoch [630/10044], Batch [5/7], Loss: 0.0236, Accuracy: 99.57%, Grad Norm: 0.14698\n",
      "Epoch [630/10044], Batch [6/7], Loss: 0.0246, Accuracy: 99.52%, Grad Norm: 0.14198\n",
      "Epoch [630/10044], Batch [7/7], Loss: 0.0205, Accuracy: 99.60%, Grad Norm: 0.16661\n",
      "Epoch [630/10044], Loss: 0.0205\n",
      "Epoch [631/10044], Batch [1/7], Loss: 0.0265, Accuracy: 99.54%, Grad Norm: 0.15196\n",
      "Epoch [631/10044], Batch [2/7], Loss: 0.0267, Accuracy: 99.44%, Grad Norm: 0.17009\n",
      "Epoch [631/10044], Batch [3/7], Loss: 0.0201, Accuracy: 99.77%, Grad Norm: 0.09968\n",
      "Epoch [631/10044], Batch [4/7], Loss: 0.0254, Accuracy: 99.54%, Grad Norm: 0.13230\n",
      "Epoch [631/10044], Batch [5/7], Loss: 0.0234, Accuracy: 99.61%, Grad Norm: 0.12742\n",
      "Epoch [631/10044], Batch [6/7], Loss: 0.0265, Accuracy: 99.45%, Grad Norm: 0.16381\n",
      "Epoch [631/10044], Batch [7/7], Loss: 0.0220, Accuracy: 99.65%, Grad Norm: 0.16792\n",
      "Epoch [631/10044], Loss: 0.0220\n",
      "Epoch [632/10044], Batch [1/7], Loss: 0.0241, Accuracy: 99.67%, Grad Norm: 0.14230\n",
      "Epoch [632/10044], Batch [2/7], Loss: 0.0270, Accuracy: 99.40%, Grad Norm: 0.15262\n",
      "Epoch [632/10044], Batch [3/7], Loss: 0.0228, Accuracy: 99.63%, Grad Norm: 0.11270\n",
      "Epoch [632/10044], Batch [4/7], Loss: 0.0249, Accuracy: 99.58%, Grad Norm: 0.11634\n",
      "Epoch [632/10044], Batch [5/7], Loss: 0.0243, Accuracy: 99.57%, Grad Norm: 0.13312\n",
      "Epoch [632/10044], Batch [6/7], Loss: 0.0271, Accuracy: 99.45%, Grad Norm: 0.15562\n",
      "Epoch [632/10044], Batch [7/7], Loss: 0.0204, Accuracy: 99.53%, Grad Norm: 0.22285\n",
      "Epoch [632/10044], Loss: 0.0204\n",
      "Epoch [633/10044], Batch [1/7], Loss: 0.0241, Accuracy: 99.58%, Grad Norm: 0.14351\n",
      "Epoch [633/10044], Batch [2/7], Loss: 0.0233, Accuracy: 99.58%, Grad Norm: 0.11617\n",
      "Epoch [633/10044], Batch [3/7], Loss: 0.0198, Accuracy: 99.77%, Grad Norm: 0.09012\n",
      "Epoch [633/10044], Batch [4/7], Loss: 0.0236, Accuracy: 99.55%, Grad Norm: 0.10160\n",
      "Epoch [633/10044], Batch [5/7], Loss: 0.0243, Accuracy: 99.60%, Grad Norm: 0.13976\n",
      "Epoch [633/10044], Batch [6/7], Loss: 0.0244, Accuracy: 99.57%, Grad Norm: 0.14910\n",
      "Epoch [633/10044], Batch [7/7], Loss: 0.0251, Accuracy: 99.48%, Grad Norm: 0.20731\n",
      "Epoch [633/10044], Loss: 0.0251\n",
      "Epoch [634/10044], Batch [1/7], Loss: 0.0239, Accuracy: 99.61%, Grad Norm: 0.12132\n",
      "Epoch [634/10044], Batch [2/7], Loss: 0.0245, Accuracy: 99.47%, Grad Norm: 0.14487\n",
      "Epoch [634/10044], Batch [3/7], Loss: 0.0222, Accuracy: 99.68%, Grad Norm: 0.11055\n",
      "Epoch [634/10044], Batch [4/7], Loss: 0.0250, Accuracy: 99.53%, Grad Norm: 0.11715\n",
      "Epoch [634/10044], Batch [5/7], Loss: 0.0215, Accuracy: 99.67%, Grad Norm: 0.12890\n",
      "Epoch [634/10044], Batch [6/7], Loss: 0.0241, Accuracy: 99.55%, Grad Norm: 0.15204\n",
      "Epoch [634/10044], Batch [7/7], Loss: 0.0185, Accuracy: 99.65%, Grad Norm: 0.14222\n",
      "Epoch [634/10044], Loss: 0.0185\n",
      "Epoch [635/10044], Batch [1/7], Loss: 0.0213, Accuracy: 99.75%, Grad Norm: 0.10591\n",
      "Epoch [635/10044], Batch [2/7], Loss: 0.0253, Accuracy: 99.48%, Grad Norm: 0.15844\n",
      "Epoch [635/10044], Batch [3/7], Loss: 0.0213, Accuracy: 99.68%, Grad Norm: 0.12105\n",
      "Epoch [635/10044], Batch [4/7], Loss: 0.0234, Accuracy: 99.61%, Grad Norm: 0.10516\n",
      "Epoch [635/10044], Batch [5/7], Loss: 0.0214, Accuracy: 99.63%, Grad Norm: 0.11942\n",
      "Epoch [635/10044], Batch [6/7], Loss: 0.0269, Accuracy: 99.42%, Grad Norm: 0.15278\n",
      "Epoch [635/10044], Batch [7/7], Loss: 0.0184, Accuracy: 99.67%, Grad Norm: 0.14262\n",
      "Epoch [635/10044], Loss: 0.0184\n",
      "Epoch [636/10044], Batch [1/7], Loss: 0.0243, Accuracy: 99.60%, Grad Norm: 0.13788\n",
      "Epoch [636/10044], Batch [2/7], Loss: 0.0272, Accuracy: 99.39%, Grad Norm: 0.15232\n",
      "Epoch [636/10044], Batch [3/7], Loss: 0.0217, Accuracy: 99.66%, Grad Norm: 0.11566\n",
      "Epoch [636/10044], Batch [4/7], Loss: 0.0241, Accuracy: 99.59%, Grad Norm: 0.11059\n",
      "Epoch [636/10044], Batch [5/7], Loss: 0.0236, Accuracy: 99.62%, Grad Norm: 0.12689\n",
      "Epoch [636/10044], Batch [6/7], Loss: 0.0268, Accuracy: 99.42%, Grad Norm: 0.18565\n",
      "Epoch [636/10044], Batch [7/7], Loss: 0.0189, Accuracy: 99.53%, Grad Norm: 0.15115\n",
      "Epoch [636/10044], Loss: 0.0189\n",
      "Epoch [637/10044], Batch [1/7], Loss: 0.0241, Accuracy: 99.55%, Grad Norm: 0.13562\n",
      "Epoch [637/10044], Batch [2/7], Loss: 0.0254, Accuracy: 99.45%, Grad Norm: 0.16694\n",
      "Epoch [637/10044], Batch [3/7], Loss: 0.0200, Accuracy: 99.76%, Grad Norm: 0.09252\n",
      "Epoch [637/10044], Batch [4/7], Loss: 0.0254, Accuracy: 99.55%, Grad Norm: 0.10906\n",
      "Epoch [637/10044], Batch [5/7], Loss: 0.0245, Accuracy: 99.54%, Grad Norm: 0.13912\n",
      "Epoch [637/10044], Batch [6/7], Loss: 0.0264, Accuracy: 99.39%, Grad Norm: 0.14210\n",
      "Epoch [637/10044], Batch [7/7], Loss: 0.0197, Accuracy: 99.53%, Grad Norm: 0.17054\n",
      "Epoch [637/10044], Loss: 0.0197\n",
      "Epoch [638/10044], Batch [1/7], Loss: 0.0219, Accuracy: 99.67%, Grad Norm: 0.10919\n",
      "Epoch [638/10044], Batch [2/7], Loss: 0.0250, Accuracy: 99.50%, Grad Norm: 0.15034\n",
      "Epoch [638/10044], Batch [3/7], Loss: 0.0205, Accuracy: 99.73%, Grad Norm: 0.10622\n",
      "Epoch [638/10044], Batch [4/7], Loss: 0.0225, Accuracy: 99.67%, Grad Norm: 0.09513\n",
      "Epoch [638/10044], Batch [5/7], Loss: 0.0229, Accuracy: 99.64%, Grad Norm: 0.12994\n",
      "Epoch [638/10044], Batch [6/7], Loss: 0.0264, Accuracy: 99.41%, Grad Norm: 0.15561\n",
      "Epoch [638/10044], Batch [7/7], Loss: 0.0189, Accuracy: 99.58%, Grad Norm: 0.15006\n",
      "Epoch [638/10044], Loss: 0.0189\n",
      "Epoch [639/10044], Batch [1/7], Loss: 0.0213, Accuracy: 99.63%, Grad Norm: 0.10390\n",
      "Epoch [639/10044], Batch [2/7], Loss: 0.0269, Accuracy: 99.42%, Grad Norm: 0.16246\n",
      "Epoch [639/10044], Batch [3/7], Loss: 0.0194, Accuracy: 99.73%, Grad Norm: 0.08870\n",
      "Epoch [639/10044], Batch [4/7], Loss: 0.0231, Accuracy: 99.57%, Grad Norm: 0.11873\n",
      "Epoch [639/10044], Batch [5/7], Loss: 0.0222, Accuracy: 99.60%, Grad Norm: 0.13366\n",
      "Epoch [639/10044], Batch [6/7], Loss: 0.0242, Accuracy: 99.55%, Grad Norm: 0.14161\n",
      "Epoch [639/10044], Batch [7/7], Loss: 0.0167, Accuracy: 99.80%, Grad Norm: 0.13903\n",
      "Epoch [639/10044], Loss: 0.0167\n",
      "Epoch [640/10044], Batch [1/7], Loss: 0.0205, Accuracy: 99.65%, Grad Norm: 0.10698\n",
      "Epoch [640/10044], Batch [2/7], Loss: 0.0269, Accuracy: 99.44%, Grad Norm: 0.15592\n",
      "Epoch [640/10044], Batch [3/7], Loss: 0.0206, Accuracy: 99.71%, Grad Norm: 0.10316\n",
      "Epoch [640/10044], Batch [4/7], Loss: 0.0255, Accuracy: 99.51%, Grad Norm: 0.12942\n",
      "Epoch [640/10044], Batch [5/7], Loss: 0.0208, Accuracy: 99.64%, Grad Norm: 0.13176\n",
      "Epoch [640/10044], Batch [6/7], Loss: 0.0238, Accuracy: 99.47%, Grad Norm: 0.13581\n",
      "Epoch [640/10044], Batch [7/7], Loss: 0.0202, Accuracy: 99.58%, Grad Norm: 0.15737\n",
      "Epoch [640/10044], Loss: 0.0202\n",
      "Epoch [641/10044], Batch [1/7], Loss: 0.0219, Accuracy: 99.60%, Grad Norm: 0.11782\n",
      "Epoch [641/10044], Batch [2/7], Loss: 0.0264, Accuracy: 99.42%, Grad Norm: 0.15645\n",
      "Epoch [641/10044], Batch [3/7], Loss: 0.0207, Accuracy: 99.71%, Grad Norm: 0.10213\n",
      "Epoch [641/10044], Batch [4/7], Loss: 0.0230, Accuracy: 99.65%, Grad Norm: 0.09780\n",
      "Epoch [641/10044], Batch [5/7], Loss: 0.0227, Accuracy: 99.59%, Grad Norm: 0.13432\n",
      "Epoch [641/10044], Batch [6/7], Loss: 0.0257, Accuracy: 99.42%, Grad Norm: 0.16037\n",
      "Epoch [641/10044], Batch [7/7], Loss: 0.0178, Accuracy: 99.68%, Grad Norm: 0.12861\n",
      "Epoch [641/10044], Loss: 0.0178\n",
      "Epoch [642/10044], Batch [1/7], Loss: 0.0215, Accuracy: 99.67%, Grad Norm: 0.12474\n",
      "Epoch [642/10044], Batch [2/7], Loss: 0.0255, Accuracy: 99.43%, Grad Norm: 0.17026\n",
      "Epoch [642/10044], Batch [3/7], Loss: 0.0201, Accuracy: 99.71%, Grad Norm: 0.09452\n",
      "Epoch [642/10044], Batch [4/7], Loss: 0.0228, Accuracy: 99.60%, Grad Norm: 0.09532\n",
      "Epoch [642/10044], Batch [5/7], Loss: 0.0209, Accuracy: 99.57%, Grad Norm: 0.12262\n",
      "Epoch [642/10044], Batch [6/7], Loss: 0.0260, Accuracy: 99.41%, Grad Norm: 0.14673\n",
      "Epoch [642/10044], Batch [7/7], Loss: 0.0206, Accuracy: 99.57%, Grad Norm: 0.15910\n",
      "Epoch [642/10044], Loss: 0.0206\n",
      "Epoch [643/10044], Batch [1/7], Loss: 0.0229, Accuracy: 99.63%, Grad Norm: 0.13156\n",
      "Epoch [643/10044], Batch [2/7], Loss: 0.0230, Accuracy: 99.54%, Grad Norm: 0.14826\n",
      "Epoch [643/10044], Batch [3/7], Loss: 0.0191, Accuracy: 99.78%, Grad Norm: 0.08842\n",
      "Epoch [643/10044], Batch [4/7], Loss: 0.0243, Accuracy: 99.48%, Grad Norm: 0.10961\n",
      "Epoch [643/10044], Batch [5/7], Loss: 0.0218, Accuracy: 99.60%, Grad Norm: 0.12529\n",
      "Epoch [643/10044], Batch [6/7], Loss: 0.0237, Accuracy: 99.50%, Grad Norm: 0.13043\n",
      "Epoch [643/10044], Batch [7/7], Loss: 0.0179, Accuracy: 99.63%, Grad Norm: 0.14879\n",
      "Epoch [643/10044], Loss: 0.0179\n",
      "Epoch [644/10044], Batch [1/7], Loss: 0.0224, Accuracy: 99.62%, Grad Norm: 0.11790\n",
      "Epoch [644/10044], Batch [2/7], Loss: 0.0243, Accuracy: 99.48%, Grad Norm: 0.14673\n",
      "Epoch [644/10044], Batch [3/7], Loss: 0.0207, Accuracy: 99.70%, Grad Norm: 0.10198\n",
      "Epoch [644/10044], Batch [4/7], Loss: 0.0222, Accuracy: 99.65%, Grad Norm: 0.10391\n",
      "Epoch [644/10044], Batch [5/7], Loss: 0.0231, Accuracy: 99.56%, Grad Norm: 0.12660\n",
      "Epoch [644/10044], Batch [6/7], Loss: 0.0235, Accuracy: 99.50%, Grad Norm: 0.14036\n",
      "Epoch [644/10044], Batch [7/7], Loss: 0.0178, Accuracy: 99.68%, Grad Norm: 0.14640\n",
      "Epoch [644/10044], Loss: 0.0178\n",
      "Epoch [645/10044], Batch [1/7], Loss: 0.0219, Accuracy: 99.59%, Grad Norm: 0.11847\n",
      "Epoch [645/10044], Batch [2/7], Loss: 0.0256, Accuracy: 99.45%, Grad Norm: 0.13844\n",
      "Epoch [645/10044], Batch [3/7], Loss: 0.0195, Accuracy: 99.76%, Grad Norm: 0.09957\n",
      "Epoch [645/10044], Batch [4/7], Loss: 0.0230, Accuracy: 99.62%, Grad Norm: 0.10966\n",
      "Epoch [645/10044], Batch [5/7], Loss: 0.0215, Accuracy: 99.61%, Grad Norm: 0.12116\n",
      "Epoch [645/10044], Batch [6/7], Loss: 0.0237, Accuracy: 99.51%, Grad Norm: 0.13394\n",
      "Epoch [645/10044], Batch [7/7], Loss: 0.0170, Accuracy: 99.72%, Grad Norm: 0.13948\n",
      "Epoch [645/10044], Loss: 0.0170\n",
      "Epoch [646/10044], Batch [1/7], Loss: 0.0205, Accuracy: 99.64%, Grad Norm: 0.10494\n",
      "Epoch [646/10044], Batch [2/7], Loss: 0.0245, Accuracy: 99.45%, Grad Norm: 0.14000\n",
      "Epoch [646/10044], Batch [3/7], Loss: 0.0200, Accuracy: 99.72%, Grad Norm: 0.10144\n",
      "Epoch [646/10044], Batch [4/7], Loss: 0.0247, Accuracy: 99.49%, Grad Norm: 0.13883\n",
      "Epoch [646/10044], Batch [5/7], Loss: 0.0220, Accuracy: 99.68%, Grad Norm: 0.13662\n",
      "Epoch [646/10044], Batch [6/7], Loss: 0.0234, Accuracy: 99.52%, Grad Norm: 0.12617\n",
      "Epoch [646/10044], Batch [7/7], Loss: 0.0174, Accuracy: 99.63%, Grad Norm: 0.12728\n",
      "Epoch [646/10044], Loss: 0.0174\n",
      "Epoch [647/10044], Batch [1/7], Loss: 0.0225, Accuracy: 99.62%, Grad Norm: 0.13706\n",
      "Epoch [647/10044], Batch [2/7], Loss: 0.0232, Accuracy: 99.50%, Grad Norm: 0.12676\n",
      "Epoch [647/10044], Batch [3/7], Loss: 0.0200, Accuracy: 99.76%, Grad Norm: 0.11771\n",
      "Epoch [647/10044], Batch [4/7], Loss: 0.0240, Accuracy: 99.51%, Grad Norm: 0.13853\n",
      "Epoch [647/10044], Batch [5/7], Loss: 0.0231, Accuracy: 99.57%, Grad Norm: 0.15660\n",
      "Epoch [647/10044], Batch [6/7], Loss: 0.0249, Accuracy: 99.40%, Grad Norm: 0.14656\n",
      "Epoch [647/10044], Batch [7/7], Loss: 0.0194, Accuracy: 99.57%, Grad Norm: 0.15648\n",
      "Epoch [647/10044], Loss: 0.0194\n",
      "Epoch [648/10044], Batch [1/7], Loss: 0.0233, Accuracy: 99.57%, Grad Norm: 0.15514\n",
      "Epoch [648/10044], Batch [2/7], Loss: 0.0242, Accuracy: 99.45%, Grad Norm: 0.15390\n",
      "Epoch [648/10044], Batch [3/7], Loss: 0.0205, Accuracy: 99.76%, Grad Norm: 0.10501\n",
      "Epoch [648/10044], Batch [4/7], Loss: 0.0236, Accuracy: 99.59%, Grad Norm: 0.12175\n",
      "Epoch [648/10044], Batch [5/7], Loss: 0.0206, Accuracy: 99.61%, Grad Norm: 0.14015\n",
      "Epoch [648/10044], Batch [6/7], Loss: 0.0243, Accuracy: 99.51%, Grad Norm: 0.14073\n",
      "Epoch [648/10044], Batch [7/7], Loss: 0.0183, Accuracy: 99.65%, Grad Norm: 0.17047\n",
      "Epoch [648/10044], Loss: 0.0183\n",
      "Epoch [649/10044], Batch [1/7], Loss: 0.0226, Accuracy: 99.64%, Grad Norm: 0.13395\n",
      "Epoch [649/10044], Batch [2/7], Loss: 0.0257, Accuracy: 99.42%, Grad Norm: 0.16160\n",
      "Epoch [649/10044], Batch [3/7], Loss: 0.0198, Accuracy: 99.78%, Grad Norm: 0.11006\n",
      "Epoch [649/10044], Batch [4/7], Loss: 0.0243, Accuracy: 99.57%, Grad Norm: 0.11348\n",
      "Epoch [649/10044], Batch [5/7], Loss: 0.0236, Accuracy: 99.56%, Grad Norm: 0.16539\n",
      "Epoch [649/10044], Batch [6/7], Loss: 0.0270, Accuracy: 99.36%, Grad Norm: 0.18574\n",
      "Epoch [649/10044], Batch [7/7], Loss: 0.0170, Accuracy: 99.68%, Grad Norm: 0.14702\n",
      "Epoch [649/10044], Loss: 0.0170\n",
      "Epoch [650/10044], Batch [1/7], Loss: 0.0242, Accuracy: 99.55%, Grad Norm: 0.13167\n",
      "Epoch [650/10044], Batch [2/7], Loss: 0.0243, Accuracy: 99.52%, Grad Norm: 0.14829\n",
      "Epoch [650/10044], Batch [3/7], Loss: 0.0201, Accuracy: 99.68%, Grad Norm: 0.10317\n",
      "Epoch [650/10044], Batch [4/7], Loss: 0.0220, Accuracy: 99.62%, Grad Norm: 0.11574\n",
      "Epoch [650/10044], Batch [5/7], Loss: 0.0220, Accuracy: 99.59%, Grad Norm: 0.14850\n",
      "Epoch [650/10044], Batch [6/7], Loss: 0.0237, Accuracy: 99.50%, Grad Norm: 0.14231\n",
      "Epoch [650/10044], Batch [7/7], Loss: 0.0169, Accuracy: 99.73%, Grad Norm: 0.13905\n",
      "Epoch [650/10044], Loss: 0.0169\n",
      "Epoch [651/10044], Batch [1/7], Loss: 0.0202, Accuracy: 99.67%, Grad Norm: 0.11031\n",
      "Epoch [651/10044], Batch [2/7], Loss: 0.0247, Accuracy: 99.49%, Grad Norm: 0.16761\n",
      "Epoch [651/10044], Batch [3/7], Loss: 0.0191, Accuracy: 99.75%, Grad Norm: 0.09951\n",
      "Epoch [651/10044], Batch [4/7], Loss: 0.0230, Accuracy: 99.56%, Grad Norm: 0.10727\n",
      "Epoch [651/10044], Batch [5/7], Loss: 0.0197, Accuracy: 99.67%, Grad Norm: 0.11468\n",
      "Epoch [651/10044], Batch [6/7], Loss: 0.0241, Accuracy: 99.48%, Grad Norm: 0.13587\n",
      "Epoch [651/10044], Batch [7/7], Loss: 0.0164, Accuracy: 99.70%, Grad Norm: 0.13981\n",
      "Epoch [651/10044], Loss: 0.0164\n",
      "Epoch [652/10044], Batch [1/7], Loss: 0.0204, Accuracy: 99.68%, Grad Norm: 0.10588\n",
      "Epoch [652/10044], Batch [2/7], Loss: 0.0253, Accuracy: 99.41%, Grad Norm: 0.16655\n",
      "Epoch [652/10044], Batch [3/7], Loss: 0.0197, Accuracy: 99.75%, Grad Norm: 0.09476\n",
      "Epoch [652/10044], Batch [4/7], Loss: 0.0223, Accuracy: 99.57%, Grad Norm: 0.09667\n",
      "Epoch [652/10044], Batch [5/7], Loss: 0.0211, Accuracy: 99.62%, Grad Norm: 0.11057\n",
      "Epoch [652/10044], Batch [6/7], Loss: 0.0224, Accuracy: 99.51%, Grad Norm: 0.12662\n",
      "Epoch [652/10044], Batch [7/7], Loss: 0.0168, Accuracy: 99.75%, Grad Norm: 0.16453\n",
      "Epoch [652/10044], Loss: 0.0168\n",
      "Epoch [653/10044], Batch [1/7], Loss: 0.0203, Accuracy: 99.65%, Grad Norm: 0.10325\n",
      "Epoch [653/10044], Batch [2/7], Loss: 0.0238, Accuracy: 99.47%, Grad Norm: 0.16054\n",
      "Epoch [653/10044], Batch [3/7], Loss: 0.0187, Accuracy: 99.77%, Grad Norm: 0.09407\n",
      "Epoch [653/10044], Batch [4/7], Loss: 0.0233, Accuracy: 99.57%, Grad Norm: 0.11212\n",
      "Epoch [653/10044], Batch [5/7], Loss: 0.0211, Accuracy: 99.62%, Grad Norm: 0.13818\n",
      "Epoch [653/10044], Batch [6/7], Loss: 0.0232, Accuracy: 99.51%, Grad Norm: 0.13675\n",
      "Epoch [653/10044], Batch [7/7], Loss: 0.0175, Accuracy: 99.63%, Grad Norm: 0.14466\n",
      "Epoch [653/10044], Loss: 0.0175\n",
      "Epoch [654/10044], Batch [1/7], Loss: 0.0205, Accuracy: 99.72%, Grad Norm: 0.10909\n",
      "Epoch [654/10044], Batch [2/7], Loss: 0.0233, Accuracy: 99.48%, Grad Norm: 0.14546\n",
      "Epoch [654/10044], Batch [3/7], Loss: 0.0199, Accuracy: 99.72%, Grad Norm: 0.10829\n",
      "Epoch [654/10044], Batch [4/7], Loss: 0.0240, Accuracy: 99.52%, Grad Norm: 0.12305\n",
      "Epoch [654/10044], Batch [5/7], Loss: 0.0206, Accuracy: 99.69%, Grad Norm: 0.13045\n",
      "Epoch [654/10044], Batch [6/7], Loss: 0.0244, Accuracy: 99.44%, Grad Norm: 0.15254\n",
      "Epoch [654/10044], Batch [7/7], Loss: 0.0173, Accuracy: 99.70%, Grad Norm: 0.14809\n",
      "Epoch [654/10044], Loss: 0.0173\n",
      "Epoch [655/10044], Batch [1/7], Loss: 0.0214, Accuracy: 99.62%, Grad Norm: 0.11199\n",
      "Epoch [655/10044], Batch [2/7], Loss: 0.0235, Accuracy: 99.50%, Grad Norm: 0.13664\n",
      "Epoch [655/10044], Batch [3/7], Loss: 0.0199, Accuracy: 99.73%, Grad Norm: 0.09615\n",
      "Epoch [655/10044], Batch [4/7], Loss: 0.0218, Accuracy: 99.63%, Grad Norm: 0.10372\n",
      "Epoch [655/10044], Batch [5/7], Loss: 0.0196, Accuracy: 99.63%, Grad Norm: 0.12665\n",
      "Epoch [655/10044], Batch [6/7], Loss: 0.0260, Accuracy: 99.44%, Grad Norm: 0.17184\n",
      "Epoch [655/10044], Batch [7/7], Loss: 0.0151, Accuracy: 99.78%, Grad Norm: 0.12023\n",
      "Epoch [655/10044], Loss: 0.0151\n",
      "Epoch [656/10044], Batch [1/7], Loss: 0.0208, Accuracy: 99.72%, Grad Norm: 0.12905\n",
      "Epoch [656/10044], Batch [2/7], Loss: 0.0250, Accuracy: 99.47%, Grad Norm: 0.14741\n",
      "Epoch [656/10044], Batch [3/7], Loss: 0.0179, Accuracy: 99.77%, Grad Norm: 0.09301\n",
      "Epoch [656/10044], Batch [4/7], Loss: 0.0215, Accuracy: 99.66%, Grad Norm: 0.11391\n",
      "Epoch [656/10044], Batch [5/7], Loss: 0.0237, Accuracy: 99.51%, Grad Norm: 0.16461\n",
      "Epoch [656/10044], Batch [6/7], Loss: 0.0221, Accuracy: 99.57%, Grad Norm: 0.12913\n",
      "Epoch [656/10044], Batch [7/7], Loss: 0.0180, Accuracy: 99.63%, Grad Norm: 0.16116\n",
      "Epoch [656/10044], Loss: 0.0180\n",
      "Epoch [657/10044], Batch [1/7], Loss: 0.0211, Accuracy: 99.68%, Grad Norm: 0.12689\n",
      "Epoch [657/10044], Batch [2/7], Loss: 0.0233, Accuracy: 99.52%, Grad Norm: 0.13318\n",
      "Epoch [657/10044], Batch [3/7], Loss: 0.0188, Accuracy: 99.75%, Grad Norm: 0.10272\n",
      "Epoch [657/10044], Batch [4/7], Loss: 0.0221, Accuracy: 99.62%, Grad Norm: 0.11073\n",
      "Epoch [657/10044], Batch [5/7], Loss: 0.0215, Accuracy: 99.65%, Grad Norm: 0.14019\n",
      "Epoch [657/10044], Batch [6/7], Loss: 0.0220, Accuracy: 99.49%, Grad Norm: 0.13281\n",
      "Epoch [657/10044], Batch [7/7], Loss: 0.0188, Accuracy: 99.60%, Grad Norm: 0.19526\n",
      "Epoch [657/10044], Loss: 0.0188\n",
      "Epoch [658/10044], Batch [1/7], Loss: 0.0219, Accuracy: 99.65%, Grad Norm: 0.12958\n",
      "Epoch [658/10044], Batch [2/7], Loss: 0.0231, Accuracy: 99.50%, Grad Norm: 0.16106\n",
      "Epoch [658/10044], Batch [3/7], Loss: 0.0184, Accuracy: 99.78%, Grad Norm: 0.09841\n",
      "Epoch [658/10044], Batch [4/7], Loss: 0.0222, Accuracy: 99.57%, Grad Norm: 0.12296\n",
      "Epoch [658/10044], Batch [5/7], Loss: 0.0212, Accuracy: 99.67%, Grad Norm: 0.13221\n",
      "Epoch [658/10044], Batch [6/7], Loss: 0.0238, Accuracy: 99.55%, Grad Norm: 0.14062\n",
      "Epoch [658/10044], Batch [7/7], Loss: 0.0167, Accuracy: 99.55%, Grad Norm: 0.13180\n",
      "Epoch [658/10044], Loss: 0.0167\n",
      "Epoch [659/10044], Batch [1/7], Loss: 0.0209, Accuracy: 99.65%, Grad Norm: 0.10777\n",
      "Epoch [659/10044], Batch [2/7], Loss: 0.0211, Accuracy: 99.57%, Grad Norm: 0.13356\n",
      "Epoch [659/10044], Batch [3/7], Loss: 0.0181, Accuracy: 99.77%, Grad Norm: 0.09245\n",
      "Epoch [659/10044], Batch [4/7], Loss: 0.0222, Accuracy: 99.52%, Grad Norm: 0.11270\n",
      "Epoch [659/10044], Batch [5/7], Loss: 0.0216, Accuracy: 99.59%, Grad Norm: 0.13036\n",
      "Epoch [659/10044], Batch [6/7], Loss: 0.0242, Accuracy: 99.49%, Grad Norm: 0.13356\n",
      "Epoch [659/10044], Batch [7/7], Loss: 0.0157, Accuracy: 99.73%, Grad Norm: 0.11762\n",
      "Epoch [659/10044], Loss: 0.0157\n",
      "Epoch [660/10044], Batch [1/7], Loss: 0.0184, Accuracy: 99.75%, Grad Norm: 0.09255\n",
      "Epoch [660/10044], Batch [2/7], Loss: 0.0224, Accuracy: 99.53%, Grad Norm: 0.13716\n",
      "Epoch [660/10044], Batch [3/7], Loss: 0.0194, Accuracy: 99.72%, Grad Norm: 0.09842\n",
      "Epoch [660/10044], Batch [4/7], Loss: 0.0219, Accuracy: 99.60%, Grad Norm: 0.10914\n",
      "Epoch [660/10044], Batch [5/7], Loss: 0.0216, Accuracy: 99.65%, Grad Norm: 0.12859\n",
      "Epoch [660/10044], Batch [6/7], Loss: 0.0217, Accuracy: 99.53%, Grad Norm: 0.11438\n",
      "Epoch [660/10044], Batch [7/7], Loss: 0.0177, Accuracy: 99.58%, Grad Norm: 0.16818\n",
      "Epoch [660/10044], Loss: 0.0177\n",
      "Epoch [661/10044], Batch [1/7], Loss: 0.0213, Accuracy: 99.68%, Grad Norm: 0.11526\n",
      "Epoch [661/10044], Batch [2/7], Loss: 0.0223, Accuracy: 99.52%, Grad Norm: 0.13463\n",
      "Epoch [661/10044], Batch [3/7], Loss: 0.0187, Accuracy: 99.74%, Grad Norm: 0.10046\n",
      "Epoch [661/10044], Batch [4/7], Loss: 0.0204, Accuracy: 99.65%, Grad Norm: 0.09673\n",
      "Epoch [661/10044], Batch [5/7], Loss: 0.0207, Accuracy: 99.63%, Grad Norm: 0.13285\n",
      "Epoch [661/10044], Batch [6/7], Loss: 0.0240, Accuracy: 99.43%, Grad Norm: 0.14297\n",
      "Epoch [661/10044], Batch [7/7], Loss: 0.0154, Accuracy: 99.70%, Grad Norm: 0.13536\n",
      "Epoch [661/10044], Loss: 0.0154\n",
      "Epoch [662/10044], Batch [1/7], Loss: 0.0203, Accuracy: 99.70%, Grad Norm: 0.10984\n",
      "Epoch [662/10044], Batch [2/7], Loss: 0.0207, Accuracy: 99.62%, Grad Norm: 0.12190\n",
      "Epoch [662/10044], Batch [3/7], Loss: 0.0195, Accuracy: 99.70%, Grad Norm: 0.10449\n",
      "Epoch [662/10044], Batch [4/7], Loss: 0.0209, Accuracy: 99.67%, Grad Norm: 0.09462\n",
      "Epoch [662/10044], Batch [5/7], Loss: 0.0191, Accuracy: 99.67%, Grad Norm: 0.12313\n",
      "Epoch [662/10044], Batch [6/7], Loss: 0.0237, Accuracy: 99.45%, Grad Norm: 0.17053\n",
      "Epoch [662/10044], Batch [7/7], Loss: 0.0182, Accuracy: 99.67%, Grad Norm: 0.15228\n",
      "Epoch [662/10044], Loss: 0.0182\n",
      "Epoch [663/10044], Batch [1/7], Loss: 0.0192, Accuracy: 99.72%, Grad Norm: 0.10654\n",
      "Epoch [663/10044], Batch [2/7], Loss: 0.0231, Accuracy: 99.47%, Grad Norm: 0.14203\n",
      "Epoch [663/10044], Batch [3/7], Loss: 0.0183, Accuracy: 99.82%, Grad Norm: 0.08804\n",
      "Epoch [663/10044], Batch [4/7], Loss: 0.0207, Accuracy: 99.68%, Grad Norm: 0.10203\n",
      "Epoch [663/10044], Batch [5/7], Loss: 0.0217, Accuracy: 99.57%, Grad Norm: 0.14193\n",
      "Epoch [663/10044], Batch [6/7], Loss: 0.0245, Accuracy: 99.41%, Grad Norm: 0.15079\n",
      "Epoch [663/10044], Batch [7/7], Loss: 0.0177, Accuracy: 99.60%, Grad Norm: 0.13952\n",
      "Epoch [663/10044], Loss: 0.0177\n",
      "Epoch [664/10044], Batch [1/7], Loss: 0.0204, Accuracy: 99.67%, Grad Norm: 0.10585\n",
      "Epoch [664/10044], Batch [2/7], Loss: 0.0219, Accuracy: 99.56%, Grad Norm: 0.13101\n",
      "Epoch [664/10044], Batch [3/7], Loss: 0.0193, Accuracy: 99.73%, Grad Norm: 0.09552\n",
      "Epoch [664/10044], Batch [4/7], Loss: 0.0213, Accuracy: 99.62%, Grad Norm: 0.11116\n",
      "Epoch [664/10044], Batch [5/7], Loss: 0.0197, Accuracy: 99.64%, Grad Norm: 0.12209\n",
      "Epoch [664/10044], Batch [6/7], Loss: 0.0227, Accuracy: 99.57%, Grad Norm: 0.12178\n",
      "Epoch [664/10044], Batch [7/7], Loss: 0.0201, Accuracy: 99.57%, Grad Norm: 0.16255\n",
      "Epoch [664/10044], Loss: 0.0201\n",
      "Epoch [665/10044], Batch [1/7], Loss: 0.0215, Accuracy: 99.61%, Grad Norm: 0.14430\n",
      "Epoch [665/10044], Batch [2/7], Loss: 0.0247, Accuracy: 99.39%, Grad Norm: 0.14864\n",
      "Epoch [665/10044], Batch [3/7], Loss: 0.0195, Accuracy: 99.72%, Grad Norm: 0.10665\n",
      "Epoch [665/10044], Batch [4/7], Loss: 0.0208, Accuracy: 99.65%, Grad Norm: 0.11344\n",
      "Epoch [665/10044], Batch [5/7], Loss: 0.0200, Accuracy: 99.62%, Grad Norm: 0.12865\n",
      "Epoch [665/10044], Batch [6/7], Loss: 0.0228, Accuracy: 99.52%, Grad Norm: 0.12672\n",
      "Epoch [665/10044], Batch [7/7], Loss: 0.0183, Accuracy: 99.62%, Grad Norm: 0.15762\n",
      "Epoch [665/10044], Loss: 0.0183\n",
      "Epoch [666/10044], Batch [1/7], Loss: 0.0203, Accuracy: 99.67%, Grad Norm: 0.17584\n",
      "Epoch [666/10044], Batch [2/7], Loss: 0.0244, Accuracy: 99.45%, Grad Norm: 0.16506\n",
      "Epoch [666/10044], Batch [3/7], Loss: 0.0178, Accuracy: 99.77%, Grad Norm: 0.09097\n",
      "Epoch [666/10044], Batch [4/7], Loss: 0.0223, Accuracy: 99.57%, Grad Norm: 0.10693\n",
      "Epoch [666/10044], Batch [5/7], Loss: 0.0188, Accuracy: 99.65%, Grad Norm: 0.11772\n",
      "Epoch [666/10044], Batch [6/7], Loss: 0.0235, Accuracy: 99.45%, Grad Norm: 0.14560\n",
      "Epoch [666/10044], Batch [7/7], Loss: 0.0169, Accuracy: 99.68%, Grad Norm: 0.15673\n",
      "Epoch [666/10044], Loss: 0.0169\n",
      "Epoch [667/10044], Batch [1/7], Loss: 0.0224, Accuracy: 99.52%, Grad Norm: 0.15215\n",
      "Epoch [667/10044], Batch [2/7], Loss: 0.0216, Accuracy: 99.57%, Grad Norm: 0.14135\n",
      "Epoch [667/10044], Batch [3/7], Loss: 0.0196, Accuracy: 99.69%, Grad Norm: 0.11532\n",
      "Epoch [667/10044], Batch [4/7], Loss: 0.0218, Accuracy: 99.59%, Grad Norm: 0.11006\n",
      "Epoch [667/10044], Batch [5/7], Loss: 0.0213, Accuracy: 99.57%, Grad Norm: 0.13280\n",
      "Epoch [667/10044], Batch [6/7], Loss: 0.0227, Accuracy: 99.47%, Grad Norm: 0.13858\n",
      "Epoch [667/10044], Batch [7/7], Loss: 0.0228, Accuracy: 99.60%, Grad Norm: 0.19434\n",
      "Epoch [667/10044], Loss: 0.0228\n",
      "Epoch [668/10044], Batch [1/7], Loss: 0.0210, Accuracy: 99.62%, Grad Norm: 0.12731\n",
      "Epoch [668/10044], Batch [2/7], Loss: 0.0227, Accuracy: 99.59%, Grad Norm: 0.15412\n",
      "Epoch [668/10044], Batch [3/7], Loss: 0.0197, Accuracy: 99.73%, Grad Norm: 0.09829\n",
      "Epoch [668/10044], Batch [4/7], Loss: 0.0220, Accuracy: 99.62%, Grad Norm: 0.11401\n",
      "Epoch [668/10044], Batch [5/7], Loss: 0.0214, Accuracy: 99.57%, Grad Norm: 0.15050\n",
      "Epoch [668/10044], Batch [6/7], Loss: 0.0237, Accuracy: 99.47%, Grad Norm: 0.15589\n",
      "Epoch [668/10044], Batch [7/7], Loss: 0.0184, Accuracy: 99.62%, Grad Norm: 0.19752\n",
      "Epoch [668/10044], Loss: 0.0184\n",
      "Epoch [669/10044], Batch [1/7], Loss: 0.0217, Accuracy: 99.61%, Grad Norm: 0.14222\n",
      "Epoch [669/10044], Batch [2/7], Loss: 0.0231, Accuracy: 99.52%, Grad Norm: 0.15135\n",
      "Epoch [669/10044], Batch [3/7], Loss: 0.0200, Accuracy: 99.68%, Grad Norm: 0.11174\n",
      "Epoch [669/10044], Batch [4/7], Loss: 0.0209, Accuracy: 99.68%, Grad Norm: 0.10323\n",
      "Epoch [669/10044], Batch [5/7], Loss: 0.0207, Accuracy: 99.58%, Grad Norm: 0.14260\n",
      "Epoch [669/10044], Batch [6/7], Loss: 0.0232, Accuracy: 99.47%, Grad Norm: 0.14319\n",
      "Epoch [669/10044], Batch [7/7], Loss: 0.0182, Accuracy: 99.62%, Grad Norm: 0.17691\n",
      "Epoch [669/10044], Loss: 0.0182\n",
      "Epoch [670/10044], Batch [1/7], Loss: 0.0208, Accuracy: 99.66%, Grad Norm: 0.13570\n",
      "Epoch [670/10044], Batch [2/7], Loss: 0.0215, Accuracy: 99.57%, Grad Norm: 0.13711\n",
      "Epoch [670/10044], Batch [3/7], Loss: 0.0195, Accuracy: 99.72%, Grad Norm: 0.12198\n",
      "Epoch [670/10044], Batch [4/7], Loss: 0.0217, Accuracy: 99.59%, Grad Norm: 0.12624\n",
      "Epoch [670/10044], Batch [5/7], Loss: 0.0205, Accuracy: 99.63%, Grad Norm: 0.13884\n",
      "Epoch [670/10044], Batch [6/7], Loss: 0.0223, Accuracy: 99.53%, Grad Norm: 0.16069\n",
      "Epoch [670/10044], Batch [7/7], Loss: 0.0174, Accuracy: 99.68%, Grad Norm: 0.15748\n",
      "Epoch [670/10044], Loss: 0.0174\n",
      "Epoch [671/10044], Batch [1/7], Loss: 0.0218, Accuracy: 99.62%, Grad Norm: 0.11976\n",
      "Epoch [671/10044], Batch [2/7], Loss: 0.0222, Accuracy: 99.57%, Grad Norm: 0.15111\n",
      "Epoch [671/10044], Batch [3/7], Loss: 0.0181, Accuracy: 99.78%, Grad Norm: 0.09293\n",
      "Epoch [671/10044], Batch [4/7], Loss: 0.0226, Accuracy: 99.64%, Grad Norm: 0.13421\n",
      "Epoch [671/10044], Batch [5/7], Loss: 0.0199, Accuracy: 99.70%, Grad Norm: 0.12288\n",
      "Epoch [671/10044], Batch [6/7], Loss: 0.0220, Accuracy: 99.54%, Grad Norm: 0.14525\n",
      "Epoch [671/10044], Batch [7/7], Loss: 0.0166, Accuracy: 99.70%, Grad Norm: 0.15752\n",
      "Epoch [671/10044], Loss: 0.0166\n",
      "Epoch [672/10044], Batch [1/7], Loss: 0.0227, Accuracy: 99.52%, Grad Norm: 0.18704\n",
      "Epoch [672/10044], Batch [2/7], Loss: 0.0236, Accuracy: 99.45%, Grad Norm: 0.16269\n",
      "Epoch [672/10044], Batch [3/7], Loss: 0.0181, Accuracy: 99.77%, Grad Norm: 0.09808\n",
      "Epoch [672/10044], Batch [4/7], Loss: 0.0237, Accuracy: 99.61%, Grad Norm: 0.12715\n",
      "Epoch [672/10044], Batch [5/7], Loss: 0.0206, Accuracy: 99.58%, Grad Norm: 0.14062\n",
      "Epoch [672/10044], Batch [6/7], Loss: 0.0237, Accuracy: 99.52%, Grad Norm: 0.15376\n",
      "Epoch [672/10044], Batch [7/7], Loss: 0.0181, Accuracy: 99.67%, Grad Norm: 0.15239\n",
      "Epoch [672/10044], Loss: 0.0181\n",
      "Epoch [673/10044], Batch [1/7], Loss: 0.0243, Accuracy: 99.57%, Grad Norm: 0.17097\n",
      "Epoch [673/10044], Batch [2/7], Loss: 0.0236, Accuracy: 99.50%, Grad Norm: 0.14631\n",
      "Epoch [673/10044], Batch [3/7], Loss: 0.0178, Accuracy: 99.75%, Grad Norm: 0.11182\n",
      "Epoch [673/10044], Batch [4/7], Loss: 0.0206, Accuracy: 99.70%, Grad Norm: 0.10392\n",
      "Epoch [673/10044], Batch [5/7], Loss: 0.0191, Accuracy: 99.71%, Grad Norm: 0.12273\n",
      "Epoch [673/10044], Batch [6/7], Loss: 0.0218, Accuracy: 99.55%, Grad Norm: 0.15126\n",
      "Epoch [673/10044], Batch [7/7], Loss: 0.0171, Accuracy: 99.68%, Grad Norm: 0.15321\n",
      "Epoch [673/10044], Loss: 0.0171\n",
      "Epoch [674/10044], Batch [1/7], Loss: 0.0201, Accuracy: 99.67%, Grad Norm: 0.12241\n",
      "Epoch [674/10044], Batch [2/7], Loss: 0.0238, Accuracy: 99.47%, Grad Norm: 0.17397\n",
      "Epoch [674/10044], Batch [3/7], Loss: 0.0194, Accuracy: 99.68%, Grad Norm: 0.11111\n",
      "Epoch [674/10044], Batch [4/7], Loss: 0.0224, Accuracy: 99.57%, Grad Norm: 0.12126\n",
      "Epoch [674/10044], Batch [5/7], Loss: 0.0207, Accuracy: 99.61%, Grad Norm: 0.12668\n",
      "Epoch [674/10044], Batch [6/7], Loss: 0.0217, Accuracy: 99.62%, Grad Norm: 0.13032\n",
      "Epoch [674/10044], Batch [7/7], Loss: 0.0171, Accuracy: 99.65%, Grad Norm: 0.17097\n",
      "Epoch [674/10044], Loss: 0.0171\n",
      "Epoch [675/10044], Batch [1/7], Loss: 0.0210, Accuracy: 99.67%, Grad Norm: 0.12944\n",
      "Epoch [675/10044], Batch [2/7], Loss: 0.0228, Accuracy: 99.47%, Grad Norm: 0.16258\n",
      "Epoch [675/10044], Batch [3/7], Loss: 0.0182, Accuracy: 99.81%, Grad Norm: 0.09003\n",
      "Epoch [675/10044], Batch [4/7], Loss: 0.0201, Accuracy: 99.62%, Grad Norm: 0.09956\n",
      "Epoch [675/10044], Batch [5/7], Loss: 0.0187, Accuracy: 99.67%, Grad Norm: 0.10799\n",
      "Epoch [675/10044], Batch [6/7], Loss: 0.0222, Accuracy: 99.52%, Grad Norm: 0.13654\n",
      "Epoch [675/10044], Batch [7/7], Loss: 0.0177, Accuracy: 99.65%, Grad Norm: 0.17732\n",
      "Epoch [675/10044], Loss: 0.0177\n",
      "Epoch [676/10044], Batch [1/7], Loss: 0.0206, Accuracy: 99.62%, Grad Norm: 0.11598\n",
      "Epoch [676/10044], Batch [2/7], Loss: 0.0225, Accuracy: 99.56%, Grad Norm: 0.14005\n",
      "Epoch [676/10044], Batch [3/7], Loss: 0.0188, Accuracy: 99.72%, Grad Norm: 0.10059\n",
      "Epoch [676/10044], Batch [4/7], Loss: 0.0204, Accuracy: 99.62%, Grad Norm: 0.10347\n",
      "Epoch [676/10044], Batch [5/7], Loss: 0.0196, Accuracy: 99.64%, Grad Norm: 0.11426\n",
      "Epoch [676/10044], Batch [6/7], Loss: 0.0220, Accuracy: 99.53%, Grad Norm: 0.12805\n",
      "Epoch [676/10044], Batch [7/7], Loss: 0.0167, Accuracy: 99.70%, Grad Norm: 0.17790\n",
      "Epoch [676/10044], Loss: 0.0167\n",
      "Epoch [677/10044], Batch [1/7], Loss: 0.0196, Accuracy: 99.69%, Grad Norm: 0.12800\n",
      "Epoch [677/10044], Batch [2/7], Loss: 0.0209, Accuracy: 99.60%, Grad Norm: 0.13579\n",
      "Epoch [677/10044], Batch [3/7], Loss: 0.0179, Accuracy: 99.73%, Grad Norm: 0.08670\n",
      "Epoch [677/10044], Batch [4/7], Loss: 0.0215, Accuracy: 99.55%, Grad Norm: 0.11233\n",
      "Epoch [677/10044], Batch [5/7], Loss: 0.0189, Accuracy: 99.66%, Grad Norm: 0.12230\n",
      "Epoch [677/10044], Batch [6/7], Loss: 0.0218, Accuracy: 99.50%, Grad Norm: 0.12662\n",
      "Epoch [677/10044], Batch [7/7], Loss: 0.0161, Accuracy: 99.75%, Grad Norm: 0.12427\n",
      "Epoch [677/10044], Loss: 0.0161\n",
      "Epoch [678/10044], Batch [1/7], Loss: 0.0211, Accuracy: 99.61%, Grad Norm: 0.13274\n",
      "Epoch [678/10044], Batch [2/7], Loss: 0.0239, Accuracy: 99.45%, Grad Norm: 0.16551\n",
      "Epoch [678/10044], Batch [3/7], Loss: 0.0188, Accuracy: 99.75%, Grad Norm: 0.09392\n",
      "Epoch [678/10044], Batch [4/7], Loss: 0.0206, Accuracy: 99.62%, Grad Norm: 0.12350\n",
      "Epoch [678/10044], Batch [5/7], Loss: 0.0210, Accuracy: 99.56%, Grad Norm: 0.14451\n",
      "Epoch [678/10044], Batch [6/7], Loss: 0.0222, Accuracy: 99.49%, Grad Norm: 0.14456\n",
      "Epoch [678/10044], Batch [7/7], Loss: 0.0170, Accuracy: 99.60%, Grad Norm: 0.15872\n",
      "Epoch [678/10044], Loss: 0.0170\n",
      "Epoch [679/10044], Batch [1/7], Loss: 0.0196, Accuracy: 99.72%, Grad Norm: 0.12443\n",
      "Epoch [679/10044], Batch [2/7], Loss: 0.0241, Accuracy: 99.48%, Grad Norm: 0.15742\n",
      "Epoch [679/10044], Batch [3/7], Loss: 0.0183, Accuracy: 99.72%, Grad Norm: 0.09872\n",
      "Epoch [679/10044], Batch [4/7], Loss: 0.0220, Accuracy: 99.56%, Grad Norm: 0.12922\n",
      "Epoch [679/10044], Batch [5/7], Loss: 0.0173, Accuracy: 99.71%, Grad Norm: 0.10688\n",
      "Epoch [679/10044], Batch [6/7], Loss: 0.0221, Accuracy: 99.47%, Grad Norm: 0.14970\n",
      "Epoch [679/10044], Batch [7/7], Loss: 0.0143, Accuracy: 99.73%, Grad Norm: 0.13270\n",
      "Epoch [679/10044], Loss: 0.0143\n",
      "Epoch [680/10044], Batch [1/7], Loss: 0.0202, Accuracy: 99.64%, Grad Norm: 0.14804\n",
      "Epoch [680/10044], Batch [2/7], Loss: 0.0217, Accuracy: 99.56%, Grad Norm: 0.14308\n",
      "Epoch [680/10044], Batch [3/7], Loss: 0.0183, Accuracy: 99.73%, Grad Norm: 0.11324\n",
      "Epoch [680/10044], Batch [4/7], Loss: 0.0218, Accuracy: 99.64%, Grad Norm: 0.12306\n",
      "Epoch [680/10044], Batch [5/7], Loss: 0.0180, Accuracy: 99.65%, Grad Norm: 0.11712\n",
      "Epoch [680/10044], Batch [6/7], Loss: 0.0213, Accuracy: 99.55%, Grad Norm: 0.14858\n",
      "Epoch [680/10044], Batch [7/7], Loss: 0.0152, Accuracy: 99.72%, Grad Norm: 0.15195\n",
      "Epoch [680/10044], Loss: 0.0152\n",
      "Epoch [681/10044], Batch [1/7], Loss: 0.0214, Accuracy: 99.62%, Grad Norm: 0.13676\n",
      "Epoch [681/10044], Batch [2/7], Loss: 0.0194, Accuracy: 99.62%, Grad Norm: 0.14381\n",
      "Epoch [681/10044], Batch [3/7], Loss: 0.0174, Accuracy: 99.72%, Grad Norm: 0.09161\n",
      "Epoch [681/10044], Batch [4/7], Loss: 0.0190, Accuracy: 99.65%, Grad Norm: 0.08635\n",
      "Epoch [681/10044], Batch [5/7], Loss: 0.0192, Accuracy: 99.63%, Grad Norm: 0.12613\n",
      "Epoch [681/10044], Batch [6/7], Loss: 0.0223, Accuracy: 99.51%, Grad Norm: 0.14627\n",
      "Epoch [681/10044], Batch [7/7], Loss: 0.0182, Accuracy: 99.57%, Grad Norm: 0.18757\n",
      "Epoch [681/10044], Loss: 0.0182\n",
      "Epoch [682/10044], Batch [1/7], Loss: 0.0213, Accuracy: 99.62%, Grad Norm: 0.14113\n",
      "Epoch [682/10044], Batch [2/7], Loss: 0.0232, Accuracy: 99.46%, Grad Norm: 0.15775\n",
      "Epoch [682/10044], Batch [3/7], Loss: 0.0163, Accuracy: 99.73%, Grad Norm: 0.08614\n",
      "Epoch [682/10044], Batch [4/7], Loss: 0.0191, Accuracy: 99.70%, Grad Norm: 0.09641\n",
      "Epoch [682/10044], Batch [5/7], Loss: 0.0194, Accuracy: 99.67%, Grad Norm: 0.12607\n",
      "Epoch [682/10044], Batch [6/7], Loss: 0.0234, Accuracy: 99.45%, Grad Norm: 0.14993\n",
      "Epoch [682/10044], Batch [7/7], Loss: 0.0165, Accuracy: 99.73%, Grad Norm: 0.18471\n",
      "Epoch [682/10044], Loss: 0.0165\n",
      "Epoch [683/10044], Batch [1/7], Loss: 0.0204, Accuracy: 99.62%, Grad Norm: 0.12274\n",
      "Epoch [683/10044], Batch [2/7], Loss: 0.0213, Accuracy: 99.53%, Grad Norm: 0.13197\n",
      "Epoch [683/10044], Batch [3/7], Loss: 0.0194, Accuracy: 99.67%, Grad Norm: 0.12116\n",
      "Epoch [683/10044], Batch [4/7], Loss: 0.0190, Accuracy: 99.68%, Grad Norm: 0.10310\n",
      "Epoch [683/10044], Batch [5/7], Loss: 0.0193, Accuracy: 99.70%, Grad Norm: 0.14864\n",
      "Epoch [683/10044], Batch [6/7], Loss: 0.0236, Accuracy: 99.47%, Grad Norm: 0.14721\n",
      "Epoch [683/10044], Batch [7/7], Loss: 0.0161, Accuracy: 99.73%, Grad Norm: 0.14243\n",
      "Epoch [683/10044], Loss: 0.0161\n",
      "Epoch [684/10044], Batch [1/7], Loss: 0.0180, Accuracy: 99.73%, Grad Norm: 0.11342\n",
      "Epoch [684/10044], Batch [2/7], Loss: 0.0216, Accuracy: 99.54%, Grad Norm: 0.14467\n",
      "Epoch [684/10044], Batch [3/7], Loss: 0.0196, Accuracy: 99.68%, Grad Norm: 0.12346\n",
      "Epoch [684/10044], Batch [4/7], Loss: 0.0208, Accuracy: 99.63%, Grad Norm: 0.12260\n",
      "Epoch [684/10044], Batch [5/7], Loss: 0.0196, Accuracy: 99.61%, Grad Norm: 0.14069\n",
      "Epoch [684/10044], Batch [6/7], Loss: 0.0210, Accuracy: 99.52%, Grad Norm: 0.12852\n",
      "Epoch [684/10044], Batch [7/7], Loss: 0.0143, Accuracy: 99.67%, Grad Norm: 0.13736\n",
      "Epoch [684/10044], Loss: 0.0143\n",
      "Epoch [685/10044], Batch [1/7], Loss: 0.0195, Accuracy: 99.68%, Grad Norm: 0.11526\n",
      "Epoch [685/10044], Batch [2/7], Loss: 0.0215, Accuracy: 99.49%, Grad Norm: 0.13151\n",
      "Epoch [685/10044], Batch [3/7], Loss: 0.0178, Accuracy: 99.77%, Grad Norm: 0.10062\n",
      "Epoch [685/10044], Batch [4/7], Loss: 0.0195, Accuracy: 99.68%, Grad Norm: 0.09981\n",
      "Epoch [685/10044], Batch [5/7], Loss: 0.0176, Accuracy: 99.68%, Grad Norm: 0.11116\n",
      "Epoch [685/10044], Batch [6/7], Loss: 0.0215, Accuracy: 99.58%, Grad Norm: 0.14349\n",
      "Epoch [685/10044], Batch [7/7], Loss: 0.0148, Accuracy: 99.73%, Grad Norm: 0.14365\n",
      "Epoch [685/10044], Loss: 0.0148\n",
      "Epoch [686/10044], Batch [1/7], Loss: 0.0203, Accuracy: 99.66%, Grad Norm: 0.12665\n",
      "Epoch [686/10044], Batch [2/7], Loss: 0.0187, Accuracy: 99.64%, Grad Norm: 0.11473\n",
      "Epoch [686/10044], Batch [3/7], Loss: 0.0169, Accuracy: 99.77%, Grad Norm: 0.08565\n",
      "Epoch [686/10044], Batch [4/7], Loss: 0.0181, Accuracy: 99.70%, Grad Norm: 0.08738\n",
      "Epoch [686/10044], Batch [5/7], Loss: 0.0192, Accuracy: 99.66%, Grad Norm: 0.12742\n",
      "Epoch [686/10044], Batch [6/7], Loss: 0.0213, Accuracy: 99.48%, Grad Norm: 0.14819\n",
      "Epoch [686/10044], Batch [7/7], Loss: 0.0175, Accuracy: 99.72%, Grad Norm: 0.16738\n",
      "Epoch [686/10044], Loss: 0.0175\n",
      "Epoch [687/10044], Batch [1/7], Loss: 0.0177, Accuracy: 99.67%, Grad Norm: 0.09818\n",
      "Epoch [687/10044], Batch [2/7], Loss: 0.0210, Accuracy: 99.58%, Grad Norm: 0.13179\n",
      "Epoch [687/10044], Batch [3/7], Loss: 0.0173, Accuracy: 99.72%, Grad Norm: 0.10014\n",
      "Epoch [687/10044], Batch [4/7], Loss: 0.0189, Accuracy: 99.72%, Grad Norm: 0.10159\n",
      "Epoch [687/10044], Batch [5/7], Loss: 0.0193, Accuracy: 99.67%, Grad Norm: 0.13555\n",
      "Epoch [687/10044], Batch [6/7], Loss: 0.0208, Accuracy: 99.50%, Grad Norm: 0.14286\n",
      "Epoch [687/10044], Batch [7/7], Loss: 0.0150, Accuracy: 99.75%, Grad Norm: 0.15328\n",
      "Epoch [687/10044], Loss: 0.0150\n",
      "Epoch [688/10044], Batch [1/7], Loss: 0.0186, Accuracy: 99.67%, Grad Norm: 0.11429\n",
      "Epoch [688/10044], Batch [2/7], Loss: 0.0209, Accuracy: 99.51%, Grad Norm: 0.12845\n",
      "Epoch [688/10044], Batch [3/7], Loss: 0.0166, Accuracy: 99.78%, Grad Norm: 0.09136\n",
      "Epoch [688/10044], Batch [4/7], Loss: 0.0190, Accuracy: 99.61%, Grad Norm: 0.10210\n",
      "Epoch [688/10044], Batch [5/7], Loss: 0.0177, Accuracy: 99.70%, Grad Norm: 0.11181\n",
      "Epoch [688/10044], Batch [6/7], Loss: 0.0207, Accuracy: 99.49%, Grad Norm: 0.12582\n",
      "Epoch [688/10044], Batch [7/7], Loss: 0.0179, Accuracy: 99.67%, Grad Norm: 0.19282\n",
      "Epoch [688/10044], Loss: 0.0179\n",
      "Epoch [689/10044], Batch [1/7], Loss: 0.0214, Accuracy: 99.62%, Grad Norm: 0.11555\n",
      "Epoch [689/10044], Batch [2/7], Loss: 0.0226, Accuracy: 99.57%, Grad Norm: 0.13225\n",
      "Epoch [689/10044], Batch [3/7], Loss: 0.0172, Accuracy: 99.74%, Grad Norm: 0.11244\n",
      "Epoch [689/10044], Batch [4/7], Loss: 0.0202, Accuracy: 99.60%, Grad Norm: 0.11969\n",
      "Epoch [689/10044], Batch [5/7], Loss: 0.0181, Accuracy: 99.71%, Grad Norm: 0.11527\n",
      "Epoch [689/10044], Batch [6/7], Loss: 0.0197, Accuracy: 99.62%, Grad Norm: 0.12066\n",
      "Epoch [689/10044], Batch [7/7], Loss: 0.0167, Accuracy: 99.58%, Grad Norm: 0.16198\n",
      "Epoch [689/10044], Loss: 0.0167\n",
      "Epoch [690/10044], Batch [1/7], Loss: 0.0190, Accuracy: 99.69%, Grad Norm: 0.11458\n",
      "Epoch [690/10044], Batch [2/7], Loss: 0.0232, Accuracy: 99.44%, Grad Norm: 0.17450\n",
      "Epoch [690/10044], Batch [3/7], Loss: 0.0184, Accuracy: 99.71%, Grad Norm: 0.11439\n",
      "Epoch [690/10044], Batch [4/7], Loss: 0.0190, Accuracy: 99.66%, Grad Norm: 0.10600\n",
      "Epoch [690/10044], Batch [5/7], Loss: 0.0177, Accuracy: 99.67%, Grad Norm: 0.11409\n",
      "Epoch [690/10044], Batch [6/7], Loss: 0.0215, Accuracy: 99.53%, Grad Norm: 0.13925\n",
      "Epoch [690/10044], Batch [7/7], Loss: 0.0147, Accuracy: 99.73%, Grad Norm: 0.12410\n",
      "Epoch [690/10044], Loss: 0.0147\n",
      "Epoch [691/10044], Batch [1/7], Loss: 0.0177, Accuracy: 99.74%, Grad Norm: 0.10899\n",
      "Epoch [691/10044], Batch [2/7], Loss: 0.0214, Accuracy: 99.52%, Grad Norm: 0.15859\n",
      "Epoch [691/10044], Batch [3/7], Loss: 0.0171, Accuracy: 99.72%, Grad Norm: 0.09768\n",
      "Epoch [691/10044], Batch [4/7], Loss: 0.0199, Accuracy: 99.68%, Grad Norm: 0.10424\n",
      "Epoch [691/10044], Batch [5/7], Loss: 0.0175, Accuracy: 99.72%, Grad Norm: 0.11266\n",
      "Epoch [691/10044], Batch [6/7], Loss: 0.0219, Accuracy: 99.48%, Grad Norm: 0.15169\n",
      "Epoch [691/10044], Batch [7/7], Loss: 0.0182, Accuracy: 99.67%, Grad Norm: 0.17235\n",
      "Epoch [691/10044], Loss: 0.0182\n",
      "Epoch [692/10044], Batch [1/7], Loss: 0.0184, Accuracy: 99.72%, Grad Norm: 0.13554\n",
      "Epoch [692/10044], Batch [2/7], Loss: 0.0202, Accuracy: 99.57%, Grad Norm: 0.14825\n",
      "Epoch [692/10044], Batch [3/7], Loss: 0.0179, Accuracy: 99.71%, Grad Norm: 0.10314\n",
      "Epoch [692/10044], Batch [4/7], Loss: 0.0202, Accuracy: 99.58%, Grad Norm: 0.10866\n",
      "Epoch [692/10044], Batch [5/7], Loss: 0.0188, Accuracy: 99.68%, Grad Norm: 0.13173\n",
      "Epoch [692/10044], Batch [6/7], Loss: 0.0197, Accuracy: 99.58%, Grad Norm: 0.12808\n",
      "Epoch [692/10044], Batch [7/7], Loss: 0.0136, Accuracy: 99.75%, Grad Norm: 0.12815\n",
      "Epoch [692/10044], Loss: 0.0136\n",
      "Epoch [693/10044], Batch [1/7], Loss: 0.0201, Accuracy: 99.64%, Grad Norm: 0.12899\n",
      "Epoch [693/10044], Batch [2/7], Loss: 0.0213, Accuracy: 99.55%, Grad Norm: 0.15088\n",
      "Epoch [693/10044], Batch [3/7], Loss: 0.0154, Accuracy: 99.83%, Grad Norm: 0.08405\n",
      "Epoch [693/10044], Batch [4/7], Loss: 0.0203, Accuracy: 99.69%, Grad Norm: 0.13813\n",
      "Epoch [693/10044], Batch [5/7], Loss: 0.0187, Accuracy: 99.67%, Grad Norm: 0.14587\n",
      "Epoch [693/10044], Batch [6/7], Loss: 0.0212, Accuracy: 99.57%, Grad Norm: 0.13575\n",
      "Epoch [693/10044], Batch [7/7], Loss: 0.0175, Accuracy: 99.67%, Grad Norm: 0.19587\n",
      "Epoch [693/10044], Loss: 0.0175\n",
      "Epoch [694/10044], Batch [1/7], Loss: 0.0184, Accuracy: 99.68%, Grad Norm: 0.10121\n",
      "Epoch [694/10044], Batch [2/7], Loss: 0.0201, Accuracy: 99.53%, Grad Norm: 0.13726\n",
      "Epoch [694/10044], Batch [3/7], Loss: 0.0173, Accuracy: 99.76%, Grad Norm: 0.09943\n",
      "Epoch [694/10044], Batch [4/7], Loss: 0.0204, Accuracy: 99.62%, Grad Norm: 0.11715\n",
      "Epoch [694/10044], Batch [5/7], Loss: 0.0181, Accuracy: 99.70%, Grad Norm: 0.13757\n",
      "Epoch [694/10044], Batch [6/7], Loss: 0.0223, Accuracy: 99.53%, Grad Norm: 0.15854\n",
      "Epoch [694/10044], Batch [7/7], Loss: 0.0174, Accuracy: 99.55%, Grad Norm: 0.22290\n",
      "Epoch [694/10044], Loss: 0.0174\n",
      "Epoch [695/10044], Batch [1/7], Loss: 0.0163, Accuracy: 99.80%, Grad Norm: 0.10282\n",
      "Epoch [695/10044], Batch [2/7], Loss: 0.0209, Accuracy: 99.56%, Grad Norm: 0.11246\n",
      "Epoch [695/10044], Batch [3/7], Loss: 0.0180, Accuracy: 99.67%, Grad Norm: 0.10802\n",
      "Epoch [695/10044], Batch [4/7], Loss: 0.0207, Accuracy: 99.57%, Grad Norm: 0.11925\n",
      "Epoch [695/10044], Batch [5/7], Loss: 0.0191, Accuracy: 99.68%, Grad Norm: 0.13495\n",
      "Epoch [695/10044], Batch [6/7], Loss: 0.0210, Accuracy: 99.56%, Grad Norm: 0.14015\n",
      "Epoch [695/10044], Batch [7/7], Loss: 0.0204, Accuracy: 99.55%, Grad Norm: 0.19983\n",
      "Epoch [695/10044], Loss: 0.0204\n",
      "Epoch [696/10044], Batch [1/7], Loss: 0.0177, Accuracy: 99.76%, Grad Norm: 0.09432\n",
      "Epoch [696/10044], Batch [2/7], Loss: 0.0214, Accuracy: 99.50%, Grad Norm: 0.14562\n",
      "Epoch [696/10044], Batch [3/7], Loss: 0.0171, Accuracy: 99.77%, Grad Norm: 0.10302\n",
      "Epoch [696/10044], Batch [4/7], Loss: 0.0187, Accuracy: 99.62%, Grad Norm: 0.11523\n",
      "Epoch [696/10044], Batch [5/7], Loss: 0.0197, Accuracy: 99.61%, Grad Norm: 0.15709\n",
      "Epoch [696/10044], Batch [6/7], Loss: 0.0198, Accuracy: 99.61%, Grad Norm: 0.12986\n",
      "Epoch [696/10044], Batch [7/7], Loss: 0.0165, Accuracy: 99.70%, Grad Norm: 0.16579\n",
      "Epoch [696/10044], Loss: 0.0165\n",
      "Epoch [697/10044], Batch [1/7], Loss: 0.0178, Accuracy: 99.71%, Grad Norm: 0.10021\n",
      "Epoch [697/10044], Batch [2/7], Loss: 0.0215, Accuracy: 99.53%, Grad Norm: 0.16795\n",
      "Epoch [697/10044], Batch [3/7], Loss: 0.0162, Accuracy: 99.78%, Grad Norm: 0.09326\n",
      "Epoch [697/10044], Batch [4/7], Loss: 0.0192, Accuracy: 99.62%, Grad Norm: 0.09729\n",
      "Epoch [697/10044], Batch [5/7], Loss: 0.0191, Accuracy: 99.62%, Grad Norm: 0.13766\n",
      "Epoch [697/10044], Batch [6/7], Loss: 0.0194, Accuracy: 99.64%, Grad Norm: 0.12355\n",
      "Epoch [697/10044], Batch [7/7], Loss: 0.0156, Accuracy: 99.68%, Grad Norm: 0.14173\n",
      "Epoch [697/10044], Loss: 0.0156\n",
      "Epoch [698/10044], Batch [1/7], Loss: 0.0194, Accuracy: 99.60%, Grad Norm: 0.14599\n",
      "Epoch [698/10044], Batch [2/7], Loss: 0.0211, Accuracy: 99.55%, Grad Norm: 0.17054\n",
      "Epoch [698/10044], Batch [3/7], Loss: 0.0158, Accuracy: 99.82%, Grad Norm: 0.08594\n",
      "Epoch [698/10044], Batch [4/7], Loss: 0.0186, Accuracy: 99.67%, Grad Norm: 0.09450\n",
      "Epoch [698/10044], Batch [5/7], Loss: 0.0177, Accuracy: 99.72%, Grad Norm: 0.13343\n",
      "Epoch [698/10044], Batch [6/7], Loss: 0.0204, Accuracy: 99.54%, Grad Norm: 0.12806\n",
      "Epoch [698/10044], Batch [7/7], Loss: 0.0172, Accuracy: 99.60%, Grad Norm: 0.17427\n",
      "Epoch [698/10044], Loss: 0.0172\n",
      "Epoch [699/10044], Batch [1/7], Loss: 0.0211, Accuracy: 99.58%, Grad Norm: 0.13899\n",
      "Epoch [699/10044], Batch [2/7], Loss: 0.0232, Accuracy: 99.42%, Grad Norm: 0.20060\n",
      "Epoch [699/10044], Batch [3/7], Loss: 0.0180, Accuracy: 99.68%, Grad Norm: 0.10464\n",
      "Epoch [699/10044], Batch [4/7], Loss: 0.0186, Accuracy: 99.69%, Grad Norm: 0.09209\n",
      "Epoch [699/10044], Batch [5/7], Loss: 0.0203, Accuracy: 99.62%, Grad Norm: 0.16181\n",
      "Epoch [699/10044], Batch [6/7], Loss: 0.0219, Accuracy: 99.58%, Grad Norm: 0.16050\n",
      "Epoch [699/10044], Batch [7/7], Loss: 0.0181, Accuracy: 99.65%, Grad Norm: 0.18615\n",
      "Epoch [699/10044], Loss: 0.0181\n",
      "Epoch [700/10044], Batch [1/7], Loss: 0.0181, Accuracy: 99.71%, Grad Norm: 0.11449\n",
      "Epoch [700/10044], Batch [2/7], Loss: 0.0202, Accuracy: 99.55%, Grad Norm: 0.15066\n",
      "Epoch [700/10044], Batch [3/7], Loss: 0.0162, Accuracy: 99.78%, Grad Norm: 0.10503\n",
      "Epoch [700/10044], Batch [4/7], Loss: 0.0190, Accuracy: 99.64%, Grad Norm: 0.10940\n",
      "Epoch [700/10044], Batch [5/7], Loss: 0.0191, Accuracy: 99.60%, Grad Norm: 0.13733\n",
      "Epoch [700/10044], Batch [6/7], Loss: 0.0210, Accuracy: 99.56%, Grad Norm: 0.15409\n",
      "Epoch [700/10044], Batch [7/7], Loss: 0.0145, Accuracy: 99.72%, Grad Norm: 0.13862\n",
      "Epoch [700/10044], Loss: 0.0145\n",
      "Epoch [701/10044], Batch [1/7], Loss: 0.0180, Accuracy: 99.72%, Grad Norm: 0.10800\n",
      "Epoch [701/10044], Batch [2/7], Loss: 0.0196, Accuracy: 99.63%, Grad Norm: 0.12554\n",
      "Epoch [701/10044], Batch [3/7], Loss: 0.0164, Accuracy: 99.79%, Grad Norm: 0.11209\n",
      "Epoch [701/10044], Batch [4/7], Loss: 0.0203, Accuracy: 99.63%, Grad Norm: 0.13006\n",
      "Epoch [701/10044], Batch [5/7], Loss: 0.0170, Accuracy: 99.72%, Grad Norm: 0.11371\n",
      "Epoch [701/10044], Batch [6/7], Loss: 0.0180, Accuracy: 99.67%, Grad Norm: 0.11573\n",
      "Epoch [701/10044], Batch [7/7], Loss: 0.0146, Accuracy: 99.67%, Grad Norm: 0.14266\n",
      "Epoch [701/10044], Loss: 0.0146\n",
      "Epoch [702/10044], Batch [1/7], Loss: 0.0191, Accuracy: 99.67%, Grad Norm: 0.12589\n",
      "Epoch [702/10044], Batch [2/7], Loss: 0.0200, Accuracy: 99.58%, Grad Norm: 0.13749\n",
      "Epoch [702/10044], Batch [3/7], Loss: 0.0160, Accuracy: 99.78%, Grad Norm: 0.10508\n",
      "Epoch [702/10044], Batch [4/7], Loss: 0.0188, Accuracy: 99.64%, Grad Norm: 0.10517\n",
      "Epoch [702/10044], Batch [5/7], Loss: 0.0152, Accuracy: 99.80%, Grad Norm: 0.09267\n",
      "Epoch [702/10044], Batch [6/7], Loss: 0.0195, Accuracy: 99.62%, Grad Norm: 0.13242\n",
      "Epoch [702/10044], Batch [7/7], Loss: 0.0158, Accuracy: 99.63%, Grad Norm: 0.17490\n",
      "Epoch [702/10044], Loss: 0.0158\n",
      "Epoch [703/10044], Batch [1/7], Loss: 0.0175, Accuracy: 99.72%, Grad Norm: 0.13529\n",
      "Epoch [703/10044], Batch [2/7], Loss: 0.0195, Accuracy: 99.61%, Grad Norm: 0.13945\n",
      "Epoch [703/10044], Batch [3/7], Loss: 0.0169, Accuracy: 99.75%, Grad Norm: 0.09770\n",
      "Epoch [703/10044], Batch [4/7], Loss: 0.0177, Accuracy: 99.71%, Grad Norm: 0.08708\n",
      "Epoch [703/10044], Batch [5/7], Loss: 0.0180, Accuracy: 99.69%, Grad Norm: 0.12456\n",
      "Epoch [703/10044], Batch [6/7], Loss: 0.0196, Accuracy: 99.59%, Grad Norm: 0.14278\n",
      "Epoch [703/10044], Batch [7/7], Loss: 0.0162, Accuracy: 99.75%, Grad Norm: 0.13287\n",
      "Epoch [703/10044], Loss: 0.0162\n",
      "Epoch [704/10044], Batch [1/7], Loss: 0.0177, Accuracy: 99.72%, Grad Norm: 0.11130\n",
      "Epoch [704/10044], Batch [2/7], Loss: 0.0208, Accuracy: 99.57%, Grad Norm: 0.14390\n",
      "Epoch [704/10044], Batch [3/7], Loss: 0.0162, Accuracy: 99.75%, Grad Norm: 0.09609\n",
      "Epoch [704/10044], Batch [4/7], Loss: 0.0197, Accuracy: 99.60%, Grad Norm: 0.10294\n",
      "Epoch [704/10044], Batch [5/7], Loss: 0.0181, Accuracy: 99.73%, Grad Norm: 0.13526\n",
      "Epoch [704/10044], Batch [6/7], Loss: 0.0202, Accuracy: 99.59%, Grad Norm: 0.15308\n",
      "Epoch [704/10044], Batch [7/7], Loss: 0.0134, Accuracy: 99.75%, Grad Norm: 0.13791\n",
      "Epoch [704/10044], Loss: 0.0134\n",
      "Epoch [705/10044], Batch [1/7], Loss: 0.0171, Accuracy: 99.73%, Grad Norm: 0.09945\n",
      "Epoch [705/10044], Batch [2/7], Loss: 0.0219, Accuracy: 99.46%, Grad Norm: 0.17041\n",
      "Epoch [705/10044], Batch [3/7], Loss: 0.0168, Accuracy: 99.76%, Grad Norm: 0.10881\n",
      "Epoch [705/10044], Batch [4/7], Loss: 0.0190, Accuracy: 99.63%, Grad Norm: 0.11963\n",
      "Epoch [705/10044], Batch [5/7], Loss: 0.0201, Accuracy: 99.58%, Grad Norm: 0.14716\n",
      "Epoch [705/10044], Batch [6/7], Loss: 0.0219, Accuracy: 99.52%, Grad Norm: 0.15492\n",
      "Epoch [705/10044], Batch [7/7], Loss: 0.0155, Accuracy: 99.63%, Grad Norm: 0.14431\n",
      "Epoch [705/10044], Loss: 0.0155\n",
      "Epoch [706/10044], Batch [1/7], Loss: 0.0183, Accuracy: 99.69%, Grad Norm: 0.11740\n",
      "Epoch [706/10044], Batch [2/7], Loss: 0.0225, Accuracy: 99.53%, Grad Norm: 0.17908\n",
      "Epoch [706/10044], Batch [3/7], Loss: 0.0176, Accuracy: 99.66%, Grad Norm: 0.12056\n",
      "Epoch [706/10044], Batch [4/7], Loss: 0.0209, Accuracy: 99.53%, Grad Norm: 0.11652\n",
      "Epoch [706/10044], Batch [5/7], Loss: 0.0181, Accuracy: 99.64%, Grad Norm: 0.11852\n",
      "Epoch [706/10044], Batch [6/7], Loss: 0.0193, Accuracy: 99.57%, Grad Norm: 0.14596\n",
      "Epoch [706/10044], Batch [7/7], Loss: 0.0161, Accuracy: 99.67%, Grad Norm: 0.14306\n",
      "Epoch [706/10044], Loss: 0.0161\n",
      "Epoch [707/10044], Batch [1/7], Loss: 0.0183, Accuracy: 99.68%, Grad Norm: 0.11301\n",
      "Epoch [707/10044], Batch [2/7], Loss: 0.0213, Accuracy: 99.52%, Grad Norm: 0.15666\n",
      "Epoch [707/10044], Batch [3/7], Loss: 0.0162, Accuracy: 99.75%, Grad Norm: 0.10195\n",
      "Epoch [707/10044], Batch [4/7], Loss: 0.0194, Accuracy: 99.57%, Grad Norm: 0.11435\n",
      "Epoch [707/10044], Batch [5/7], Loss: 0.0178, Accuracy: 99.66%, Grad Norm: 0.12395\n",
      "Epoch [707/10044], Batch [6/7], Loss: 0.0186, Accuracy: 99.67%, Grad Norm: 0.12671\n",
      "Epoch [707/10044], Batch [7/7], Loss: 0.0162, Accuracy: 99.72%, Grad Norm: 0.14740\n",
      "Epoch [707/10044], Loss: 0.0162\n",
      "Epoch [708/10044], Batch [1/7], Loss: 0.0169, Accuracy: 99.78%, Grad Norm: 0.12168\n",
      "Epoch [708/10044], Batch [2/7], Loss: 0.0185, Accuracy: 99.65%, Grad Norm: 0.12781\n",
      "Epoch [708/10044], Batch [3/7], Loss: 0.0167, Accuracy: 99.78%, Grad Norm: 0.10300\n",
      "Epoch [708/10044], Batch [4/7], Loss: 0.0178, Accuracy: 99.69%, Grad Norm: 0.10200\n",
      "Epoch [708/10044], Batch [5/7], Loss: 0.0187, Accuracy: 99.60%, Grad Norm: 0.12755\n",
      "Epoch [708/10044], Batch [6/7], Loss: 0.0185, Accuracy: 99.66%, Grad Norm: 0.12811\n",
      "Epoch [708/10044], Batch [7/7], Loss: 0.0138, Accuracy: 99.72%, Grad Norm: 0.13303\n",
      "Epoch [708/10044], Loss: 0.0138\n",
      "Epoch [709/10044], Batch [1/7], Loss: 0.0178, Accuracy: 99.73%, Grad Norm: 0.10884\n",
      "Epoch [709/10044], Batch [2/7], Loss: 0.0199, Accuracy: 99.57%, Grad Norm: 0.14842\n",
      "Epoch [709/10044], Batch [3/7], Loss: 0.0151, Accuracy: 99.80%, Grad Norm: 0.08038\n",
      "Epoch [709/10044], Batch [4/7], Loss: 0.0198, Accuracy: 99.64%, Grad Norm: 0.11204\n",
      "Epoch [709/10044], Batch [5/7], Loss: 0.0186, Accuracy: 99.67%, Grad Norm: 0.13101\n",
      "Epoch [709/10044], Batch [6/7], Loss: 0.0181, Accuracy: 99.62%, Grad Norm: 0.12675\n",
      "Epoch [709/10044], Batch [7/7], Loss: 0.0139, Accuracy: 99.72%, Grad Norm: 0.12171\n",
      "Epoch [709/10044], Loss: 0.0139\n",
      "Epoch [710/10044], Batch [1/7], Loss: 0.0171, Accuracy: 99.70%, Grad Norm: 0.11424\n",
      "Epoch [710/10044], Batch [2/7], Loss: 0.0185, Accuracy: 99.61%, Grad Norm: 0.12784\n",
      "Epoch [710/10044], Batch [3/7], Loss: 0.0152, Accuracy: 99.81%, Grad Norm: 0.07920\n",
      "Epoch [710/10044], Batch [4/7], Loss: 0.0194, Accuracy: 99.62%, Grad Norm: 0.10980\n",
      "Epoch [710/10044], Batch [5/7], Loss: 0.0159, Accuracy: 99.72%, Grad Norm: 0.09553\n",
      "Epoch [710/10044], Batch [6/7], Loss: 0.0196, Accuracy: 99.53%, Grad Norm: 0.13347\n",
      "Epoch [710/10044], Batch [7/7], Loss: 0.0133, Accuracy: 99.75%, Grad Norm: 0.12431\n",
      "Epoch [710/10044], Loss: 0.0133\n",
      "Epoch [711/10044], Batch [1/7], Loss: 0.0183, Accuracy: 99.66%, Grad Norm: 0.11615\n",
      "Epoch [711/10044], Batch [2/7], Loss: 0.0196, Accuracy: 99.52%, Grad Norm: 0.12334\n",
      "Epoch [711/10044], Batch [3/7], Loss: 0.0158, Accuracy: 99.77%, Grad Norm: 0.09371\n",
      "Epoch [711/10044], Batch [4/7], Loss: 0.0179, Accuracy: 99.64%, Grad Norm: 0.09083\n",
      "Epoch [711/10044], Batch [5/7], Loss: 0.0185, Accuracy: 99.64%, Grad Norm: 0.11887\n",
      "Epoch [711/10044], Batch [6/7], Loss: 0.0186, Accuracy: 99.58%, Grad Norm: 0.13002\n",
      "Epoch [711/10044], Batch [7/7], Loss: 0.0128, Accuracy: 99.78%, Grad Norm: 0.10751\n",
      "Epoch [711/10044], Loss: 0.0128\n",
      "Epoch [712/10044], Batch [1/7], Loss: 0.0174, Accuracy: 99.65%, Grad Norm: 0.12638\n",
      "Epoch [712/10044], Batch [2/7], Loss: 0.0179, Accuracy: 99.62%, Grad Norm: 0.11636\n",
      "Epoch [712/10044], Batch [3/7], Loss: 0.0161, Accuracy: 99.80%, Grad Norm: 0.09451\n",
      "Epoch [712/10044], Batch [4/7], Loss: 0.0176, Accuracy: 99.70%, Grad Norm: 0.08963\n",
      "Epoch [712/10044], Batch [5/7], Loss: 0.0176, Accuracy: 99.69%, Grad Norm: 0.12415\n",
      "Epoch [712/10044], Batch [6/7], Loss: 0.0186, Accuracy: 99.60%, Grad Norm: 0.10972\n",
      "Epoch [712/10044], Batch [7/7], Loss: 0.0143, Accuracy: 99.65%, Grad Norm: 0.14726\n",
      "Epoch [712/10044], Loss: 0.0143\n",
      "Epoch [713/10044], Batch [1/7], Loss: 0.0166, Accuracy: 99.71%, Grad Norm: 0.09476\n",
      "Epoch [713/10044], Batch [2/7], Loss: 0.0192, Accuracy: 99.62%, Grad Norm: 0.12002\n",
      "Epoch [713/10044], Batch [3/7], Loss: 0.0148, Accuracy: 99.81%, Grad Norm: 0.08147\n",
      "Epoch [713/10044], Batch [4/7], Loss: 0.0181, Accuracy: 99.63%, Grad Norm: 0.09947\n",
      "Epoch [713/10044], Batch [5/7], Loss: 0.0168, Accuracy: 99.64%, Grad Norm: 0.11867\n",
      "Epoch [713/10044], Batch [6/7], Loss: 0.0172, Accuracy: 99.67%, Grad Norm: 0.11488\n",
      "Epoch [713/10044], Batch [7/7], Loss: 0.0168, Accuracy: 99.65%, Grad Norm: 0.19884\n",
      "Epoch [713/10044], Loss: 0.0168\n",
      "Epoch [714/10044], Batch [1/7], Loss: 0.0167, Accuracy: 99.69%, Grad Norm: 0.11965\n",
      "Epoch [714/10044], Batch [2/7], Loss: 0.0198, Accuracy: 99.58%, Grad Norm: 0.13736\n",
      "Epoch [714/10044], Batch [3/7], Loss: 0.0159, Accuracy: 99.73%, Grad Norm: 0.10181\n",
      "Epoch [714/10044], Batch [4/7], Loss: 0.0175, Accuracy: 99.62%, Grad Norm: 0.09689\n",
      "Epoch [714/10044], Batch [5/7], Loss: 0.0166, Accuracy: 99.71%, Grad Norm: 0.11607\n",
      "Epoch [714/10044], Batch [6/7], Loss: 0.0185, Accuracy: 99.64%, Grad Norm: 0.12270\n",
      "Epoch [714/10044], Batch [7/7], Loss: 0.0140, Accuracy: 99.70%, Grad Norm: 0.14242\n",
      "Epoch [714/10044], Loss: 0.0140\n",
      "Epoch [715/10044], Batch [1/7], Loss: 0.0172, Accuracy: 99.71%, Grad Norm: 0.13068\n",
      "Epoch [715/10044], Batch [2/7], Loss: 0.0195, Accuracy: 99.66%, Grad Norm: 0.14287\n",
      "Epoch [715/10044], Batch [3/7], Loss: 0.0168, Accuracy: 99.77%, Grad Norm: 0.11783\n",
      "Epoch [715/10044], Batch [4/7], Loss: 0.0183, Accuracy: 99.62%, Grad Norm: 0.11489\n",
      "Epoch [715/10044], Batch [5/7], Loss: 0.0167, Accuracy: 99.68%, Grad Norm: 0.12598\n",
      "Epoch [715/10044], Batch [6/7], Loss: 0.0196, Accuracy: 99.56%, Grad Norm: 0.12881\n",
      "Epoch [715/10044], Batch [7/7], Loss: 0.0155, Accuracy: 99.60%, Grad Norm: 0.16702\n",
      "Epoch [715/10044], Loss: 0.0155\n",
      "Epoch [716/10044], Batch [1/7], Loss: 0.0174, Accuracy: 99.73%, Grad Norm: 0.11519\n",
      "Epoch [716/10044], Batch [2/7], Loss: 0.0182, Accuracy: 99.63%, Grad Norm: 0.13583\n",
      "Epoch [716/10044], Batch [3/7], Loss: 0.0164, Accuracy: 99.70%, Grad Norm: 0.11970\n",
      "Epoch [716/10044], Batch [4/7], Loss: 0.0195, Accuracy: 99.58%, Grad Norm: 0.14264\n",
      "Epoch [716/10044], Batch [5/7], Loss: 0.0166, Accuracy: 99.70%, Grad Norm: 0.13802\n",
      "Epoch [716/10044], Batch [6/7], Loss: 0.0191, Accuracy: 99.61%, Grad Norm: 0.14644\n",
      "Epoch [716/10044], Batch [7/7], Loss: 0.0154, Accuracy: 99.72%, Grad Norm: 0.17959\n",
      "Epoch [716/10044], Loss: 0.0154\n",
      "Epoch [717/10044], Batch [1/7], Loss: 0.0180, Accuracy: 99.68%, Grad Norm: 0.12169\n",
      "Epoch [717/10044], Batch [2/7], Loss: 0.0204, Accuracy: 99.56%, Grad Norm: 0.14584\n",
      "Epoch [717/10044], Batch [3/7], Loss: 0.0169, Accuracy: 99.72%, Grad Norm: 0.11933\n",
      "Epoch [717/10044], Batch [4/7], Loss: 0.0183, Accuracy: 99.61%, Grad Norm: 0.12148\n",
      "Epoch [717/10044], Batch [5/7], Loss: 0.0169, Accuracy: 99.70%, Grad Norm: 0.13233\n",
      "Epoch [717/10044], Batch [6/7], Loss: 0.0189, Accuracy: 99.65%, Grad Norm: 0.13077\n",
      "Epoch [717/10044], Batch [7/7], Loss: 0.0155, Accuracy: 99.65%, Grad Norm: 0.15211\n",
      "Epoch [717/10044], Loss: 0.0155\n",
      "Epoch [718/10044], Batch [1/7], Loss: 0.0172, Accuracy: 99.69%, Grad Norm: 0.12208\n",
      "Epoch [718/10044], Batch [2/7], Loss: 0.0195, Accuracy: 99.62%, Grad Norm: 0.14935\n",
      "Epoch [718/10044], Batch [3/7], Loss: 0.0157, Accuracy: 99.77%, Grad Norm: 0.09616\n",
      "Epoch [718/10044], Batch [4/7], Loss: 0.0171, Accuracy: 99.69%, Grad Norm: 0.09894\n",
      "Epoch [718/10044], Batch [5/7], Loss: 0.0166, Accuracy: 99.71%, Grad Norm: 0.10831\n",
      "Epoch [718/10044], Batch [6/7], Loss: 0.0190, Accuracy: 99.60%, Grad Norm: 0.14542\n",
      "Epoch [718/10044], Batch [7/7], Loss: 0.0164, Accuracy: 99.62%, Grad Norm: 0.17176\n",
      "Epoch [718/10044], Loss: 0.0164\n",
      "Epoch [719/10044], Batch [1/7], Loss: 0.0172, Accuracy: 99.66%, Grad Norm: 0.10559\n",
      "Epoch [719/10044], Batch [2/7], Loss: 0.0195, Accuracy: 99.62%, Grad Norm: 0.13290\n",
      "Epoch [719/10044], Batch [3/7], Loss: 0.0152, Accuracy: 99.79%, Grad Norm: 0.09388\n",
      "Epoch [719/10044], Batch [4/7], Loss: 0.0181, Accuracy: 99.62%, Grad Norm: 0.09863\n",
      "Epoch [719/10044], Batch [5/7], Loss: 0.0162, Accuracy: 99.70%, Grad Norm: 0.11665\n",
      "Epoch [719/10044], Batch [6/7], Loss: 0.0189, Accuracy: 99.62%, Grad Norm: 0.13845\n",
      "Epoch [719/10044], Batch [7/7], Loss: 0.0150, Accuracy: 99.75%, Grad Norm: 0.14499\n",
      "Epoch [719/10044], Loss: 0.0150\n",
      "Epoch [720/10044], Batch [1/7], Loss: 0.0176, Accuracy: 99.69%, Grad Norm: 0.13421\n",
      "Epoch [720/10044], Batch [2/7], Loss: 0.0183, Accuracy: 99.64%, Grad Norm: 0.13354\n",
      "Epoch [720/10044], Batch [3/7], Loss: 0.0152, Accuracy: 99.77%, Grad Norm: 0.08105\n",
      "Epoch [720/10044], Batch [4/7], Loss: 0.0204, Accuracy: 99.49%, Grad Norm: 0.11346\n",
      "Epoch [720/10044], Batch [5/7], Loss: 0.0172, Accuracy: 99.64%, Grad Norm: 0.12230\n",
      "Epoch [720/10044], Batch [6/7], Loss: 0.0227, Accuracy: 99.47%, Grad Norm: 0.17147\n",
      "Epoch [720/10044], Batch [7/7], Loss: 0.0173, Accuracy: 99.53%, Grad Norm: 0.17182\n",
      "Epoch [720/10044], Loss: 0.0173\n",
      "Epoch [721/10044], Batch [1/7], Loss: 0.0160, Accuracy: 99.74%, Grad Norm: 0.09737\n",
      "Epoch [721/10044], Batch [2/7], Loss: 0.0190, Accuracy: 99.59%, Grad Norm: 0.11994\n",
      "Epoch [721/10044], Batch [3/7], Loss: 0.0157, Accuracy: 99.77%, Grad Norm: 0.10052\n",
      "Epoch [721/10044], Batch [4/7], Loss: 0.0173, Accuracy: 99.72%, Grad Norm: 0.14084\n",
      "Epoch [721/10044], Batch [5/7], Loss: 0.0168, Accuracy: 99.67%, Grad Norm: 0.14038\n",
      "Epoch [721/10044], Batch [6/7], Loss: 0.0182, Accuracy: 99.60%, Grad Norm: 0.14866\n",
      "Epoch [721/10044], Batch [7/7], Loss: 0.0149, Accuracy: 99.67%, Grad Norm: 0.15096\n",
      "Epoch [721/10044], Loss: 0.0149\n",
      "Epoch [722/10044], Batch [1/7], Loss: 0.0161, Accuracy: 99.75%, Grad Norm: 0.10580\n",
      "Epoch [722/10044], Batch [2/7], Loss: 0.0196, Accuracy: 99.57%, Grad Norm: 0.15284\n",
      "Epoch [722/10044], Batch [3/7], Loss: 0.0151, Accuracy: 99.82%, Grad Norm: 0.09805\n",
      "Epoch [722/10044], Batch [4/7], Loss: 0.0176, Accuracy: 99.74%, Grad Norm: 0.11646\n",
      "Epoch [722/10044], Batch [5/7], Loss: 0.0171, Accuracy: 99.69%, Grad Norm: 0.13781\n",
      "Epoch [722/10044], Batch [6/7], Loss: 0.0176, Accuracy: 99.68%, Grad Norm: 0.11283\n",
      "Epoch [722/10044], Batch [7/7], Loss: 0.0135, Accuracy: 99.72%, Grad Norm: 0.15074\n",
      "Epoch [722/10044], Loss: 0.0135\n",
      "Epoch [723/10044], Batch [1/7], Loss: 0.0180, Accuracy: 99.64%, Grad Norm: 0.14056\n",
      "Epoch [723/10044], Batch [2/7], Loss: 0.0187, Accuracy: 99.61%, Grad Norm: 0.13887\n",
      "Epoch [723/10044], Batch [3/7], Loss: 0.0152, Accuracy: 99.82%, Grad Norm: 0.09236\n",
      "Epoch [723/10044], Batch [4/7], Loss: 0.0183, Accuracy: 99.61%, Grad Norm: 0.10857\n",
      "Epoch [723/10044], Batch [5/7], Loss: 0.0178, Accuracy: 99.65%, Grad Norm: 0.13406\n",
      "Epoch [723/10044], Batch [6/7], Loss: 0.0201, Accuracy: 99.56%, Grad Norm: 0.16152\n",
      "Epoch [723/10044], Batch [7/7], Loss: 0.0143, Accuracy: 99.70%, Grad Norm: 0.13453\n",
      "Epoch [723/10044], Loss: 0.0143\n",
      "Epoch [724/10044], Batch [1/7], Loss: 0.0168, Accuracy: 99.68%, Grad Norm: 0.11774\n",
      "Epoch [724/10044], Batch [2/7], Loss: 0.0191, Accuracy: 99.61%, Grad Norm: 0.12425\n",
      "Epoch [724/10044], Batch [3/7], Loss: 0.0142, Accuracy: 99.83%, Grad Norm: 0.08248\n",
      "Epoch [724/10044], Batch [4/7], Loss: 0.0182, Accuracy: 99.62%, Grad Norm: 0.10571\n",
      "Epoch [724/10044], Batch [5/7], Loss: 0.0180, Accuracy: 99.59%, Grad Norm: 0.15626\n",
      "Epoch [724/10044], Batch [6/7], Loss: 0.0187, Accuracy: 99.57%, Grad Norm: 0.14516\n",
      "Epoch [724/10044], Batch [7/7], Loss: 0.0126, Accuracy: 99.73%, Grad Norm: 0.13017\n",
      "Epoch [724/10044], Loss: 0.0126\n",
      "Epoch [725/10044], Batch [1/7], Loss: 0.0173, Accuracy: 99.77%, Grad Norm: 0.11763\n",
      "Epoch [725/10044], Batch [2/7], Loss: 0.0192, Accuracy: 99.54%, Grad Norm: 0.13235\n",
      "Epoch [725/10044], Batch [3/7], Loss: 0.0181, Accuracy: 99.67%, Grad Norm: 0.13046\n",
      "Epoch [725/10044], Batch [4/7], Loss: 0.0168, Accuracy: 99.72%, Grad Norm: 0.10529\n",
      "Epoch [725/10044], Batch [5/7], Loss: 0.0179, Accuracy: 99.57%, Grad Norm: 0.14013\n",
      "Epoch [725/10044], Batch [6/7], Loss: 0.0186, Accuracy: 99.62%, Grad Norm: 0.14331\n",
      "Epoch [725/10044], Batch [7/7], Loss: 0.0125, Accuracy: 99.80%, Grad Norm: 0.13120\n",
      "Epoch [725/10044], Loss: 0.0125\n",
      "Epoch [726/10044], Batch [1/7], Loss: 0.0156, Accuracy: 99.77%, Grad Norm: 0.10607\n",
      "Epoch [726/10044], Batch [2/7], Loss: 0.0179, Accuracy: 99.64%, Grad Norm: 0.14273\n",
      "Epoch [726/10044], Batch [3/7], Loss: 0.0153, Accuracy: 99.80%, Grad Norm: 0.10368\n",
      "Epoch [726/10044], Batch [4/7], Loss: 0.0173, Accuracy: 99.67%, Grad Norm: 0.11322\n",
      "Epoch [726/10044], Batch [5/7], Loss: 0.0155, Accuracy: 99.68%, Grad Norm: 0.11129\n",
      "Epoch [726/10044], Batch [6/7], Loss: 0.0191, Accuracy: 99.56%, Grad Norm: 0.16397\n",
      "Epoch [726/10044], Batch [7/7], Loss: 0.0181, Accuracy: 99.53%, Grad Norm: 0.17684\n",
      "Epoch [726/10044], Loss: 0.0181\n",
      "Epoch [727/10044], Batch [1/7], Loss: 0.0157, Accuracy: 99.78%, Grad Norm: 0.11654\n",
      "Epoch [727/10044], Batch [2/7], Loss: 0.0195, Accuracy: 99.56%, Grad Norm: 0.13184\n",
      "Epoch [727/10044], Batch [3/7], Loss: 0.0141, Accuracy: 99.82%, Grad Norm: 0.08477\n",
      "Epoch [727/10044], Batch [4/7], Loss: 0.0166, Accuracy: 99.74%, Grad Norm: 0.10203\n",
      "Epoch [727/10044], Batch [5/7], Loss: 0.0160, Accuracy: 99.72%, Grad Norm: 0.12306\n",
      "Epoch [727/10044], Batch [6/7], Loss: 0.0171, Accuracy: 99.62%, Grad Norm: 0.12181\n",
      "Epoch [727/10044], Batch [7/7], Loss: 0.0120, Accuracy: 99.83%, Grad Norm: 0.10956\n",
      "Epoch [727/10044], Loss: 0.0120\n",
      "Epoch [728/10044], Batch [1/7], Loss: 0.0166, Accuracy: 99.75%, Grad Norm: 0.11127\n",
      "Epoch [728/10044], Batch [2/7], Loss: 0.0176, Accuracy: 99.60%, Grad Norm: 0.12802\n",
      "Epoch [728/10044], Batch [3/7], Loss: 0.0148, Accuracy: 99.77%, Grad Norm: 0.08537\n",
      "Epoch [728/10044], Batch [4/7], Loss: 0.0177, Accuracy: 99.70%, Grad Norm: 0.11054\n",
      "Epoch [728/10044], Batch [5/7], Loss: 0.0169, Accuracy: 99.62%, Grad Norm: 0.13070\n",
      "Epoch [728/10044], Batch [6/7], Loss: 0.0176, Accuracy: 99.59%, Grad Norm: 0.12168\n",
      "Epoch [728/10044], Batch [7/7], Loss: 0.0139, Accuracy: 99.73%, Grad Norm: 0.14122\n",
      "Epoch [728/10044], Loss: 0.0139\n",
      "Epoch [729/10044], Batch [1/7], Loss: 0.0161, Accuracy: 99.70%, Grad Norm: 0.10229\n",
      "Epoch [729/10044], Batch [2/7], Loss: 0.0201, Accuracy: 99.50%, Grad Norm: 0.15750\n",
      "Epoch [729/10044], Batch [3/7], Loss: 0.0152, Accuracy: 99.77%, Grad Norm: 0.09872\n",
      "Epoch [729/10044], Batch [4/7], Loss: 0.0182, Accuracy: 99.67%, Grad Norm: 0.11374\n",
      "Epoch [729/10044], Batch [5/7], Loss: 0.0177, Accuracy: 99.68%, Grad Norm: 0.13898\n",
      "Epoch [729/10044], Batch [6/7], Loss: 0.0183, Accuracy: 99.61%, Grad Norm: 0.12629\n",
      "Epoch [729/10044], Batch [7/7], Loss: 0.0171, Accuracy: 99.62%, Grad Norm: 0.18947\n",
      "Epoch [729/10044], Loss: 0.0171\n",
      "Epoch [730/10044], Batch [1/7], Loss: 0.0167, Accuracy: 99.68%, Grad Norm: 0.11440\n",
      "Epoch [730/10044], Batch [2/7], Loss: 0.0209, Accuracy: 99.48%, Grad Norm: 0.18654\n",
      "Epoch [730/10044], Batch [3/7], Loss: 0.0142, Accuracy: 99.77%, Grad Norm: 0.08559\n",
      "Epoch [730/10044], Batch [4/7], Loss: 0.0191, Accuracy: 99.58%, Grad Norm: 0.10965\n",
      "Epoch [730/10044], Batch [5/7], Loss: 0.0164, Accuracy: 99.70%, Grad Norm: 0.13067\n",
      "Epoch [730/10044], Batch [6/7], Loss: 0.0181, Accuracy: 99.56%, Grad Norm: 0.16058\n",
      "Epoch [730/10044], Batch [7/7], Loss: 0.0157, Accuracy: 99.63%, Grad Norm: 0.15082\n",
      "Epoch [730/10044], Loss: 0.0157\n",
      "Epoch [731/10044], Batch [1/7], Loss: 0.0166, Accuracy: 99.68%, Grad Norm: 0.11083\n",
      "Epoch [731/10044], Batch [2/7], Loss: 0.0222, Accuracy: 99.46%, Grad Norm: 0.17800\n",
      "Epoch [731/10044], Batch [3/7], Loss: 0.0148, Accuracy: 99.75%, Grad Norm: 0.09395\n",
      "Epoch [731/10044], Batch [4/7], Loss: 0.0183, Accuracy: 99.63%, Grad Norm: 0.13035\n",
      "Epoch [731/10044], Batch [5/7], Loss: 0.0179, Accuracy: 99.71%, Grad Norm: 0.15125\n",
      "Epoch [731/10044], Batch [6/7], Loss: 0.0187, Accuracy: 99.60%, Grad Norm: 0.13696\n",
      "Epoch [731/10044], Batch [7/7], Loss: 0.0151, Accuracy: 99.68%, Grad Norm: 0.17183\n",
      "Epoch [731/10044], Loss: 0.0151\n",
      "Epoch [732/10044], Batch [1/7], Loss: 0.0155, Accuracy: 99.71%, Grad Norm: 0.10257\n",
      "Epoch [732/10044], Batch [2/7], Loss: 0.0220, Accuracy: 99.52%, Grad Norm: 0.15506\n",
      "Epoch [732/10044], Batch [3/7], Loss: 0.0146, Accuracy: 99.76%, Grad Norm: 0.09146\n",
      "Epoch [732/10044], Batch [4/7], Loss: 0.0190, Accuracy: 99.64%, Grad Norm: 0.13578\n",
      "Epoch [732/10044], Batch [5/7], Loss: 0.0171, Accuracy: 99.61%, Grad Norm: 0.15074\n",
      "Epoch [732/10044], Batch [6/7], Loss: 0.0184, Accuracy: 99.60%, Grad Norm: 0.12344\n",
      "Epoch [732/10044], Batch [7/7], Loss: 0.0159, Accuracy: 99.58%, Grad Norm: 0.17658\n",
      "Epoch [732/10044], Loss: 0.0159\n",
      "Epoch [733/10044], Batch [1/7], Loss: 0.0165, Accuracy: 99.68%, Grad Norm: 0.12390\n",
      "Epoch [733/10044], Batch [2/7], Loss: 0.0184, Accuracy: 99.57%, Grad Norm: 0.15238\n",
      "Epoch [733/10044], Batch [3/7], Loss: 0.0133, Accuracy: 99.82%, Grad Norm: 0.07749\n",
      "Epoch [733/10044], Batch [4/7], Loss: 0.0160, Accuracy: 99.75%, Grad Norm: 0.09416\n",
      "Epoch [733/10044], Batch [5/7], Loss: 0.0161, Accuracy: 99.72%, Grad Norm: 0.11696\n",
      "Epoch [733/10044], Batch [6/7], Loss: 0.0185, Accuracy: 99.54%, Grad Norm: 0.13228\n",
      "Epoch [733/10044], Batch [7/7], Loss: 0.0131, Accuracy: 99.75%, Grad Norm: 0.11843\n",
      "Epoch [733/10044], Loss: 0.0131\n",
      "Epoch [734/10044], Batch [1/7], Loss: 0.0167, Accuracy: 99.73%, Grad Norm: 0.12904\n",
      "Epoch [734/10044], Batch [2/7], Loss: 0.0208, Accuracy: 99.46%, Grad Norm: 0.16454\n",
      "Epoch [734/10044], Batch [3/7], Loss: 0.0153, Accuracy: 99.75%, Grad Norm: 0.10327\n",
      "Epoch [734/10044], Batch [4/7], Loss: 0.0162, Accuracy: 99.67%, Grad Norm: 0.08939\n",
      "Epoch [734/10044], Batch [5/7], Loss: 0.0150, Accuracy: 99.72%, Grad Norm: 0.11001\n",
      "Epoch [734/10044], Batch [6/7], Loss: 0.0186, Accuracy: 99.60%, Grad Norm: 0.14487\n",
      "Epoch [734/10044], Batch [7/7], Loss: 0.0131, Accuracy: 99.77%, Grad Norm: 0.14938\n",
      "Epoch [734/10044], Loss: 0.0131\n",
      "Epoch [735/10044], Batch [1/7], Loss: 0.0171, Accuracy: 99.71%, Grad Norm: 0.12246\n",
      "Epoch [735/10044], Batch [2/7], Loss: 0.0188, Accuracy: 99.61%, Grad Norm: 0.13460\n",
      "Epoch [735/10044], Batch [3/7], Loss: 0.0147, Accuracy: 99.67%, Grad Norm: 0.08764\n",
      "Epoch [735/10044], Batch [4/7], Loss: 0.0166, Accuracy: 99.70%, Grad Norm: 0.09426\n",
      "Epoch [735/10044], Batch [5/7], Loss: 0.0147, Accuracy: 99.75%, Grad Norm: 0.11325\n",
      "Epoch [735/10044], Batch [6/7], Loss: 0.0197, Accuracy: 99.55%, Grad Norm: 0.15572\n",
      "Epoch [735/10044], Batch [7/7], Loss: 0.0156, Accuracy: 99.75%, Grad Norm: 0.17291\n",
      "Epoch [735/10044], Loss: 0.0156\n",
      "Epoch [736/10044], Batch [1/7], Loss: 0.0150, Accuracy: 99.75%, Grad Norm: 0.10625\n",
      "Epoch [736/10044], Batch [2/7], Loss: 0.0179, Accuracy: 99.55%, Grad Norm: 0.12826\n",
      "Epoch [736/10044], Batch [3/7], Loss: 0.0159, Accuracy: 99.71%, Grad Norm: 0.11416\n",
      "Epoch [736/10044], Batch [4/7], Loss: 0.0179, Accuracy: 99.67%, Grad Norm: 0.10738\n",
      "Epoch [736/10044], Batch [5/7], Loss: 0.0179, Accuracy: 99.64%, Grad Norm: 0.14335\n",
      "Epoch [736/10044], Batch [6/7], Loss: 0.0196, Accuracy: 99.57%, Grad Norm: 0.14628\n",
      "Epoch [736/10044], Batch [7/7], Loss: 0.0178, Accuracy: 99.53%, Grad Norm: 0.20713\n",
      "Epoch [736/10044], Loss: 0.0178\n",
      "Epoch [737/10044], Batch [1/7], Loss: 0.0158, Accuracy: 99.72%, Grad Norm: 0.09931\n",
      "Epoch [737/10044], Batch [2/7], Loss: 0.0179, Accuracy: 99.59%, Grad Norm: 0.14345\n",
      "Epoch [737/10044], Batch [3/7], Loss: 0.0146, Accuracy: 99.82%, Grad Norm: 0.08676\n",
      "Epoch [737/10044], Batch [4/7], Loss: 0.0182, Accuracy: 99.65%, Grad Norm: 0.10975\n",
      "Epoch [737/10044], Batch [5/7], Loss: 0.0175, Accuracy: 99.62%, Grad Norm: 0.13632\n",
      "Epoch [737/10044], Batch [6/7], Loss: 0.0207, Accuracy: 99.56%, Grad Norm: 0.14829\n",
      "Epoch [737/10044], Batch [7/7], Loss: 0.0143, Accuracy: 99.75%, Grad Norm: 0.16424\n",
      "Epoch [737/10044], Loss: 0.0143\n",
      "Epoch [738/10044], Batch [1/7], Loss: 0.0148, Accuracy: 99.77%, Grad Norm: 0.11557\n",
      "Epoch [738/10044], Batch [2/7], Loss: 0.0171, Accuracy: 99.67%, Grad Norm: 0.14740\n",
      "Epoch [738/10044], Batch [3/7], Loss: 0.0149, Accuracy: 99.82%, Grad Norm: 0.09685\n",
      "Epoch [738/10044], Batch [4/7], Loss: 0.0167, Accuracy: 99.66%, Grad Norm: 0.10144\n",
      "Epoch [738/10044], Batch [5/7], Loss: 0.0152, Accuracy: 99.75%, Grad Norm: 0.10817\n",
      "Epoch [738/10044], Batch [6/7], Loss: 0.0179, Accuracy: 99.67%, Grad Norm: 0.12209\n",
      "Epoch [738/10044], Batch [7/7], Loss: 0.0147, Accuracy: 99.77%, Grad Norm: 0.15288\n",
      "Epoch [738/10044], Loss: 0.0147\n",
      "Epoch [739/10044], Batch [1/7], Loss: 0.0155, Accuracy: 99.77%, Grad Norm: 0.10555\n",
      "Epoch [739/10044], Batch [2/7], Loss: 0.0198, Accuracy: 99.55%, Grad Norm: 0.15403\n",
      "Epoch [739/10044], Batch [3/7], Loss: 0.0142, Accuracy: 99.81%, Grad Norm: 0.09100\n",
      "Epoch [739/10044], Batch [4/7], Loss: 0.0153, Accuracy: 99.70%, Grad Norm: 0.08999\n",
      "Epoch [739/10044], Batch [5/7], Loss: 0.0160, Accuracy: 99.71%, Grad Norm: 0.11538\n",
      "Epoch [739/10044], Batch [6/7], Loss: 0.0185, Accuracy: 99.60%, Grad Norm: 0.14112\n",
      "Epoch [739/10044], Batch [7/7], Loss: 0.0138, Accuracy: 99.67%, Grad Norm: 0.13386\n",
      "Epoch [739/10044], Loss: 0.0138\n",
      "Epoch [740/10044], Batch [1/7], Loss: 0.0155, Accuracy: 99.74%, Grad Norm: 0.10755\n",
      "Epoch [740/10044], Batch [2/7], Loss: 0.0175, Accuracy: 99.65%, Grad Norm: 0.12603\n",
      "Epoch [740/10044], Batch [3/7], Loss: 0.0152, Accuracy: 99.79%, Grad Norm: 0.09702\n",
      "Epoch [740/10044], Batch [4/7], Loss: 0.0165, Accuracy: 99.76%, Grad Norm: 0.09040\n",
      "Epoch [740/10044], Batch [5/7], Loss: 0.0143, Accuracy: 99.75%, Grad Norm: 0.10005\n",
      "Epoch [740/10044], Batch [6/7], Loss: 0.0168, Accuracy: 99.62%, Grad Norm: 0.12784\n",
      "Epoch [740/10044], Batch [7/7], Loss: 0.0132, Accuracy: 99.72%, Grad Norm: 0.16226\n",
      "Epoch [740/10044], Loss: 0.0132\n",
      "Epoch [741/10044], Batch [1/7], Loss: 0.0151, Accuracy: 99.71%, Grad Norm: 0.10217\n",
      "Epoch [741/10044], Batch [2/7], Loss: 0.0162, Accuracy: 99.64%, Grad Norm: 0.12878\n",
      "Epoch [741/10044], Batch [3/7], Loss: 0.0144, Accuracy: 99.77%, Grad Norm: 0.08811\n",
      "Epoch [741/10044], Batch [4/7], Loss: 0.0154, Accuracy: 99.74%, Grad Norm: 0.07837\n",
      "Epoch [741/10044], Batch [5/7], Loss: 0.0137, Accuracy: 99.76%, Grad Norm: 0.09389\n",
      "Epoch [741/10044], Batch [6/7], Loss: 0.0173, Accuracy: 99.61%, Grad Norm: 0.13930\n",
      "Epoch [741/10044], Batch [7/7], Loss: 0.0139, Accuracy: 99.73%, Grad Norm: 0.15242\n",
      "Epoch [741/10044], Loss: 0.0139\n",
      "Epoch [742/10044], Batch [1/7], Loss: 0.0156, Accuracy: 99.77%, Grad Norm: 0.10617\n",
      "Epoch [742/10044], Batch [2/7], Loss: 0.0172, Accuracy: 99.53%, Grad Norm: 0.14149\n",
      "Epoch [742/10044], Batch [3/7], Loss: 0.0135, Accuracy: 99.84%, Grad Norm: 0.07762\n",
      "Epoch [742/10044], Batch [4/7], Loss: 0.0184, Accuracy: 99.62%, Grad Norm: 0.09553\n",
      "Epoch [742/10044], Batch [5/7], Loss: 0.0171, Accuracy: 99.68%, Grad Norm: 0.15004\n",
      "Epoch [742/10044], Batch [6/7], Loss: 0.0191, Accuracy: 99.60%, Grad Norm: 0.15174\n",
      "Epoch [742/10044], Batch [7/7], Loss: 0.0144, Accuracy: 99.72%, Grad Norm: 0.14121\n",
      "Epoch [742/10044], Loss: 0.0144\n",
      "Epoch [743/10044], Batch [1/7], Loss: 0.0158, Accuracy: 99.68%, Grad Norm: 0.11032\n",
      "Epoch [743/10044], Batch [2/7], Loss: 0.0162, Accuracy: 99.62%, Grad Norm: 0.10456\n",
      "Epoch [743/10044], Batch [3/7], Loss: 0.0140, Accuracy: 99.82%, Grad Norm: 0.09433\n",
      "Epoch [743/10044], Batch [4/7], Loss: 0.0166, Accuracy: 99.65%, Grad Norm: 0.11320\n",
      "Epoch [743/10044], Batch [5/7], Loss: 0.0161, Accuracy: 99.72%, Grad Norm: 0.12929\n",
      "Epoch [743/10044], Batch [6/7], Loss: 0.0192, Accuracy: 99.57%, Grad Norm: 0.13887\n",
      "Epoch [743/10044], Batch [7/7], Loss: 0.0157, Accuracy: 99.62%, Grad Norm: 0.18704\n",
      "Epoch [743/10044], Loss: 0.0157\n",
      "Epoch [744/10044], Batch [1/7], Loss: 0.0156, Accuracy: 99.77%, Grad Norm: 0.10260\n",
      "Epoch [744/10044], Batch [2/7], Loss: 0.0188, Accuracy: 99.52%, Grad Norm: 0.15220\n",
      "Epoch [744/10044], Batch [3/7], Loss: 0.0153, Accuracy: 99.72%, Grad Norm: 0.10053\n",
      "Epoch [744/10044], Batch [4/7], Loss: 0.0165, Accuracy: 99.67%, Grad Norm: 0.11015\n",
      "Epoch [744/10044], Batch [5/7], Loss: 0.0150, Accuracy: 99.71%, Grad Norm: 0.11127\n",
      "Epoch [744/10044], Batch [6/7], Loss: 0.0171, Accuracy: 99.67%, Grad Norm: 0.11121\n",
      "Epoch [744/10044], Batch [7/7], Loss: 0.0154, Accuracy: 99.58%, Grad Norm: 0.17783\n",
      "Epoch [744/10044], Loss: 0.0154\n",
      "Epoch [745/10044], Batch [1/7], Loss: 0.0176, Accuracy: 99.63%, Grad Norm: 0.12726\n",
      "Epoch [745/10044], Batch [2/7], Loss: 0.0199, Accuracy: 99.57%, Grad Norm: 0.15090\n",
      "Epoch [745/10044], Batch [3/7], Loss: 0.0133, Accuracy: 99.81%, Grad Norm: 0.08316\n",
      "Epoch [745/10044], Batch [4/7], Loss: 0.0161, Accuracy: 99.75%, Grad Norm: 0.09699\n",
      "Epoch [745/10044], Batch [5/7], Loss: 0.0154, Accuracy: 99.75%, Grad Norm: 0.10594\n",
      "Epoch [745/10044], Batch [6/7], Loss: 0.0180, Accuracy: 99.57%, Grad Norm: 0.12508\n",
      "Epoch [745/10044], Batch [7/7], Loss: 0.0144, Accuracy: 99.68%, Grad Norm: 0.18844\n",
      "Epoch [745/10044], Loss: 0.0144\n",
      "Epoch [746/10044], Batch [1/7], Loss: 0.0158, Accuracy: 99.67%, Grad Norm: 0.11243\n",
      "Epoch [746/10044], Batch [2/7], Loss: 0.0187, Accuracy: 99.56%, Grad Norm: 0.12561\n",
      "Epoch [746/10044], Batch [3/7], Loss: 0.0157, Accuracy: 99.75%, Grad Norm: 0.09734\n",
      "Epoch [746/10044], Batch [4/7], Loss: 0.0163, Accuracy: 99.66%, Grad Norm: 0.10724\n",
      "Epoch [746/10044], Batch [5/7], Loss: 0.0167, Accuracy: 99.65%, Grad Norm: 0.14126\n",
      "Epoch [746/10044], Batch [6/7], Loss: 0.0183, Accuracy: 99.58%, Grad Norm: 0.12602\n",
      "Epoch [746/10044], Batch [7/7], Loss: 0.0133, Accuracy: 99.72%, Grad Norm: 0.14665\n",
      "Epoch [746/10044], Loss: 0.0133\n",
      "Epoch [747/10044], Batch [1/7], Loss: 0.0163, Accuracy: 99.72%, Grad Norm: 0.12026\n",
      "Epoch [747/10044], Batch [2/7], Loss: 0.0168, Accuracy: 99.67%, Grad Norm: 0.13559\n",
      "Epoch [747/10044], Batch [3/7], Loss: 0.0133, Accuracy: 99.82%, Grad Norm: 0.07728\n",
      "Epoch [747/10044], Batch [4/7], Loss: 0.0159, Accuracy: 99.71%, Grad Norm: 0.11352\n",
      "Epoch [747/10044], Batch [5/7], Loss: 0.0149, Accuracy: 99.77%, Grad Norm: 0.13517\n",
      "Epoch [747/10044], Batch [6/7], Loss: 0.0182, Accuracy: 99.52%, Grad Norm: 0.15355\n",
      "Epoch [747/10044], Batch [7/7], Loss: 0.0120, Accuracy: 99.78%, Grad Norm: 0.12069\n",
      "Epoch [747/10044], Loss: 0.0120\n",
      "Epoch [748/10044], Batch [1/7], Loss: 0.0159, Accuracy: 99.67%, Grad Norm: 0.11774\n",
      "Epoch [748/10044], Batch [2/7], Loss: 0.0182, Accuracy: 99.57%, Grad Norm: 0.14453\n",
      "Epoch [748/10044], Batch [3/7], Loss: 0.0150, Accuracy: 99.76%, Grad Norm: 0.09149\n",
      "Epoch [748/10044], Batch [4/7], Loss: 0.0161, Accuracy: 99.75%, Grad Norm: 0.10077\n",
      "Epoch [748/10044], Batch [5/7], Loss: 0.0164, Accuracy: 99.71%, Grad Norm: 0.12309\n",
      "Epoch [748/10044], Batch [6/7], Loss: 0.0171, Accuracy: 99.58%, Grad Norm: 0.12455\n",
      "Epoch [748/10044], Batch [7/7], Loss: 0.0123, Accuracy: 99.73%, Grad Norm: 0.13697\n",
      "Epoch [748/10044], Loss: 0.0123\n",
      "Epoch [749/10044], Batch [1/7], Loss: 0.0153, Accuracy: 99.77%, Grad Norm: 0.12070\n",
      "Epoch [749/10044], Batch [2/7], Loss: 0.0194, Accuracy: 99.56%, Grad Norm: 0.15206\n",
      "Epoch [749/10044], Batch [3/7], Loss: 0.0135, Accuracy: 99.83%, Grad Norm: 0.08045\n",
      "Epoch [749/10044], Batch [4/7], Loss: 0.0167, Accuracy: 99.72%, Grad Norm: 0.10539\n",
      "Epoch [749/10044], Batch [5/7], Loss: 0.0166, Accuracy: 99.70%, Grad Norm: 0.11593\n",
      "Epoch [749/10044], Batch [6/7], Loss: 0.0165, Accuracy: 99.63%, Grad Norm: 0.12826\n",
      "Epoch [749/10044], Batch [7/7], Loss: 0.0119, Accuracy: 99.77%, Grad Norm: 0.11042\n",
      "Epoch [749/10044], Loss: 0.0119\n",
      "Epoch [750/10044], Batch [1/7], Loss: 0.0149, Accuracy: 99.77%, Grad Norm: 0.10818\n",
      "Epoch [750/10044], Batch [2/7], Loss: 0.0166, Accuracy: 99.65%, Grad Norm: 0.11921\n",
      "Epoch [750/10044], Batch [3/7], Loss: 0.0135, Accuracy: 99.81%, Grad Norm: 0.07842\n",
      "Epoch [750/10044], Batch [4/7], Loss: 0.0160, Accuracy: 99.71%, Grad Norm: 0.10060\n",
      "Epoch [750/10044], Batch [5/7], Loss: 0.0163, Accuracy: 99.64%, Grad Norm: 0.13077\n",
      "Epoch [750/10044], Batch [6/7], Loss: 0.0168, Accuracy: 99.66%, Grad Norm: 0.13405\n",
      "Epoch [750/10044], Batch [7/7], Loss: 0.0131, Accuracy: 99.68%, Grad Norm: 0.14159\n",
      "Epoch [750/10044], Loss: 0.0131\n",
      "Epoch [751/10044], Batch [1/7], Loss: 0.0147, Accuracy: 99.78%, Grad Norm: 0.09317\n",
      "Epoch [751/10044], Batch [2/7], Loss: 0.0173, Accuracy: 99.66%, Grad Norm: 0.13567\n",
      "Epoch [751/10044], Batch [3/7], Loss: 0.0130, Accuracy: 99.82%, Grad Norm: 0.08453\n",
      "Epoch [751/10044], Batch [4/7], Loss: 0.0158, Accuracy: 99.71%, Grad Norm: 0.10105\n",
      "Epoch [751/10044], Batch [5/7], Loss: 0.0141, Accuracy: 99.75%, Grad Norm: 0.11326\n",
      "Epoch [751/10044], Batch [6/7], Loss: 0.0160, Accuracy: 99.69%, Grad Norm: 0.12531\n",
      "Epoch [751/10044], Batch [7/7], Loss: 0.0167, Accuracy: 99.55%, Grad Norm: 0.19715\n",
      "Epoch [751/10044], Loss: 0.0167\n",
      "Epoch [752/10044], Batch [1/7], Loss: 0.0151, Accuracy: 99.74%, Grad Norm: 0.12077\n",
      "Epoch [752/10044], Batch [2/7], Loss: 0.0181, Accuracy: 99.58%, Grad Norm: 0.16104\n",
      "Epoch [752/10044], Batch [3/7], Loss: 0.0143, Accuracy: 99.77%, Grad Norm: 0.10401\n",
      "Epoch [752/10044], Batch [4/7], Loss: 0.0172, Accuracy: 99.62%, Grad Norm: 0.11690\n",
      "Epoch [752/10044], Batch [5/7], Loss: 0.0143, Accuracy: 99.72%, Grad Norm: 0.09939\n",
      "Epoch [752/10044], Batch [6/7], Loss: 0.0177, Accuracy: 99.58%, Grad Norm: 0.13041\n",
      "Epoch [752/10044], Batch [7/7], Loss: 0.0120, Accuracy: 99.78%, Grad Norm: 0.11012\n",
      "Epoch [752/10044], Loss: 0.0120\n",
      "Epoch [753/10044], Batch [1/7], Loss: 0.0151, Accuracy: 99.71%, Grad Norm: 0.11646\n",
      "Epoch [753/10044], Batch [2/7], Loss: 0.0170, Accuracy: 99.62%, Grad Norm: 0.13435\n",
      "Epoch [753/10044], Batch [3/7], Loss: 0.0140, Accuracy: 99.79%, Grad Norm: 0.08975\n",
      "Epoch [753/10044], Batch [4/7], Loss: 0.0148, Accuracy: 99.77%, Grad Norm: 0.09053\n",
      "Epoch [753/10044], Batch [5/7], Loss: 0.0146, Accuracy: 99.73%, Grad Norm: 0.10834\n",
      "Epoch [753/10044], Batch [6/7], Loss: 0.0177, Accuracy: 99.62%, Grad Norm: 0.11863\n",
      "Epoch [753/10044], Batch [7/7], Loss: 0.0135, Accuracy: 99.63%, Grad Norm: 0.12911\n",
      "Epoch [753/10044], Loss: 0.0135\n",
      "Epoch [754/10044], Batch [1/7], Loss: 0.0162, Accuracy: 99.67%, Grad Norm: 0.13259\n",
      "Epoch [754/10044], Batch [2/7], Loss: 0.0166, Accuracy: 99.69%, Grad Norm: 0.12430\n",
      "Epoch [754/10044], Batch [3/7], Loss: 0.0142, Accuracy: 99.82%, Grad Norm: 0.07913\n",
      "Epoch [754/10044], Batch [4/7], Loss: 0.0155, Accuracy: 99.70%, Grad Norm: 0.09000\n",
      "Epoch [754/10044], Batch [5/7], Loss: 0.0151, Accuracy: 99.75%, Grad Norm: 0.10299\n",
      "Epoch [754/10044], Batch [6/7], Loss: 0.0183, Accuracy: 99.59%, Grad Norm: 0.12744\n",
      "Epoch [754/10044], Batch [7/7], Loss: 0.0130, Accuracy: 99.75%, Grad Norm: 0.13870\n",
      "Epoch [754/10044], Loss: 0.0130\n",
      "Epoch [755/10044], Batch [1/7], Loss: 0.0154, Accuracy: 99.73%, Grad Norm: 0.11157\n",
      "Epoch [755/10044], Batch [2/7], Loss: 0.0156, Accuracy: 99.70%, Grad Norm: 0.10840\n",
      "Epoch [755/10044], Batch [3/7], Loss: 0.0118, Accuracy: 99.85%, Grad Norm: 0.06894\n",
      "Epoch [755/10044], Batch [4/7], Loss: 0.0155, Accuracy: 99.74%, Grad Norm: 0.08871\n",
      "Epoch [755/10044], Batch [5/7], Loss: 0.0148, Accuracy: 99.72%, Grad Norm: 0.12210\n",
      "Epoch [755/10044], Batch [6/7], Loss: 0.0168, Accuracy: 99.66%, Grad Norm: 0.11518\n",
      "Epoch [755/10044], Batch [7/7], Loss: 0.0165, Accuracy: 99.68%, Grad Norm: 0.18185\n",
      "Epoch [755/10044], Loss: 0.0165\n",
      "Epoch [756/10044], Batch [1/7], Loss: 0.0149, Accuracy: 99.74%, Grad Norm: 0.10877\n",
      "Epoch [756/10044], Batch [2/7], Loss: 0.0156, Accuracy: 99.66%, Grad Norm: 0.11164\n",
      "Epoch [756/10044], Batch [3/7], Loss: 0.0123, Accuracy: 99.84%, Grad Norm: 0.07414\n",
      "Epoch [756/10044], Batch [4/7], Loss: 0.0174, Accuracy: 99.59%, Grad Norm: 0.11054\n",
      "Epoch [756/10044], Batch [5/7], Loss: 0.0151, Accuracy: 99.68%, Grad Norm: 0.12380\n",
      "Epoch [756/10044], Batch [6/7], Loss: 0.0173, Accuracy: 99.62%, Grad Norm: 0.12886\n",
      "Epoch [756/10044], Batch [7/7], Loss: 0.0138, Accuracy: 99.70%, Grad Norm: 0.16357\n",
      "Epoch [756/10044], Loss: 0.0138\n",
      "Epoch [757/10044], Batch [1/7], Loss: 0.0156, Accuracy: 99.66%, Grad Norm: 0.11868\n",
      "Epoch [757/10044], Batch [2/7], Loss: 0.0177, Accuracy: 99.62%, Grad Norm: 0.13888\n",
      "Epoch [757/10044], Batch [3/7], Loss: 0.0138, Accuracy: 99.82%, Grad Norm: 0.09224\n",
      "Epoch [757/10044], Batch [4/7], Loss: 0.0157, Accuracy: 99.71%, Grad Norm: 0.09106\n",
      "Epoch [757/10044], Batch [5/7], Loss: 0.0154, Accuracy: 99.66%, Grad Norm: 0.13879\n",
      "Epoch [757/10044], Batch [6/7], Loss: 0.0169, Accuracy: 99.62%, Grad Norm: 0.13162\n",
      "Epoch [757/10044], Batch [7/7], Loss: 0.0146, Accuracy: 99.70%, Grad Norm: 0.16450\n",
      "Epoch [757/10044], Loss: 0.0146\n",
      "Epoch [758/10044], Batch [1/7], Loss: 0.0147, Accuracy: 99.73%, Grad Norm: 0.11243\n",
      "Epoch [758/10044], Batch [2/7], Loss: 0.0171, Accuracy: 99.65%, Grad Norm: 0.13033\n",
      "Epoch [758/10044], Batch [3/7], Loss: 0.0132, Accuracy: 99.77%, Grad Norm: 0.09496\n",
      "Epoch [758/10044], Batch [4/7], Loss: 0.0166, Accuracy: 99.61%, Grad Norm: 0.09724\n",
      "Epoch [758/10044], Batch [5/7], Loss: 0.0173, Accuracy: 99.64%, Grad Norm: 0.14767\n",
      "Epoch [758/10044], Batch [6/7], Loss: 0.0145, Accuracy: 99.72%, Grad Norm: 0.09750\n",
      "Epoch [758/10044], Batch [7/7], Loss: 0.0149, Accuracy: 99.68%, Grad Norm: 0.16015\n",
      "Epoch [758/10044], Loss: 0.0149\n",
      "Epoch [759/10044], Batch [1/7], Loss: 0.0142, Accuracy: 99.69%, Grad Norm: 0.10236\n",
      "Epoch [759/10044], Batch [2/7], Loss: 0.0178, Accuracy: 99.58%, Grad Norm: 0.12001\n",
      "Epoch [759/10044], Batch [3/7], Loss: 0.0127, Accuracy: 99.83%, Grad Norm: 0.09497\n",
      "Epoch [759/10044], Batch [4/7], Loss: 0.0153, Accuracy: 99.68%, Grad Norm: 0.09648\n",
      "Epoch [759/10044], Batch [5/7], Loss: 0.0132, Accuracy: 99.78%, Grad Norm: 0.09336\n",
      "Epoch [759/10044], Batch [6/7], Loss: 0.0160, Accuracy: 99.64%, Grad Norm: 0.10996\n",
      "Epoch [759/10044], Batch [7/7], Loss: 0.0135, Accuracy: 99.60%, Grad Norm: 0.14492\n",
      "Epoch [759/10044], Loss: 0.0135\n",
      "Epoch [760/10044], Batch [1/7], Loss: 0.0145, Accuracy: 99.74%, Grad Norm: 0.11404\n",
      "Epoch [760/10044], Batch [2/7], Loss: 0.0169, Accuracy: 99.64%, Grad Norm: 0.13048\n",
      "Epoch [760/10044], Batch [3/7], Loss: 0.0121, Accuracy: 99.82%, Grad Norm: 0.07750\n",
      "Epoch [760/10044], Batch [4/7], Loss: 0.0145, Accuracy: 99.72%, Grad Norm: 0.08670\n",
      "Epoch [760/10044], Batch [5/7], Loss: 0.0142, Accuracy: 99.76%, Grad Norm: 0.09944\n",
      "Epoch [760/10044], Batch [6/7], Loss: 0.0156, Accuracy: 99.68%, Grad Norm: 0.10762\n",
      "Epoch [760/10044], Batch [7/7], Loss: 0.0112, Accuracy: 99.77%, Grad Norm: 0.11858\n",
      "Epoch [760/10044], Loss: 0.0112\n",
      "Epoch [761/10044], Batch [1/7], Loss: 0.0161, Accuracy: 99.70%, Grad Norm: 0.13607\n",
      "Epoch [761/10044], Batch [2/7], Loss: 0.0155, Accuracy: 99.65%, Grad Norm: 0.13105\n",
      "Epoch [761/10044], Batch [3/7], Loss: 0.0121, Accuracy: 99.85%, Grad Norm: 0.07440\n",
      "Epoch [761/10044], Batch [4/7], Loss: 0.0149, Accuracy: 99.72%, Grad Norm: 0.09398\n",
      "Epoch [761/10044], Batch [5/7], Loss: 0.0144, Accuracy: 99.72%, Grad Norm: 0.10858\n",
      "Epoch [761/10044], Batch [6/7], Loss: 0.0140, Accuracy: 99.79%, Grad Norm: 0.10453\n",
      "Epoch [761/10044], Batch [7/7], Loss: 0.0142, Accuracy: 99.65%, Grad Norm: 0.13613\n",
      "Epoch [761/10044], Loss: 0.0142\n",
      "Epoch [762/10044], Batch [1/7], Loss: 0.0153, Accuracy: 99.74%, Grad Norm: 0.12092\n",
      "Epoch [762/10044], Batch [2/7], Loss: 0.0165, Accuracy: 99.65%, Grad Norm: 0.10951\n",
      "Epoch [762/10044], Batch [3/7], Loss: 0.0142, Accuracy: 99.77%, Grad Norm: 0.09053\n",
      "Epoch [762/10044], Batch [4/7], Loss: 0.0142, Accuracy: 99.72%, Grad Norm: 0.08481\n",
      "Epoch [762/10044], Batch [5/7], Loss: 0.0142, Accuracy: 99.76%, Grad Norm: 0.10956\n",
      "Epoch [762/10044], Batch [6/7], Loss: 0.0172, Accuracy: 99.60%, Grad Norm: 0.12225\n",
      "Epoch [762/10044], Batch [7/7], Loss: 0.0115, Accuracy: 99.78%, Grad Norm: 0.12057\n",
      "Epoch [762/10044], Loss: 0.0115\n",
      "Epoch [763/10044], Batch [1/7], Loss: 0.0162, Accuracy: 99.70%, Grad Norm: 0.13837\n",
      "Epoch [763/10044], Batch [2/7], Loss: 0.0181, Accuracy: 99.59%, Grad Norm: 0.12523\n",
      "Epoch [763/10044], Batch [3/7], Loss: 0.0123, Accuracy: 99.80%, Grad Norm: 0.07532\n",
      "Epoch [763/10044], Batch [4/7], Loss: 0.0155, Accuracy: 99.62%, Grad Norm: 0.09969\n",
      "Epoch [763/10044], Batch [5/7], Loss: 0.0148, Accuracy: 99.72%, Grad Norm: 0.10305\n",
      "Epoch [763/10044], Batch [6/7], Loss: 0.0145, Accuracy: 99.72%, Grad Norm: 0.11620\n",
      "Epoch [763/10044], Batch [7/7], Loss: 0.0129, Accuracy: 99.68%, Grad Norm: 0.14157\n",
      "Epoch [763/10044], Loss: 0.0129\n",
      "Epoch [764/10044], Batch [1/7], Loss: 0.0163, Accuracy: 99.71%, Grad Norm: 0.13131\n",
      "Epoch [764/10044], Batch [2/7], Loss: 0.0157, Accuracy: 99.65%, Grad Norm: 0.11466\n",
      "Epoch [764/10044], Batch [3/7], Loss: 0.0133, Accuracy: 99.81%, Grad Norm: 0.08471\n",
      "Epoch [764/10044], Batch [4/7], Loss: 0.0133, Accuracy: 99.78%, Grad Norm: 0.08662\n",
      "Epoch [764/10044], Batch [5/7], Loss: 0.0139, Accuracy: 99.78%, Grad Norm: 0.08817\n",
      "Epoch [764/10044], Batch [6/7], Loss: 0.0152, Accuracy: 99.59%, Grad Norm: 0.10807\n",
      "Epoch [764/10044], Batch [7/7], Loss: 0.0129, Accuracy: 99.72%, Grad Norm: 0.13439\n",
      "Epoch [764/10044], Loss: 0.0129\n",
      "Epoch [765/10044], Batch [1/7], Loss: 0.0143, Accuracy: 99.74%, Grad Norm: 0.10619\n",
      "Epoch [765/10044], Batch [2/7], Loss: 0.0163, Accuracy: 99.64%, Grad Norm: 0.14345\n",
      "Epoch [765/10044], Batch [3/7], Loss: 0.0123, Accuracy: 99.80%, Grad Norm: 0.08655\n",
      "Epoch [765/10044], Batch [4/7], Loss: 0.0129, Accuracy: 99.82%, Grad Norm: 0.07091\n",
      "Epoch [765/10044], Batch [5/7], Loss: 0.0127, Accuracy: 99.79%, Grad Norm: 0.09368\n",
      "Epoch [765/10044], Batch [6/7], Loss: 0.0152, Accuracy: 99.66%, Grad Norm: 0.12596\n",
      "Epoch [765/10044], Batch [7/7], Loss: 0.0097, Accuracy: 99.83%, Grad Norm: 0.09167\n",
      "Epoch [765/10044], Loss: 0.0097\n",
      "Epoch [766/10044], Batch [1/7], Loss: 0.0136, Accuracy: 99.74%, Grad Norm: 0.09770\n",
      "Epoch [766/10044], Batch [2/7], Loss: 0.0172, Accuracy: 99.62%, Grad Norm: 0.12267\n",
      "Epoch [766/10044], Batch [3/7], Loss: 0.0135, Accuracy: 99.77%, Grad Norm: 0.08919\n",
      "Epoch [766/10044], Batch [4/7], Loss: 0.0167, Accuracy: 99.65%, Grad Norm: 0.10273\n",
      "Epoch [766/10044], Batch [5/7], Loss: 0.0137, Accuracy: 99.76%, Grad Norm: 0.10523\n",
      "Epoch [766/10044], Batch [6/7], Loss: 0.0155, Accuracy: 99.70%, Grad Norm: 0.10614\n",
      "Epoch [766/10044], Batch [7/7], Loss: 0.0119, Accuracy: 99.75%, Grad Norm: 0.13057\n",
      "Epoch [766/10044], Loss: 0.0119\n",
      "Epoch [767/10044], Batch [1/7], Loss: 0.0165, Accuracy: 99.66%, Grad Norm: 0.11176\n",
      "Epoch [767/10044], Batch [2/7], Loss: 0.0145, Accuracy: 99.77%, Grad Norm: 0.09967\n",
      "Epoch [767/10044], Batch [3/7], Loss: 0.0115, Accuracy: 99.83%, Grad Norm: 0.07159\n",
      "Epoch [767/10044], Batch [4/7], Loss: 0.0148, Accuracy: 99.75%, Grad Norm: 0.09295\n",
      "Epoch [767/10044], Batch [5/7], Loss: 0.0132, Accuracy: 99.77%, Grad Norm: 0.09133\n",
      "Epoch [767/10044], Batch [6/7], Loss: 0.0142, Accuracy: 99.73%, Grad Norm: 0.09941\n",
      "Epoch [767/10044], Batch [7/7], Loss: 0.0108, Accuracy: 99.80%, Grad Norm: 0.12534\n",
      "Epoch [767/10044], Loss: 0.0108\n",
      "Epoch [768/10044], Batch [1/7], Loss: 0.0148, Accuracy: 99.73%, Grad Norm: 0.11036\n",
      "Epoch [768/10044], Batch [2/7], Loss: 0.0159, Accuracy: 99.59%, Grad Norm: 0.13181\n",
      "Epoch [768/10044], Batch [3/7], Loss: 0.0115, Accuracy: 99.87%, Grad Norm: 0.07059\n",
      "Epoch [768/10044], Batch [4/7], Loss: 0.0135, Accuracy: 99.76%, Grad Norm: 0.08128\n",
      "Epoch [768/10044], Batch [5/7], Loss: 0.0136, Accuracy: 99.73%, Grad Norm: 0.11044\n",
      "Epoch [768/10044], Batch [6/7], Loss: 0.0164, Accuracy: 99.65%, Grad Norm: 0.12332\n",
      "Epoch [768/10044], Batch [7/7], Loss: 0.0125, Accuracy: 99.72%, Grad Norm: 0.13515\n",
      "Epoch [768/10044], Loss: 0.0125\n",
      "Epoch [769/10044], Batch [1/7], Loss: 0.0137, Accuracy: 99.80%, Grad Norm: 0.08759\n",
      "Epoch [769/10044], Batch [2/7], Loss: 0.0152, Accuracy: 99.70%, Grad Norm: 0.10554\n",
      "Epoch [769/10044], Batch [3/7], Loss: 0.0129, Accuracy: 99.77%, Grad Norm: 0.08159\n",
      "Epoch [769/10044], Batch [4/7], Loss: 0.0143, Accuracy: 99.75%, Grad Norm: 0.08905\n",
      "Epoch [769/10044], Batch [5/7], Loss: 0.0144, Accuracy: 99.72%, Grad Norm: 0.12142\n",
      "Epoch [769/10044], Batch [6/7], Loss: 0.0156, Accuracy: 99.63%, Grad Norm: 0.12070\n",
      "Epoch [769/10044], Batch [7/7], Loss: 0.0126, Accuracy: 99.70%, Grad Norm: 0.14873\n",
      "Epoch [769/10044], Loss: 0.0126\n",
      "Epoch [770/10044], Batch [1/7], Loss: 0.0141, Accuracy: 99.79%, Grad Norm: 0.11055\n",
      "Epoch [770/10044], Batch [2/7], Loss: 0.0151, Accuracy: 99.70%, Grad Norm: 0.11556\n",
      "Epoch [770/10044], Batch [3/7], Loss: 0.0127, Accuracy: 99.81%, Grad Norm: 0.08741\n",
      "Epoch [770/10044], Batch [4/7], Loss: 0.0150, Accuracy: 99.69%, Grad Norm: 0.09602\n",
      "Epoch [770/10044], Batch [5/7], Loss: 0.0135, Accuracy: 99.75%, Grad Norm: 0.09813\n",
      "Epoch [770/10044], Batch [6/7], Loss: 0.0161, Accuracy: 99.62%, Grad Norm: 0.12439\n",
      "Epoch [770/10044], Batch [7/7], Loss: 0.0105, Accuracy: 99.82%, Grad Norm: 0.11381\n",
      "Epoch [770/10044], Loss: 0.0105\n",
      "Epoch [771/10044], Batch [1/7], Loss: 0.0147, Accuracy: 99.70%, Grad Norm: 0.10389\n",
      "Epoch [771/10044], Batch [2/7], Loss: 0.0162, Accuracy: 99.63%, Grad Norm: 0.12779\n",
      "Epoch [771/10044], Batch [3/7], Loss: 0.0137, Accuracy: 99.77%, Grad Norm: 0.09861\n",
      "Epoch [771/10044], Batch [4/7], Loss: 0.0147, Accuracy: 99.74%, Grad Norm: 0.09857\n",
      "Epoch [771/10044], Batch [5/7], Loss: 0.0136, Accuracy: 99.75%, Grad Norm: 0.10189\n",
      "Epoch [771/10044], Batch [6/7], Loss: 0.0159, Accuracy: 99.64%, Grad Norm: 0.15291\n",
      "Epoch [771/10044], Batch [7/7], Loss: 0.0121, Accuracy: 99.73%, Grad Norm: 0.13295\n",
      "Epoch [771/10044], Loss: 0.0121\n",
      "Epoch [772/10044], Batch [1/7], Loss: 0.0134, Accuracy: 99.73%, Grad Norm: 0.09505\n",
      "Epoch [772/10044], Batch [2/7], Loss: 0.0150, Accuracy: 99.65%, Grad Norm: 0.11834\n",
      "Epoch [772/10044], Batch [3/7], Loss: 0.0125, Accuracy: 99.83%, Grad Norm: 0.08279\n",
      "Epoch [772/10044], Batch [4/7], Loss: 0.0139, Accuracy: 99.77%, Grad Norm: 0.08432\n",
      "Epoch [772/10044], Batch [5/7], Loss: 0.0139, Accuracy: 99.72%, Grad Norm: 0.11365\n",
      "Epoch [772/10044], Batch [6/7], Loss: 0.0177, Accuracy: 99.66%, Grad Norm: 0.16621\n",
      "Epoch [772/10044], Batch [7/7], Loss: 0.0103, Accuracy: 99.80%, Grad Norm: 0.11261\n",
      "Epoch [772/10044], Loss: 0.0103\n",
      "Epoch [773/10044], Batch [1/7], Loss: 0.0138, Accuracy: 99.71%, Grad Norm: 0.10193\n",
      "Epoch [773/10044], Batch [2/7], Loss: 0.0151, Accuracy: 99.67%, Grad Norm: 0.10602\n",
      "Epoch [773/10044], Batch [3/7], Loss: 0.0125, Accuracy: 99.79%, Grad Norm: 0.09608\n",
      "Epoch [773/10044], Batch [4/7], Loss: 0.0149, Accuracy: 99.72%, Grad Norm: 0.10027\n",
      "Epoch [773/10044], Batch [5/7], Loss: 0.0145, Accuracy: 99.72%, Grad Norm: 0.12386\n",
      "Epoch [773/10044], Batch [6/7], Loss: 0.0167, Accuracy: 99.57%, Grad Norm: 0.13428\n",
      "Epoch [773/10044], Batch [7/7], Loss: 0.0118, Accuracy: 99.77%, Grad Norm: 0.13757\n",
      "Epoch [773/10044], Loss: 0.0118\n",
      "Epoch [774/10044], Batch [1/7], Loss: 0.0152, Accuracy: 99.74%, Grad Norm: 0.11228\n",
      "Epoch [774/10044], Batch [2/7], Loss: 0.0155, Accuracy: 99.64%, Grad Norm: 0.12168\n",
      "Epoch [774/10044], Batch [3/7], Loss: 0.0127, Accuracy: 99.80%, Grad Norm: 0.09473\n",
      "Epoch [774/10044], Batch [4/7], Loss: 0.0143, Accuracy: 99.77%, Grad Norm: 0.09435\n",
      "Epoch [774/10044], Batch [5/7], Loss: 0.0145, Accuracy: 99.78%, Grad Norm: 0.12343\n",
      "Epoch [774/10044], Batch [6/7], Loss: 0.0153, Accuracy: 99.69%, Grad Norm: 0.12223\n",
      "Epoch [774/10044], Batch [7/7], Loss: 0.0125, Accuracy: 99.70%, Grad Norm: 0.16737\n",
      "Epoch [774/10044], Loss: 0.0125\n",
      "Epoch [775/10044], Batch [1/7], Loss: 0.0144, Accuracy: 99.73%, Grad Norm: 0.10400\n",
      "Epoch [775/10044], Batch [2/7], Loss: 0.0176, Accuracy: 99.56%, Grad Norm: 0.15732\n",
      "Epoch [775/10044], Batch [3/7], Loss: 0.0124, Accuracy: 99.82%, Grad Norm: 0.08821\n",
      "Epoch [775/10044], Batch [4/7], Loss: 0.0153, Accuracy: 99.69%, Grad Norm: 0.09968\n",
      "Epoch [775/10044], Batch [5/7], Loss: 0.0135, Accuracy: 99.83%, Grad Norm: 0.09878\n",
      "Epoch [775/10044], Batch [6/7], Loss: 0.0161, Accuracy: 99.57%, Grad Norm: 0.13735\n",
      "Epoch [775/10044], Batch [7/7], Loss: 0.0102, Accuracy: 99.82%, Grad Norm: 0.11756\n",
      "Epoch [775/10044], Loss: 0.0102\n",
      "Epoch [776/10044], Batch [1/7], Loss: 0.0138, Accuracy: 99.75%, Grad Norm: 0.10680\n",
      "Epoch [776/10044], Batch [2/7], Loss: 0.0161, Accuracy: 99.66%, Grad Norm: 0.17580\n",
      "Epoch [776/10044], Batch [3/7], Loss: 0.0118, Accuracy: 99.82%, Grad Norm: 0.08451\n",
      "Epoch [776/10044], Batch [4/7], Loss: 0.0159, Accuracy: 99.65%, Grad Norm: 0.09108\n",
      "Epoch [776/10044], Batch [5/7], Loss: 0.0120, Accuracy: 99.79%, Grad Norm: 0.09954\n",
      "Epoch [776/10044], Batch [6/7], Loss: 0.0159, Accuracy: 99.62%, Grad Norm: 0.12018\n",
      "Epoch [776/10044], Batch [7/7], Loss: 0.0108, Accuracy: 99.78%, Grad Norm: 0.12788\n",
      "Epoch [776/10044], Loss: 0.0108\n",
      "Epoch [777/10044], Batch [1/7], Loss: 0.0145, Accuracy: 99.72%, Grad Norm: 0.12221\n",
      "Epoch [777/10044], Batch [2/7], Loss: 0.0173, Accuracy: 99.59%, Grad Norm: 0.14345\n",
      "Epoch [777/10044], Batch [3/7], Loss: 0.0126, Accuracy: 99.81%, Grad Norm: 0.08440\n",
      "Epoch [777/10044], Batch [4/7], Loss: 0.0147, Accuracy: 99.72%, Grad Norm: 0.09291\n",
      "Epoch [777/10044], Batch [5/7], Loss: 0.0138, Accuracy: 99.72%, Grad Norm: 0.11305\n",
      "Epoch [777/10044], Batch [6/7], Loss: 0.0140, Accuracy: 99.72%, Grad Norm: 0.10361\n",
      "Epoch [777/10044], Batch [7/7], Loss: 0.0121, Accuracy: 99.73%, Grad Norm: 0.14805\n",
      "Epoch [777/10044], Loss: 0.0121\n",
      "Epoch [778/10044], Batch [1/7], Loss: 0.0131, Accuracy: 99.78%, Grad Norm: 0.08953\n",
      "Epoch [778/10044], Batch [2/7], Loss: 0.0163, Accuracy: 99.59%, Grad Norm: 0.13147\n",
      "Epoch [778/10044], Batch [3/7], Loss: 0.0122, Accuracy: 99.83%, Grad Norm: 0.07225\n",
      "Epoch [778/10044], Batch [4/7], Loss: 0.0143, Accuracy: 99.72%, Grad Norm: 0.08655\n",
      "Epoch [778/10044], Batch [5/7], Loss: 0.0133, Accuracy: 99.76%, Grad Norm: 0.09631\n",
      "Epoch [778/10044], Batch [6/7], Loss: 0.0144, Accuracy: 99.71%, Grad Norm: 0.10449\n",
      "Epoch [778/10044], Batch [7/7], Loss: 0.0132, Accuracy: 99.67%, Grad Norm: 0.16581\n",
      "Epoch [778/10044], Loss: 0.0132\n",
      "Epoch [779/10044], Batch [1/7], Loss: 0.0134, Accuracy: 99.77%, Grad Norm: 0.09751\n",
      "Epoch [779/10044], Batch [2/7], Loss: 0.0163, Accuracy: 99.61%, Grad Norm: 0.13816\n",
      "Epoch [779/10044], Batch [3/7], Loss: 0.0117, Accuracy: 99.85%, Grad Norm: 0.07614\n",
      "Epoch [779/10044], Batch [4/7], Loss: 0.0142, Accuracy: 99.71%, Grad Norm: 0.11227\n",
      "Epoch [779/10044], Batch [5/7], Loss: 0.0127, Accuracy: 99.76%, Grad Norm: 0.10658\n",
      "Epoch [779/10044], Batch [6/7], Loss: 0.0137, Accuracy: 99.68%, Grad Norm: 0.10483\n",
      "Epoch [779/10044], Batch [7/7], Loss: 0.0133, Accuracy: 99.73%, Grad Norm: 0.14341\n",
      "Epoch [779/10044], Loss: 0.0133\n",
      "Epoch [780/10044], Batch [1/7], Loss: 0.0135, Accuracy: 99.77%, Grad Norm: 0.11407\n",
      "Epoch [780/10044], Batch [2/7], Loss: 0.0142, Accuracy: 99.67%, Grad Norm: 0.11465\n",
      "Epoch [780/10044], Batch [3/7], Loss: 0.0120, Accuracy: 99.83%, Grad Norm: 0.07543\n",
      "Epoch [780/10044], Batch [4/7], Loss: 0.0154, Accuracy: 99.67%, Grad Norm: 0.11450\n",
      "Epoch [780/10044], Batch [5/7], Loss: 0.0140, Accuracy: 99.70%, Grad Norm: 0.11241\n",
      "Epoch [780/10044], Batch [6/7], Loss: 0.0138, Accuracy: 99.67%, Grad Norm: 0.10195\n",
      "Epoch [780/10044], Batch [7/7], Loss: 0.0123, Accuracy: 99.78%, Grad Norm: 0.17318\n",
      "Epoch [780/10044], Loss: 0.0123\n",
      "Epoch [781/10044], Batch [1/7], Loss: 0.0132, Accuracy: 99.78%, Grad Norm: 0.10576\n",
      "Epoch [781/10044], Batch [2/7], Loss: 0.0143, Accuracy: 99.67%, Grad Norm: 0.11589\n",
      "Epoch [781/10044], Batch [3/7], Loss: 0.0121, Accuracy: 99.84%, Grad Norm: 0.08419\n",
      "Epoch [781/10044], Batch [4/7], Loss: 0.0132, Accuracy: 99.77%, Grad Norm: 0.08697\n",
      "Epoch [781/10044], Batch [5/7], Loss: 0.0120, Accuracy: 99.77%, Grad Norm: 0.08391\n",
      "Epoch [781/10044], Batch [6/7], Loss: 0.0160, Accuracy: 99.62%, Grad Norm: 0.12189\n",
      "Epoch [781/10044], Batch [7/7], Loss: 0.0116, Accuracy: 99.73%, Grad Norm: 0.15684\n",
      "Epoch [781/10044], Loss: 0.0116\n",
      "Epoch [782/10044], Batch [1/7], Loss: 0.0152, Accuracy: 99.75%, Grad Norm: 0.10179\n",
      "Epoch [782/10044], Batch [2/7], Loss: 0.0149, Accuracy: 99.72%, Grad Norm: 0.11951\n",
      "Epoch [782/10044], Batch [3/7], Loss: 0.0130, Accuracy: 99.78%, Grad Norm: 0.09651\n",
      "Epoch [782/10044], Batch [4/7], Loss: 0.0145, Accuracy: 99.67%, Grad Norm: 0.09894\n",
      "Epoch [782/10044], Batch [5/7], Loss: 0.0142, Accuracy: 99.73%, Grad Norm: 0.11498\n",
      "Epoch [782/10044], Batch [6/7], Loss: 0.0150, Accuracy: 99.65%, Grad Norm: 0.13546\n",
      "Epoch [782/10044], Batch [7/7], Loss: 0.0110, Accuracy: 99.75%, Grad Norm: 0.11949\n",
      "Epoch [782/10044], Loss: 0.0110\n",
      "Epoch [783/10044], Batch [1/7], Loss: 0.0131, Accuracy: 99.75%, Grad Norm: 0.09573\n",
      "Epoch [783/10044], Batch [2/7], Loss: 0.0165, Accuracy: 99.70%, Grad Norm: 0.12756\n",
      "Epoch [783/10044], Batch [3/7], Loss: 0.0128, Accuracy: 99.80%, Grad Norm: 0.09205\n",
      "Epoch [783/10044], Batch [4/7], Loss: 0.0134, Accuracy: 99.75%, Grad Norm: 0.09825\n",
      "Epoch [783/10044], Batch [5/7], Loss: 0.0129, Accuracy: 99.74%, Grad Norm: 0.09747\n",
      "Epoch [783/10044], Batch [6/7], Loss: 0.0154, Accuracy: 99.61%, Grad Norm: 0.11006\n",
      "Epoch [783/10044], Batch [7/7], Loss: 0.0131, Accuracy: 99.72%, Grad Norm: 0.16363\n",
      "Epoch [783/10044], Loss: 0.0131\n",
      "Epoch [784/10044], Batch [1/7], Loss: 0.0153, Accuracy: 99.69%, Grad Norm: 0.11604\n",
      "Epoch [784/10044], Batch [2/7], Loss: 0.0161, Accuracy: 99.69%, Grad Norm: 0.13811\n",
      "Epoch [784/10044], Batch [3/7], Loss: 0.0140, Accuracy: 99.74%, Grad Norm: 0.09945\n",
      "Epoch [784/10044], Batch [4/7], Loss: 0.0144, Accuracy: 99.68%, Grad Norm: 0.09737\n",
      "Epoch [784/10044], Batch [5/7], Loss: 0.0123, Accuracy: 99.78%, Grad Norm: 0.10167\n",
      "Epoch [784/10044], Batch [6/7], Loss: 0.0156, Accuracy: 99.65%, Grad Norm: 0.10893\n",
      "Epoch [784/10044], Batch [7/7], Loss: 0.0122, Accuracy: 99.73%, Grad Norm: 0.15331\n",
      "Epoch [784/10044], Loss: 0.0122\n",
      "Epoch [785/10044], Batch [1/7], Loss: 0.0147, Accuracy: 99.72%, Grad Norm: 0.11713\n",
      "Epoch [785/10044], Batch [2/7], Loss: 0.0179, Accuracy: 99.55%, Grad Norm: 0.15915\n",
      "Epoch [785/10044], Batch [3/7], Loss: 0.0129, Accuracy: 99.78%, Grad Norm: 0.09972\n",
      "Epoch [785/10044], Batch [4/7], Loss: 0.0137, Accuracy: 99.73%, Grad Norm: 0.08272\n",
      "Epoch [785/10044], Batch [5/7], Loss: 0.0129, Accuracy: 99.77%, Grad Norm: 0.11062\n",
      "Epoch [785/10044], Batch [6/7], Loss: 0.0151, Accuracy: 99.65%, Grad Norm: 0.11882\n",
      "Epoch [785/10044], Batch [7/7], Loss: 0.0115, Accuracy: 99.75%, Grad Norm: 0.14086\n",
      "Epoch [785/10044], Loss: 0.0115\n",
      "Epoch [786/10044], Batch [1/7], Loss: 0.0124, Accuracy: 99.82%, Grad Norm: 0.08915\n",
      "Epoch [786/10044], Batch [2/7], Loss: 0.0152, Accuracy: 99.68%, Grad Norm: 0.12653\n",
      "Epoch [786/10044], Batch [3/7], Loss: 0.0119, Accuracy: 99.82%, Grad Norm: 0.08257\n",
      "Epoch [786/10044], Batch [4/7], Loss: 0.0136, Accuracy: 99.77%, Grad Norm: 0.08589\n",
      "Epoch [786/10044], Batch [5/7], Loss: 0.0128, Accuracy: 99.75%, Grad Norm: 0.09787\n",
      "Epoch [786/10044], Batch [6/7], Loss: 0.0171, Accuracy: 99.57%, Grad Norm: 0.13328\n",
      "Epoch [786/10044], Batch [7/7], Loss: 0.0115, Accuracy: 99.75%, Grad Norm: 0.13141\n",
      "Epoch [786/10044], Loss: 0.0115\n",
      "Epoch [787/10044], Batch [1/7], Loss: 0.0152, Accuracy: 99.67%, Grad Norm: 0.12904\n",
      "Epoch [787/10044], Batch [2/7], Loss: 0.0151, Accuracy: 99.66%, Grad Norm: 0.11617\n",
      "Epoch [787/10044], Batch [3/7], Loss: 0.0109, Accuracy: 99.87%, Grad Norm: 0.07894\n",
      "Epoch [787/10044], Batch [4/7], Loss: 0.0136, Accuracy: 99.74%, Grad Norm: 0.09284\n",
      "Epoch [787/10044], Batch [5/7], Loss: 0.0124, Accuracy: 99.76%, Grad Norm: 0.11812\n",
      "Epoch [787/10044], Batch [6/7], Loss: 0.0147, Accuracy: 99.65%, Grad Norm: 0.13941\n",
      "Epoch [787/10044], Batch [7/7], Loss: 0.0128, Accuracy: 99.73%, Grad Norm: 0.15759\n",
      "Epoch [787/10044], Loss: 0.0128\n",
      "Epoch [788/10044], Batch [1/7], Loss: 0.0142, Accuracy: 99.73%, Grad Norm: 0.10809\n",
      "Epoch [788/10044], Batch [2/7], Loss: 0.0154, Accuracy: 99.63%, Grad Norm: 0.13517\n",
      "Epoch [788/10044], Batch [3/7], Loss: 0.0120, Accuracy: 99.80%, Grad Norm: 0.07780\n",
      "Epoch [788/10044], Batch [4/7], Loss: 0.0153, Accuracy: 99.65%, Grad Norm: 0.11313\n",
      "Epoch [788/10044], Batch [5/7], Loss: 0.0133, Accuracy: 99.69%, Grad Norm: 0.13175\n",
      "Epoch [788/10044], Batch [6/7], Loss: 0.0161, Accuracy: 99.64%, Grad Norm: 0.14077\n",
      "Epoch [788/10044], Batch [7/7], Loss: 0.0150, Accuracy: 99.65%, Grad Norm: 0.15506\n",
      "Epoch [788/10044], Loss: 0.0150\n",
      "Epoch [789/10044], Batch [1/7], Loss: 0.0125, Accuracy: 99.76%, Grad Norm: 0.09135\n",
      "Epoch [789/10044], Batch [2/7], Loss: 0.0151, Accuracy: 99.62%, Grad Norm: 0.12270\n",
      "Epoch [789/10044], Batch [3/7], Loss: 0.0124, Accuracy: 99.82%, Grad Norm: 0.07645\n",
      "Epoch [789/10044], Batch [4/7], Loss: 0.0143, Accuracy: 99.77%, Grad Norm: 0.10549\n",
      "Epoch [789/10044], Batch [5/7], Loss: 0.0149, Accuracy: 99.73%, Grad Norm: 0.16517\n",
      "Epoch [789/10044], Batch [6/7], Loss: 0.0149, Accuracy: 99.65%, Grad Norm: 0.12920\n",
      "Epoch [789/10044], Batch [7/7], Loss: 0.0135, Accuracy: 99.68%, Grad Norm: 0.16557\n",
      "Epoch [789/10044], Loss: 0.0135\n",
      "Epoch [790/10044], Batch [1/7], Loss: 0.0127, Accuracy: 99.79%, Grad Norm: 0.08818\n",
      "Epoch [790/10044], Batch [2/7], Loss: 0.0180, Accuracy: 99.58%, Grad Norm: 0.14280\n",
      "Epoch [790/10044], Batch [3/7], Loss: 0.0122, Accuracy: 99.81%, Grad Norm: 0.08499\n",
      "Epoch [790/10044], Batch [4/7], Loss: 0.0137, Accuracy: 99.76%, Grad Norm: 0.08635\n",
      "Epoch [790/10044], Batch [5/7], Loss: 0.0131, Accuracy: 99.76%, Grad Norm: 0.10531\n",
      "Epoch [790/10044], Batch [6/7], Loss: 0.0151, Accuracy: 99.68%, Grad Norm: 0.11664\n",
      "Epoch [790/10044], Batch [7/7], Loss: 0.0127, Accuracy: 99.63%, Grad Norm: 0.14272\n",
      "Epoch [790/10044], Loss: 0.0127\n",
      "Epoch [791/10044], Batch [1/7], Loss: 0.0132, Accuracy: 99.78%, Grad Norm: 0.09312\n",
      "Epoch [791/10044], Batch [2/7], Loss: 0.0166, Accuracy: 99.61%, Grad Norm: 0.14369\n",
      "Epoch [791/10044], Batch [3/7], Loss: 0.0133, Accuracy: 99.76%, Grad Norm: 0.09757\n",
      "Epoch [791/10044], Batch [4/7], Loss: 0.0146, Accuracy: 99.72%, Grad Norm: 0.10778\n",
      "Epoch [791/10044], Batch [5/7], Loss: 0.0122, Accuracy: 99.77%, Grad Norm: 0.10053\n",
      "Epoch [791/10044], Batch [6/7], Loss: 0.0150, Accuracy: 99.70%, Grad Norm: 0.10948\n",
      "Epoch [791/10044], Batch [7/7], Loss: 0.0104, Accuracy: 99.75%, Grad Norm: 0.11797\n",
      "Epoch [791/10044], Loss: 0.0104\n",
      "Epoch [792/10044], Batch [1/7], Loss: 0.0130, Accuracy: 99.80%, Grad Norm: 0.09615\n",
      "Epoch [792/10044], Batch [2/7], Loss: 0.0172, Accuracy: 99.52%, Grad Norm: 0.14184\n",
      "Epoch [792/10044], Batch [3/7], Loss: 0.0119, Accuracy: 99.82%, Grad Norm: 0.08902\n",
      "Epoch [792/10044], Batch [4/7], Loss: 0.0139, Accuracy: 99.74%, Grad Norm: 0.09683\n",
      "Epoch [792/10044], Batch [5/7], Loss: 0.0121, Accuracy: 99.76%, Grad Norm: 0.10371\n",
      "Epoch [792/10044], Batch [6/7], Loss: 0.0156, Accuracy: 99.62%, Grad Norm: 0.13749\n",
      "Epoch [792/10044], Batch [7/7], Loss: 0.0114, Accuracy: 99.72%, Grad Norm: 0.11977\n",
      "Epoch [792/10044], Loss: 0.0114\n",
      "Epoch [793/10044], Batch [1/7], Loss: 0.0137, Accuracy: 99.77%, Grad Norm: 0.09523\n",
      "Epoch [793/10044], Batch [2/7], Loss: 0.0129, Accuracy: 99.75%, Grad Norm: 0.10565\n",
      "Epoch [793/10044], Batch [3/7], Loss: 0.0117, Accuracy: 99.82%, Grad Norm: 0.07782\n",
      "Epoch [793/10044], Batch [4/7], Loss: 0.0147, Accuracy: 99.69%, Grad Norm: 0.12093\n",
      "Epoch [793/10044], Batch [5/7], Loss: 0.0146, Accuracy: 99.63%, Grad Norm: 0.13152\n",
      "Epoch [793/10044], Batch [6/7], Loss: 0.0154, Accuracy: 99.64%, Grad Norm: 0.11733\n",
      "Epoch [793/10044], Batch [7/7], Loss: 0.0095, Accuracy: 99.88%, Grad Norm: 0.09903\n",
      "Epoch [793/10044], Loss: 0.0095\n",
      "Epoch [794/10044], Batch [1/7], Loss: 0.0138, Accuracy: 99.73%, Grad Norm: 0.09168\n",
      "Epoch [794/10044], Batch [2/7], Loss: 0.0138, Accuracy: 99.78%, Grad Norm: 0.10680\n",
      "Epoch [794/10044], Batch [3/7], Loss: 0.0114, Accuracy: 99.82%, Grad Norm: 0.07887\n",
      "Epoch [794/10044], Batch [4/7], Loss: 0.0148, Accuracy: 99.75%, Grad Norm: 0.10688\n",
      "Epoch [794/10044], Batch [5/7], Loss: 0.0146, Accuracy: 99.72%, Grad Norm: 0.13313\n",
      "Epoch [794/10044], Batch [6/7], Loss: 0.0157, Accuracy: 99.62%, Grad Norm: 0.13106\n",
      "Epoch [794/10044], Batch [7/7], Loss: 0.0132, Accuracy: 99.68%, Grad Norm: 0.21123\n",
      "Epoch [794/10044], Loss: 0.0132\n",
      "Epoch [795/10044], Batch [1/7], Loss: 0.0122, Accuracy: 99.81%, Grad Norm: 0.09433\n",
      "Epoch [795/10044], Batch [2/7], Loss: 0.0154, Accuracy: 99.67%, Grad Norm: 0.14146\n",
      "Epoch [795/10044], Batch [3/7], Loss: 0.0114, Accuracy: 99.85%, Grad Norm: 0.07658\n",
      "Epoch [795/10044], Batch [4/7], Loss: 0.0132, Accuracy: 99.71%, Grad Norm: 0.08782\n",
      "Epoch [795/10044], Batch [5/7], Loss: 0.0143, Accuracy: 99.73%, Grad Norm: 0.13185\n",
      "Epoch [795/10044], Batch [6/7], Loss: 0.0157, Accuracy: 99.59%, Grad Norm: 0.14392\n",
      "Epoch [795/10044], Batch [7/7], Loss: 0.0125, Accuracy: 99.82%, Grad Norm: 0.19495\n",
      "Epoch [795/10044], Loss: 0.0125\n",
      "Epoch [796/10044], Batch [1/7], Loss: 0.0129, Accuracy: 99.79%, Grad Norm: 0.09635\n",
      "Epoch [796/10044], Batch [2/7], Loss: 0.0153, Accuracy: 99.62%, Grad Norm: 0.13990\n",
      "Epoch [796/10044], Batch [3/7], Loss: 0.0114, Accuracy: 99.85%, Grad Norm: 0.08296\n",
      "Epoch [796/10044], Batch [4/7], Loss: 0.0142, Accuracy: 99.73%, Grad Norm: 0.09711\n",
      "Epoch [796/10044], Batch [5/7], Loss: 0.0156, Accuracy: 99.70%, Grad Norm: 0.14419\n",
      "Epoch [796/10044], Batch [6/7], Loss: 0.0155, Accuracy: 99.66%, Grad Norm: 0.15228\n",
      "Epoch [796/10044], Batch [7/7], Loss: 0.0121, Accuracy: 99.67%, Grad Norm: 0.17161\n",
      "Epoch [796/10044], Loss: 0.0121\n",
      "Epoch [797/10044], Batch [1/7], Loss: 0.0148, Accuracy: 99.67%, Grad Norm: 0.12344\n",
      "Epoch [797/10044], Batch [2/7], Loss: 0.0156, Accuracy: 99.66%, Grad Norm: 0.14128\n",
      "Epoch [797/10044], Batch [3/7], Loss: 0.0113, Accuracy: 99.83%, Grad Norm: 0.07225\n",
      "Epoch [797/10044], Batch [4/7], Loss: 0.0134, Accuracy: 99.73%, Grad Norm: 0.08662\n",
      "Epoch [797/10044], Batch [5/7], Loss: 0.0135, Accuracy: 99.73%, Grad Norm: 0.11230\n",
      "Epoch [797/10044], Batch [6/7], Loss: 0.0149, Accuracy: 99.64%, Grad Norm: 0.12414\n",
      "Epoch [797/10044], Batch [7/7], Loss: 0.0100, Accuracy: 99.83%, Grad Norm: 0.10403\n",
      "Epoch [797/10044], Loss: 0.0100\n",
      "Epoch [798/10044], Batch [1/7], Loss: 0.0130, Accuracy: 99.72%, Grad Norm: 0.10807\n",
      "Epoch [798/10044], Batch [2/7], Loss: 0.0140, Accuracy: 99.66%, Grad Norm: 0.11987\n",
      "Epoch [798/10044], Batch [3/7], Loss: 0.0113, Accuracy: 99.86%, Grad Norm: 0.07640\n",
      "Epoch [798/10044], Batch [4/7], Loss: 0.0147, Accuracy: 99.68%, Grad Norm: 0.09475\n",
      "Epoch [798/10044], Batch [5/7], Loss: 0.0120, Accuracy: 99.76%, Grad Norm: 0.10065\n",
      "Epoch [798/10044], Batch [6/7], Loss: 0.0134, Accuracy: 99.74%, Grad Norm: 0.09723\n",
      "Epoch [798/10044], Batch [7/7], Loss: 0.0113, Accuracy: 99.73%, Grad Norm: 0.12759\n",
      "Epoch [798/10044], Loss: 0.0113\n",
      "Epoch [799/10044], Batch [1/7], Loss: 0.0145, Accuracy: 99.71%, Grad Norm: 0.13046\n",
      "Epoch [799/10044], Batch [2/7], Loss: 0.0155, Accuracy: 99.62%, Grad Norm: 0.12450\n",
      "Epoch [799/10044], Batch [3/7], Loss: 0.0126, Accuracy: 99.78%, Grad Norm: 0.09021\n",
      "Epoch [799/10044], Batch [4/7], Loss: 0.0147, Accuracy: 99.66%, Grad Norm: 0.10211\n",
      "Epoch [799/10044], Batch [5/7], Loss: 0.0135, Accuracy: 99.75%, Grad Norm: 0.12242\n",
      "Epoch [799/10044], Batch [6/7], Loss: 0.0131, Accuracy: 99.73%, Grad Norm: 0.09355\n",
      "Epoch [799/10044], Batch [7/7], Loss: 0.0115, Accuracy: 99.73%, Grad Norm: 0.12547\n",
      "Epoch [799/10044], Loss: 0.0115\n",
      "Epoch [800/10044], Batch [1/7], Loss: 0.0135, Accuracy: 99.76%, Grad Norm: 0.10265\n",
      "Epoch [800/10044], Batch [2/7], Loss: 0.0148, Accuracy: 99.72%, Grad Norm: 0.12343\n",
      "Epoch [800/10044], Batch [3/7], Loss: 0.0120, Accuracy: 99.80%, Grad Norm: 0.07905\n",
      "Epoch [800/10044], Batch [4/7], Loss: 0.0137, Accuracy: 99.74%, Grad Norm: 0.09684\n",
      "Epoch [800/10044], Batch [5/7], Loss: 0.0120, Accuracy: 99.80%, Grad Norm: 0.09521\n",
      "Epoch [800/10044], Batch [6/7], Loss: 0.0153, Accuracy: 99.63%, Grad Norm: 0.12330\n",
      "Epoch [800/10044], Batch [7/7], Loss: 0.0098, Accuracy: 99.82%, Grad Norm: 0.10117\n",
      "Epoch [800/10044], Loss: 0.0098\n",
      "Epoch [801/10044], Batch [1/7], Loss: 0.0129, Accuracy: 99.76%, Grad Norm: 0.09167\n",
      "Epoch [801/10044], Batch [2/7], Loss: 0.0140, Accuracy: 99.74%, Grad Norm: 0.12104\n",
      "Epoch [801/10044], Batch [3/7], Loss: 0.0112, Accuracy: 99.88%, Grad Norm: 0.07285\n",
      "Epoch [801/10044], Batch [4/7], Loss: 0.0125, Accuracy: 99.79%, Grad Norm: 0.08199\n",
      "Epoch [801/10044], Batch [5/7], Loss: 0.0120, Accuracy: 99.77%, Grad Norm: 0.09930\n",
      "Epoch [801/10044], Batch [6/7], Loss: 0.0158, Accuracy: 99.62%, Grad Norm: 0.13989\n",
      "Epoch [801/10044], Batch [7/7], Loss: 0.0104, Accuracy: 99.75%, Grad Norm: 0.11855\n",
      "Epoch [801/10044], Loss: 0.0104\n",
      "Epoch [802/10044], Batch [1/7], Loss: 0.0118, Accuracy: 99.81%, Grad Norm: 0.09160\n",
      "Epoch [802/10044], Batch [2/7], Loss: 0.0149, Accuracy: 99.66%, Grad Norm: 0.12903\n",
      "Epoch [802/10044], Batch [3/7], Loss: 0.0103, Accuracy: 99.89%, Grad Norm: 0.06091\n",
      "Epoch [802/10044], Batch [4/7], Loss: 0.0132, Accuracy: 99.73%, Grad Norm: 0.08888\n",
      "Epoch [802/10044], Batch [5/7], Loss: 0.0127, Accuracy: 99.83%, Grad Norm: 0.10792\n",
      "Epoch [802/10044], Batch [6/7], Loss: 0.0128, Accuracy: 99.74%, Grad Norm: 0.10174\n",
      "Epoch [802/10044], Batch [7/7], Loss: 0.0101, Accuracy: 99.82%, Grad Norm: 0.12064\n",
      "Epoch [802/10044], Loss: 0.0101\n",
      "Epoch [803/10044], Batch [1/7], Loss: 0.0134, Accuracy: 99.73%, Grad Norm: 0.11695\n",
      "Epoch [803/10044], Batch [2/7], Loss: 0.0150, Accuracy: 99.65%, Grad Norm: 0.12035\n",
      "Epoch [803/10044], Batch [3/7], Loss: 0.0122, Accuracy: 99.78%, Grad Norm: 0.08326\n",
      "Epoch [803/10044], Batch [4/7], Loss: 0.0117, Accuracy: 99.83%, Grad Norm: 0.07607\n",
      "Epoch [803/10044], Batch [5/7], Loss: 0.0121, Accuracy: 99.77%, Grad Norm: 0.09476\n",
      "Epoch [803/10044], Batch [6/7], Loss: 0.0151, Accuracy: 99.66%, Grad Norm: 0.11061\n",
      "Epoch [803/10044], Batch [7/7], Loss: 0.0112, Accuracy: 99.73%, Grad Norm: 0.13559\n",
      "Epoch [803/10044], Loss: 0.0112\n",
      "Epoch [804/10044], Batch [1/7], Loss: 0.0135, Accuracy: 99.77%, Grad Norm: 0.11351\n",
      "Epoch [804/10044], Batch [2/7], Loss: 0.0137, Accuracy: 99.67%, Grad Norm: 0.11481\n",
      "Epoch [804/10044], Batch [3/7], Loss: 0.0116, Accuracy: 99.82%, Grad Norm: 0.09266\n",
      "Epoch [804/10044], Batch [4/7], Loss: 0.0135, Accuracy: 99.76%, Grad Norm: 0.08502\n",
      "Epoch [804/10044], Batch [5/7], Loss: 0.0112, Accuracy: 99.77%, Grad Norm: 0.09132\n",
      "Epoch [804/10044], Batch [6/7], Loss: 0.0159, Accuracy: 99.61%, Grad Norm: 0.12717\n",
      "Epoch [804/10044], Batch [7/7], Loss: 0.0103, Accuracy: 99.78%, Grad Norm: 0.13978\n",
      "Epoch [804/10044], Loss: 0.0103\n",
      "Epoch [805/10044], Batch [1/7], Loss: 0.0137, Accuracy: 99.75%, Grad Norm: 0.11386\n",
      "Epoch [805/10044], Batch [2/7], Loss: 0.0150, Accuracy: 99.64%, Grad Norm: 0.12823\n",
      "Epoch [805/10044], Batch [3/7], Loss: 0.0112, Accuracy: 99.84%, Grad Norm: 0.07485\n",
      "Epoch [805/10044], Batch [4/7], Loss: 0.0138, Accuracy: 99.74%, Grad Norm: 0.09175\n",
      "Epoch [805/10044], Batch [5/7], Loss: 0.0120, Accuracy: 99.77%, Grad Norm: 0.09774\n",
      "Epoch [805/10044], Batch [6/7], Loss: 0.0129, Accuracy: 99.72%, Grad Norm: 0.11058\n",
      "Epoch [805/10044], Batch [7/7], Loss: 0.0101, Accuracy: 99.78%, Grad Norm: 0.10077\n",
      "Epoch [805/10044], Loss: 0.0101\n",
      "Epoch [806/10044], Batch [1/7], Loss: 0.0147, Accuracy: 99.72%, Grad Norm: 0.12989\n",
      "Epoch [806/10044], Batch [2/7], Loss: 0.0152, Accuracy: 99.69%, Grad Norm: 0.12593\n",
      "Epoch [806/10044], Batch [3/7], Loss: 0.0110, Accuracy: 99.85%, Grad Norm: 0.08408\n",
      "Epoch [806/10044], Batch [4/7], Loss: 0.0137, Accuracy: 99.77%, Grad Norm: 0.09306\n",
      "Epoch [806/10044], Batch [5/7], Loss: 0.0109, Accuracy: 99.84%, Grad Norm: 0.09680\n",
      "Epoch [806/10044], Batch [6/7], Loss: 0.0136, Accuracy: 99.67%, Grad Norm: 0.10961\n",
      "Epoch [806/10044], Batch [7/7], Loss: 0.0142, Accuracy: 99.65%, Grad Norm: 0.20831\n",
      "Epoch [806/10044], Loss: 0.0142\n",
      "Epoch [807/10044], Batch [1/7], Loss: 0.0127, Accuracy: 99.81%, Grad Norm: 0.12950\n",
      "Epoch [807/10044], Batch [2/7], Loss: 0.0143, Accuracy: 99.64%, Grad Norm: 0.15366\n",
      "Epoch [807/10044], Batch [3/7], Loss: 0.0119, Accuracy: 99.81%, Grad Norm: 0.08960\n",
      "Epoch [807/10044], Batch [4/7], Loss: 0.0129, Accuracy: 99.73%, Grad Norm: 0.08678\n",
      "Epoch [807/10044], Batch [5/7], Loss: 0.0125, Accuracy: 99.79%, Grad Norm: 0.10946\n",
      "Epoch [807/10044], Batch [6/7], Loss: 0.0157, Accuracy: 99.64%, Grad Norm: 0.13592\n",
      "Epoch [807/10044], Batch [7/7], Loss: 0.0097, Accuracy: 99.80%, Grad Norm: 0.12757\n",
      "Epoch [807/10044], Loss: 0.0097\n",
      "Epoch [808/10044], Batch [1/7], Loss: 0.0121, Accuracy: 99.77%, Grad Norm: 0.10191\n",
      "Epoch [808/10044], Batch [2/7], Loss: 0.0134, Accuracy: 99.71%, Grad Norm: 0.10468\n",
      "Epoch [808/10044], Batch [3/7], Loss: 0.0125, Accuracy: 99.78%, Grad Norm: 0.09769\n",
      "Epoch [808/10044], Batch [4/7], Loss: 0.0142, Accuracy: 99.66%, Grad Norm: 0.10076\n",
      "Epoch [808/10044], Batch [5/7], Loss: 0.0120, Accuracy: 99.79%, Grad Norm: 0.10007\n",
      "Epoch [808/10044], Batch [6/7], Loss: 0.0144, Accuracy: 99.75%, Grad Norm: 0.11831\n",
      "Epoch [808/10044], Batch [7/7], Loss: 0.0118, Accuracy: 99.68%, Grad Norm: 0.14211\n",
      "Epoch [808/10044], Loss: 0.0118\n",
      "Epoch [809/10044], Batch [1/7], Loss: 0.0136, Accuracy: 99.72%, Grad Norm: 0.10925\n",
      "Epoch [809/10044], Batch [2/7], Loss: 0.0149, Accuracy: 99.63%, Grad Norm: 0.12285\n",
      "Epoch [809/10044], Batch [3/7], Loss: 0.0116, Accuracy: 99.81%, Grad Norm: 0.08146\n",
      "Epoch [809/10044], Batch [4/7], Loss: 0.0134, Accuracy: 99.73%, Grad Norm: 0.09598\n",
      "Epoch [809/10044], Batch [5/7], Loss: 0.0120, Accuracy: 99.82%, Grad Norm: 0.09876\n",
      "Epoch [809/10044], Batch [6/7], Loss: 0.0145, Accuracy: 99.62%, Grad Norm: 0.11263\n",
      "Epoch [809/10044], Batch [7/7], Loss: 0.0085, Accuracy: 99.80%, Grad Norm: 0.10053\n",
      "Epoch [809/10044], Loss: 0.0085\n",
      "Epoch [810/10044], Batch [1/7], Loss: 0.0124, Accuracy: 99.81%, Grad Norm: 0.08768\n",
      "Epoch [810/10044], Batch [2/7], Loss: 0.0162, Accuracy: 99.63%, Grad Norm: 0.12209\n",
      "Epoch [810/10044], Batch [3/7], Loss: 0.0104, Accuracy: 99.87%, Grad Norm: 0.07369\n",
      "Epoch [810/10044], Batch [4/7], Loss: 0.0118, Accuracy: 99.76%, Grad Norm: 0.07620\n",
      "Epoch [810/10044], Batch [5/7], Loss: 0.0125, Accuracy: 99.77%, Grad Norm: 0.09904\n",
      "Epoch [810/10044], Batch [6/7], Loss: 0.0133, Accuracy: 99.68%, Grad Norm: 0.11601\n",
      "Epoch [810/10044], Batch [7/7], Loss: 0.0110, Accuracy: 99.77%, Grad Norm: 0.14511\n",
      "Epoch [810/10044], Loss: 0.0110\n",
      "Epoch [811/10044], Batch [1/7], Loss: 0.0120, Accuracy: 99.76%, Grad Norm: 0.09145\n",
      "Epoch [811/10044], Batch [2/7], Loss: 0.0134, Accuracy: 99.73%, Grad Norm: 0.11920\n",
      "Epoch [811/10044], Batch [3/7], Loss: 0.0111, Accuracy: 99.86%, Grad Norm: 0.07037\n",
      "Epoch [811/10044], Batch [4/7], Loss: 0.0127, Accuracy: 99.77%, Grad Norm: 0.09516\n",
      "Epoch [811/10044], Batch [5/7], Loss: 0.0124, Accuracy: 99.78%, Grad Norm: 0.10080\n",
      "Epoch [811/10044], Batch [6/7], Loss: 0.0130, Accuracy: 99.69%, Grad Norm: 0.10819\n",
      "Epoch [811/10044], Batch [7/7], Loss: 0.0107, Accuracy: 99.75%, Grad Norm: 0.14558\n",
      "Epoch [811/10044], Loss: 0.0107\n",
      "Epoch [812/10044], Batch [1/7], Loss: 0.0109, Accuracy: 99.85%, Grad Norm: 0.08134\n",
      "Epoch [812/10044], Batch [2/7], Loss: 0.0143, Accuracy: 99.67%, Grad Norm: 0.11227\n",
      "Epoch [812/10044], Batch [3/7], Loss: 0.0108, Accuracy: 99.87%, Grad Norm: 0.07242\n",
      "Epoch [812/10044], Batch [4/7], Loss: 0.0117, Accuracy: 99.79%, Grad Norm: 0.08423\n",
      "Epoch [812/10044], Batch [5/7], Loss: 0.0119, Accuracy: 99.78%, Grad Norm: 0.11238\n",
      "Epoch [812/10044], Batch [6/7], Loss: 0.0160, Accuracy: 99.53%, Grad Norm: 0.13616\n",
      "Epoch [812/10044], Batch [7/7], Loss: 0.0114, Accuracy: 99.73%, Grad Norm: 0.14893\n",
      "Epoch [812/10044], Loss: 0.0114\n",
      "Epoch [813/10044], Batch [1/7], Loss: 0.0122, Accuracy: 99.76%, Grad Norm: 0.09170\n",
      "Epoch [813/10044], Batch [2/7], Loss: 0.0144, Accuracy: 99.67%, Grad Norm: 0.11820\n",
      "Epoch [813/10044], Batch [3/7], Loss: 0.0109, Accuracy: 99.87%, Grad Norm: 0.08177\n",
      "Epoch [813/10044], Batch [4/7], Loss: 0.0146, Accuracy: 99.65%, Grad Norm: 0.11701\n",
      "Epoch [813/10044], Batch [5/7], Loss: 0.0119, Accuracy: 99.80%, Grad Norm: 0.10118\n",
      "Epoch [813/10044], Batch [6/7], Loss: 0.0150, Accuracy: 99.71%, Grad Norm: 0.12173\n",
      "Epoch [813/10044], Batch [7/7], Loss: 0.0106, Accuracy: 99.85%, Grad Norm: 0.13420\n",
      "Epoch [813/10044], Loss: 0.0106\n",
      "Epoch [814/10044], Batch [1/7], Loss: 0.0126, Accuracy: 99.80%, Grad Norm: 0.08969\n",
      "Epoch [814/10044], Batch [2/7], Loss: 0.0146, Accuracy: 99.68%, Grad Norm: 0.11974\n",
      "Epoch [814/10044], Batch [3/7], Loss: 0.0106, Accuracy: 99.84%, Grad Norm: 0.07830\n",
      "Epoch [814/10044], Batch [4/7], Loss: 0.0142, Accuracy: 99.69%, Grad Norm: 0.11302\n",
      "Epoch [814/10044], Batch [5/7], Loss: 0.0111, Accuracy: 99.80%, Grad Norm: 0.09222\n",
      "Epoch [814/10044], Batch [6/7], Loss: 0.0128, Accuracy: 99.77%, Grad Norm: 0.10546\n",
      "Epoch [814/10044], Batch [7/7], Loss: 0.0100, Accuracy: 99.82%, Grad Norm: 0.13348\n",
      "Epoch [814/10044], Loss: 0.0100\n",
      "Epoch [815/10044], Batch [1/7], Loss: 0.0121, Accuracy: 99.73%, Grad Norm: 0.10355\n",
      "Epoch [815/10044], Batch [2/7], Loss: 0.0128, Accuracy: 99.75%, Grad Norm: 0.10849\n",
      "Epoch [815/10044], Batch [3/7], Loss: 0.0104, Accuracy: 99.88%, Grad Norm: 0.06916\n",
      "Epoch [815/10044], Batch [4/7], Loss: 0.0121, Accuracy: 99.81%, Grad Norm: 0.08572\n",
      "Epoch [815/10044], Batch [5/7], Loss: 0.0106, Accuracy: 99.78%, Grad Norm: 0.10196\n",
      "Epoch [815/10044], Batch [6/7], Loss: 0.0139, Accuracy: 99.65%, Grad Norm: 0.11683\n",
      "Epoch [815/10044], Batch [7/7], Loss: 0.0103, Accuracy: 99.73%, Grad Norm: 0.12405\n",
      "Epoch [815/10044], Loss: 0.0103\n",
      "Epoch [816/10044], Batch [1/7], Loss: 0.0115, Accuracy: 99.78%, Grad Norm: 0.10425\n",
      "Epoch [816/10044], Batch [2/7], Loss: 0.0137, Accuracy: 99.71%, Grad Norm: 0.11204\n",
      "Epoch [816/10044], Batch [3/7], Loss: 0.0099, Accuracy: 99.82%, Grad Norm: 0.07096\n",
      "Epoch [816/10044], Batch [4/7], Loss: 0.0126, Accuracy: 99.76%, Grad Norm: 0.08044\n",
      "Epoch [816/10044], Batch [5/7], Loss: 0.0115, Accuracy: 99.78%, Grad Norm: 0.11105\n",
      "Epoch [816/10044], Batch [6/7], Loss: 0.0132, Accuracy: 99.72%, Grad Norm: 0.09850\n",
      "Epoch [816/10044], Batch [7/7], Loss: 0.0105, Accuracy: 99.73%, Grad Norm: 0.14448\n",
      "Epoch [816/10044], Loss: 0.0105\n",
      "Epoch [817/10044], Batch [1/7], Loss: 0.0107, Accuracy: 99.84%, Grad Norm: 0.07883\n",
      "Epoch [817/10044], Batch [2/7], Loss: 0.0131, Accuracy: 99.72%, Grad Norm: 0.11061\n",
      "Epoch [817/10044], Batch [3/7], Loss: 0.0106, Accuracy: 99.87%, Grad Norm: 0.07661\n",
      "Epoch [817/10044], Batch [4/7], Loss: 0.0130, Accuracy: 99.79%, Grad Norm: 0.08695\n",
      "Epoch [817/10044], Batch [5/7], Loss: 0.0116, Accuracy: 99.75%, Grad Norm: 0.09670\n",
      "Epoch [817/10044], Batch [6/7], Loss: 0.0147, Accuracy: 99.67%, Grad Norm: 0.11800\n",
      "Epoch [817/10044], Batch [7/7], Loss: 0.0094, Accuracy: 99.82%, Grad Norm: 0.11454\n",
      "Epoch [817/10044], Loss: 0.0094\n",
      "Epoch [818/10044], Batch [1/7], Loss: 0.0114, Accuracy: 99.77%, Grad Norm: 0.08927\n",
      "Epoch [818/10044], Batch [2/7], Loss: 0.0126, Accuracy: 99.72%, Grad Norm: 0.09826\n",
      "Epoch [818/10044], Batch [3/7], Loss: 0.0102, Accuracy: 99.89%, Grad Norm: 0.06772\n",
      "Epoch [818/10044], Batch [4/7], Loss: 0.0134, Accuracy: 99.68%, Grad Norm: 0.09304\n",
      "Epoch [818/10044], Batch [5/7], Loss: 0.0119, Accuracy: 99.72%, Grad Norm: 0.11994\n",
      "Epoch [818/10044], Batch [6/7], Loss: 0.0129, Accuracy: 99.73%, Grad Norm: 0.10492\n",
      "Epoch [818/10044], Batch [7/7], Loss: 0.0094, Accuracy: 99.77%, Grad Norm: 0.12178\n",
      "Epoch [818/10044], Loss: 0.0094\n",
      "Epoch [819/10044], Batch [1/7], Loss: 0.0102, Accuracy: 99.87%, Grad Norm: 0.07262\n",
      "Epoch [819/10044], Batch [2/7], Loss: 0.0147, Accuracy: 99.69%, Grad Norm: 0.13964\n",
      "Epoch [819/10044], Batch [3/7], Loss: 0.0105, Accuracy: 99.84%, Grad Norm: 0.07820\n",
      "Epoch [819/10044], Batch [4/7], Loss: 0.0142, Accuracy: 99.69%, Grad Norm: 0.10222\n",
      "Epoch [819/10044], Batch [5/7], Loss: 0.0112, Accuracy: 99.80%, Grad Norm: 0.10308\n",
      "Epoch [819/10044], Batch [6/7], Loss: 0.0155, Accuracy: 99.64%, Grad Norm: 0.12655\n",
      "Epoch [819/10044], Batch [7/7], Loss: 0.0103, Accuracy: 99.75%, Grad Norm: 0.13812\n",
      "Epoch [819/10044], Loss: 0.0103\n",
      "Epoch [820/10044], Batch [1/7], Loss: 0.0111, Accuracy: 99.79%, Grad Norm: 0.09410\n",
      "Epoch [820/10044], Batch [2/7], Loss: 0.0158, Accuracy: 99.59%, Grad Norm: 0.14494\n",
      "Epoch [820/10044], Batch [3/7], Loss: 0.0112, Accuracy: 99.83%, Grad Norm: 0.07770\n",
      "Epoch [820/10044], Batch [4/7], Loss: 0.0131, Accuracy: 99.74%, Grad Norm: 0.10052\n",
      "Epoch [820/10044], Batch [5/7], Loss: 0.0133, Accuracy: 99.65%, Grad Norm: 0.11729\n",
      "Epoch [820/10044], Batch [6/7], Loss: 0.0123, Accuracy: 99.68%, Grad Norm: 0.10458\n",
      "Epoch [820/10044], Batch [7/7], Loss: 0.0086, Accuracy: 99.88%, Grad Norm: 0.10893\n",
      "Epoch [820/10044], Loss: 0.0086\n",
      "Epoch [821/10044], Batch [1/7], Loss: 0.0127, Accuracy: 99.75%, Grad Norm: 0.11466\n",
      "Epoch [821/10044], Batch [2/7], Loss: 0.0150, Accuracy: 99.67%, Grad Norm: 0.12858\n",
      "Epoch [821/10044], Batch [3/7], Loss: 0.0115, Accuracy: 99.79%, Grad Norm: 0.08734\n",
      "Epoch [821/10044], Batch [4/7], Loss: 0.0118, Accuracy: 99.75%, Grad Norm: 0.08340\n",
      "Epoch [821/10044], Batch [5/7], Loss: 0.0122, Accuracy: 99.77%, Grad Norm: 0.11509\n",
      "Epoch [821/10044], Batch [6/7], Loss: 0.0134, Accuracy: 99.69%, Grad Norm: 0.11518\n",
      "Epoch [821/10044], Batch [7/7], Loss: 0.0133, Accuracy: 99.62%, Grad Norm: 0.17118\n",
      "Epoch [821/10044], Loss: 0.0133\n",
      "Epoch [822/10044], Batch [1/7], Loss: 0.0119, Accuracy: 99.77%, Grad Norm: 0.10918\n",
      "Epoch [822/10044], Batch [2/7], Loss: 0.0147, Accuracy: 99.63%, Grad Norm: 0.13411\n",
      "Epoch [822/10044], Batch [3/7], Loss: 0.0112, Accuracy: 99.83%, Grad Norm: 0.08258\n",
      "Epoch [822/10044], Batch [4/7], Loss: 0.0147, Accuracy: 99.66%, Grad Norm: 0.11095\n",
      "Epoch [822/10044], Batch [5/7], Loss: 0.0116, Accuracy: 99.75%, Grad Norm: 0.11296\n",
      "Epoch [822/10044], Batch [6/7], Loss: 0.0135, Accuracy: 99.68%, Grad Norm: 0.11555\n",
      "Epoch [822/10044], Batch [7/7], Loss: 0.0136, Accuracy: 99.65%, Grad Norm: 0.17091\n",
      "Epoch [822/10044], Loss: 0.0136\n",
      "Epoch [823/10044], Batch [1/7], Loss: 0.0140, Accuracy: 99.65%, Grad Norm: 0.12877\n",
      "Epoch [823/10044], Batch [2/7], Loss: 0.0153, Accuracy: 99.67%, Grad Norm: 0.13419\n",
      "Epoch [823/10044], Batch [3/7], Loss: 0.0104, Accuracy: 99.91%, Grad Norm: 0.07470\n",
      "Epoch [823/10044], Batch [4/7], Loss: 0.0125, Accuracy: 99.79%, Grad Norm: 0.09368\n",
      "Epoch [823/10044], Batch [5/7], Loss: 0.0117, Accuracy: 99.78%, Grad Norm: 0.11484\n",
      "Epoch [823/10044], Batch [6/7], Loss: 0.0153, Accuracy: 99.60%, Grad Norm: 0.13506\n",
      "Epoch [823/10044], Batch [7/7], Loss: 0.0118, Accuracy: 99.70%, Grad Norm: 0.16674\n",
      "Epoch [823/10044], Loss: 0.0118\n",
      "Epoch [824/10044], Batch [1/7], Loss: 0.0131, Accuracy: 99.71%, Grad Norm: 0.10287\n",
      "Epoch [824/10044], Batch [2/7], Loss: 0.0132, Accuracy: 99.69%, Grad Norm: 0.10868\n",
      "Epoch [824/10044], Batch [3/7], Loss: 0.0110, Accuracy: 99.85%, Grad Norm: 0.08227\n",
      "Epoch [824/10044], Batch [4/7], Loss: 0.0144, Accuracy: 99.74%, Grad Norm: 0.10699\n",
      "Epoch [824/10044], Batch [5/7], Loss: 0.0105, Accuracy: 99.78%, Grad Norm: 0.09161\n",
      "Epoch [824/10044], Batch [6/7], Loss: 0.0141, Accuracy: 99.68%, Grad Norm: 0.13440\n",
      "Epoch [824/10044], Batch [7/7], Loss: 0.0120, Accuracy: 99.70%, Grad Norm: 0.15305\n",
      "Epoch [824/10044], Loss: 0.0120\n",
      "Epoch [825/10044], Batch [1/7], Loss: 0.0105, Accuracy: 99.86%, Grad Norm: 0.07538\n",
      "Epoch [825/10044], Batch [2/7], Loss: 0.0131, Accuracy: 99.68%, Grad Norm: 0.11371\n",
      "Epoch [825/10044], Batch [3/7], Loss: 0.0108, Accuracy: 99.84%, Grad Norm: 0.08680\n",
      "Epoch [825/10044], Batch [4/7], Loss: 0.0136, Accuracy: 99.72%, Grad Norm: 0.10984\n",
      "Epoch [825/10044], Batch [5/7], Loss: 0.0121, Accuracy: 99.73%, Grad Norm: 0.11528\n",
      "Epoch [825/10044], Batch [6/7], Loss: 0.0129, Accuracy: 99.67%, Grad Norm: 0.10407\n",
      "Epoch [825/10044], Batch [7/7], Loss: 0.0106, Accuracy: 99.80%, Grad Norm: 0.14554\n",
      "Epoch [825/10044], Loss: 0.0106\n",
      "Epoch [826/10044], Batch [1/7], Loss: 0.0122, Accuracy: 99.74%, Grad Norm: 0.09396\n",
      "Epoch [826/10044], Batch [2/7], Loss: 0.0118, Accuracy: 99.77%, Grad Norm: 0.10179\n",
      "Epoch [826/10044], Batch [3/7], Loss: 0.0102, Accuracy: 99.87%, Grad Norm: 0.06920\n",
      "Epoch [826/10044], Batch [4/7], Loss: 0.0134, Accuracy: 99.72%, Grad Norm: 0.09974\n",
      "Epoch [826/10044], Batch [5/7], Loss: 0.0126, Accuracy: 99.77%, Grad Norm: 0.11931\n",
      "Epoch [826/10044], Batch [6/7], Loss: 0.0149, Accuracy: 99.59%, Grad Norm: 0.12679\n",
      "Epoch [826/10044], Batch [7/7], Loss: 0.0097, Accuracy: 99.77%, Grad Norm: 0.12075\n",
      "Epoch [826/10044], Loss: 0.0097\n",
      "Epoch [827/10044], Batch [1/7], Loss: 0.0117, Accuracy: 99.77%, Grad Norm: 0.10484\n",
      "Epoch [827/10044], Batch [2/7], Loss: 0.0125, Accuracy: 99.73%, Grad Norm: 0.10582\n",
      "Epoch [827/10044], Batch [3/7], Loss: 0.0104, Accuracy: 99.83%, Grad Norm: 0.09236\n",
      "Epoch [827/10044], Batch [4/7], Loss: 0.0133, Accuracy: 99.75%, Grad Norm: 0.10162\n",
      "Epoch [827/10044], Batch [5/7], Loss: 0.0108, Accuracy: 99.77%, Grad Norm: 0.10049\n",
      "Epoch [827/10044], Batch [6/7], Loss: 0.0140, Accuracy: 99.67%, Grad Norm: 0.13233\n",
      "Epoch [827/10044], Batch [7/7], Loss: 0.0090, Accuracy: 99.85%, Grad Norm: 0.10920\n",
      "Epoch [827/10044], Loss: 0.0090\n",
      "Epoch [828/10044], Batch [1/7], Loss: 0.0120, Accuracy: 99.79%, Grad Norm: 0.09463\n",
      "Epoch [828/10044], Batch [2/7], Loss: 0.0134, Accuracy: 99.72%, Grad Norm: 0.11146\n",
      "Epoch [828/10044], Batch [3/7], Loss: 0.0104, Accuracy: 99.85%, Grad Norm: 0.07699\n",
      "Epoch [828/10044], Batch [4/7], Loss: 0.0122, Accuracy: 99.81%, Grad Norm: 0.07623\n",
      "Epoch [828/10044], Batch [5/7], Loss: 0.0123, Accuracy: 99.79%, Grad Norm: 0.13156\n",
      "Epoch [828/10044], Batch [6/7], Loss: 0.0138, Accuracy: 99.71%, Grad Norm: 0.12131\n",
      "Epoch [828/10044], Batch [7/7], Loss: 0.0084, Accuracy: 99.90%, Grad Norm: 0.09136\n",
      "Epoch [828/10044], Loss: 0.0084\n",
      "Epoch [829/10044], Batch [1/7], Loss: 0.0114, Accuracy: 99.77%, Grad Norm: 0.09025\n",
      "Epoch [829/10044], Batch [2/7], Loss: 0.0164, Accuracy: 99.56%, Grad Norm: 0.15905\n",
      "Epoch [829/10044], Batch [3/7], Loss: 0.0106, Accuracy: 99.84%, Grad Norm: 0.08315\n",
      "Epoch [829/10044], Batch [4/7], Loss: 0.0114, Accuracy: 99.79%, Grad Norm: 0.07807\n",
      "Epoch [829/10044], Batch [5/7], Loss: 0.0136, Accuracy: 99.74%, Grad Norm: 0.11983\n",
      "Epoch [829/10044], Batch [6/7], Loss: 0.0134, Accuracy: 99.71%, Grad Norm: 0.13217\n",
      "Epoch [829/10044], Batch [7/7], Loss: 0.0114, Accuracy: 99.72%, Grad Norm: 0.18321\n",
      "Epoch [829/10044], Loss: 0.0114\n",
      "Epoch [830/10044], Batch [1/7], Loss: 0.0129, Accuracy: 99.75%, Grad Norm: 0.11137\n",
      "Epoch [830/10044], Batch [2/7], Loss: 0.0142, Accuracy: 99.67%, Grad Norm: 0.12885\n",
      "Epoch [830/10044], Batch [3/7], Loss: 0.0099, Accuracy: 99.85%, Grad Norm: 0.08379\n",
      "Epoch [830/10044], Batch [4/7], Loss: 0.0115, Accuracy: 99.78%, Grad Norm: 0.09533\n",
      "Epoch [830/10044], Batch [5/7], Loss: 0.0114, Accuracy: 99.79%, Grad Norm: 0.11510\n",
      "Epoch [830/10044], Batch [6/7], Loss: 0.0169, Accuracy: 99.60%, Grad Norm: 0.16285\n",
      "Epoch [830/10044], Batch [7/7], Loss: 0.0120, Accuracy: 99.67%, Grad Norm: 0.17745\n",
      "Epoch [830/10044], Loss: 0.0120\n",
      "Epoch [831/10044], Batch [1/7], Loss: 0.0123, Accuracy: 99.73%, Grad Norm: 0.09156\n",
      "Epoch [831/10044], Batch [2/7], Loss: 0.0139, Accuracy: 99.67%, Grad Norm: 0.12876\n",
      "Epoch [831/10044], Batch [3/7], Loss: 0.0100, Accuracy: 99.87%, Grad Norm: 0.07394\n",
      "Epoch [831/10044], Batch [4/7], Loss: 0.0145, Accuracy: 99.67%, Grad Norm: 0.15844\n",
      "Epoch [831/10044], Batch [5/7], Loss: 0.0143, Accuracy: 99.67%, Grad Norm: 0.13415\n",
      "Epoch [831/10044], Batch [6/7], Loss: 0.0133, Accuracy: 99.67%, Grad Norm: 0.11477\n",
      "Epoch [831/10044], Batch [7/7], Loss: 0.0110, Accuracy: 99.75%, Grad Norm: 0.16031\n",
      "Epoch [831/10044], Loss: 0.0110\n",
      "Epoch [832/10044], Batch [1/7], Loss: 0.0118, Accuracy: 99.79%, Grad Norm: 0.09446\n",
      "Epoch [832/10044], Batch [2/7], Loss: 0.0139, Accuracy: 99.62%, Grad Norm: 0.12747\n",
      "Epoch [832/10044], Batch [3/7], Loss: 0.0118, Accuracy: 99.78%, Grad Norm: 0.09900\n",
      "Epoch [832/10044], Batch [4/7], Loss: 0.0145, Accuracy: 99.70%, Grad Norm: 0.16192\n",
      "Epoch [832/10044], Batch [5/7], Loss: 0.0126, Accuracy: 99.72%, Grad Norm: 0.10798\n",
      "Epoch [832/10044], Batch [6/7], Loss: 0.0145, Accuracy: 99.67%, Grad Norm: 0.14337\n",
      "Epoch [832/10044], Batch [7/7], Loss: 0.0133, Accuracy: 99.63%, Grad Norm: 0.20024\n",
      "Epoch [832/10044], Loss: 0.0133\n",
      "Epoch [833/10044], Batch [1/7], Loss: 0.0114, Accuracy: 99.81%, Grad Norm: 0.10132\n",
      "Epoch [833/10044], Batch [2/7], Loss: 0.0155, Accuracy: 99.70%, Grad Norm: 0.12800\n",
      "Epoch [833/10044], Batch [3/7], Loss: 0.0109, Accuracy: 99.83%, Grad Norm: 0.09644\n",
      "Epoch [833/10044], Batch [4/7], Loss: 0.0125, Accuracy: 99.76%, Grad Norm: 0.09628\n",
      "Epoch [833/10044], Batch [5/7], Loss: 0.0120, Accuracy: 99.77%, Grad Norm: 0.11053\n",
      "Epoch [833/10044], Batch [6/7], Loss: 0.0138, Accuracy: 99.68%, Grad Norm: 0.12314\n",
      "Epoch [833/10044], Batch [7/7], Loss: 0.0094, Accuracy: 99.83%, Grad Norm: 0.12424\n",
      "Epoch [833/10044], Loss: 0.0094\n",
      "Epoch [834/10044], Batch [1/7], Loss: 0.0124, Accuracy: 99.77%, Grad Norm: 0.10737\n",
      "Epoch [834/10044], Batch [2/7], Loss: 0.0137, Accuracy: 99.69%, Grad Norm: 0.12240\n",
      "Epoch [834/10044], Batch [3/7], Loss: 0.0102, Accuracy: 99.82%, Grad Norm: 0.07477\n",
      "Epoch [834/10044], Batch [4/7], Loss: 0.0121, Accuracy: 99.76%, Grad Norm: 0.08632\n",
      "Epoch [834/10044], Batch [5/7], Loss: 0.0112, Accuracy: 99.78%, Grad Norm: 0.09165\n",
      "Epoch [834/10044], Batch [6/7], Loss: 0.0149, Accuracy: 99.67%, Grad Norm: 0.11171\n",
      "Epoch [834/10044], Batch [7/7], Loss: 0.0108, Accuracy: 99.82%, Grad Norm: 0.13960\n",
      "Epoch [834/10044], Loss: 0.0108\n",
      "Epoch [835/10044], Batch [1/7], Loss: 0.0108, Accuracy: 99.81%, Grad Norm: 0.08373\n",
      "Epoch [835/10044], Batch [2/7], Loss: 0.0119, Accuracy: 99.72%, Grad Norm: 0.10969\n",
      "Epoch [835/10044], Batch [3/7], Loss: 0.0106, Accuracy: 99.82%, Grad Norm: 0.08975\n",
      "Epoch [835/10044], Batch [4/7], Loss: 0.0129, Accuracy: 99.67%, Grad Norm: 0.09005\n",
      "Epoch [835/10044], Batch [5/7], Loss: 0.0109, Accuracy: 99.83%, Grad Norm: 0.09164\n",
      "Epoch [835/10044], Batch [6/7], Loss: 0.0129, Accuracy: 99.73%, Grad Norm: 0.11013\n",
      "Epoch [835/10044], Batch [7/7], Loss: 0.0077, Accuracy: 99.87%, Grad Norm: 0.08770\n",
      "Epoch [835/10044], Loss: 0.0077\n",
      "Epoch [836/10044], Batch [1/7], Loss: 0.0110, Accuracy: 99.81%, Grad Norm: 0.08534\n",
      "Epoch [836/10044], Batch [2/7], Loss: 0.0123, Accuracy: 99.71%, Grad Norm: 0.10358\n",
      "Epoch [836/10044], Batch [3/7], Loss: 0.0106, Accuracy: 99.81%, Grad Norm: 0.07445\n",
      "Epoch [836/10044], Batch [4/7], Loss: 0.0118, Accuracy: 99.76%, Grad Norm: 0.08423\n",
      "Epoch [836/10044], Batch [5/7], Loss: 0.0119, Accuracy: 99.76%, Grad Norm: 0.11005\n",
      "Epoch [836/10044], Batch [6/7], Loss: 0.0142, Accuracy: 99.71%, Grad Norm: 0.12340\n",
      "Epoch [836/10044], Batch [7/7], Loss: 0.0090, Accuracy: 99.75%, Grad Norm: 0.12473\n",
      "Epoch [836/10044], Loss: 0.0090\n",
      "Epoch [837/10044], Batch [1/7], Loss: 0.0120, Accuracy: 99.80%, Grad Norm: 0.10318\n",
      "Epoch [837/10044], Batch [2/7], Loss: 0.0114, Accuracy: 99.76%, Grad Norm: 0.09830\n",
      "Epoch [837/10044], Batch [3/7], Loss: 0.0104, Accuracy: 99.85%, Grad Norm: 0.08831\n",
      "Epoch [837/10044], Batch [4/7], Loss: 0.0122, Accuracy: 99.78%, Grad Norm: 0.09233\n",
      "Epoch [837/10044], Batch [5/7], Loss: 0.0106, Accuracy: 99.85%, Grad Norm: 0.09392\n",
      "Epoch [837/10044], Batch [6/7], Loss: 0.0138, Accuracy: 99.71%, Grad Norm: 0.11428\n",
      "Epoch [837/10044], Batch [7/7], Loss: 0.0103, Accuracy: 99.75%, Grad Norm: 0.13539\n",
      "Epoch [837/10044], Loss: 0.0103\n",
      "Epoch [838/10044], Batch [1/7], Loss: 0.0113, Accuracy: 99.79%, Grad Norm: 0.10077\n",
      "Epoch [838/10044], Batch [2/7], Loss: 0.0123, Accuracy: 99.69%, Grad Norm: 0.11374\n",
      "Epoch [838/10044], Batch [3/7], Loss: 0.0101, Accuracy: 99.85%, Grad Norm: 0.07708\n",
      "Epoch [838/10044], Batch [4/7], Loss: 0.0117, Accuracy: 99.78%, Grad Norm: 0.09695\n",
      "Epoch [838/10044], Batch [5/7], Loss: 0.0100, Accuracy: 99.79%, Grad Norm: 0.09087\n",
      "Epoch [838/10044], Batch [6/7], Loss: 0.0115, Accuracy: 99.77%, Grad Norm: 0.09385\n",
      "Epoch [838/10044], Batch [7/7], Loss: 0.0105, Accuracy: 99.77%, Grad Norm: 0.14959\n",
      "Epoch [838/10044], Loss: 0.0105\n",
      "Epoch [839/10044], Batch [1/7], Loss: 0.0112, Accuracy: 99.73%, Grad Norm: 0.09965\n",
      "Epoch [839/10044], Batch [2/7], Loss: 0.0133, Accuracy: 99.67%, Grad Norm: 0.11302\n",
      "Epoch [839/10044], Batch [3/7], Loss: 0.0096, Accuracy: 99.82%, Grad Norm: 0.06533\n",
      "Epoch [839/10044], Batch [4/7], Loss: 0.0131, Accuracy: 99.76%, Grad Norm: 0.09942\n",
      "Epoch [839/10044], Batch [5/7], Loss: 0.0111, Accuracy: 99.76%, Grad Norm: 0.10874\n",
      "Epoch [839/10044], Batch [6/7], Loss: 0.0130, Accuracy: 99.68%, Grad Norm: 0.11276\n",
      "Epoch [839/10044], Batch [7/7], Loss: 0.0118, Accuracy: 99.72%, Grad Norm: 0.15106\n",
      "Epoch [839/10044], Loss: 0.0118\n",
      "Epoch [840/10044], Batch [1/7], Loss: 0.0126, Accuracy: 99.71%, Grad Norm: 0.10924\n",
      "Epoch [840/10044], Batch [2/7], Loss: 0.0130, Accuracy: 99.72%, Grad Norm: 0.11536\n",
      "Epoch [840/10044], Batch [3/7], Loss: 0.0095, Accuracy: 99.87%, Grad Norm: 0.07248\n",
      "Epoch [840/10044], Batch [4/7], Loss: 0.0123, Accuracy: 99.77%, Grad Norm: 0.10341\n",
      "Epoch [840/10044], Batch [5/7], Loss: 0.0108, Accuracy: 99.76%, Grad Norm: 0.09075\n",
      "Epoch [840/10044], Batch [6/7], Loss: 0.0150, Accuracy: 99.65%, Grad Norm: 0.13826\n",
      "Epoch [840/10044], Batch [7/7], Loss: 0.0107, Accuracy: 99.72%, Grad Norm: 0.14185\n",
      "Epoch [840/10044], Loss: 0.0107\n",
      "Epoch [841/10044], Batch [1/7], Loss: 0.0118, Accuracy: 99.81%, Grad Norm: 0.11689\n",
      "Epoch [841/10044], Batch [2/7], Loss: 0.0127, Accuracy: 99.72%, Grad Norm: 0.09853\n",
      "Epoch [841/10044], Batch [3/7], Loss: 0.0105, Accuracy: 99.83%, Grad Norm: 0.08775\n",
      "Epoch [841/10044], Batch [4/7], Loss: 0.0135, Accuracy: 99.66%, Grad Norm: 0.10728\n",
      "Epoch [841/10044], Batch [5/7], Loss: 0.0112, Accuracy: 99.77%, Grad Norm: 0.09386\n",
      "Epoch [841/10044], Batch [6/7], Loss: 0.0144, Accuracy: 99.67%, Grad Norm: 0.13259\n",
      "Epoch [841/10044], Batch [7/7], Loss: 0.0113, Accuracy: 99.75%, Grad Norm: 0.17713\n",
      "Epoch [841/10044], Loss: 0.0113\n",
      "Epoch [842/10044], Batch [1/7], Loss: 0.0133, Accuracy: 99.75%, Grad Norm: 0.12073\n",
      "Epoch [842/10044], Batch [2/7], Loss: 0.0124, Accuracy: 99.72%, Grad Norm: 0.11355\n",
      "Epoch [842/10044], Batch [3/7], Loss: 0.0107, Accuracy: 99.82%, Grad Norm: 0.08196\n",
      "Epoch [842/10044], Batch [4/7], Loss: 0.0118, Accuracy: 99.74%, Grad Norm: 0.09232\n",
      "Epoch [842/10044], Batch [5/7], Loss: 0.0105, Accuracy: 99.82%, Grad Norm: 0.10280\n",
      "Epoch [842/10044], Batch [6/7], Loss: 0.0140, Accuracy: 99.71%, Grad Norm: 0.14198\n",
      "Epoch [842/10044], Batch [7/7], Loss: 0.0100, Accuracy: 99.78%, Grad Norm: 0.15467\n",
      "Epoch [842/10044], Loss: 0.0100\n",
      "Epoch [843/10044], Batch [1/7], Loss: 0.0109, Accuracy: 99.81%, Grad Norm: 0.10246\n",
      "Epoch [843/10044], Batch [2/7], Loss: 0.0125, Accuracy: 99.74%, Grad Norm: 0.11244\n",
      "Epoch [843/10044], Batch [3/7], Loss: 0.0105, Accuracy: 99.85%, Grad Norm: 0.09133\n",
      "Epoch [843/10044], Batch [4/7], Loss: 0.0112, Accuracy: 99.82%, Grad Norm: 0.08352\n",
      "Epoch [843/10044], Batch [5/7], Loss: 0.0144, Accuracy: 99.69%, Grad Norm: 0.15040\n",
      "Epoch [843/10044], Batch [6/7], Loss: 0.0153, Accuracy: 99.60%, Grad Norm: 0.13007\n",
      "Epoch [843/10044], Batch [7/7], Loss: 0.0129, Accuracy: 99.65%, Grad Norm: 0.17988\n",
      "Epoch [843/10044], Loss: 0.0129\n",
      "Epoch [844/10044], Batch [1/7], Loss: 0.0125, Accuracy: 99.78%, Grad Norm: 0.10128\n",
      "Epoch [844/10044], Batch [2/7], Loss: 0.0131, Accuracy: 99.66%, Grad Norm: 0.12513\n",
      "Epoch [844/10044], Batch [3/7], Loss: 0.0100, Accuracy: 99.86%, Grad Norm: 0.08200\n",
      "Epoch [844/10044], Batch [4/7], Loss: 0.0133, Accuracy: 99.67%, Grad Norm: 0.10896\n",
      "Epoch [844/10044], Batch [5/7], Loss: 0.0123, Accuracy: 99.75%, Grad Norm: 0.11916\n",
      "Epoch [844/10044], Batch [6/7], Loss: 0.0113, Accuracy: 99.77%, Grad Norm: 0.09665\n",
      "Epoch [844/10044], Batch [7/7], Loss: 0.0088, Accuracy: 99.85%, Grad Norm: 0.10389\n",
      "Epoch [844/10044], Loss: 0.0088\n",
      "Epoch [845/10044], Batch [1/7], Loss: 0.0122, Accuracy: 99.77%, Grad Norm: 0.10193\n",
      "Epoch [845/10044], Batch [2/7], Loss: 0.0137, Accuracy: 99.67%, Grad Norm: 0.12814\n",
      "Epoch [845/10044], Batch [3/7], Loss: 0.0101, Accuracy: 99.86%, Grad Norm: 0.07838\n",
      "Epoch [845/10044], Batch [4/7], Loss: 0.0109, Accuracy: 99.80%, Grad Norm: 0.07525\n",
      "Epoch [845/10044], Batch [5/7], Loss: 0.0124, Accuracy: 99.75%, Grad Norm: 0.11255\n",
      "Epoch [845/10044], Batch [6/7], Loss: 0.0117, Accuracy: 99.76%, Grad Norm: 0.10146\n",
      "Epoch [845/10044], Batch [7/7], Loss: 0.0099, Accuracy: 99.82%, Grad Norm: 0.12741\n",
      "Epoch [845/10044], Loss: 0.0099\n",
      "Epoch [846/10044], Batch [1/7], Loss: 0.0124, Accuracy: 99.72%, Grad Norm: 0.10400\n",
      "Epoch [846/10044], Batch [2/7], Loss: 0.0129, Accuracy: 99.72%, Grad Norm: 0.11759\n",
      "Epoch [846/10044], Batch [3/7], Loss: 0.0094, Accuracy: 99.92%, Grad Norm: 0.07532\n",
      "Epoch [846/10044], Batch [4/7], Loss: 0.0118, Accuracy: 99.78%, Grad Norm: 0.09376\n",
      "Epoch [846/10044], Batch [5/7], Loss: 0.0107, Accuracy: 99.80%, Grad Norm: 0.09789\n",
      "Epoch [846/10044], Batch [6/7], Loss: 0.0129, Accuracy: 99.68%, Grad Norm: 0.11161\n",
      "Epoch [846/10044], Batch [7/7], Loss: 0.0118, Accuracy: 99.80%, Grad Norm: 0.13563\n",
      "Epoch [846/10044], Loss: 0.0118\n",
      "Epoch [847/10044], Batch [1/7], Loss: 0.0107, Accuracy: 99.80%, Grad Norm: 0.08149\n",
      "Epoch [847/10044], Batch [2/7], Loss: 0.0124, Accuracy: 99.72%, Grad Norm: 0.09454\n",
      "Epoch [847/10044], Batch [3/7], Loss: 0.0104, Accuracy: 99.87%, Grad Norm: 0.08119\n",
      "Epoch [847/10044], Batch [4/7], Loss: 0.0106, Accuracy: 99.80%, Grad Norm: 0.07834\n",
      "Epoch [847/10044], Batch [5/7], Loss: 0.0099, Accuracy: 99.81%, Grad Norm: 0.10369\n",
      "Epoch [847/10044], Batch [6/7], Loss: 0.0136, Accuracy: 99.72%, Grad Norm: 0.12045\n",
      "Epoch [847/10044], Batch [7/7], Loss: 0.0109, Accuracy: 99.70%, Grad Norm: 0.17077\n",
      "Epoch [847/10044], Loss: 0.0109\n",
      "Epoch [848/10044], Batch [1/7], Loss: 0.0104, Accuracy: 99.83%, Grad Norm: 0.08916\n",
      "Epoch [848/10044], Batch [2/7], Loss: 0.0113, Accuracy: 99.74%, Grad Norm: 0.10375\n",
      "Epoch [848/10044], Batch [3/7], Loss: 0.0104, Accuracy: 99.82%, Grad Norm: 0.07120\n",
      "Epoch [848/10044], Batch [4/7], Loss: 0.0111, Accuracy: 99.84%, Grad Norm: 0.09004\n",
      "Epoch [848/10044], Batch [5/7], Loss: 0.0106, Accuracy: 99.81%, Grad Norm: 0.10077\n",
      "Epoch [848/10044], Batch [6/7], Loss: 0.0124, Accuracy: 99.77%, Grad Norm: 0.10547\n",
      "Epoch [848/10044], Batch [7/7], Loss: 0.0089, Accuracy: 99.80%, Grad Norm: 0.13148\n",
      "Epoch [848/10044], Loss: 0.0089\n",
      "Epoch [849/10044], Batch [1/7], Loss: 0.0098, Accuracy: 99.78%, Grad Norm: 0.08101\n",
      "Epoch [849/10044], Batch [2/7], Loss: 0.0116, Accuracy: 99.78%, Grad Norm: 0.10025\n",
      "Epoch [849/10044], Batch [3/7], Loss: 0.0102, Accuracy: 99.82%, Grad Norm: 0.08480\n",
      "Epoch [849/10044], Batch [4/7], Loss: 0.0112, Accuracy: 99.77%, Grad Norm: 0.09094\n",
      "Epoch [849/10044], Batch [5/7], Loss: 0.0116, Accuracy: 99.80%, Grad Norm: 0.10707\n",
      "Epoch [849/10044], Batch [6/7], Loss: 0.0130, Accuracy: 99.70%, Grad Norm: 0.11678\n",
      "Epoch [849/10044], Batch [7/7], Loss: 0.0088, Accuracy: 99.78%, Grad Norm: 0.10829\n",
      "Epoch [849/10044], Loss: 0.0088\n",
      "Epoch [850/10044], Batch [1/7], Loss: 0.0126, Accuracy: 99.76%, Grad Norm: 0.12477\n",
      "Epoch [850/10044], Batch [2/7], Loss: 0.0138, Accuracy: 99.65%, Grad Norm: 0.12482\n",
      "Epoch [850/10044], Batch [3/7], Loss: 0.0090, Accuracy: 99.87%, Grad Norm: 0.07042\n",
      "Epoch [850/10044], Batch [4/7], Loss: 0.0120, Accuracy: 99.77%, Grad Norm: 0.09898\n",
      "Epoch [850/10044], Batch [5/7], Loss: 0.0109, Accuracy: 99.80%, Grad Norm: 0.11616\n",
      "Epoch [850/10044], Batch [6/7], Loss: 0.0131, Accuracy: 99.71%, Grad Norm: 0.11518\n",
      "Epoch [850/10044], Batch [7/7], Loss: 0.0093, Accuracy: 99.82%, Grad Norm: 0.11554\n",
      "Epoch [850/10044], Loss: 0.0093\n",
      "Epoch [851/10044], Batch [1/7], Loss: 0.0134, Accuracy: 99.67%, Grad Norm: 0.12686\n",
      "Epoch [851/10044], Batch [2/7], Loss: 0.0111, Accuracy: 99.79%, Grad Norm: 0.11651\n",
      "Epoch [851/10044], Batch [3/7], Loss: 0.0100, Accuracy: 99.87%, Grad Norm: 0.07176\n",
      "Epoch [851/10044], Batch [4/7], Loss: 0.0119, Accuracy: 99.74%, Grad Norm: 0.08435\n",
      "Epoch [851/10044], Batch [5/7], Loss: 0.0108, Accuracy: 99.77%, Grad Norm: 0.11387\n",
      "Epoch [851/10044], Batch [6/7], Loss: 0.0114, Accuracy: 99.73%, Grad Norm: 0.10545\n",
      "Epoch [851/10044], Batch [7/7], Loss: 0.0081, Accuracy: 99.80%, Grad Norm: 0.12042\n",
      "Epoch [851/10044], Loss: 0.0081\n",
      "Epoch [852/10044], Batch [1/7], Loss: 0.0131, Accuracy: 99.72%, Grad Norm: 0.11603\n",
      "Epoch [852/10044], Batch [2/7], Loss: 0.0120, Accuracy: 99.72%, Grad Norm: 0.11078\n",
      "Epoch [852/10044], Batch [3/7], Loss: 0.0089, Accuracy: 99.88%, Grad Norm: 0.06226\n",
      "Epoch [852/10044], Batch [4/7], Loss: 0.0112, Accuracy: 99.82%, Grad Norm: 0.08953\n",
      "Epoch [852/10044], Batch [5/7], Loss: 0.0116, Accuracy: 99.78%, Grad Norm: 0.12012\n",
      "Epoch [852/10044], Batch [6/7], Loss: 0.0112, Accuracy: 99.77%, Grad Norm: 0.10047\n",
      "Epoch [852/10044], Batch [7/7], Loss: 0.0097, Accuracy: 99.75%, Grad Norm: 0.14786\n",
      "Epoch [852/10044], Loss: 0.0097\n",
      "Epoch [853/10044], Batch [1/7], Loss: 0.0112, Accuracy: 99.77%, Grad Norm: 0.09719\n",
      "Epoch [853/10044], Batch [2/7], Loss: 0.0135, Accuracy: 99.64%, Grad Norm: 0.12911\n",
      "Epoch [853/10044], Batch [3/7], Loss: 0.0110, Accuracy: 99.81%, Grad Norm: 0.10713\n",
      "Epoch [853/10044], Batch [4/7], Loss: 0.0113, Accuracy: 99.77%, Grad Norm: 0.08832\n",
      "Epoch [853/10044], Batch [5/7], Loss: 0.0112, Accuracy: 99.79%, Grad Norm: 0.10300\n",
      "Epoch [853/10044], Batch [6/7], Loss: 0.0136, Accuracy: 99.63%, Grad Norm: 0.13829\n",
      "Epoch [853/10044], Batch [7/7], Loss: 0.0076, Accuracy: 99.87%, Grad Norm: 0.13659\n",
      "Epoch [853/10044], Loss: 0.0076\n",
      "Epoch [854/10044], Batch [1/7], Loss: 0.0131, Accuracy: 99.76%, Grad Norm: 0.09769\n",
      "Epoch [854/10044], Batch [2/7], Loss: 0.0116, Accuracy: 99.74%, Grad Norm: 0.10203\n",
      "Epoch [854/10044], Batch [3/7], Loss: 0.0104, Accuracy: 99.83%, Grad Norm: 0.09149\n",
      "Epoch [854/10044], Batch [4/7], Loss: 0.0119, Accuracy: 99.80%, Grad Norm: 0.08008\n",
      "Epoch [854/10044], Batch [5/7], Loss: 0.0102, Accuracy: 99.81%, Grad Norm: 0.09316\n",
      "Epoch [854/10044], Batch [6/7], Loss: 0.0129, Accuracy: 99.70%, Grad Norm: 0.13674\n",
      "Epoch [854/10044], Batch [7/7], Loss: 0.0142, Accuracy: 99.62%, Grad Norm: 0.22222\n",
      "Epoch [854/10044], Loss: 0.0142\n",
      "Epoch [855/10044], Batch [1/7], Loss: 0.0105, Accuracy: 99.79%, Grad Norm: 0.07890\n",
      "Epoch [855/10044], Batch [2/7], Loss: 0.0143, Accuracy: 99.69%, Grad Norm: 0.14246\n",
      "Epoch [855/10044], Batch [3/7], Loss: 0.0080, Accuracy: 99.93%, Grad Norm: 0.05593\n",
      "Epoch [855/10044], Batch [4/7], Loss: 0.0106, Accuracy: 99.83%, Grad Norm: 0.07350\n",
      "Epoch [855/10044], Batch [5/7], Loss: 0.0120, Accuracy: 99.81%, Grad Norm: 0.10679\n",
      "Epoch [855/10044], Batch [6/7], Loss: 0.0121, Accuracy: 99.77%, Grad Norm: 0.11603\n",
      "Epoch [855/10044], Batch [7/7], Loss: 0.0107, Accuracy: 99.78%, Grad Norm: 0.16875\n",
      "Epoch [855/10044], Loss: 0.0107\n",
      "Epoch [856/10044], Batch [1/7], Loss: 0.0101, Accuracy: 99.84%, Grad Norm: 0.08761\n",
      "Epoch [856/10044], Batch [2/7], Loss: 0.0133, Accuracy: 99.67%, Grad Norm: 0.12680\n",
      "Epoch [856/10044], Batch [3/7], Loss: 0.0107, Accuracy: 99.83%, Grad Norm: 0.09216\n",
      "Epoch [856/10044], Batch [4/7], Loss: 0.0118, Accuracy: 99.76%, Grad Norm: 0.08682\n",
      "Epoch [856/10044], Batch [5/7], Loss: 0.0106, Accuracy: 99.79%, Grad Norm: 0.08957\n",
      "Epoch [856/10044], Batch [6/7], Loss: 0.0126, Accuracy: 99.68%, Grad Norm: 0.12576\n",
      "Epoch [856/10044], Batch [7/7], Loss: 0.0082, Accuracy: 99.87%, Grad Norm: 0.12426\n",
      "Epoch [856/10044], Loss: 0.0082\n",
      "Epoch [857/10044], Batch [1/7], Loss: 0.0122, Accuracy: 99.79%, Grad Norm: 0.11443\n",
      "Epoch [857/10044], Batch [2/7], Loss: 0.0121, Accuracy: 99.72%, Grad Norm: 0.11137\n",
      "Epoch [857/10044], Batch [3/7], Loss: 0.0098, Accuracy: 99.88%, Grad Norm: 0.08317\n",
      "Epoch [857/10044], Batch [4/7], Loss: 0.0115, Accuracy: 99.77%, Grad Norm: 0.09409\n",
      "Epoch [857/10044], Batch [5/7], Loss: 0.0096, Accuracy: 99.80%, Grad Norm: 0.09831\n",
      "Epoch [857/10044], Batch [6/7], Loss: 0.0141, Accuracy: 99.65%, Grad Norm: 0.15177\n",
      "Epoch [857/10044], Batch [7/7], Loss: 0.0097, Accuracy: 99.80%, Grad Norm: 0.11193\n",
      "Epoch [857/10044], Loss: 0.0097\n",
      "Epoch [858/10044], Batch [1/7], Loss: 0.0138, Accuracy: 99.77%, Grad Norm: 0.10974\n",
      "Epoch [858/10044], Batch [2/7], Loss: 0.0133, Accuracy: 99.70%, Grad Norm: 0.11387\n",
      "Epoch [858/10044], Batch [3/7], Loss: 0.0088, Accuracy: 99.87%, Grad Norm: 0.06661\n",
      "Epoch [858/10044], Batch [4/7], Loss: 0.0137, Accuracy: 99.73%, Grad Norm: 0.11076\n",
      "Epoch [858/10044], Batch [5/7], Loss: 0.0117, Accuracy: 99.72%, Grad Norm: 0.11982\n",
      "Epoch [858/10044], Batch [6/7], Loss: 0.0135, Accuracy: 99.74%, Grad Norm: 0.14612\n",
      "Epoch [858/10044], Batch [7/7], Loss: 0.0116, Accuracy: 99.78%, Grad Norm: 0.13537\n",
      "Epoch [858/10044], Loss: 0.0116\n",
      "Epoch [859/10044], Batch [1/7], Loss: 0.0112, Accuracy: 99.76%, Grad Norm: 0.11220\n",
      "Epoch [859/10044], Batch [2/7], Loss: 0.0121, Accuracy: 99.77%, Grad Norm: 0.10179\n",
      "Epoch [859/10044], Batch [3/7], Loss: 0.0086, Accuracy: 99.87%, Grad Norm: 0.06477\n",
      "Epoch [859/10044], Batch [4/7], Loss: 0.0114, Accuracy: 99.76%, Grad Norm: 0.09565\n",
      "Epoch [859/10044], Batch [5/7], Loss: 0.0090, Accuracy: 99.85%, Grad Norm: 0.08550\n",
      "Epoch [859/10044], Batch [6/7], Loss: 0.0133, Accuracy: 99.71%, Grad Norm: 0.11636\n",
      "Epoch [859/10044], Batch [7/7], Loss: 0.0100, Accuracy: 99.75%, Grad Norm: 0.12816\n",
      "Epoch [859/10044], Loss: 0.0100\n",
      "Epoch [860/10044], Batch [1/7], Loss: 0.0111, Accuracy: 99.81%, Grad Norm: 0.09693\n",
      "Epoch [860/10044], Batch [2/7], Loss: 0.0132, Accuracy: 99.69%, Grad Norm: 0.11855\n",
      "Epoch [860/10044], Batch [3/7], Loss: 0.0096, Accuracy: 99.84%, Grad Norm: 0.07405\n",
      "Epoch [860/10044], Batch [4/7], Loss: 0.0106, Accuracy: 99.80%, Grad Norm: 0.08250\n",
      "Epoch [860/10044], Batch [5/7], Loss: 0.0116, Accuracy: 99.70%, Grad Norm: 0.09752\n",
      "Epoch [860/10044], Batch [6/7], Loss: 0.0097, Accuracy: 99.87%, Grad Norm: 0.09521\n",
      "Epoch [860/10044], Batch [7/7], Loss: 0.0087, Accuracy: 99.82%, Grad Norm: 0.12738\n",
      "Epoch [860/10044], Loss: 0.0087\n",
      "Epoch [861/10044], Batch [1/7], Loss: 0.0107, Accuracy: 99.80%, Grad Norm: 0.08307\n",
      "Epoch [861/10044], Batch [2/7], Loss: 0.0106, Accuracy: 99.79%, Grad Norm: 0.09222\n",
      "Epoch [861/10044], Batch [3/7], Loss: 0.0097, Accuracy: 99.82%, Grad Norm: 0.07209\n",
      "Epoch [861/10044], Batch [4/7], Loss: 0.0105, Accuracy: 99.80%, Grad Norm: 0.08461\n",
      "Epoch [861/10044], Batch [5/7], Loss: 0.0105, Accuracy: 99.79%, Grad Norm: 0.09586\n",
      "Epoch [861/10044], Batch [6/7], Loss: 0.0109, Accuracy: 99.77%, Grad Norm: 0.08608\n",
      "Epoch [861/10044], Batch [7/7], Loss: 0.0091, Accuracy: 99.85%, Grad Norm: 0.14845\n",
      "Epoch [861/10044], Loss: 0.0091\n",
      "Epoch [862/10044], Batch [1/7], Loss: 0.0102, Accuracy: 99.77%, Grad Norm: 0.08153\n",
      "Epoch [862/10044], Batch [2/7], Loss: 0.0115, Accuracy: 99.75%, Grad Norm: 0.11367\n",
      "Epoch [862/10044], Batch [3/7], Loss: 0.0096, Accuracy: 99.86%, Grad Norm: 0.07291\n",
      "Epoch [862/10044], Batch [4/7], Loss: 0.0115, Accuracy: 99.77%, Grad Norm: 0.10096\n",
      "Epoch [862/10044], Batch [5/7], Loss: 0.0101, Accuracy: 99.83%, Grad Norm: 0.09668\n",
      "Epoch [862/10044], Batch [6/7], Loss: 0.0121, Accuracy: 99.71%, Grad Norm: 0.11977\n",
      "Epoch [862/10044], Batch [7/7], Loss: 0.0084, Accuracy: 99.88%, Grad Norm: 0.11323\n",
      "Epoch [862/10044], Loss: 0.0084\n",
      "Epoch [863/10044], Batch [1/7], Loss: 0.0100, Accuracy: 99.83%, Grad Norm: 0.09320\n",
      "Epoch [863/10044], Batch [2/7], Loss: 0.0120, Accuracy: 99.74%, Grad Norm: 0.10652\n",
      "Epoch [863/10044], Batch [3/7], Loss: 0.0093, Accuracy: 99.85%, Grad Norm: 0.07179\n",
      "Epoch [863/10044], Batch [4/7], Loss: 0.0102, Accuracy: 99.82%, Grad Norm: 0.08791\n",
      "Epoch [863/10044], Batch [5/7], Loss: 0.0100, Accuracy: 99.79%, Grad Norm: 0.09401\n",
      "Epoch [863/10044], Batch [6/7], Loss: 0.0120, Accuracy: 99.63%, Grad Norm: 0.10769\n",
      "Epoch [863/10044], Batch [7/7], Loss: 0.0088, Accuracy: 99.85%, Grad Norm: 0.11117\n",
      "Epoch [863/10044], Loss: 0.0088\n",
      "Epoch [864/10044], Batch [1/7], Loss: 0.0104, Accuracy: 99.76%, Grad Norm: 0.09471\n",
      "Epoch [864/10044], Batch [2/7], Loss: 0.0126, Accuracy: 99.69%, Grad Norm: 0.12006\n",
      "Epoch [864/10044], Batch [3/7], Loss: 0.0090, Accuracy: 99.85%, Grad Norm: 0.08065\n",
      "Epoch [864/10044], Batch [4/7], Loss: 0.0110, Accuracy: 99.77%, Grad Norm: 0.10398\n",
      "Epoch [864/10044], Batch [5/7], Loss: 0.0105, Accuracy: 99.80%, Grad Norm: 0.10326\n",
      "Epoch [864/10044], Batch [6/7], Loss: 0.0107, Accuracy: 99.76%, Grad Norm: 0.09680\n",
      "Epoch [864/10044], Batch [7/7], Loss: 0.0086, Accuracy: 99.75%, Grad Norm: 0.13567\n",
      "Epoch [864/10044], Loss: 0.0086\n",
      "Epoch [865/10044], Batch [1/7], Loss: 0.0105, Accuracy: 99.78%, Grad Norm: 0.09622\n",
      "Epoch [865/10044], Batch [2/7], Loss: 0.0126, Accuracy: 99.67%, Grad Norm: 0.10569\n",
      "Epoch [865/10044], Batch [3/7], Loss: 0.0089, Accuracy: 99.87%, Grad Norm: 0.06821\n",
      "Epoch [865/10044], Batch [4/7], Loss: 0.0114, Accuracy: 99.72%, Grad Norm: 0.09654\n",
      "Epoch [865/10044], Batch [5/7], Loss: 0.0110, Accuracy: 99.74%, Grad Norm: 0.11426\n",
      "Epoch [865/10044], Batch [6/7], Loss: 0.0121, Accuracy: 99.70%, Grad Norm: 0.11268\n",
      "Epoch [865/10044], Batch [7/7], Loss: 0.0083, Accuracy: 99.87%, Grad Norm: 0.10262\n",
      "Epoch [865/10044], Loss: 0.0083\n",
      "Epoch [866/10044], Batch [1/7], Loss: 0.0095, Accuracy: 99.82%, Grad Norm: 0.08003\n",
      "Epoch [866/10044], Batch [2/7], Loss: 0.0117, Accuracy: 99.75%, Grad Norm: 0.11879\n",
      "Epoch [866/10044], Batch [3/7], Loss: 0.0083, Accuracy: 99.88%, Grad Norm: 0.06509\n",
      "Epoch [866/10044], Batch [4/7], Loss: 0.0115, Accuracy: 99.80%, Grad Norm: 0.08638\n",
      "Epoch [866/10044], Batch [5/7], Loss: 0.0102, Accuracy: 99.84%, Grad Norm: 0.10353\n",
      "Epoch [866/10044], Batch [6/7], Loss: 0.0139, Accuracy: 99.67%, Grad Norm: 0.13929\n",
      "Epoch [866/10044], Batch [7/7], Loss: 0.0071, Accuracy: 99.90%, Grad Norm: 0.08314\n",
      "Epoch [866/10044], Loss: 0.0071\n",
      "Epoch [867/10044], Batch [1/7], Loss: 0.0097, Accuracy: 99.86%, Grad Norm: 0.07042\n",
      "Epoch [867/10044], Batch [2/7], Loss: 0.0127, Accuracy: 99.64%, Grad Norm: 0.12481\n",
      "Epoch [867/10044], Batch [3/7], Loss: 0.0089, Accuracy: 99.87%, Grad Norm: 0.06888\n",
      "Epoch [867/10044], Batch [4/7], Loss: 0.0113, Accuracy: 99.77%, Grad Norm: 0.08921\n",
      "Epoch [867/10044], Batch [5/7], Loss: 0.0104, Accuracy: 99.77%, Grad Norm: 0.11206\n",
      "Epoch [867/10044], Batch [6/7], Loss: 0.0123, Accuracy: 99.68%, Grad Norm: 0.13127\n",
      "Epoch [867/10044], Batch [7/7], Loss: 0.0090, Accuracy: 99.78%, Grad Norm: 0.12205\n",
      "Epoch [867/10044], Loss: 0.0090\n",
      "Epoch [868/10044], Batch [1/7], Loss: 0.0103, Accuracy: 99.78%, Grad Norm: 0.09158\n",
      "Epoch [868/10044], Batch [2/7], Loss: 0.0123, Accuracy: 99.74%, Grad Norm: 0.10869\n",
      "Epoch [868/10044], Batch [3/7], Loss: 0.0084, Accuracy: 99.93%, Grad Norm: 0.06569\n",
      "Epoch [868/10044], Batch [4/7], Loss: 0.0116, Accuracy: 99.75%, Grad Norm: 0.10141\n",
      "Epoch [868/10044], Batch [5/7], Loss: 0.0104, Accuracy: 99.79%, Grad Norm: 0.10045\n",
      "Epoch [868/10044], Batch [6/7], Loss: 0.0132, Accuracy: 99.66%, Grad Norm: 0.12283\n",
      "Epoch [868/10044], Batch [7/7], Loss: 0.0084, Accuracy: 99.83%, Grad Norm: 0.11042\n",
      "Epoch [868/10044], Loss: 0.0084\n",
      "Epoch [869/10044], Batch [1/7], Loss: 0.0110, Accuracy: 99.78%, Grad Norm: 0.09362\n",
      "Epoch [869/10044], Batch [2/7], Loss: 0.0116, Accuracy: 99.73%, Grad Norm: 0.10198\n",
      "Epoch [869/10044], Batch [3/7], Loss: 0.0086, Accuracy: 99.88%, Grad Norm: 0.06975\n",
      "Epoch [869/10044], Batch [4/7], Loss: 0.0128, Accuracy: 99.70%, Grad Norm: 0.09933\n",
      "Epoch [869/10044], Batch [5/7], Loss: 0.0092, Accuracy: 99.83%, Grad Norm: 0.08851\n",
      "Epoch [869/10044], Batch [6/7], Loss: 0.0126, Accuracy: 99.70%, Grad Norm: 0.11701\n",
      "Epoch [869/10044], Batch [7/7], Loss: 0.0088, Accuracy: 99.85%, Grad Norm: 0.11669\n",
      "Epoch [869/10044], Loss: 0.0088\n",
      "Epoch [870/10044], Batch [1/7], Loss: 0.0100, Accuracy: 99.80%, Grad Norm: 0.08607\n",
      "Epoch [870/10044], Batch [2/7], Loss: 0.0120, Accuracy: 99.72%, Grad Norm: 0.10721\n",
      "Epoch [870/10044], Batch [3/7], Loss: 0.0089, Accuracy: 99.85%, Grad Norm: 0.07567\n",
      "Epoch [870/10044], Batch [4/7], Loss: 0.0100, Accuracy: 99.80%, Grad Norm: 0.07649\n",
      "Epoch [870/10044], Batch [5/7], Loss: 0.0111, Accuracy: 99.77%, Grad Norm: 0.11020\n",
      "Epoch [870/10044], Batch [6/7], Loss: 0.0115, Accuracy: 99.72%, Grad Norm: 0.12024\n",
      "Epoch [870/10044], Batch [7/7], Loss: 0.0087, Accuracy: 99.80%, Grad Norm: 0.12248\n",
      "Epoch [870/10044], Loss: 0.0087\n",
      "Epoch [871/10044], Batch [1/7], Loss: 0.0097, Accuracy: 99.83%, Grad Norm: 0.09739\n",
      "Epoch [871/10044], Batch [2/7], Loss: 0.0121, Accuracy: 99.72%, Grad Norm: 0.10300\n",
      "Epoch [871/10044], Batch [3/7], Loss: 0.0086, Accuracy: 99.88%, Grad Norm: 0.06762\n",
      "Epoch [871/10044], Batch [4/7], Loss: 0.0118, Accuracy: 99.73%, Grad Norm: 0.12628\n",
      "Epoch [871/10044], Batch [5/7], Loss: 0.0091, Accuracy: 99.82%, Grad Norm: 0.08880\n",
      "Epoch [871/10044], Batch [6/7], Loss: 0.0132, Accuracy: 99.65%, Grad Norm: 0.13097\n",
      "Epoch [871/10044], Batch [7/7], Loss: 0.0092, Accuracy: 99.80%, Grad Norm: 0.16643\n",
      "Epoch [871/10044], Loss: 0.0092\n",
      "Epoch [872/10044], Batch [1/7], Loss: 0.0101, Accuracy: 99.82%, Grad Norm: 0.07210\n",
      "Epoch [872/10044], Batch [2/7], Loss: 0.0112, Accuracy: 99.75%, Grad Norm: 0.10905\n",
      "Epoch [872/10044], Batch [3/7], Loss: 0.0086, Accuracy: 99.87%, Grad Norm: 0.07783\n",
      "Epoch [872/10044], Batch [4/7], Loss: 0.0124, Accuracy: 99.74%, Grad Norm: 0.11750\n",
      "Epoch [872/10044], Batch [5/7], Loss: 0.0098, Accuracy: 99.82%, Grad Norm: 0.08872\n",
      "Epoch [872/10044], Batch [6/7], Loss: 0.0121, Accuracy: 99.72%, Grad Norm: 0.11937\n",
      "Epoch [872/10044], Batch [7/7], Loss: 0.0101, Accuracy: 99.78%, Grad Norm: 0.15625\n",
      "Epoch [872/10044], Loss: 0.0101\n",
      "Epoch [873/10044], Batch [1/7], Loss: 0.0090, Accuracy: 99.86%, Grad Norm: 0.08913\n",
      "Epoch [873/10044], Batch [2/7], Loss: 0.0117, Accuracy: 99.74%, Grad Norm: 0.11156\n",
      "Epoch [873/10044], Batch [3/7], Loss: 0.0086, Accuracy: 99.89%, Grad Norm: 0.06441\n",
      "Epoch [873/10044], Batch [4/7], Loss: 0.0115, Accuracy: 99.79%, Grad Norm: 0.09071\n",
      "Epoch [873/10044], Batch [5/7], Loss: 0.0096, Accuracy: 99.82%, Grad Norm: 0.09756\n",
      "Epoch [873/10044], Batch [6/7], Loss: 0.0122, Accuracy: 99.73%, Grad Norm: 0.12834\n",
      "Epoch [873/10044], Batch [7/7], Loss: 0.0081, Accuracy: 99.80%, Grad Norm: 0.13283\n",
      "Epoch [873/10044], Loss: 0.0081\n",
      "Epoch [874/10044], Batch [1/7], Loss: 0.0133, Accuracy: 99.74%, Grad Norm: 0.12202\n",
      "Epoch [874/10044], Batch [2/7], Loss: 0.0125, Accuracy: 99.71%, Grad Norm: 0.13108\n",
      "Epoch [874/10044], Batch [3/7], Loss: 0.0091, Accuracy: 99.87%, Grad Norm: 0.08413\n",
      "Epoch [874/10044], Batch [4/7], Loss: 0.0110, Accuracy: 99.81%, Grad Norm: 0.09422\n",
      "Epoch [874/10044], Batch [5/7], Loss: 0.0119, Accuracy: 99.70%, Grad Norm: 0.15329\n",
      "Epoch [874/10044], Batch [6/7], Loss: 0.0123, Accuracy: 99.73%, Grad Norm: 0.12321\n",
      "Epoch [874/10044], Batch [7/7], Loss: 0.0107, Accuracy: 99.72%, Grad Norm: 0.16242\n",
      "Epoch [874/10044], Loss: 0.0107\n",
      "Epoch [875/10044], Batch [1/7], Loss: 0.0106, Accuracy: 99.79%, Grad Norm: 0.10261\n",
      "Epoch [875/10044], Batch [2/7], Loss: 0.0141, Accuracy: 99.67%, Grad Norm: 0.15247\n",
      "Epoch [875/10044], Batch [3/7], Loss: 0.0099, Accuracy: 99.86%, Grad Norm: 0.10918\n",
      "Epoch [875/10044], Batch [4/7], Loss: 0.0120, Accuracy: 99.72%, Grad Norm: 0.11142\n",
      "Epoch [875/10044], Batch [5/7], Loss: 0.0113, Accuracy: 99.75%, Grad Norm: 0.11468\n",
      "Epoch [875/10044], Batch [6/7], Loss: 0.0120, Accuracy: 99.74%, Grad Norm: 0.11250\n",
      "Epoch [875/10044], Batch [7/7], Loss: 0.0103, Accuracy: 99.75%, Grad Norm: 0.15670\n",
      "Epoch [875/10044], Loss: 0.0103\n",
      "Epoch [876/10044], Batch [1/7], Loss: 0.0108, Accuracy: 99.77%, Grad Norm: 0.11564\n",
      "Epoch [876/10044], Batch [2/7], Loss: 0.0145, Accuracy: 99.65%, Grad Norm: 0.14701\n",
      "Epoch [876/10044], Batch [3/7], Loss: 0.0102, Accuracy: 99.82%, Grad Norm: 0.10741\n",
      "Epoch [876/10044], Batch [4/7], Loss: 0.0108, Accuracy: 99.75%, Grad Norm: 0.09243\n",
      "Epoch [876/10044], Batch [5/7], Loss: 0.0109, Accuracy: 99.80%, Grad Norm: 0.10315\n",
      "Epoch [876/10044], Batch [6/7], Loss: 0.0140, Accuracy: 99.64%, Grad Norm: 0.14445\n",
      "Epoch [876/10044], Batch [7/7], Loss: 0.0102, Accuracy: 99.77%, Grad Norm: 0.14897\n",
      "Epoch [876/10044], Loss: 0.0102\n",
      "Epoch [877/10044], Batch [1/7], Loss: 0.0099, Accuracy: 99.80%, Grad Norm: 0.09654\n",
      "Epoch [877/10044], Batch [2/7], Loss: 0.0123, Accuracy: 99.69%, Grad Norm: 0.12577\n",
      "Epoch [877/10044], Batch [3/7], Loss: 0.0108, Accuracy: 99.82%, Grad Norm: 0.10004\n",
      "Epoch [877/10044], Batch [4/7], Loss: 0.0116, Accuracy: 99.75%, Grad Norm: 0.11586\n",
      "Epoch [877/10044], Batch [5/7], Loss: 0.0095, Accuracy: 99.82%, Grad Norm: 0.10272\n",
      "Epoch [877/10044], Batch [6/7], Loss: 0.0155, Accuracy: 99.72%, Grad Norm: 0.13674\n",
      "Epoch [877/10044], Batch [7/7], Loss: 0.0075, Accuracy: 99.88%, Grad Norm: 0.11397\n",
      "Epoch [877/10044], Loss: 0.0075\n",
      "Epoch [878/10044], Batch [1/7], Loss: 0.0109, Accuracy: 99.82%, Grad Norm: 0.08820\n",
      "Epoch [878/10044], Batch [2/7], Loss: 0.0118, Accuracy: 99.74%, Grad Norm: 0.11883\n",
      "Epoch [878/10044], Batch [3/7], Loss: 0.0107, Accuracy: 99.80%, Grad Norm: 0.09691\n",
      "Epoch [878/10044], Batch [4/7], Loss: 0.0124, Accuracy: 99.72%, Grad Norm: 0.09770\n",
      "Epoch [878/10044], Batch [5/7], Loss: 0.0112, Accuracy: 99.76%, Grad Norm: 0.11553\n",
      "Epoch [878/10044], Batch [6/7], Loss: 0.0123, Accuracy: 99.75%, Grad Norm: 0.11458\n",
      "Epoch [878/10044], Batch [7/7], Loss: 0.0087, Accuracy: 99.82%, Grad Norm: 0.12170\n",
      "Epoch [878/10044], Loss: 0.0087\n",
      "Epoch [879/10044], Batch [1/7], Loss: 0.0119, Accuracy: 99.72%, Grad Norm: 0.12781\n",
      "Epoch [879/10044], Batch [2/7], Loss: 0.0134, Accuracy: 99.72%, Grad Norm: 0.14486\n",
      "Epoch [879/10044], Batch [3/7], Loss: 0.0100, Accuracy: 99.82%, Grad Norm: 0.08968\n",
      "Epoch [879/10044], Batch [4/7], Loss: 0.0110, Accuracy: 99.77%, Grad Norm: 0.09180\n",
      "Epoch [879/10044], Batch [5/7], Loss: 0.0110, Accuracy: 99.78%, Grad Norm: 0.11100\n",
      "Epoch [879/10044], Batch [6/7], Loss: 0.0123, Accuracy: 99.66%, Grad Norm: 0.11565\n",
      "Epoch [879/10044], Batch [7/7], Loss: 0.0098, Accuracy: 99.82%, Grad Norm: 0.16889\n",
      "Epoch [879/10044], Loss: 0.0098\n",
      "Epoch [880/10044], Batch [1/7], Loss: 0.0124, Accuracy: 99.73%, Grad Norm: 0.12808\n",
      "Epoch [880/10044], Batch [2/7], Loss: 0.0113, Accuracy: 99.73%, Grad Norm: 0.10412\n",
      "Epoch [880/10044], Batch [3/7], Loss: 0.0090, Accuracy: 99.85%, Grad Norm: 0.07200\n",
      "Epoch [880/10044], Batch [4/7], Loss: 0.0093, Accuracy: 99.86%, Grad Norm: 0.07022\n",
      "Epoch [880/10044], Batch [5/7], Loss: 0.0108, Accuracy: 99.73%, Grad Norm: 0.11552\n",
      "Epoch [880/10044], Batch [6/7], Loss: 0.0115, Accuracy: 99.75%, Grad Norm: 0.10444\n",
      "Epoch [880/10044], Batch [7/7], Loss: 0.0120, Accuracy: 99.75%, Grad Norm: 0.17042\n",
      "Epoch [880/10044], Loss: 0.0120\n",
      "Epoch [881/10044], Batch [1/7], Loss: 0.0117, Accuracy: 99.74%, Grad Norm: 0.11725\n",
      "Epoch [881/10044], Batch [2/7], Loss: 0.0109, Accuracy: 99.75%, Grad Norm: 0.10269\n",
      "Epoch [881/10044], Batch [3/7], Loss: 0.0084, Accuracy: 99.92%, Grad Norm: 0.06449\n",
      "Epoch [881/10044], Batch [4/7], Loss: 0.0108, Accuracy: 99.76%, Grad Norm: 0.08077\n",
      "Epoch [881/10044], Batch [5/7], Loss: 0.0105, Accuracy: 99.77%, Grad Norm: 0.11686\n",
      "Epoch [881/10044], Batch [6/7], Loss: 0.0125, Accuracy: 99.71%, Grad Norm: 0.11754\n",
      "Epoch [881/10044], Batch [7/7], Loss: 0.0127, Accuracy: 99.67%, Grad Norm: 0.22139\n",
      "Epoch [881/10044], Loss: 0.0127\n",
      "Epoch [882/10044], Batch [1/7], Loss: 0.0115, Accuracy: 99.78%, Grad Norm: 0.11037\n",
      "Epoch [882/10044], Batch [2/7], Loss: 0.0119, Accuracy: 99.79%, Grad Norm: 0.13267\n",
      "Epoch [882/10044], Batch [3/7], Loss: 0.0097, Accuracy: 99.80%, Grad Norm: 0.08710\n",
      "Epoch [882/10044], Batch [4/7], Loss: 0.0113, Accuracy: 99.77%, Grad Norm: 0.08504\n",
      "Epoch [882/10044], Batch [5/7], Loss: 0.0128, Accuracy: 99.71%, Grad Norm: 0.15740\n",
      "Epoch [882/10044], Batch [6/7], Loss: 0.0127, Accuracy: 99.74%, Grad Norm: 0.10982\n",
      "Epoch [882/10044], Batch [7/7], Loss: 0.0105, Accuracy: 99.75%, Grad Norm: 0.17280\n",
      "Epoch [882/10044], Loss: 0.0105\n",
      "Epoch [883/10044], Batch [1/7], Loss: 0.0100, Accuracy: 99.82%, Grad Norm: 0.09336\n",
      "Epoch [883/10044], Batch [2/7], Loss: 0.0130, Accuracy: 99.70%, Grad Norm: 0.12525\n",
      "Epoch [883/10044], Batch [3/7], Loss: 0.0092, Accuracy: 99.85%, Grad Norm: 0.07903\n",
      "Epoch [883/10044], Batch [4/7], Loss: 0.0104, Accuracy: 99.79%, Grad Norm: 0.09181\n",
      "Epoch [883/10044], Batch [5/7], Loss: 0.0103, Accuracy: 99.78%, Grad Norm: 0.11237\n",
      "Epoch [883/10044], Batch [6/7], Loss: 0.0130, Accuracy: 99.67%, Grad Norm: 0.13767\n",
      "Epoch [883/10044], Batch [7/7], Loss: 0.0090, Accuracy: 99.83%, Grad Norm: 0.11624\n",
      "Epoch [883/10044], Loss: 0.0090\n",
      "Epoch [884/10044], Batch [1/7], Loss: 0.0098, Accuracy: 99.82%, Grad Norm: 0.08809\n",
      "Epoch [884/10044], Batch [2/7], Loss: 0.0113, Accuracy: 99.73%, Grad Norm: 0.10444\n",
      "Epoch [884/10044], Batch [3/7], Loss: 0.0087, Accuracy: 99.87%, Grad Norm: 0.07021\n",
      "Epoch [884/10044], Batch [4/7], Loss: 0.0106, Accuracy: 99.82%, Grad Norm: 0.08987\n",
      "Epoch [884/10044], Batch [5/7], Loss: 0.0101, Accuracy: 99.77%, Grad Norm: 0.09542\n",
      "Epoch [884/10044], Batch [6/7], Loss: 0.0122, Accuracy: 99.72%, Grad Norm: 0.13219\n",
      "Epoch [884/10044], Batch [7/7], Loss: 0.0084, Accuracy: 99.78%, Grad Norm: 0.12556\n",
      "Epoch [884/10044], Loss: 0.0084\n",
      "Epoch [885/10044], Batch [1/7], Loss: 0.0107, Accuracy: 99.79%, Grad Norm: 0.10457\n",
      "Epoch [885/10044], Batch [2/7], Loss: 0.0125, Accuracy: 99.70%, Grad Norm: 0.12859\n",
      "Epoch [885/10044], Batch [3/7], Loss: 0.0095, Accuracy: 99.87%, Grad Norm: 0.08296\n",
      "Epoch [885/10044], Batch [4/7], Loss: 0.0106, Accuracy: 99.78%, Grad Norm: 0.08966\n",
      "Epoch [885/10044], Batch [5/7], Loss: 0.0120, Accuracy: 99.76%, Grad Norm: 0.10816\n",
      "Epoch [885/10044], Batch [6/7], Loss: 0.0117, Accuracy: 99.73%, Grad Norm: 0.10668\n",
      "Epoch [885/10044], Batch [7/7], Loss: 0.0081, Accuracy: 99.82%, Grad Norm: 0.11898\n",
      "Epoch [885/10044], Loss: 0.0081\n",
      "Epoch [886/10044], Batch [1/7], Loss: 0.0108, Accuracy: 99.78%, Grad Norm: 0.10468\n",
      "Epoch [886/10044], Batch [2/7], Loss: 0.0121, Accuracy: 99.72%, Grad Norm: 0.10928\n",
      "Epoch [886/10044], Batch [3/7], Loss: 0.0095, Accuracy: 99.82%, Grad Norm: 0.09475\n",
      "Epoch [886/10044], Batch [4/7], Loss: 0.0105, Accuracy: 99.81%, Grad Norm: 0.08851\n",
      "Epoch [886/10044], Batch [5/7], Loss: 0.0107, Accuracy: 99.74%, Grad Norm: 0.09615\n",
      "Epoch [886/10044], Batch [6/7], Loss: 0.0102, Accuracy: 99.77%, Grad Norm: 0.09045\n",
      "Epoch [886/10044], Batch [7/7], Loss: 0.0081, Accuracy: 99.88%, Grad Norm: 0.12770\n",
      "Epoch [886/10044], Loss: 0.0081\n",
      "Epoch [887/10044], Batch [1/7], Loss: 0.0132, Accuracy: 99.71%, Grad Norm: 0.13198\n",
      "Epoch [887/10044], Batch [2/7], Loss: 0.0117, Accuracy: 99.73%, Grad Norm: 0.12907\n",
      "Epoch [887/10044], Batch [3/7], Loss: 0.0092, Accuracy: 99.83%, Grad Norm: 0.08342\n",
      "Epoch [887/10044], Batch [4/7], Loss: 0.0099, Accuracy: 99.82%, Grad Norm: 0.07970\n",
      "Epoch [887/10044], Batch [5/7], Loss: 0.0105, Accuracy: 99.77%, Grad Norm: 0.09256\n",
      "Epoch [887/10044], Batch [6/7], Loss: 0.0143, Accuracy: 99.62%, Grad Norm: 0.14207\n",
      "Epoch [887/10044], Batch [7/7], Loss: 0.0080, Accuracy: 99.80%, Grad Norm: 0.12317\n",
      "Epoch [887/10044], Loss: 0.0080\n",
      "Epoch [888/10044], Batch [1/7], Loss: 0.0097, Accuracy: 99.82%, Grad Norm: 0.09775\n",
      "Epoch [888/10044], Batch [2/7], Loss: 0.0117, Accuracy: 99.71%, Grad Norm: 0.11765\n",
      "Epoch [888/10044], Batch [3/7], Loss: 0.0082, Accuracy: 99.88%, Grad Norm: 0.07373\n",
      "Epoch [888/10044], Batch [4/7], Loss: 0.0103, Accuracy: 99.78%, Grad Norm: 0.09481\n",
      "Epoch [888/10044], Batch [5/7], Loss: 0.0100, Accuracy: 99.83%, Grad Norm: 0.10569\n",
      "Epoch [888/10044], Batch [6/7], Loss: 0.0119, Accuracy: 99.75%, Grad Norm: 0.12638\n",
      "Epoch [888/10044], Batch [7/7], Loss: 0.0084, Accuracy: 99.83%, Grad Norm: 0.10614\n",
      "Epoch [888/10044], Loss: 0.0084\n",
      "Epoch [889/10044], Batch [1/7], Loss: 0.0096, Accuracy: 99.82%, Grad Norm: 0.08921\n",
      "Epoch [889/10044], Batch [2/7], Loss: 0.0121, Accuracy: 99.72%, Grad Norm: 0.10675\n",
      "Epoch [889/10044], Batch [3/7], Loss: 0.0098, Accuracy: 99.82%, Grad Norm: 0.08706\n",
      "Epoch [889/10044], Batch [4/7], Loss: 0.0095, Accuracy: 99.82%, Grad Norm: 0.09238\n",
      "Epoch [889/10044], Batch [5/7], Loss: 0.0100, Accuracy: 99.75%, Grad Norm: 0.10719\n",
      "Epoch [889/10044], Batch [6/7], Loss: 0.0126, Accuracy: 99.63%, Grad Norm: 0.15200\n",
      "Epoch [889/10044], Batch [7/7], Loss: 0.0077, Accuracy: 99.87%, Grad Norm: 0.10579\n",
      "Epoch [889/10044], Loss: 0.0077\n",
      "Epoch [890/10044], Batch [1/7], Loss: 0.0098, Accuracy: 99.79%, Grad Norm: 0.08320\n",
      "Epoch [890/10044], Batch [2/7], Loss: 0.0112, Accuracy: 99.73%, Grad Norm: 0.10616\n",
      "Epoch [890/10044], Batch [3/7], Loss: 0.0092, Accuracy: 99.85%, Grad Norm: 0.08109\n",
      "Epoch [890/10044], Batch [4/7], Loss: 0.0092, Accuracy: 99.81%, Grad Norm: 0.07541\n",
      "Epoch [890/10044], Batch [5/7], Loss: 0.0105, Accuracy: 99.79%, Grad Norm: 0.10792\n",
      "Epoch [890/10044], Batch [6/7], Loss: 0.0111, Accuracy: 99.71%, Grad Norm: 0.11109\n",
      "Epoch [890/10044], Batch [7/7], Loss: 0.0103, Accuracy: 99.80%, Grad Norm: 0.14889\n",
      "Epoch [890/10044], Loss: 0.0103\n",
      "Epoch [891/10044], Batch [1/7], Loss: 0.0083, Accuracy: 99.89%, Grad Norm: 0.06667\n",
      "Epoch [891/10044], Batch [2/7], Loss: 0.0135, Accuracy: 99.64%, Grad Norm: 0.12883\n",
      "Epoch [891/10044], Batch [3/7], Loss: 0.0083, Accuracy: 99.88%, Grad Norm: 0.06416\n",
      "Epoch [891/10044], Batch [4/7], Loss: 0.0110, Accuracy: 99.72%, Grad Norm: 0.10018\n",
      "Epoch [891/10044], Batch [5/7], Loss: 0.0094, Accuracy: 99.81%, Grad Norm: 0.09142\n",
      "Epoch [891/10044], Batch [6/7], Loss: 0.0109, Accuracy: 99.76%, Grad Norm: 0.09766\n",
      "Epoch [891/10044], Batch [7/7], Loss: 0.0101, Accuracy: 99.73%, Grad Norm: 0.16466\n",
      "Epoch [891/10044], Loss: 0.0101\n",
      "Epoch [892/10044], Batch [1/7], Loss: 0.0107, Accuracy: 99.77%, Grad Norm: 0.09941\n",
      "Epoch [892/10044], Batch [2/7], Loss: 0.0130, Accuracy: 99.71%, Grad Norm: 0.13301\n",
      "Epoch [892/10044], Batch [3/7], Loss: 0.0085, Accuracy: 99.88%, Grad Norm: 0.06513\n",
      "Epoch [892/10044], Batch [4/7], Loss: 0.0122, Accuracy: 99.72%, Grad Norm: 0.11549\n",
      "Epoch [892/10044], Batch [5/7], Loss: 0.0089, Accuracy: 99.85%, Grad Norm: 0.09798\n",
      "Epoch [892/10044], Batch [6/7], Loss: 0.0123, Accuracy: 99.67%, Grad Norm: 0.12051\n",
      "Epoch [892/10044], Batch [7/7], Loss: 0.0097, Accuracy: 99.77%, Grad Norm: 0.12213\n",
      "Epoch [892/10044], Loss: 0.0097\n",
      "Epoch [893/10044], Batch [1/7], Loss: 0.0088, Accuracy: 99.81%, Grad Norm: 0.07428\n",
      "Epoch [893/10044], Batch [2/7], Loss: 0.0124, Accuracy: 99.67%, Grad Norm: 0.14327\n",
      "Epoch [893/10044], Batch [3/7], Loss: 0.0091, Accuracy: 99.84%, Grad Norm: 0.07683\n",
      "Epoch [893/10044], Batch [4/7], Loss: 0.0098, Accuracy: 99.82%, Grad Norm: 0.11436\n",
      "Epoch [893/10044], Batch [5/7], Loss: 0.0107, Accuracy: 99.76%, Grad Norm: 0.12635\n",
      "Epoch [893/10044], Batch [6/7], Loss: 0.0101, Accuracy: 99.78%, Grad Norm: 0.10007\n",
      "Epoch [893/10044], Batch [7/7], Loss: 0.0063, Accuracy: 99.92%, Grad Norm: 0.08710\n",
      "Epoch [893/10044], Loss: 0.0063\n",
      "Epoch [894/10044], Batch [1/7], Loss: 0.0101, Accuracy: 99.79%, Grad Norm: 0.10273\n",
      "Epoch [894/10044], Batch [2/7], Loss: 0.0124, Accuracy: 99.66%, Grad Norm: 0.13420\n",
      "Epoch [894/10044], Batch [3/7], Loss: 0.0097, Accuracy: 99.82%, Grad Norm: 0.09626\n",
      "Epoch [894/10044], Batch [4/7], Loss: 0.0111, Accuracy: 99.77%, Grad Norm: 0.09508\n",
      "Epoch [894/10044], Batch [5/7], Loss: 0.0104, Accuracy: 99.79%, Grad Norm: 0.12547\n",
      "Epoch [894/10044], Batch [6/7], Loss: 0.0101, Accuracy: 99.77%, Grad Norm: 0.09930\n",
      "Epoch [894/10044], Batch [7/7], Loss: 0.0086, Accuracy: 99.82%, Grad Norm: 0.13172\n",
      "Epoch [894/10044], Loss: 0.0086\n",
      "Epoch [895/10044], Batch [1/7], Loss: 0.0102, Accuracy: 99.79%, Grad Norm: 0.08776\n",
      "Epoch [895/10044], Batch [2/7], Loss: 0.0106, Accuracy: 99.76%, Grad Norm: 0.10719\n",
      "Epoch [895/10044], Batch [3/7], Loss: 0.0087, Accuracy: 99.91%, Grad Norm: 0.08191\n",
      "Epoch [895/10044], Batch [4/7], Loss: 0.0110, Accuracy: 99.79%, Grad Norm: 0.11005\n",
      "Epoch [895/10044], Batch [5/7], Loss: 0.0110, Accuracy: 99.74%, Grad Norm: 0.10823\n",
      "Epoch [895/10044], Batch [6/7], Loss: 0.0152, Accuracy: 99.63%, Grad Norm: 0.13295\n",
      "Epoch [895/10044], Batch [7/7], Loss: 0.0076, Accuracy: 99.83%, Grad Norm: 0.13654\n",
      "Epoch [895/10044], Loss: 0.0076\n",
      "Epoch [896/10044], Batch [1/7], Loss: 0.0109, Accuracy: 99.82%, Grad Norm: 0.09199\n",
      "Epoch [896/10044], Batch [2/7], Loss: 0.0124, Accuracy: 99.69%, Grad Norm: 0.13924\n",
      "Epoch [896/10044], Batch [3/7], Loss: 0.0091, Accuracy: 99.86%, Grad Norm: 0.08334\n",
      "Epoch [896/10044], Batch [4/7], Loss: 0.0122, Accuracy: 99.69%, Grad Norm: 0.12196\n",
      "Epoch [896/10044], Batch [5/7], Loss: 0.0108, Accuracy: 99.77%, Grad Norm: 0.12173\n",
      "Epoch [896/10044], Batch [6/7], Loss: 0.0121, Accuracy: 99.62%, Grad Norm: 0.13541\n",
      "Epoch [896/10044], Batch [7/7], Loss: 0.0079, Accuracy: 99.87%, Grad Norm: 0.11045\n",
      "Epoch [896/10044], Loss: 0.0079\n",
      "Epoch [897/10044], Batch [1/7], Loss: 0.0104, Accuracy: 99.76%, Grad Norm: 0.09298\n",
      "Epoch [897/10044], Batch [2/7], Loss: 0.0147, Accuracy: 99.57%, Grad Norm: 0.15409\n",
      "Epoch [897/10044], Batch [3/7], Loss: 0.0092, Accuracy: 99.89%, Grad Norm: 0.07758\n",
      "Epoch [897/10044], Batch [4/7], Loss: 0.0128, Accuracy: 99.72%, Grad Norm: 0.12255\n",
      "Epoch [897/10044], Batch [5/7], Loss: 0.0105, Accuracy: 99.82%, Grad Norm: 0.10507\n",
      "Epoch [897/10044], Batch [6/7], Loss: 0.0132, Accuracy: 99.66%, Grad Norm: 0.12115\n",
      "Epoch [897/10044], Batch [7/7], Loss: 0.0088, Accuracy: 99.75%, Grad Norm: 0.11798\n",
      "Epoch [897/10044], Loss: 0.0088\n",
      "Epoch [898/10044], Batch [1/7], Loss: 0.0089, Accuracy: 99.85%, Grad Norm: 0.07949\n",
      "Epoch [898/10044], Batch [2/7], Loss: 0.0107, Accuracy: 99.79%, Grad Norm: 0.12309\n",
      "Epoch [898/10044], Batch [3/7], Loss: 0.0086, Accuracy: 99.86%, Grad Norm: 0.07111\n",
      "Epoch [898/10044], Batch [4/7], Loss: 0.0105, Accuracy: 99.80%, Grad Norm: 0.10521\n",
      "Epoch [898/10044], Batch [5/7], Loss: 0.0096, Accuracy: 99.77%, Grad Norm: 0.10852\n",
      "Epoch [898/10044], Batch [6/7], Loss: 0.0107, Accuracy: 99.76%, Grad Norm: 0.09367\n",
      "Epoch [898/10044], Batch [7/7], Loss: 0.0082, Accuracy: 99.85%, Grad Norm: 0.14266\n",
      "Epoch [898/10044], Loss: 0.0082\n",
      "Epoch [899/10044], Batch [1/7], Loss: 0.0091, Accuracy: 99.84%, Grad Norm: 0.08141\n",
      "Epoch [899/10044], Batch [2/7], Loss: 0.0116, Accuracy: 99.72%, Grad Norm: 0.11043\n",
      "Epoch [899/10044], Batch [3/7], Loss: 0.0083, Accuracy: 99.89%, Grad Norm: 0.06441\n",
      "Epoch [899/10044], Batch [4/7], Loss: 0.0094, Accuracy: 99.88%, Grad Norm: 0.07331\n",
      "Epoch [899/10044], Batch [5/7], Loss: 0.0112, Accuracy: 99.82%, Grad Norm: 0.11212\n",
      "Epoch [899/10044], Batch [6/7], Loss: 0.0133, Accuracy: 99.67%, Grad Norm: 0.12199\n",
      "Epoch [899/10044], Batch [7/7], Loss: 0.0093, Accuracy: 99.83%, Grad Norm: 0.12220\n",
      "Epoch [899/10044], Loss: 0.0093\n",
      "Epoch [900/10044], Batch [1/7], Loss: 0.0093, Accuracy: 99.83%, Grad Norm: 0.07690\n",
      "Epoch [900/10044], Batch [2/7], Loss: 0.0113, Accuracy: 99.73%, Grad Norm: 0.11080\n",
      "Epoch [900/10044], Batch [3/7], Loss: 0.0082, Accuracy: 99.89%, Grad Norm: 0.06569\n",
      "Epoch [900/10044], Batch [4/7], Loss: 0.0092, Accuracy: 99.87%, Grad Norm: 0.07222\n",
      "Epoch [900/10044], Batch [5/7], Loss: 0.0092, Accuracy: 99.78%, Grad Norm: 0.10822\n",
      "Epoch [900/10044], Batch [6/7], Loss: 0.0130, Accuracy: 99.66%, Grad Norm: 0.13723\n",
      "Epoch [900/10044], Batch [7/7], Loss: 0.0085, Accuracy: 99.82%, Grad Norm: 0.12144\n",
      "Epoch [900/10044], Loss: 0.0085\n",
      "Epoch [901/10044], Batch [1/7], Loss: 0.0109, Accuracy: 99.72%, Grad Norm: 0.11433\n",
      "Epoch [901/10044], Batch [2/7], Loss: 0.0108, Accuracy: 99.80%, Grad Norm: 0.09133\n",
      "Epoch [901/10044], Batch [3/7], Loss: 0.0078, Accuracy: 99.87%, Grad Norm: 0.07460\n",
      "Epoch [901/10044], Batch [4/7], Loss: 0.0097, Accuracy: 99.82%, Grad Norm: 0.08636\n",
      "Epoch [901/10044], Batch [5/7], Loss: 0.0104, Accuracy: 99.78%, Grad Norm: 0.11389\n",
      "Epoch [901/10044], Batch [6/7], Loss: 0.0124, Accuracy: 99.67%, Grad Norm: 0.12853\n",
      "Epoch [901/10044], Batch [7/7], Loss: 0.0082, Accuracy: 99.87%, Grad Norm: 0.12245\n",
      "Epoch [901/10044], Loss: 0.0082\n",
      "Epoch [902/10044], Batch [1/7], Loss: 0.0096, Accuracy: 99.79%, Grad Norm: 0.09953\n",
      "Epoch [902/10044], Batch [2/7], Loss: 0.0122, Accuracy: 99.77%, Grad Norm: 0.09938\n",
      "Epoch [902/10044], Batch [3/7], Loss: 0.0100, Accuracy: 99.84%, Grad Norm: 0.08771\n",
      "Epoch [902/10044], Batch [4/7], Loss: 0.0107, Accuracy: 99.75%, Grad Norm: 0.09636\n",
      "Epoch [902/10044], Batch [5/7], Loss: 0.0100, Accuracy: 99.82%, Grad Norm: 0.11305\n",
      "Epoch [902/10044], Batch [6/7], Loss: 0.0118, Accuracy: 99.69%, Grad Norm: 0.12153\n",
      "Epoch [902/10044], Batch [7/7], Loss: 0.0083, Accuracy: 99.82%, Grad Norm: 0.11475\n",
      "Epoch [902/10044], Loss: 0.0083\n",
      "Epoch [903/10044], Batch [1/7], Loss: 0.0094, Accuracy: 99.81%, Grad Norm: 0.08576\n",
      "Epoch [903/10044], Batch [2/7], Loss: 0.0110, Accuracy: 99.72%, Grad Norm: 0.10459\n",
      "Epoch [903/10044], Batch [3/7], Loss: 0.0082, Accuracy: 99.89%, Grad Norm: 0.06963\n",
      "Epoch [903/10044], Batch [4/7], Loss: 0.0098, Accuracy: 99.79%, Grad Norm: 0.08681\n",
      "Epoch [903/10044], Batch [5/7], Loss: 0.0096, Accuracy: 99.80%, Grad Norm: 0.09907\n",
      "Epoch [903/10044], Batch [6/7], Loss: 0.0118, Accuracy: 99.73%, Grad Norm: 0.12073\n",
      "Epoch [903/10044], Batch [7/7], Loss: 0.0066, Accuracy: 99.92%, Grad Norm: 0.10854\n",
      "Epoch [903/10044], Loss: 0.0066\n",
      "Epoch [904/10044], Batch [1/7], Loss: 0.0094, Accuracy: 99.82%, Grad Norm: 0.08267\n",
      "Epoch [904/10044], Batch [2/7], Loss: 0.0121, Accuracy: 99.76%, Grad Norm: 0.10466\n",
      "Epoch [904/10044], Batch [3/7], Loss: 0.0083, Accuracy: 99.86%, Grad Norm: 0.07551\n",
      "Epoch [904/10044], Batch [4/7], Loss: 0.0097, Accuracy: 99.81%, Grad Norm: 0.08553\n",
      "Epoch [904/10044], Batch [5/7], Loss: 0.0095, Accuracy: 99.83%, Grad Norm: 0.09755\n",
      "Epoch [904/10044], Batch [6/7], Loss: 0.0112, Accuracy: 99.77%, Grad Norm: 0.11005\n",
      "Epoch [904/10044], Batch [7/7], Loss: 0.0087, Accuracy: 99.88%, Grad Norm: 0.13280\n",
      "Epoch [904/10044], Loss: 0.0087\n",
      "Epoch [905/10044], Batch [1/7], Loss: 0.0087, Accuracy: 99.82%, Grad Norm: 0.08220\n",
      "Epoch [905/10044], Batch [2/7], Loss: 0.0113, Accuracy: 99.78%, Grad Norm: 0.11308\n",
      "Epoch [905/10044], Batch [3/7], Loss: 0.0083, Accuracy: 99.86%, Grad Norm: 0.07345\n",
      "Epoch [905/10044], Batch [4/7], Loss: 0.0112, Accuracy: 99.72%, Grad Norm: 0.09811\n",
      "Epoch [905/10044], Batch [5/7], Loss: 0.0100, Accuracy: 99.75%, Grad Norm: 0.10953\n",
      "Epoch [905/10044], Batch [6/7], Loss: 0.0114, Accuracy: 99.70%, Grad Norm: 0.10885\n",
      "Epoch [905/10044], Batch [7/7], Loss: 0.0065, Accuracy: 99.90%, Grad Norm: 0.08830\n",
      "Epoch [905/10044], Loss: 0.0065\n",
      "Epoch [906/10044], Batch [1/7], Loss: 0.0098, Accuracy: 99.75%, Grad Norm: 0.09831\n",
      "Epoch [906/10044], Batch [2/7], Loss: 0.0100, Accuracy: 99.77%, Grad Norm: 0.10409\n",
      "Epoch [906/10044], Batch [3/7], Loss: 0.0083, Accuracy: 99.87%, Grad Norm: 0.07188\n",
      "Epoch [906/10044], Batch [4/7], Loss: 0.0087, Accuracy: 99.85%, Grad Norm: 0.07505\n",
      "Epoch [906/10044], Batch [5/7], Loss: 0.0099, Accuracy: 99.80%, Grad Norm: 0.11857\n",
      "Epoch [906/10044], Batch [6/7], Loss: 0.0123, Accuracy: 99.72%, Grad Norm: 0.11582\n",
      "Epoch [906/10044], Batch [7/7], Loss: 0.0076, Accuracy: 99.83%, Grad Norm: 0.12141\n",
      "Epoch [906/10044], Loss: 0.0076\n",
      "Epoch [907/10044], Batch [1/7], Loss: 0.0091, Accuracy: 99.82%, Grad Norm: 0.09983\n",
      "Epoch [907/10044], Batch [2/7], Loss: 0.0094, Accuracy: 99.81%, Grad Norm: 0.08535\n",
      "Epoch [907/10044], Batch [3/7], Loss: 0.0088, Accuracy: 99.87%, Grad Norm: 0.09204\n",
      "Epoch [907/10044], Batch [4/7], Loss: 0.0109, Accuracy: 99.73%, Grad Norm: 0.09039\n",
      "Epoch [907/10044], Batch [5/7], Loss: 0.0096, Accuracy: 99.77%, Grad Norm: 0.12427\n",
      "Epoch [907/10044], Batch [6/7], Loss: 0.0134, Accuracy: 99.67%, Grad Norm: 0.13149\n",
      "Epoch [907/10044], Batch [7/7], Loss: 0.0070, Accuracy: 99.85%, Grad Norm: 0.10156\n",
      "Epoch [907/10044], Loss: 0.0070\n",
      "Epoch [908/10044], Batch [1/7], Loss: 0.0095, Accuracy: 99.78%, Grad Norm: 0.10799\n",
      "Epoch [908/10044], Batch [2/7], Loss: 0.0099, Accuracy: 99.78%, Grad Norm: 0.10157\n",
      "Epoch [908/10044], Batch [3/7], Loss: 0.0078, Accuracy: 99.87%, Grad Norm: 0.07441\n",
      "Epoch [908/10044], Batch [4/7], Loss: 0.0107, Accuracy: 99.77%, Grad Norm: 0.08660\n",
      "Epoch [908/10044], Batch [5/7], Loss: 0.0094, Accuracy: 99.81%, Grad Norm: 0.09270\n",
      "Epoch [908/10044], Batch [6/7], Loss: 0.0110, Accuracy: 99.73%, Grad Norm: 0.10829\n",
      "Epoch [908/10044], Batch [7/7], Loss: 0.0082, Accuracy: 99.80%, Grad Norm: 0.11626\n",
      "Epoch [908/10044], Loss: 0.0082\n",
      "Epoch [909/10044], Batch [1/7], Loss: 0.0107, Accuracy: 99.72%, Grad Norm: 0.10454\n",
      "Epoch [909/10044], Batch [2/7], Loss: 0.0117, Accuracy: 99.69%, Grad Norm: 0.10811\n",
      "Epoch [909/10044], Batch [3/7], Loss: 0.0074, Accuracy: 99.89%, Grad Norm: 0.07185\n",
      "Epoch [909/10044], Batch [4/7], Loss: 0.0088, Accuracy: 99.83%, Grad Norm: 0.07639\n",
      "Epoch [909/10044], Batch [5/7], Loss: 0.0100, Accuracy: 99.73%, Grad Norm: 0.10795\n",
      "Epoch [909/10044], Batch [6/7], Loss: 0.0122, Accuracy: 99.67%, Grad Norm: 0.11579\n",
      "Epoch [909/10044], Batch [7/7], Loss: 0.0077, Accuracy: 99.85%, Grad Norm: 0.12507\n",
      "Epoch [909/10044], Loss: 0.0077\n",
      "Epoch [910/10044], Batch [1/7], Loss: 0.0109, Accuracy: 99.73%, Grad Norm: 0.11707\n",
      "Epoch [910/10044], Batch [2/7], Loss: 0.0102, Accuracy: 99.78%, Grad Norm: 0.10880\n",
      "Epoch [910/10044], Batch [3/7], Loss: 0.0080, Accuracy: 99.87%, Grad Norm: 0.07506\n",
      "Epoch [910/10044], Batch [4/7], Loss: 0.0124, Accuracy: 99.72%, Grad Norm: 0.10205\n",
      "Epoch [910/10044], Batch [5/7], Loss: 0.0102, Accuracy: 99.77%, Grad Norm: 0.10926\n",
      "Epoch [910/10044], Batch [6/7], Loss: 0.0104, Accuracy: 99.77%, Grad Norm: 0.08427\n",
      "Epoch [910/10044], Batch [7/7], Loss: 0.0088, Accuracy: 99.75%, Grad Norm: 0.14051\n",
      "Epoch [910/10044], Loss: 0.0088\n",
      "Epoch [911/10044], Batch [1/7], Loss: 0.0092, Accuracy: 99.84%, Grad Norm: 0.09931\n",
      "Epoch [911/10044], Batch [2/7], Loss: 0.0093, Accuracy: 99.79%, Grad Norm: 0.08351\n",
      "Epoch [911/10044], Batch [3/7], Loss: 0.0082, Accuracy: 99.87%, Grad Norm: 0.08000\n",
      "Epoch [911/10044], Batch [4/7], Loss: 0.0110, Accuracy: 99.76%, Grad Norm: 0.10528\n",
      "Epoch [911/10044], Batch [5/7], Loss: 0.0102, Accuracy: 99.76%, Grad Norm: 0.11171\n",
      "Epoch [911/10044], Batch [6/7], Loss: 0.0105, Accuracy: 99.78%, Grad Norm: 0.09985\n",
      "Epoch [911/10044], Batch [7/7], Loss: 0.0088, Accuracy: 99.73%, Grad Norm: 0.14863\n",
      "Epoch [911/10044], Loss: 0.0088\n",
      "Epoch [912/10044], Batch [1/7], Loss: 0.0108, Accuracy: 99.75%, Grad Norm: 0.11998\n",
      "Epoch [912/10044], Batch [2/7], Loss: 0.0112, Accuracy: 99.72%, Grad Norm: 0.11476\n",
      "Epoch [912/10044], Batch [3/7], Loss: 0.0092, Accuracy: 99.82%, Grad Norm: 0.08504\n",
      "Epoch [912/10044], Batch [4/7], Loss: 0.0099, Accuracy: 99.79%, Grad Norm: 0.08075\n",
      "Epoch [912/10044], Batch [5/7], Loss: 0.0090, Accuracy: 99.84%, Grad Norm: 0.09929\n",
      "Epoch [912/10044], Batch [6/7], Loss: 0.0112, Accuracy: 99.77%, Grad Norm: 0.11880\n",
      "Epoch [912/10044], Batch [7/7], Loss: 0.0083, Accuracy: 99.82%, Grad Norm: 0.13233\n",
      "Epoch [912/10044], Loss: 0.0083\n",
      "Epoch [913/10044], Batch [1/7], Loss: 0.0125, Accuracy: 99.72%, Grad Norm: 0.11904\n",
      "Epoch [913/10044], Batch [2/7], Loss: 0.0097, Accuracy: 99.78%, Grad Norm: 0.09078\n",
      "Epoch [913/10044], Batch [3/7], Loss: 0.0092, Accuracy: 99.86%, Grad Norm: 0.10059\n",
      "Epoch [913/10044], Batch [4/7], Loss: 0.0097, Accuracy: 99.82%, Grad Norm: 0.08408\n",
      "Epoch [913/10044], Batch [5/7], Loss: 0.0078, Accuracy: 99.87%, Grad Norm: 0.08546\n",
      "Epoch [913/10044], Batch [6/7], Loss: 0.0132, Accuracy: 99.72%, Grad Norm: 0.14221\n",
      "Epoch [913/10044], Batch [7/7], Loss: 0.0069, Accuracy: 99.92%, Grad Norm: 0.09082\n",
      "Epoch [913/10044], Loss: 0.0069\n",
      "Epoch [914/10044], Batch [1/7], Loss: 0.0105, Accuracy: 99.78%, Grad Norm: 0.10439\n",
      "Epoch [914/10044], Batch [2/7], Loss: 0.0096, Accuracy: 99.80%, Grad Norm: 0.11713\n",
      "Epoch [914/10044], Batch [3/7], Loss: 0.0083, Accuracy: 99.89%, Grad Norm: 0.06779\n",
      "Epoch [914/10044], Batch [4/7], Loss: 0.0097, Accuracy: 99.81%, Grad Norm: 0.07987\n",
      "Epoch [914/10044], Batch [5/7], Loss: 0.0091, Accuracy: 99.82%, Grad Norm: 0.09296\n",
      "Epoch [914/10044], Batch [6/7], Loss: 0.0102, Accuracy: 99.77%, Grad Norm: 0.11977\n",
      "Epoch [914/10044], Batch [7/7], Loss: 0.0083, Accuracy: 99.83%, Grad Norm: 0.17360\n",
      "Epoch [914/10044], Loss: 0.0083\n",
      "Epoch [915/10044], Batch [1/7], Loss: 0.0113, Accuracy: 99.82%, Grad Norm: 0.11231\n",
      "Epoch [915/10044], Batch [2/7], Loss: 0.0117, Accuracy: 99.68%, Grad Norm: 0.10966\n",
      "Epoch [915/10044], Batch [3/7], Loss: 0.0088, Accuracy: 99.84%, Grad Norm: 0.09955\n",
      "Epoch [915/10044], Batch [4/7], Loss: 0.0089, Accuracy: 99.82%, Grad Norm: 0.07319\n",
      "Epoch [915/10044], Batch [5/7], Loss: 0.0086, Accuracy: 99.87%, Grad Norm: 0.09731\n",
      "Epoch [915/10044], Batch [6/7], Loss: 0.0119, Accuracy: 99.67%, Grad Norm: 0.15609\n",
      "Epoch [915/10044], Batch [7/7], Loss: 0.0110, Accuracy: 99.77%, Grad Norm: 0.17021\n",
      "Epoch [915/10044], Loss: 0.0110\n",
      "Epoch [916/10044], Batch [1/7], Loss: 0.0103, Accuracy: 99.77%, Grad Norm: 0.11019\n",
      "Epoch [916/10044], Batch [2/7], Loss: 0.0109, Accuracy: 99.70%, Grad Norm: 0.11544\n",
      "Epoch [916/10044], Batch [3/7], Loss: 0.0089, Accuracy: 99.82%, Grad Norm: 0.08491\n",
      "Epoch [916/10044], Batch [4/7], Loss: 0.0128, Accuracy: 99.71%, Grad Norm: 0.11240\n",
      "Epoch [916/10044], Batch [5/7], Loss: 0.0096, Accuracy: 99.77%, Grad Norm: 0.10657\n",
      "Epoch [916/10044], Batch [6/7], Loss: 0.0112, Accuracy: 99.77%, Grad Norm: 0.11364\n",
      "Epoch [916/10044], Batch [7/7], Loss: 0.0085, Accuracy: 99.85%, Grad Norm: 0.12822\n",
      "Epoch [916/10044], Loss: 0.0085\n",
      "Epoch [917/10044], Batch [1/7], Loss: 0.0099, Accuracy: 99.85%, Grad Norm: 0.10149\n",
      "Epoch [917/10044], Batch [2/7], Loss: 0.0114, Accuracy: 99.68%, Grad Norm: 0.12806\n",
      "Epoch [917/10044], Batch [3/7], Loss: 0.0080, Accuracy: 99.88%, Grad Norm: 0.07276\n",
      "Epoch [917/10044], Batch [4/7], Loss: 0.0095, Accuracy: 99.83%, Grad Norm: 0.09235\n",
      "Epoch [917/10044], Batch [5/7], Loss: 0.0098, Accuracy: 99.81%, Grad Norm: 0.12596\n",
      "Epoch [917/10044], Batch [6/7], Loss: 0.0113, Accuracy: 99.71%, Grad Norm: 0.11351\n",
      "Epoch [917/10044], Batch [7/7], Loss: 0.0081, Accuracy: 99.83%, Grad Norm: 0.12311\n",
      "Epoch [917/10044], Loss: 0.0081\n",
      "Epoch [918/10044], Batch [1/7], Loss: 0.0092, Accuracy: 99.78%, Grad Norm: 0.09875\n",
      "Epoch [918/10044], Batch [2/7], Loss: 0.0115, Accuracy: 99.72%, Grad Norm: 0.11378\n",
      "Epoch [918/10044], Batch [3/7], Loss: 0.0080, Accuracy: 99.88%, Grad Norm: 0.07919\n",
      "Epoch [918/10044], Batch [4/7], Loss: 0.0099, Accuracy: 99.76%, Grad Norm: 0.10032\n",
      "Epoch [918/10044], Batch [5/7], Loss: 0.0096, Accuracy: 99.75%, Grad Norm: 0.12293\n",
      "Epoch [918/10044], Batch [6/7], Loss: 0.0119, Accuracy: 99.69%, Grad Norm: 0.11466\n",
      "Epoch [918/10044], Batch [7/7], Loss: 0.0084, Accuracy: 99.85%, Grad Norm: 0.12967\n",
      "Epoch [918/10044], Loss: 0.0084\n",
      "Epoch [919/10044], Batch [1/7], Loss: 0.0098, Accuracy: 99.83%, Grad Norm: 0.10287\n",
      "Epoch [919/10044], Batch [2/7], Loss: 0.0122, Accuracy: 99.67%, Grad Norm: 0.12613\n",
      "Epoch [919/10044], Batch [3/7], Loss: 0.0086, Accuracy: 99.79%, Grad Norm: 0.07642\n",
      "Epoch [919/10044], Batch [4/7], Loss: 0.0102, Accuracy: 99.76%, Grad Norm: 0.09210\n",
      "Epoch [919/10044], Batch [5/7], Loss: 0.0097, Accuracy: 99.77%, Grad Norm: 0.09974\n",
      "Epoch [919/10044], Batch [6/7], Loss: 0.0121, Accuracy: 99.72%, Grad Norm: 0.12053\n",
      "Epoch [919/10044], Batch [7/7], Loss: 0.0083, Accuracy: 99.82%, Grad Norm: 0.12012\n",
      "Epoch [919/10044], Loss: 0.0083\n",
      "Epoch [920/10044], Batch [1/7], Loss: 0.0095, Accuracy: 99.84%, Grad Norm: 0.08881\n",
      "Epoch [920/10044], Batch [2/7], Loss: 0.0116, Accuracy: 99.72%, Grad Norm: 0.12752\n",
      "Epoch [920/10044], Batch [3/7], Loss: 0.0079, Accuracy: 99.89%, Grad Norm: 0.07202\n",
      "Epoch [920/10044], Batch [4/7], Loss: 0.0087, Accuracy: 99.83%, Grad Norm: 0.07559\n",
      "Epoch [920/10044], Batch [5/7], Loss: 0.0088, Accuracy: 99.88%, Grad Norm: 0.09408\n",
      "Epoch [920/10044], Batch [6/7], Loss: 0.0101, Accuracy: 99.77%, Grad Norm: 0.11115\n",
      "Epoch [920/10044], Batch [7/7], Loss: 0.0069, Accuracy: 99.82%, Grad Norm: 0.11971\n",
      "Epoch [920/10044], Loss: 0.0069\n",
      "Epoch [921/10044], Batch [1/7], Loss: 0.0125, Accuracy: 99.70%, Grad Norm: 0.11031\n",
      "Epoch [921/10044], Batch [2/7], Loss: 0.0116, Accuracy: 99.77%, Grad Norm: 0.12836\n",
      "Epoch [921/10044], Batch [3/7], Loss: 0.0078, Accuracy: 99.85%, Grad Norm: 0.07275\n",
      "Epoch [921/10044], Batch [4/7], Loss: 0.0095, Accuracy: 99.82%, Grad Norm: 0.07501\n",
      "Epoch [921/10044], Batch [5/7], Loss: 0.0079, Accuracy: 99.85%, Grad Norm: 0.07407\n",
      "Epoch [921/10044], Batch [6/7], Loss: 0.0114, Accuracy: 99.74%, Grad Norm: 0.11918\n",
      "Epoch [921/10044], Batch [7/7], Loss: 0.0091, Accuracy: 99.85%, Grad Norm: 0.13895\n",
      "Epoch [921/10044], Loss: 0.0091\n",
      "Epoch [922/10044], Batch [1/7], Loss: 0.0101, Accuracy: 99.79%, Grad Norm: 0.10545\n",
      "Epoch [922/10044], Batch [2/7], Loss: 0.0106, Accuracy: 99.71%, Grad Norm: 0.10843\n",
      "Epoch [922/10044], Batch [3/7], Loss: 0.0076, Accuracy: 99.88%, Grad Norm: 0.07535\n",
      "Epoch [922/10044], Batch [4/7], Loss: 0.0098, Accuracy: 99.82%, Grad Norm: 0.09134\n",
      "Epoch [922/10044], Batch [5/7], Loss: 0.0079, Accuracy: 99.82%, Grad Norm: 0.08819\n",
      "Epoch [922/10044], Batch [6/7], Loss: 0.0102, Accuracy: 99.81%, Grad Norm: 0.10895\n",
      "Epoch [922/10044], Batch [7/7], Loss: 0.0065, Accuracy: 99.88%, Grad Norm: 0.08697\n",
      "Epoch [922/10044], Loss: 0.0065\n",
      "Epoch [923/10044], Batch [1/7], Loss: 0.0101, Accuracy: 99.79%, Grad Norm: 0.09775\n",
      "Epoch [923/10044], Batch [2/7], Loss: 0.0115, Accuracy: 99.76%, Grad Norm: 0.11126\n",
      "Epoch [923/10044], Batch [3/7], Loss: 0.0082, Accuracy: 99.85%, Grad Norm: 0.07798\n",
      "Epoch [923/10044], Batch [4/7], Loss: 0.0096, Accuracy: 99.80%, Grad Norm: 0.08251\n",
      "Epoch [923/10044], Batch [5/7], Loss: 0.0085, Accuracy: 99.86%, Grad Norm: 0.08633\n",
      "Epoch [923/10044], Batch [6/7], Loss: 0.0091, Accuracy: 99.79%, Grad Norm: 0.09645\n",
      "Epoch [923/10044], Batch [7/7], Loss: 0.0068, Accuracy: 99.88%, Grad Norm: 0.09564\n",
      "Epoch [923/10044], Loss: 0.0068\n",
      "Epoch [924/10044], Batch [1/7], Loss: 0.0090, Accuracy: 99.83%, Grad Norm: 0.09157\n",
      "Epoch [924/10044], Batch [2/7], Loss: 0.0122, Accuracy: 99.69%, Grad Norm: 0.13501\n",
      "Epoch [924/10044], Batch [3/7], Loss: 0.0076, Accuracy: 99.85%, Grad Norm: 0.07695\n",
      "Epoch [924/10044], Batch [4/7], Loss: 0.0097, Accuracy: 99.81%, Grad Norm: 0.08870\n",
      "Epoch [924/10044], Batch [5/7], Loss: 0.0080, Accuracy: 99.83%, Grad Norm: 0.08553\n",
      "Epoch [924/10044], Batch [6/7], Loss: 0.0096, Accuracy: 99.76%, Grad Norm: 0.10991\n",
      "Epoch [924/10044], Batch [7/7], Loss: 0.0082, Accuracy: 99.80%, Grad Norm: 0.10810\n",
      "Epoch [924/10044], Loss: 0.0082\n",
      "Epoch [925/10044], Batch [1/7], Loss: 0.0091, Accuracy: 99.79%, Grad Norm: 0.09331\n",
      "Epoch [925/10044], Batch [2/7], Loss: 0.0094, Accuracy: 99.80%, Grad Norm: 0.09991\n",
      "Epoch [925/10044], Batch [3/7], Loss: 0.0073, Accuracy: 99.91%, Grad Norm: 0.05825\n",
      "Epoch [925/10044], Batch [4/7], Loss: 0.0079, Accuracy: 99.91%, Grad Norm: 0.06336\n",
      "Epoch [925/10044], Batch [5/7], Loss: 0.0081, Accuracy: 99.82%, Grad Norm: 0.07835\n",
      "Epoch [925/10044], Batch [6/7], Loss: 0.0101, Accuracy: 99.78%, Grad Norm: 0.12254\n",
      "Epoch [925/10044], Batch [7/7], Loss: 0.0062, Accuracy: 99.85%, Grad Norm: 0.11313\n",
      "Epoch [925/10044], Loss: 0.0062\n",
      "Epoch [926/10044], Batch [1/7], Loss: 0.0094, Accuracy: 99.82%, Grad Norm: 0.08692\n",
      "Epoch [926/10044], Batch [2/7], Loss: 0.0105, Accuracy: 99.77%, Grad Norm: 0.10444\n",
      "Epoch [926/10044], Batch [3/7], Loss: 0.0075, Accuracy: 99.87%, Grad Norm: 0.07161\n",
      "Epoch [926/10044], Batch [4/7], Loss: 0.0085, Accuracy: 99.84%, Grad Norm: 0.08558\n",
      "Epoch [926/10044], Batch [5/7], Loss: 0.0085, Accuracy: 99.81%, Grad Norm: 0.14639\n",
      "Epoch [926/10044], Batch [6/7], Loss: 0.0109, Accuracy: 99.72%, Grad Norm: 0.13915\n",
      "Epoch [926/10044], Batch [7/7], Loss: 0.0109, Accuracy: 99.82%, Grad Norm: 0.14304\n",
      "Epoch [926/10044], Loss: 0.0109\n",
      "Epoch [927/10044], Batch [1/7], Loss: 0.0083, Accuracy: 99.84%, Grad Norm: 0.08568\n",
      "Epoch [927/10044], Batch [2/7], Loss: 0.0106, Accuracy: 99.70%, Grad Norm: 0.10421\n",
      "Epoch [927/10044], Batch [3/7], Loss: 0.0075, Accuracy: 99.89%, Grad Norm: 0.07043\n",
      "Epoch [927/10044], Batch [4/7], Loss: 0.0087, Accuracy: 99.87%, Grad Norm: 0.07611\n",
      "Epoch [927/10044], Batch [5/7], Loss: 0.0106, Accuracy: 99.76%, Grad Norm: 0.12115\n",
      "Epoch [927/10044], Batch [6/7], Loss: 0.0095, Accuracy: 99.76%, Grad Norm: 0.10463\n",
      "Epoch [927/10044], Batch [7/7], Loss: 0.0068, Accuracy: 99.92%, Grad Norm: 0.10167\n",
      "Epoch [927/10044], Loss: 0.0068\n",
      "Epoch [928/10044], Batch [1/7], Loss: 0.0078, Accuracy: 99.85%, Grad Norm: 0.07769\n",
      "Epoch [928/10044], Batch [2/7], Loss: 0.0086, Accuracy: 99.82%, Grad Norm: 0.09596\n",
      "Epoch [928/10044], Batch [3/7], Loss: 0.0089, Accuracy: 99.88%, Grad Norm: 0.08932\n",
      "Epoch [928/10044], Batch [4/7], Loss: 0.0095, Accuracy: 99.79%, Grad Norm: 0.08656\n",
      "Epoch [928/10044], Batch [5/7], Loss: 0.0090, Accuracy: 99.81%, Grad Norm: 0.09653\n",
      "Epoch [928/10044], Batch [6/7], Loss: 0.0104, Accuracy: 99.80%, Grad Norm: 0.10430\n",
      "Epoch [928/10044], Batch [7/7], Loss: 0.0077, Accuracy: 99.85%, Grad Norm: 0.12285\n",
      "Epoch [928/10044], Loss: 0.0077\n",
      "Epoch [929/10044], Batch [1/7], Loss: 0.0081, Accuracy: 99.85%, Grad Norm: 0.08277\n",
      "Epoch [929/10044], Batch [2/7], Loss: 0.0107, Accuracy: 99.73%, Grad Norm: 0.10214\n",
      "Epoch [929/10044], Batch [3/7], Loss: 0.0080, Accuracy: 99.88%, Grad Norm: 0.08514\n",
      "Epoch [929/10044], Batch [4/7], Loss: 0.0091, Accuracy: 99.80%, Grad Norm: 0.07697\n",
      "Epoch [929/10044], Batch [5/7], Loss: 0.0086, Accuracy: 99.84%, Grad Norm: 0.10033\n",
      "Epoch [929/10044], Batch [6/7], Loss: 0.0089, Accuracy: 99.83%, Grad Norm: 0.09094\n",
      "Epoch [929/10044], Batch [7/7], Loss: 0.0077, Accuracy: 99.83%, Grad Norm: 0.13812\n",
      "Epoch [929/10044], Loss: 0.0077\n",
      "Epoch [930/10044], Batch [1/7], Loss: 0.0098, Accuracy: 99.79%, Grad Norm: 0.10558\n",
      "Epoch [930/10044], Batch [2/7], Loss: 0.0111, Accuracy: 99.71%, Grad Norm: 0.11235\n",
      "Epoch [930/10044], Batch [3/7], Loss: 0.0075, Accuracy: 99.86%, Grad Norm: 0.06180\n",
      "Epoch [930/10044], Batch [4/7], Loss: 0.0079, Accuracy: 99.84%, Grad Norm: 0.06358\n",
      "Epoch [930/10044], Batch [5/7], Loss: 0.0076, Accuracy: 99.89%, Grad Norm: 0.07293\n",
      "Epoch [930/10044], Batch [6/7], Loss: 0.0104, Accuracy: 99.76%, Grad Norm: 0.10644\n",
      "Epoch [930/10044], Batch [7/7], Loss: 0.0083, Accuracy: 99.80%, Grad Norm: 0.14139\n",
      "Epoch [930/10044], Loss: 0.0083\n",
      "Epoch [931/10044], Batch [1/7], Loss: 0.0092, Accuracy: 99.81%, Grad Norm: 0.09257\n",
      "Epoch [931/10044], Batch [2/7], Loss: 0.0097, Accuracy: 99.78%, Grad Norm: 0.10219\n",
      "Epoch [931/10044], Batch [3/7], Loss: 0.0084, Accuracy: 99.83%, Grad Norm: 0.08374\n",
      "Epoch [931/10044], Batch [4/7], Loss: 0.0087, Accuracy: 99.83%, Grad Norm: 0.07327\n",
      "Epoch [931/10044], Batch [5/7], Loss: 0.0092, Accuracy: 99.77%, Grad Norm: 0.10758\n",
      "Epoch [931/10044], Batch [6/7], Loss: 0.0099, Accuracy: 99.73%, Grad Norm: 0.09965\n",
      "Epoch [931/10044], Batch [7/7], Loss: 0.0079, Accuracy: 99.83%, Grad Norm: 0.14683\n",
      "Epoch [931/10044], Loss: 0.0079\n",
      "Epoch [932/10044], Batch [1/7], Loss: 0.0091, Accuracy: 99.82%, Grad Norm: 0.09752\n",
      "Epoch [932/10044], Batch [2/7], Loss: 0.0085, Accuracy: 99.76%, Grad Norm: 0.11098\n",
      "Epoch [932/10044], Batch [3/7], Loss: 0.0072, Accuracy: 99.89%, Grad Norm: 0.06531\n",
      "Epoch [932/10044], Batch [4/7], Loss: 0.0089, Accuracy: 99.86%, Grad Norm: 0.08440\n",
      "Epoch [932/10044], Batch [5/7], Loss: 0.0082, Accuracy: 99.82%, Grad Norm: 0.09107\n",
      "Epoch [932/10044], Batch [6/7], Loss: 0.0104, Accuracy: 99.75%, Grad Norm: 0.12938\n",
      "Epoch [932/10044], Batch [7/7], Loss: 0.0097, Accuracy: 99.78%, Grad Norm: 0.17814\n",
      "Epoch [932/10044], Loss: 0.0097\n",
      "Epoch [933/10044], Batch [1/7], Loss: 0.0097, Accuracy: 99.77%, Grad Norm: 0.10946\n",
      "Epoch [933/10044], Batch [2/7], Loss: 0.0111, Accuracy: 99.75%, Grad Norm: 0.13512\n",
      "Epoch [933/10044], Batch [3/7], Loss: 0.0087, Accuracy: 99.86%, Grad Norm: 0.08277\n",
      "Epoch [933/10044], Batch [4/7], Loss: 0.0113, Accuracy: 99.74%, Grad Norm: 0.09847\n",
      "Epoch [933/10044], Batch [5/7], Loss: 0.0080, Accuracy: 99.83%, Grad Norm: 0.09015\n",
      "Epoch [933/10044], Batch [6/7], Loss: 0.0112, Accuracy: 99.77%, Grad Norm: 0.13484\n",
      "Epoch [933/10044], Batch [7/7], Loss: 0.0115, Accuracy: 99.75%, Grad Norm: 0.17475\n",
      "Epoch [933/10044], Loss: 0.0115\n",
      "Epoch [934/10044], Batch [1/7], Loss: 0.0118, Accuracy: 99.71%, Grad Norm: 0.13837\n",
      "Epoch [934/10044], Batch [2/7], Loss: 0.0117, Accuracy: 99.67%, Grad Norm: 0.14289\n",
      "Epoch [934/10044], Batch [3/7], Loss: 0.0078, Accuracy: 99.90%, Grad Norm: 0.07316\n",
      "Epoch [934/10044], Batch [4/7], Loss: 0.0085, Accuracy: 99.86%, Grad Norm: 0.07033\n",
      "Epoch [934/10044], Batch [5/7], Loss: 0.0082, Accuracy: 99.87%, Grad Norm: 0.08715\n",
      "Epoch [934/10044], Batch [6/7], Loss: 0.0103, Accuracy: 99.80%, Grad Norm: 0.11822\n",
      "Epoch [934/10044], Batch [7/7], Loss: 0.0096, Accuracy: 99.78%, Grad Norm: 0.16587\n",
      "Epoch [934/10044], Loss: 0.0096\n",
      "Epoch [935/10044], Batch [1/7], Loss: 0.0095, Accuracy: 99.83%, Grad Norm: 0.10389\n",
      "Epoch [935/10044], Batch [2/7], Loss: 0.0131, Accuracy: 99.68%, Grad Norm: 0.14734\n",
      "Epoch [935/10044], Batch [3/7], Loss: 0.0095, Accuracy: 99.82%, Grad Norm: 0.08747\n",
      "Epoch [935/10044], Batch [4/7], Loss: 0.0098, Accuracy: 99.79%, Grad Norm: 0.08499\n",
      "Epoch [935/10044], Batch [5/7], Loss: 0.0096, Accuracy: 99.77%, Grad Norm: 0.11391\n",
      "Epoch [935/10044], Batch [6/7], Loss: 0.0102, Accuracy: 99.75%, Grad Norm: 0.10328\n",
      "Epoch [935/10044], Batch [7/7], Loss: 0.0063, Accuracy: 99.92%, Grad Norm: 0.11049\n",
      "Epoch [935/10044], Loss: 0.0063\n",
      "Epoch [936/10044], Batch [1/7], Loss: 0.0092, Accuracy: 99.79%, Grad Norm: 0.08696\n",
      "Epoch [936/10044], Batch [2/7], Loss: 0.0120, Accuracy: 99.72%, Grad Norm: 0.14941\n",
      "Epoch [936/10044], Batch [3/7], Loss: 0.0083, Accuracy: 99.80%, Grad Norm: 0.08038\n",
      "Epoch [936/10044], Batch [4/7], Loss: 0.0082, Accuracy: 99.83%, Grad Norm: 0.08100\n",
      "Epoch [936/10044], Batch [5/7], Loss: 0.0091, Accuracy: 99.83%, Grad Norm: 0.12203\n",
      "Epoch [936/10044], Batch [6/7], Loss: 0.0127, Accuracy: 99.71%, Grad Norm: 0.13712\n",
      "Epoch [936/10044], Batch [7/7], Loss: 0.0076, Accuracy: 99.88%, Grad Norm: 0.11269\n",
      "Epoch [936/10044], Loss: 0.0076\n",
      "Epoch [937/10044], Batch [1/7], Loss: 0.0089, Accuracy: 99.77%, Grad Norm: 0.09570\n",
      "Epoch [937/10044], Batch [2/7], Loss: 0.0141, Accuracy: 99.67%, Grad Norm: 0.14597\n",
      "Epoch [937/10044], Batch [3/7], Loss: 0.0077, Accuracy: 99.85%, Grad Norm: 0.07266\n",
      "Epoch [937/10044], Batch [4/7], Loss: 0.0079, Accuracy: 99.88%, Grad Norm: 0.06399\n",
      "Epoch [937/10044], Batch [5/7], Loss: 0.0083, Accuracy: 99.87%, Grad Norm: 0.10248\n",
      "Epoch [937/10044], Batch [6/7], Loss: 0.0101, Accuracy: 99.80%, Grad Norm: 0.11855\n",
      "Epoch [937/10044], Batch [7/7], Loss: 0.0083, Accuracy: 99.85%, Grad Norm: 0.13152\n",
      "Epoch [937/10044], Loss: 0.0083\n",
      "Epoch [938/10044], Batch [1/7], Loss: 0.0099, Accuracy: 99.77%, Grad Norm: 0.09838\n",
      "Epoch [938/10044], Batch [2/7], Loss: 0.0104, Accuracy: 99.71%, Grad Norm: 0.11388\n",
      "Epoch [938/10044], Batch [3/7], Loss: 0.0071, Accuracy: 99.89%, Grad Norm: 0.06759\n",
      "Epoch [938/10044], Batch [4/7], Loss: 0.0102, Accuracy: 99.80%, Grad Norm: 0.09262\n",
      "Epoch [938/10044], Batch [5/7], Loss: 0.0096, Accuracy: 99.80%, Grad Norm: 0.09588\n",
      "Epoch [938/10044], Batch [6/7], Loss: 0.0111, Accuracy: 99.81%, Grad Norm: 0.11226\n",
      "Epoch [938/10044], Batch [7/7], Loss: 0.0069, Accuracy: 99.90%, Grad Norm: 0.11618\n",
      "Epoch [938/10044], Loss: 0.0069\n",
      "Epoch [939/10044], Batch [1/7], Loss: 0.0100, Accuracy: 99.75%, Grad Norm: 0.10252\n",
      "Epoch [939/10044], Batch [2/7], Loss: 0.0090, Accuracy: 99.80%, Grad Norm: 0.09704\n",
      "Epoch [939/10044], Batch [3/7], Loss: 0.0076, Accuracy: 99.92%, Grad Norm: 0.06582\n",
      "Epoch [939/10044], Batch [4/7], Loss: 0.0090, Accuracy: 99.81%, Grad Norm: 0.07287\n",
      "Epoch [939/10044], Batch [5/7], Loss: 0.0086, Accuracy: 99.80%, Grad Norm: 0.09551\n",
      "Epoch [939/10044], Batch [6/7], Loss: 0.0107, Accuracy: 99.73%, Grad Norm: 0.11417\n",
      "Epoch [939/10044], Batch [7/7], Loss: 0.0080, Accuracy: 99.75%, Grad Norm: 0.13546\n",
      "Epoch [939/10044], Loss: 0.0080\n",
      "Epoch [940/10044], Batch [1/7], Loss: 0.0082, Accuracy: 99.80%, Grad Norm: 0.08777\n",
      "Epoch [940/10044], Batch [2/7], Loss: 0.0107, Accuracy: 99.77%, Grad Norm: 0.11994\n",
      "Epoch [940/10044], Batch [3/7], Loss: 0.0076, Accuracy: 99.87%, Grad Norm: 0.07943\n",
      "Epoch [940/10044], Batch [4/7], Loss: 0.0086, Accuracy: 99.84%, Grad Norm: 0.08893\n",
      "Epoch [940/10044], Batch [5/7], Loss: 0.0088, Accuracy: 99.85%, Grad Norm: 0.09445\n",
      "Epoch [940/10044], Batch [6/7], Loss: 0.0098, Accuracy: 99.82%, Grad Norm: 0.11611\n",
      "Epoch [940/10044], Batch [7/7], Loss: 0.0091, Accuracy: 99.78%, Grad Norm: 0.16426\n",
      "Epoch [940/10044], Loss: 0.0091\n",
      "Epoch [941/10044], Batch [1/7], Loss: 0.0106, Accuracy: 99.74%, Grad Norm: 0.09817\n",
      "Epoch [941/10044], Batch [2/7], Loss: 0.0098, Accuracy: 99.77%, Grad Norm: 0.11526\n",
      "Epoch [941/10044], Batch [3/7], Loss: 0.0079, Accuracy: 99.87%, Grad Norm: 0.07370\n",
      "Epoch [941/10044], Batch [4/7], Loss: 0.0092, Accuracy: 99.79%, Grad Norm: 0.10761\n",
      "Epoch [941/10044], Batch [5/7], Loss: 0.0104, Accuracy: 99.80%, Grad Norm: 0.10135\n",
      "Epoch [941/10044], Batch [6/7], Loss: 0.0104, Accuracy: 99.73%, Grad Norm: 0.11554\n",
      "Epoch [941/10044], Batch [7/7], Loss: 0.0075, Accuracy: 99.90%, Grad Norm: 0.10103\n",
      "Epoch [941/10044], Loss: 0.0075\n",
      "Epoch [942/10044], Batch [1/7], Loss: 0.0094, Accuracy: 99.82%, Grad Norm: 0.09996\n",
      "Epoch [942/10044], Batch [2/7], Loss: 0.0108, Accuracy: 99.73%, Grad Norm: 0.12202\n",
      "Epoch [942/10044], Batch [3/7], Loss: 0.0073, Accuracy: 99.85%, Grad Norm: 0.07450\n",
      "Epoch [942/10044], Batch [4/7], Loss: 0.0096, Accuracy: 99.82%, Grad Norm: 0.09329\n",
      "Epoch [942/10044], Batch [5/7], Loss: 0.0079, Accuracy: 99.84%, Grad Norm: 0.07958\n",
      "Epoch [942/10044], Batch [6/7], Loss: 0.0083, Accuracy: 99.82%, Grad Norm: 0.08417\n",
      "Epoch [942/10044], Batch [7/7], Loss: 0.0062, Accuracy: 99.95%, Grad Norm: 0.10789\n",
      "Epoch [942/10044], Loss: 0.0062\n",
      "Epoch [943/10044], Batch [1/7], Loss: 0.0094, Accuracy: 99.82%, Grad Norm: 0.09316\n",
      "Epoch [943/10044], Batch [2/7], Loss: 0.0087, Accuracy: 99.83%, Grad Norm: 0.08360\n",
      "Epoch [943/10044], Batch [3/7], Loss: 0.0091, Accuracy: 99.83%, Grad Norm: 0.08993\n",
      "Epoch [943/10044], Batch [4/7], Loss: 0.0076, Accuracy: 99.88%, Grad Norm: 0.06758\n",
      "Epoch [943/10044], Batch [5/7], Loss: 0.0081, Accuracy: 99.83%, Grad Norm: 0.09908\n",
      "Epoch [943/10044], Batch [6/7], Loss: 0.0108, Accuracy: 99.73%, Grad Norm: 0.11676\n",
      "Epoch [943/10044], Batch [7/7], Loss: 0.0052, Accuracy: 99.92%, Grad Norm: 0.09798\n",
      "Epoch [943/10044], Loss: 0.0052\n",
      "Epoch [944/10044], Batch [1/7], Loss: 0.0075, Accuracy: 99.88%, Grad Norm: 0.07613\n",
      "Epoch [944/10044], Batch [2/7], Loss: 0.0098, Accuracy: 99.76%, Grad Norm: 0.09938\n",
      "Epoch [944/10044], Batch [3/7], Loss: 0.0068, Accuracy: 99.89%, Grad Norm: 0.06461\n",
      "Epoch [944/10044], Batch [4/7], Loss: 0.0093, Accuracy: 99.77%, Grad Norm: 0.09333\n",
      "Epoch [944/10044], Batch [5/7], Loss: 0.0096, Accuracy: 99.73%, Grad Norm: 0.11716\n",
      "Epoch [944/10044], Batch [6/7], Loss: 0.0099, Accuracy: 99.80%, Grad Norm: 0.10184\n",
      "Epoch [944/10044], Batch [7/7], Loss: 0.0086, Accuracy: 99.80%, Grad Norm: 0.16603\n",
      "Epoch [944/10044], Loss: 0.0086\n",
      "Epoch [945/10044], Batch [1/7], Loss: 0.0101, Accuracy: 99.77%, Grad Norm: 0.09792\n",
      "Epoch [945/10044], Batch [2/7], Loss: 0.0098, Accuracy: 99.76%, Grad Norm: 0.10696\n",
      "Epoch [945/10044], Batch [3/7], Loss: 0.0072, Accuracy: 99.89%, Grad Norm: 0.07183\n",
      "Epoch [945/10044], Batch [4/7], Loss: 0.0094, Accuracy: 99.83%, Grad Norm: 0.09506\n",
      "Epoch [945/10044], Batch [5/7], Loss: 0.0102, Accuracy: 99.73%, Grad Norm: 0.12417\n",
      "Epoch [945/10044], Batch [6/7], Loss: 0.0105, Accuracy: 99.76%, Grad Norm: 0.11234\n",
      "Epoch [945/10044], Batch [7/7], Loss: 0.0081, Accuracy: 99.73%, Grad Norm: 0.17706\n",
      "Epoch [945/10044], Loss: 0.0081\n",
      "Epoch [946/10044], Batch [1/7], Loss: 0.0098, Accuracy: 99.81%, Grad Norm: 0.10507\n",
      "Epoch [946/10044], Batch [2/7], Loss: 0.0100, Accuracy: 99.72%, Grad Norm: 0.10515\n",
      "Epoch [946/10044], Batch [3/7], Loss: 0.0068, Accuracy: 99.89%, Grad Norm: 0.06499\n",
      "Epoch [946/10044], Batch [4/7], Loss: 0.0088, Accuracy: 99.82%, Grad Norm: 0.07772\n",
      "Epoch [946/10044], Batch [5/7], Loss: 0.0085, Accuracy: 99.87%, Grad Norm: 0.09998\n",
      "Epoch [946/10044], Batch [6/7], Loss: 0.0092, Accuracy: 99.74%, Grad Norm: 0.09420\n",
      "Epoch [946/10044], Batch [7/7], Loss: 0.0094, Accuracy: 99.72%, Grad Norm: 0.18534\n",
      "Epoch [946/10044], Loss: 0.0094\n",
      "Epoch [947/10044], Batch [1/7], Loss: 0.0089, Accuracy: 99.87%, Grad Norm: 0.08307\n",
      "Epoch [947/10044], Batch [2/7], Loss: 0.0105, Accuracy: 99.74%, Grad Norm: 0.09869\n",
      "Epoch [947/10044], Batch [3/7], Loss: 0.0069, Accuracy: 99.88%, Grad Norm: 0.05718\n",
      "Epoch [947/10044], Batch [4/7], Loss: 0.0082, Accuracy: 99.87%, Grad Norm: 0.09162\n",
      "Epoch [947/10044], Batch [5/7], Loss: 0.0085, Accuracy: 99.83%, Grad Norm: 0.11487\n",
      "Epoch [947/10044], Batch [6/7], Loss: 0.0106, Accuracy: 99.75%, Grad Norm: 0.12303\n",
      "Epoch [947/10044], Batch [7/7], Loss: 0.0089, Accuracy: 99.78%, Grad Norm: 0.16817\n",
      "Epoch [947/10044], Loss: 0.0089\n",
      "Epoch [948/10044], Batch [1/7], Loss: 0.0080, Accuracy: 99.87%, Grad Norm: 0.08262\n",
      "Epoch [948/10044], Batch [2/7], Loss: 0.0095, Accuracy: 99.81%, Grad Norm: 0.09621\n",
      "Epoch [948/10044], Batch [3/7], Loss: 0.0081, Accuracy: 99.88%, Grad Norm: 0.08364\n",
      "Epoch [948/10044], Batch [4/7], Loss: 0.0090, Accuracy: 99.80%, Grad Norm: 0.09931\n",
      "Epoch [948/10044], Batch [5/7], Loss: 0.0085, Accuracy: 99.87%, Grad Norm: 0.09895\n",
      "Epoch [948/10044], Batch [6/7], Loss: 0.0096, Accuracy: 99.75%, Grad Norm: 0.10800\n",
      "Epoch [948/10044], Batch [7/7], Loss: 0.0092, Accuracy: 99.78%, Grad Norm: 0.16086\n",
      "Epoch [948/10044], Loss: 0.0092\n",
      "Epoch [949/10044], Batch [1/7], Loss: 0.0072, Accuracy: 99.90%, Grad Norm: 0.08020\n",
      "Epoch [949/10044], Batch [2/7], Loss: 0.0117, Accuracy: 99.72%, Grad Norm: 0.14039\n",
      "Epoch [949/10044], Batch [3/7], Loss: 0.0078, Accuracy: 99.87%, Grad Norm: 0.08711\n",
      "Epoch [949/10044], Batch [4/7], Loss: 0.0108, Accuracy: 99.73%, Grad Norm: 0.11215\n",
      "Epoch [949/10044], Batch [5/7], Loss: 0.0068, Accuracy: 99.88%, Grad Norm: 0.08309\n",
      "Epoch [949/10044], Batch [6/7], Loss: 0.0113, Accuracy: 99.70%, Grad Norm: 0.12486\n",
      "Epoch [949/10044], Batch [7/7], Loss: 0.0079, Accuracy: 99.82%, Grad Norm: 0.12298\n",
      "Epoch [949/10044], Loss: 0.0079\n",
      "Epoch [950/10044], Batch [1/7], Loss: 0.0094, Accuracy: 99.78%, Grad Norm: 0.11648\n",
      "Epoch [950/10044], Batch [2/7], Loss: 0.0114, Accuracy: 99.72%, Grad Norm: 0.13777\n",
      "Epoch [950/10044], Batch [3/7], Loss: 0.0073, Accuracy: 99.87%, Grad Norm: 0.07739\n",
      "Epoch [950/10044], Batch [4/7], Loss: 0.0083, Accuracy: 99.85%, Grad Norm: 0.09919\n",
      "Epoch [950/10044], Batch [5/7], Loss: 0.0078, Accuracy: 99.85%, Grad Norm: 0.07879\n",
      "Epoch [950/10044], Batch [6/7], Loss: 0.0098, Accuracy: 99.82%, Grad Norm: 0.11414\n",
      "Epoch [950/10044], Batch [7/7], Loss: 0.0071, Accuracy: 99.88%, Grad Norm: 0.10968\n",
      "Epoch [950/10044], Loss: 0.0071\n",
      "Epoch [951/10044], Batch [1/7], Loss: 0.0110, Accuracy: 99.74%, Grad Norm: 0.12245\n",
      "Epoch [951/10044], Batch [2/7], Loss: 0.0085, Accuracy: 99.82%, Grad Norm: 0.09228\n",
      "Epoch [951/10044], Batch [3/7], Loss: 0.0079, Accuracy: 99.86%, Grad Norm: 0.10049\n",
      "Epoch [951/10044], Batch [4/7], Loss: 0.0099, Accuracy: 99.77%, Grad Norm: 0.10472\n",
      "Epoch [951/10044], Batch [5/7], Loss: 0.0092, Accuracy: 99.80%, Grad Norm: 0.09801\n",
      "Epoch [951/10044], Batch [6/7], Loss: 0.0085, Accuracy: 99.85%, Grad Norm: 0.09712\n",
      "Epoch [951/10044], Batch [7/7], Loss: 0.0094, Accuracy: 99.75%, Grad Norm: 0.14233\n",
      "Epoch [951/10044], Loss: 0.0094\n",
      "Epoch [952/10044], Batch [1/7], Loss: 0.0103, Accuracy: 99.78%, Grad Norm: 0.13149\n",
      "Epoch [952/10044], Batch [2/7], Loss: 0.0094, Accuracy: 99.75%, Grad Norm: 0.10465\n",
      "Epoch [952/10044], Batch [3/7], Loss: 0.0087, Accuracy: 99.84%, Grad Norm: 0.10030\n",
      "Epoch [952/10044], Batch [4/7], Loss: 0.0086, Accuracy: 99.81%, Grad Norm: 0.08579\n",
      "Epoch [952/10044], Batch [5/7], Loss: 0.0077, Accuracy: 99.86%, Grad Norm: 0.09051\n",
      "Epoch [952/10044], Batch [6/7], Loss: 0.0097, Accuracy: 99.82%, Grad Norm: 0.09578\n",
      "Epoch [952/10044], Batch [7/7], Loss: 0.0088, Accuracy: 99.77%, Grad Norm: 0.13565\n",
      "Epoch [952/10044], Loss: 0.0088\n",
      "Epoch [953/10044], Batch [1/7], Loss: 0.0110, Accuracy: 99.78%, Grad Norm: 0.13214\n",
      "Epoch [953/10044], Batch [2/7], Loss: 0.0113, Accuracy: 99.74%, Grad Norm: 0.12002\n",
      "Epoch [953/10044], Batch [3/7], Loss: 0.0082, Accuracy: 99.87%, Grad Norm: 0.07469\n",
      "Epoch [953/10044], Batch [4/7], Loss: 0.0086, Accuracy: 99.79%, Grad Norm: 0.08103\n",
      "Epoch [953/10044], Batch [5/7], Loss: 0.0081, Accuracy: 99.82%, Grad Norm: 0.09131\n",
      "Epoch [953/10044], Batch [6/7], Loss: 0.0094, Accuracy: 99.76%, Grad Norm: 0.11128\n",
      "Epoch [953/10044], Batch [7/7], Loss: 0.0077, Accuracy: 99.80%, Grad Norm: 0.11737\n",
      "Epoch [953/10044], Loss: 0.0077\n",
      "Epoch [954/10044], Batch [1/7], Loss: 0.0098, Accuracy: 99.78%, Grad Norm: 0.11550\n",
      "Epoch [954/10044], Batch [2/7], Loss: 0.0101, Accuracy: 99.72%, Grad Norm: 0.11734\n",
      "Epoch [954/10044], Batch [3/7], Loss: 0.0069, Accuracy: 99.92%, Grad Norm: 0.06305\n",
      "Epoch [954/10044], Batch [4/7], Loss: 0.0079, Accuracy: 99.82%, Grad Norm: 0.09283\n",
      "Epoch [954/10044], Batch [5/7], Loss: 0.0079, Accuracy: 99.87%, Grad Norm: 0.09872\n",
      "Epoch [954/10044], Batch [6/7], Loss: 0.0092, Accuracy: 99.82%, Grad Norm: 0.10531\n",
      "Epoch [954/10044], Batch [7/7], Loss: 0.0073, Accuracy: 99.87%, Grad Norm: 0.14565\n",
      "Epoch [954/10044], Loss: 0.0073\n",
      "Epoch [955/10044], Batch [1/7], Loss: 0.0105, Accuracy: 99.77%, Grad Norm: 0.10620\n",
      "Epoch [955/10044], Batch [2/7], Loss: 0.0103, Accuracy: 99.74%, Grad Norm: 0.10639\n",
      "Epoch [955/10044], Batch [3/7], Loss: 0.0074, Accuracy: 99.88%, Grad Norm: 0.08080\n",
      "Epoch [955/10044], Batch [4/7], Loss: 0.0088, Accuracy: 99.77%, Grad Norm: 0.10137\n",
      "Epoch [955/10044], Batch [5/7], Loss: 0.0071, Accuracy: 99.90%, Grad Norm: 0.09006\n",
      "Epoch [955/10044], Batch [6/7], Loss: 0.0096, Accuracy: 99.80%, Grad Norm: 0.10007\n",
      "Epoch [955/10044], Batch [7/7], Loss: 0.0062, Accuracy: 99.83%, Grad Norm: 0.11976\n",
      "Epoch [955/10044], Loss: 0.0062\n",
      "Epoch [956/10044], Batch [1/7], Loss: 0.0092, Accuracy: 99.80%, Grad Norm: 0.09466\n",
      "Epoch [956/10044], Batch [2/7], Loss: 0.0088, Accuracy: 99.81%, Grad Norm: 0.10844\n",
      "Epoch [956/10044], Batch [3/7], Loss: 0.0069, Accuracy: 99.92%, Grad Norm: 0.06284\n",
      "Epoch [956/10044], Batch [4/7], Loss: 0.0082, Accuracy: 99.87%, Grad Norm: 0.08592\n",
      "Epoch [956/10044], Batch [5/7], Loss: 0.0099, Accuracy: 99.77%, Grad Norm: 0.12723\n",
      "Epoch [956/10044], Batch [6/7], Loss: 0.0088, Accuracy: 99.86%, Grad Norm: 0.10155\n",
      "Epoch [956/10044], Batch [7/7], Loss: 0.0064, Accuracy: 99.88%, Grad Norm: 0.14269\n",
      "Epoch [956/10044], Loss: 0.0064\n",
      "Epoch [957/10044], Batch [1/7], Loss: 0.0104, Accuracy: 99.77%, Grad Norm: 0.10451\n",
      "Epoch [957/10044], Batch [2/7], Loss: 0.0090, Accuracy: 99.80%, Grad Norm: 0.08893\n",
      "Epoch [957/10044], Batch [3/7], Loss: 0.0065, Accuracy: 99.92%, Grad Norm: 0.06514\n",
      "Epoch [957/10044], Batch [4/7], Loss: 0.0095, Accuracy: 99.77%, Grad Norm: 0.09133\n",
      "Epoch [957/10044], Batch [5/7], Loss: 0.0079, Accuracy: 99.80%, Grad Norm: 0.09115\n",
      "Epoch [957/10044], Batch [6/7], Loss: 0.0088, Accuracy: 99.81%, Grad Norm: 0.09464\n",
      "Epoch [957/10044], Batch [7/7], Loss: 0.0061, Accuracy: 99.87%, Grad Norm: 0.11528\n",
      "Epoch [957/10044], Loss: 0.0061\n",
      "Epoch [958/10044], Batch [1/7], Loss: 0.0082, Accuracy: 99.84%, Grad Norm: 0.09321\n",
      "Epoch [958/10044], Batch [2/7], Loss: 0.0095, Accuracy: 99.82%, Grad Norm: 0.10129\n",
      "Epoch [958/10044], Batch [3/7], Loss: 0.0061, Accuracy: 99.92%, Grad Norm: 0.05240\n",
      "Epoch [958/10044], Batch [4/7], Loss: 0.0086, Accuracy: 99.83%, Grad Norm: 0.07605\n",
      "Epoch [958/10044], Batch [5/7], Loss: 0.0084, Accuracy: 99.82%, Grad Norm: 0.10853\n",
      "Epoch [958/10044], Batch [6/7], Loss: 0.0082, Accuracy: 99.77%, Grad Norm: 0.10108\n",
      "Epoch [958/10044], Batch [7/7], Loss: 0.0054, Accuracy: 99.90%, Grad Norm: 0.06892\n",
      "Epoch [958/10044], Loss: 0.0054\n",
      "Epoch [959/10044], Batch [1/7], Loss: 0.0079, Accuracy: 99.84%, Grad Norm: 0.07240\n",
      "Epoch [959/10044], Batch [2/7], Loss: 0.0101, Accuracy: 99.76%, Grad Norm: 0.11177\n",
      "Epoch [959/10044], Batch [3/7], Loss: 0.0067, Accuracy: 99.89%, Grad Norm: 0.07039\n",
      "Epoch [959/10044], Batch [4/7], Loss: 0.0078, Accuracy: 99.84%, Grad Norm: 0.07650\n",
      "Epoch [959/10044], Batch [5/7], Loss: 0.0093, Accuracy: 99.78%, Grad Norm: 0.12288\n",
      "Epoch [959/10044], Batch [6/7], Loss: 0.0103, Accuracy: 99.76%, Grad Norm: 0.14778\n",
      "Epoch [959/10044], Batch [7/7], Loss: 0.0055, Accuracy: 99.88%, Grad Norm: 0.10552\n",
      "Epoch [959/10044], Loss: 0.0055\n",
      "Epoch [960/10044], Batch [1/7], Loss: 0.0084, Accuracy: 99.82%, Grad Norm: 0.09271\n",
      "Epoch [960/10044], Batch [2/7], Loss: 0.0099, Accuracy: 99.76%, Grad Norm: 0.09921\n",
      "Epoch [960/10044], Batch [3/7], Loss: 0.0077, Accuracy: 99.88%, Grad Norm: 0.07066\n",
      "Epoch [960/10044], Batch [4/7], Loss: 0.0084, Accuracy: 99.83%, Grad Norm: 0.08749\n",
      "Epoch [960/10044], Batch [5/7], Loss: 0.0083, Accuracy: 99.87%, Grad Norm: 0.13638\n",
      "Epoch [960/10044], Batch [6/7], Loss: 0.0113, Accuracy: 99.69%, Grad Norm: 0.14387\n",
      "Epoch [960/10044], Batch [7/7], Loss: 0.0091, Accuracy: 99.80%, Grad Norm: 0.16488\n",
      "Epoch [960/10044], Loss: 0.0091\n",
      "Epoch [961/10044], Batch [1/7], Loss: 0.0080, Accuracy: 99.83%, Grad Norm: 0.08387\n",
      "Epoch [961/10044], Batch [2/7], Loss: 0.0105, Accuracy: 99.72%, Grad Norm: 0.10829\n",
      "Epoch [961/10044], Batch [3/7], Loss: 0.0087, Accuracy: 99.85%, Grad Norm: 0.09153\n",
      "Epoch [961/10044], Batch [4/7], Loss: 0.0089, Accuracy: 99.79%, Grad Norm: 0.09250\n",
      "Epoch [961/10044], Batch [5/7], Loss: 0.0077, Accuracy: 99.88%, Grad Norm: 0.09777\n",
      "Epoch [961/10044], Batch [6/7], Loss: 0.0104, Accuracy: 99.75%, Grad Norm: 0.15790\n",
      "Epoch [961/10044], Batch [7/7], Loss: 0.0062, Accuracy: 99.92%, Grad Norm: 0.10794\n",
      "Epoch [961/10044], Loss: 0.0062\n",
      "Epoch [962/10044], Batch [1/7], Loss: 0.0095, Accuracy: 99.80%, Grad Norm: 0.11492\n",
      "Epoch [962/10044], Batch [2/7], Loss: 0.0089, Accuracy: 99.83%, Grad Norm: 0.09612\n",
      "Epoch [962/10044], Batch [3/7], Loss: 0.0070, Accuracy: 99.89%, Grad Norm: 0.07099\n",
      "Epoch [962/10044], Batch [4/7], Loss: 0.0087, Accuracy: 99.83%, Grad Norm: 0.08755\n",
      "Epoch [962/10044], Batch [5/7], Loss: 0.0087, Accuracy: 99.82%, Grad Norm: 0.09629\n",
      "Epoch [962/10044], Batch [6/7], Loss: 0.0100, Accuracy: 99.78%, Grad Norm: 0.10882\n",
      "Epoch [962/10044], Batch [7/7], Loss: 0.0065, Accuracy: 99.87%, Grad Norm: 0.10000\n",
      "Epoch [962/10044], Loss: 0.0065\n",
      "Epoch [963/10044], Batch [1/7], Loss: 0.0086, Accuracy: 99.82%, Grad Norm: 0.08636\n",
      "Epoch [963/10044], Batch [2/7], Loss: 0.0091, Accuracy: 99.78%, Grad Norm: 0.09229\n",
      "Epoch [963/10044], Batch [3/7], Loss: 0.0083, Accuracy: 99.89%, Grad Norm: 0.07351\n",
      "Epoch [963/10044], Batch [4/7], Loss: 0.0075, Accuracy: 99.87%, Grad Norm: 0.07374\n",
      "Epoch [963/10044], Batch [5/7], Loss: 0.0078, Accuracy: 99.82%, Grad Norm: 0.09505\n",
      "Epoch [963/10044], Batch [6/7], Loss: 0.0092, Accuracy: 99.77%, Grad Norm: 0.09247\n",
      "Epoch [963/10044], Batch [7/7], Loss: 0.0064, Accuracy: 99.88%, Grad Norm: 0.09110\n",
      "Epoch [963/10044], Loss: 0.0064\n",
      "Epoch [964/10044], Batch [1/7], Loss: 0.0080, Accuracy: 99.83%, Grad Norm: 0.08516\n",
      "Epoch [964/10044], Batch [2/7], Loss: 0.0087, Accuracy: 99.82%, Grad Norm: 0.09424\n",
      "Epoch [964/10044], Batch [3/7], Loss: 0.0069, Accuracy: 99.91%, Grad Norm: 0.07034\n",
      "Epoch [964/10044], Batch [4/7], Loss: 0.0082, Accuracy: 99.87%, Grad Norm: 0.08231\n",
      "Epoch [964/10044], Batch [5/7], Loss: 0.0085, Accuracy: 99.82%, Grad Norm: 0.10031\n",
      "Epoch [964/10044], Batch [6/7], Loss: 0.0104, Accuracy: 99.77%, Grad Norm: 0.10342\n",
      "Epoch [964/10044], Batch [7/7], Loss: 0.0050, Accuracy: 99.93%, Grad Norm: 0.07876\n",
      "Epoch [964/10044], Loss: 0.0050\n",
      "Epoch [965/10044], Batch [1/7], Loss: 0.0095, Accuracy: 99.78%, Grad Norm: 0.09811\n",
      "Epoch [965/10044], Batch [2/7], Loss: 0.0104, Accuracy: 99.79%, Grad Norm: 0.09542\n",
      "Epoch [965/10044], Batch [3/7], Loss: 0.0057, Accuracy: 99.92%, Grad Norm: 0.04580\n",
      "Epoch [965/10044], Batch [4/7], Loss: 0.0075, Accuracy: 99.85%, Grad Norm: 0.06765\n",
      "Epoch [965/10044], Batch [5/7], Loss: 0.0091, Accuracy: 99.77%, Grad Norm: 0.11121\n",
      "Epoch [965/10044], Batch [6/7], Loss: 0.0080, Accuracy: 99.82%, Grad Norm: 0.08363\n",
      "Epoch [965/10044], Batch [7/7], Loss: 0.0067, Accuracy: 99.87%, Grad Norm: 0.11324\n",
      "Epoch [965/10044], Loss: 0.0067\n",
      "Epoch [966/10044], Batch [1/7], Loss: 0.0076, Accuracy: 99.86%, Grad Norm: 0.08775\n",
      "Epoch [966/10044], Batch [2/7], Loss: 0.0084, Accuracy: 99.84%, Grad Norm: 0.09053\n",
      "Epoch [966/10044], Batch [3/7], Loss: 0.0078, Accuracy: 99.84%, Grad Norm: 0.08453\n",
      "Epoch [966/10044], Batch [4/7], Loss: 0.0081, Accuracy: 99.83%, Grad Norm: 0.08327\n",
      "Epoch [966/10044], Batch [5/7], Loss: 0.0074, Accuracy: 99.85%, Grad Norm: 0.08407\n",
      "Epoch [966/10044], Batch [6/7], Loss: 0.0102, Accuracy: 99.74%, Grad Norm: 0.10962\n",
      "Epoch [966/10044], Batch [7/7], Loss: 0.0064, Accuracy: 99.87%, Grad Norm: 0.11622\n",
      "Epoch [966/10044], Loss: 0.0064\n",
      "Epoch [967/10044], Batch [1/7], Loss: 0.0077, Accuracy: 99.82%, Grad Norm: 0.08410\n",
      "Epoch [967/10044], Batch [2/7], Loss: 0.0094, Accuracy: 99.75%, Grad Norm: 0.10884\n",
      "Epoch [967/10044], Batch [3/7], Loss: 0.0063, Accuracy: 99.93%, Grad Norm: 0.06209\n",
      "Epoch [967/10044], Batch [4/7], Loss: 0.0092, Accuracy: 99.80%, Grad Norm: 0.10716\n",
      "Epoch [967/10044], Batch [5/7], Loss: 0.0091, Accuracy: 99.84%, Grad Norm: 0.09833\n",
      "Epoch [967/10044], Batch [6/7], Loss: 0.0089, Accuracy: 99.79%, Grad Norm: 0.10558\n",
      "Epoch [967/10044], Batch [7/7], Loss: 0.0076, Accuracy: 99.77%, Grad Norm: 0.14407\n",
      "Epoch [967/10044], Loss: 0.0076\n",
      "Epoch [968/10044], Batch [1/7], Loss: 0.0079, Accuracy: 99.82%, Grad Norm: 0.08223\n",
      "Epoch [968/10044], Batch [2/7], Loss: 0.0094, Accuracy: 99.81%, Grad Norm: 0.10157\n",
      "Epoch [968/10044], Batch [3/7], Loss: 0.0066, Accuracy: 99.93%, Grad Norm: 0.05882\n",
      "Epoch [968/10044], Batch [4/7], Loss: 0.0087, Accuracy: 99.80%, Grad Norm: 0.10012\n",
      "Epoch [968/10044], Batch [5/7], Loss: 0.0083, Accuracy: 99.83%, Grad Norm: 0.09136\n",
      "Epoch [968/10044], Batch [6/7], Loss: 0.0093, Accuracy: 99.77%, Grad Norm: 0.11801\n",
      "Epoch [968/10044], Batch [7/7], Loss: 0.0065, Accuracy: 99.88%, Grad Norm: 0.12502\n",
      "Epoch [968/10044], Loss: 0.0065\n",
      "Epoch [969/10044], Batch [1/7], Loss: 0.0074, Accuracy: 99.87%, Grad Norm: 0.07817\n",
      "Epoch [969/10044], Batch [2/7], Loss: 0.0087, Accuracy: 99.82%, Grad Norm: 0.08877\n",
      "Epoch [969/10044], Batch [3/7], Loss: 0.0062, Accuracy: 99.93%, Grad Norm: 0.05155\n",
      "Epoch [969/10044], Batch [4/7], Loss: 0.0083, Accuracy: 99.82%, Grad Norm: 0.09428\n",
      "Epoch [969/10044], Batch [5/7], Loss: 0.0075, Accuracy: 99.87%, Grad Norm: 0.10058\n",
      "Epoch [969/10044], Batch [6/7], Loss: 0.0088, Accuracy: 99.77%, Grad Norm: 0.10546\n",
      "Epoch [969/10044], Batch [7/7], Loss: 0.0071, Accuracy: 99.88%, Grad Norm: 0.10599\n",
      "Epoch [969/10044], Loss: 0.0071\n",
      "Epoch [970/10044], Batch [1/7], Loss: 0.0082, Accuracy: 99.83%, Grad Norm: 0.08229\n",
      "Epoch [970/10044], Batch [2/7], Loss: 0.0102, Accuracy: 99.74%, Grad Norm: 0.10787\n",
      "Epoch [970/10044], Batch [3/7], Loss: 0.0055, Accuracy: 99.97%, Grad Norm: 0.03927\n",
      "Epoch [970/10044], Batch [4/7], Loss: 0.0074, Accuracy: 99.84%, Grad Norm: 0.07314\n",
      "Epoch [970/10044], Batch [5/7], Loss: 0.0081, Accuracy: 99.78%, Grad Norm: 0.10167\n",
      "Epoch [970/10044], Batch [6/7], Loss: 0.0083, Accuracy: 99.81%, Grad Norm: 0.07891\n",
      "Epoch [970/10044], Batch [7/7], Loss: 0.0063, Accuracy: 99.88%, Grad Norm: 0.11288\n",
      "Epoch [970/10044], Loss: 0.0063\n",
      "Epoch [971/10044], Batch [1/7], Loss: 0.0086, Accuracy: 99.82%, Grad Norm: 0.09157\n",
      "Epoch [971/10044], Batch [2/7], Loss: 0.0082, Accuracy: 99.81%, Grad Norm: 0.08287\n",
      "Epoch [971/10044], Batch [3/7], Loss: 0.0072, Accuracy: 99.87%, Grad Norm: 0.07738\n",
      "Epoch [971/10044], Batch [4/7], Loss: 0.0083, Accuracy: 99.87%, Grad Norm: 0.08739\n",
      "Epoch [971/10044], Batch [5/7], Loss: 0.0075, Accuracy: 99.84%, Grad Norm: 0.08929\n",
      "Epoch [971/10044], Batch [6/7], Loss: 0.0081, Accuracy: 99.85%, Grad Norm: 0.07765\n",
      "Epoch [971/10044], Batch [7/7], Loss: 0.0072, Accuracy: 99.85%, Grad Norm: 0.10725\n",
      "Epoch [971/10044], Loss: 0.0072\n",
      "Epoch [972/10044], Batch [1/7], Loss: 0.0088, Accuracy: 99.75%, Grad Norm: 0.10383\n",
      "Epoch [972/10044], Batch [2/7], Loss: 0.0082, Accuracy: 99.83%, Grad Norm: 0.10851\n",
      "Epoch [972/10044], Batch [3/7], Loss: 0.0070, Accuracy: 99.87%, Grad Norm: 0.06554\n",
      "Epoch [972/10044], Batch [4/7], Loss: 0.0081, Accuracy: 99.87%, Grad Norm: 0.07848\n",
      "Epoch [972/10044], Batch [5/7], Loss: 0.0088, Accuracy: 99.79%, Grad Norm: 0.09216\n",
      "Epoch [972/10044], Batch [6/7], Loss: 0.0115, Accuracy: 99.67%, Grad Norm: 0.12916\n",
      "Epoch [972/10044], Batch [7/7], Loss: 0.0052, Accuracy: 99.92%, Grad Norm: 0.09086\n",
      "Epoch [972/10044], Loss: 0.0052\n",
      "Epoch [973/10044], Batch [1/7], Loss: 0.0076, Accuracy: 99.86%, Grad Norm: 0.08816\n",
      "Epoch [973/10044], Batch [2/7], Loss: 0.0088, Accuracy: 99.82%, Grad Norm: 0.11239\n",
      "Epoch [973/10044], Batch [3/7], Loss: 0.0062, Accuracy: 99.91%, Grad Norm: 0.05605\n",
      "Epoch [973/10044], Batch [4/7], Loss: 0.0083, Accuracy: 99.85%, Grad Norm: 0.08817\n",
      "Epoch [973/10044], Batch [5/7], Loss: 0.0079, Accuracy: 99.83%, Grad Norm: 0.08878\n",
      "Epoch [973/10044], Batch [6/7], Loss: 0.0095, Accuracy: 99.74%, Grad Norm: 0.11272\n",
      "Epoch [973/10044], Batch [7/7], Loss: 0.0074, Accuracy: 99.80%, Grad Norm: 0.14302\n",
      "Epoch [973/10044], Loss: 0.0074\n",
      "Epoch [974/10044], Batch [1/7], Loss: 0.0084, Accuracy: 99.82%, Grad Norm: 0.09860\n",
      "Epoch [974/10044], Batch [2/7], Loss: 0.0114, Accuracy: 99.69%, Grad Norm: 0.11468\n",
      "Epoch [974/10044], Batch [3/7], Loss: 0.0073, Accuracy: 99.82%, Grad Norm: 0.07581\n",
      "Epoch [974/10044], Batch [4/7], Loss: 0.0085, Accuracy: 99.81%, Grad Norm: 0.09895\n",
      "Epoch [974/10044], Batch [5/7], Loss: 0.0082, Accuracy: 99.82%, Grad Norm: 0.10799\n",
      "Epoch [974/10044], Batch [6/7], Loss: 0.0086, Accuracy: 99.81%, Grad Norm: 0.10178\n",
      "Epoch [974/10044], Batch [7/7], Loss: 0.0092, Accuracy: 99.78%, Grad Norm: 0.14915\n",
      "Epoch [974/10044], Loss: 0.0092\n",
      "Epoch [975/10044], Batch [1/7], Loss: 0.0089, Accuracy: 99.83%, Grad Norm: 0.09606\n",
      "Epoch [975/10044], Batch [2/7], Loss: 0.0100, Accuracy: 99.76%, Grad Norm: 0.12033\n",
      "Epoch [975/10044], Batch [3/7], Loss: 0.0079, Accuracy: 99.83%, Grad Norm: 0.07406\n",
      "Epoch [975/10044], Batch [4/7], Loss: 0.0087, Accuracy: 99.81%, Grad Norm: 0.09115\n",
      "Epoch [975/10044], Batch [5/7], Loss: 0.0088, Accuracy: 99.82%, Grad Norm: 0.11378\n",
      "Epoch [975/10044], Batch [6/7], Loss: 0.0098, Accuracy: 99.75%, Grad Norm: 0.11954\n",
      "Epoch [975/10044], Batch [7/7], Loss: 0.0060, Accuracy: 99.87%, Grad Norm: 0.10203\n",
      "Epoch [975/10044], Loss: 0.0060\n",
      "Epoch [976/10044], Batch [1/7], Loss: 0.0087, Accuracy: 99.80%, Grad Norm: 0.10597\n",
      "Epoch [976/10044], Batch [2/7], Loss: 0.0094, Accuracy: 99.77%, Grad Norm: 0.11215\n",
      "Epoch [976/10044], Batch [3/7], Loss: 0.0068, Accuracy: 99.86%, Grad Norm: 0.07374\n",
      "Epoch [976/10044], Batch [4/7], Loss: 0.0081, Accuracy: 99.80%, Grad Norm: 0.08662\n",
      "Epoch [976/10044], Batch [5/7], Loss: 0.0076, Accuracy: 99.87%, Grad Norm: 0.09187\n",
      "Epoch [976/10044], Batch [6/7], Loss: 0.0109, Accuracy: 99.69%, Grad Norm: 0.12003\n",
      "Epoch [976/10044], Batch [7/7], Loss: 0.0070, Accuracy: 99.83%, Grad Norm: 0.12330\n",
      "Epoch [976/10044], Loss: 0.0070\n",
      "Epoch [977/10044], Batch [1/7], Loss: 0.0085, Accuracy: 99.83%, Grad Norm: 0.09613\n",
      "Epoch [977/10044], Batch [2/7], Loss: 0.0091, Accuracy: 99.77%, Grad Norm: 0.11232\n",
      "Epoch [977/10044], Batch [3/7], Loss: 0.0062, Accuracy: 99.92%, Grad Norm: 0.06383\n",
      "Epoch [977/10044], Batch [4/7], Loss: 0.0077, Accuracy: 99.80%, Grad Norm: 0.07538\n",
      "Epoch [977/10044], Batch [5/7], Loss: 0.0079, Accuracy: 99.83%, Grad Norm: 0.08589\n",
      "Epoch [977/10044], Batch [6/7], Loss: 0.0077, Accuracy: 99.83%, Grad Norm: 0.07996\n",
      "Epoch [977/10044], Batch [7/7], Loss: 0.0052, Accuracy: 99.90%, Grad Norm: 0.08035\n",
      "Epoch [977/10044], Loss: 0.0052\n",
      "Epoch [978/10044], Batch [1/7], Loss: 0.0084, Accuracy: 99.84%, Grad Norm: 0.09530\n",
      "Epoch [978/10044], Batch [2/7], Loss: 0.0093, Accuracy: 99.80%, Grad Norm: 0.10817\n",
      "Epoch [978/10044], Batch [3/7], Loss: 0.0056, Accuracy: 99.92%, Grad Norm: 0.05298\n",
      "Epoch [978/10044], Batch [4/7], Loss: 0.0086, Accuracy: 99.82%, Grad Norm: 0.09231\n",
      "Epoch [978/10044], Batch [5/7], Loss: 0.0067, Accuracy: 99.87%, Grad Norm: 0.07237\n",
      "Epoch [978/10044], Batch [6/7], Loss: 0.0088, Accuracy: 99.72%, Grad Norm: 0.09855\n",
      "Epoch [978/10044], Batch [7/7], Loss: 0.0064, Accuracy: 99.90%, Grad Norm: 0.11787\n",
      "Epoch [978/10044], Loss: 0.0064\n",
      "Epoch [979/10044], Batch [1/7], Loss: 0.0083, Accuracy: 99.82%, Grad Norm: 0.09048\n",
      "Epoch [979/10044], Batch [2/7], Loss: 0.0084, Accuracy: 99.82%, Grad Norm: 0.10485\n",
      "Epoch [979/10044], Batch [3/7], Loss: 0.0065, Accuracy: 99.90%, Grad Norm: 0.05452\n",
      "Epoch [979/10044], Batch [4/7], Loss: 0.0085, Accuracy: 99.82%, Grad Norm: 0.09497\n",
      "Epoch [979/10044], Batch [5/7], Loss: 0.0076, Accuracy: 99.87%, Grad Norm: 0.09156\n",
      "Epoch [979/10044], Batch [6/7], Loss: 0.0075, Accuracy: 99.82%, Grad Norm: 0.09125\n",
      "Epoch [979/10044], Batch [7/7], Loss: 0.0075, Accuracy: 99.88%, Grad Norm: 0.10910\n",
      "Epoch [979/10044], Loss: 0.0075\n",
      "Epoch [980/10044], Batch [1/7], Loss: 0.0079, Accuracy: 99.87%, Grad Norm: 0.09947\n",
      "Epoch [980/10044], Batch [2/7], Loss: 0.0081, Accuracy: 99.83%, Grad Norm: 0.10331\n",
      "Epoch [980/10044], Batch [3/7], Loss: 0.0064, Accuracy: 99.89%, Grad Norm: 0.07116\n",
      "Epoch [980/10044], Batch [4/7], Loss: 0.0070, Accuracy: 99.84%, Grad Norm: 0.07934\n",
      "Epoch [980/10044], Batch [5/7], Loss: 0.0067, Accuracy: 99.84%, Grad Norm: 0.09429\n",
      "Epoch [980/10044], Batch [6/7], Loss: 0.0083, Accuracy: 99.81%, Grad Norm: 0.09329\n",
      "Epoch [980/10044], Batch [7/7], Loss: 0.0078, Accuracy: 99.82%, Grad Norm: 0.11917\n",
      "Epoch [980/10044], Loss: 0.0078\n",
      "Epoch [981/10044], Batch [1/7], Loss: 0.0094, Accuracy: 99.79%, Grad Norm: 0.10809\n",
      "Epoch [981/10044], Batch [2/7], Loss: 0.0096, Accuracy: 99.74%, Grad Norm: 0.12072\n",
      "Epoch [981/10044], Batch [3/7], Loss: 0.0064, Accuracy: 99.90%, Grad Norm: 0.05814\n",
      "Epoch [981/10044], Batch [4/7], Loss: 0.0073, Accuracy: 99.88%, Grad Norm: 0.07485\n",
      "Epoch [981/10044], Batch [5/7], Loss: 0.0078, Accuracy: 99.83%, Grad Norm: 0.08341\n",
      "Epoch [981/10044], Batch [6/7], Loss: 0.0082, Accuracy: 99.81%, Grad Norm: 0.08955\n",
      "Epoch [981/10044], Batch [7/7], Loss: 0.0076, Accuracy: 99.83%, Grad Norm: 0.16654\n",
      "Epoch [981/10044], Loss: 0.0076\n",
      "Epoch [982/10044], Batch [1/7], Loss: 0.0083, Accuracy: 99.81%, Grad Norm: 0.10017\n",
      "Epoch [982/10044], Batch [2/7], Loss: 0.0089, Accuracy: 99.82%, Grad Norm: 0.09960\n",
      "Epoch [982/10044], Batch [3/7], Loss: 0.0073, Accuracy: 99.85%, Grad Norm: 0.07483\n",
      "Epoch [982/10044], Batch [4/7], Loss: 0.0084, Accuracy: 99.80%, Grad Norm: 0.09044\n",
      "Epoch [982/10044], Batch [5/7], Loss: 0.0069, Accuracy: 99.88%, Grad Norm: 0.09140\n",
      "Epoch [982/10044], Batch [6/7], Loss: 0.0093, Accuracy: 99.76%, Grad Norm: 0.10965\n",
      "Epoch [982/10044], Batch [7/7], Loss: 0.0085, Accuracy: 99.77%, Grad Norm: 0.21660\n",
      "Epoch [982/10044], Loss: 0.0085\n",
      "Epoch [983/10044], Batch [1/7], Loss: 0.0081, Accuracy: 99.84%, Grad Norm: 0.09205\n",
      "Epoch [983/10044], Batch [2/7], Loss: 0.0105, Accuracy: 99.71%, Grad Norm: 0.11565\n",
      "Epoch [983/10044], Batch [3/7], Loss: 0.0078, Accuracy: 99.88%, Grad Norm: 0.07707\n",
      "Epoch [983/10044], Batch [4/7], Loss: 0.0086, Accuracy: 99.82%, Grad Norm: 0.08313\n",
      "Epoch [983/10044], Batch [5/7], Loss: 0.0069, Accuracy: 99.88%, Grad Norm: 0.08497\n",
      "Epoch [983/10044], Batch [6/7], Loss: 0.0094, Accuracy: 99.73%, Grad Norm: 0.12564\n",
      "Epoch [983/10044], Batch [7/7], Loss: 0.0090, Accuracy: 99.72%, Grad Norm: 0.18201\n",
      "Epoch [983/10044], Loss: 0.0090\n",
      "Epoch [984/10044], Batch [1/7], Loss: 0.0092, Accuracy: 99.77%, Grad Norm: 0.10101\n",
      "Epoch [984/10044], Batch [2/7], Loss: 0.0095, Accuracy: 99.81%, Grad Norm: 0.10731\n",
      "Epoch [984/10044], Batch [3/7], Loss: 0.0072, Accuracy: 99.86%, Grad Norm: 0.07879\n",
      "Epoch [984/10044], Batch [4/7], Loss: 0.0096, Accuracy: 99.77%, Grad Norm: 0.09488\n",
      "Epoch [984/10044], Batch [5/7], Loss: 0.0091, Accuracy: 99.81%, Grad Norm: 0.10669\n",
      "Epoch [984/10044], Batch [6/7], Loss: 0.0094, Accuracy: 99.75%, Grad Norm: 0.12072\n",
      "Epoch [984/10044], Batch [7/7], Loss: 0.0117, Accuracy: 99.67%, Grad Norm: 0.25730\n",
      "Epoch [984/10044], Loss: 0.0117\n",
      "Epoch [985/10044], Batch [1/7], Loss: 0.0083, Accuracy: 99.83%, Grad Norm: 0.10593\n",
      "Epoch [985/10044], Batch [2/7], Loss: 0.0104, Accuracy: 99.75%, Grad Norm: 0.13616\n",
      "Epoch [985/10044], Batch [3/7], Loss: 0.0087, Accuracy: 99.82%, Grad Norm: 0.10512\n",
      "Epoch [985/10044], Batch [4/7], Loss: 0.0096, Accuracy: 99.76%, Grad Norm: 0.11375\n",
      "Epoch [985/10044], Batch [5/7], Loss: 0.0087, Accuracy: 99.83%, Grad Norm: 0.11046\n",
      "Epoch [985/10044], Batch [6/7], Loss: 0.0090, Accuracy: 99.77%, Grad Norm: 0.10277\n",
      "Epoch [985/10044], Batch [7/7], Loss: 0.0100, Accuracy: 99.87%, Grad Norm: 0.16481\n",
      "Epoch [985/10044], Loss: 0.0100\n",
      "Epoch [986/10044], Batch [1/7], Loss: 0.0072, Accuracy: 99.90%, Grad Norm: 0.08533\n",
      "Epoch [986/10044], Batch [2/7], Loss: 0.0120, Accuracy: 99.70%, Grad Norm: 0.15593\n",
      "Epoch [986/10044], Batch [3/7], Loss: 0.0066, Accuracy: 99.93%, Grad Norm: 0.07325\n",
      "Epoch [986/10044], Batch [4/7], Loss: 0.0077, Accuracy: 99.86%, Grad Norm: 0.08938\n",
      "Epoch [986/10044], Batch [5/7], Loss: 0.0074, Accuracy: 99.87%, Grad Norm: 0.09988\n",
      "Epoch [986/10044], Batch [6/7], Loss: 0.0093, Accuracy: 99.76%, Grad Norm: 0.09976\n",
      "Epoch [986/10044], Batch [7/7], Loss: 0.0073, Accuracy: 99.82%, Grad Norm: 0.13855\n",
      "Epoch [986/10044], Loss: 0.0073\n",
      "Epoch [987/10044], Batch [1/7], Loss: 0.0099, Accuracy: 99.86%, Grad Norm: 0.11083\n",
      "Epoch [987/10044], Batch [2/7], Loss: 0.0095, Accuracy: 99.76%, Grad Norm: 0.12287\n",
      "Epoch [987/10044], Batch [3/7], Loss: 0.0073, Accuracy: 99.89%, Grad Norm: 0.07477\n",
      "Epoch [987/10044], Batch [4/7], Loss: 0.0084, Accuracy: 99.83%, Grad Norm: 0.08609\n",
      "Epoch [987/10044], Batch [5/7], Loss: 0.0080, Accuracy: 99.79%, Grad Norm: 0.10753\n",
      "Epoch [987/10044], Batch [6/7], Loss: 0.0093, Accuracy: 99.80%, Grad Norm: 0.13748\n",
      "Epoch [987/10044], Batch [7/7], Loss: 0.0056, Accuracy: 99.92%, Grad Norm: 0.09517\n",
      "Epoch [987/10044], Loss: 0.0056\n",
      "Epoch [988/10044], Batch [1/7], Loss: 0.0104, Accuracy: 99.80%, Grad Norm: 0.11937\n",
      "Epoch [988/10044], Batch [2/7], Loss: 0.0096, Accuracy: 99.73%, Grad Norm: 0.10287\n",
      "Epoch [988/10044], Batch [3/7], Loss: 0.0060, Accuracy: 99.89%, Grad Norm: 0.06303\n",
      "Epoch [988/10044], Batch [4/7], Loss: 0.0088, Accuracy: 99.82%, Grad Norm: 0.09129\n",
      "Epoch [988/10044], Batch [5/7], Loss: 0.0062, Accuracy: 99.93%, Grad Norm: 0.07456\n",
      "Epoch [988/10044], Batch [6/7], Loss: 0.0091, Accuracy: 99.77%, Grad Norm: 0.10397\n",
      "Epoch [988/10044], Batch [7/7], Loss: 0.0070, Accuracy: 99.83%, Grad Norm: 0.12115\n",
      "Epoch [988/10044], Loss: 0.0070\n",
      "Epoch [989/10044], Batch [1/7], Loss: 0.0082, Accuracy: 99.82%, Grad Norm: 0.09402\n",
      "Epoch [989/10044], Batch [2/7], Loss: 0.0078, Accuracy: 99.83%, Grad Norm: 0.07987\n",
      "Epoch [989/10044], Batch [3/7], Loss: 0.0068, Accuracy: 99.87%, Grad Norm: 0.06679\n",
      "Epoch [989/10044], Batch [4/7], Loss: 0.0086, Accuracy: 99.81%, Grad Norm: 0.09057\n",
      "Epoch [989/10044], Batch [5/7], Loss: 0.0073, Accuracy: 99.87%, Grad Norm: 0.08852\n",
      "Epoch [989/10044], Batch [6/7], Loss: 0.0089, Accuracy: 99.79%, Grad Norm: 0.09929\n",
      "Epoch [989/10044], Batch [7/7], Loss: 0.0080, Accuracy: 99.85%, Grad Norm: 0.16975\n",
      "Epoch [989/10044], Loss: 0.0080\n",
      "Epoch [990/10044], Batch [1/7], Loss: 0.0085, Accuracy: 99.84%, Grad Norm: 0.09996\n",
      "Epoch [990/10044], Batch [2/7], Loss: 0.0094, Accuracy: 99.80%, Grad Norm: 0.09481\n",
      "Epoch [990/10044], Batch [3/7], Loss: 0.0065, Accuracy: 99.90%, Grad Norm: 0.07538\n",
      "Epoch [990/10044], Batch [4/7], Loss: 0.0091, Accuracy: 99.79%, Grad Norm: 0.09782\n",
      "Epoch [990/10044], Batch [5/7], Loss: 0.0098, Accuracy: 99.79%, Grad Norm: 0.12095\n",
      "Epoch [990/10044], Batch [6/7], Loss: 0.0099, Accuracy: 99.75%, Grad Norm: 0.10345\n",
      "Epoch [990/10044], Batch [7/7], Loss: 0.0081, Accuracy: 99.75%, Grad Norm: 0.17621\n",
      "Epoch [990/10044], Loss: 0.0081\n",
      "Epoch [991/10044], Batch [1/7], Loss: 0.0078, Accuracy: 99.87%, Grad Norm: 0.07319\n",
      "Epoch [991/10044], Batch [2/7], Loss: 0.0077, Accuracy: 99.82%, Grad Norm: 0.08495\n",
      "Epoch [991/10044], Batch [3/7], Loss: 0.0071, Accuracy: 99.87%, Grad Norm: 0.06710\n",
      "Epoch [991/10044], Batch [4/7], Loss: 0.0074, Accuracy: 99.88%, Grad Norm: 0.06922\n",
      "Epoch [991/10044], Batch [5/7], Loss: 0.0082, Accuracy: 99.79%, Grad Norm: 0.09751\n",
      "Epoch [991/10044], Batch [6/7], Loss: 0.0092, Accuracy: 99.80%, Grad Norm: 0.10153\n",
      "Epoch [991/10044], Batch [7/7], Loss: 0.0065, Accuracy: 99.87%, Grad Norm: 0.12622\n",
      "Epoch [991/10044], Loss: 0.0065\n",
      "Epoch [992/10044], Batch [1/7], Loss: 0.0075, Accuracy: 99.84%, Grad Norm: 0.07787\n",
      "Epoch [992/10044], Batch [2/7], Loss: 0.0085, Accuracy: 99.80%, Grad Norm: 0.08640\n",
      "Epoch [992/10044], Batch [3/7], Loss: 0.0069, Accuracy: 99.89%, Grad Norm: 0.07402\n",
      "Epoch [992/10044], Batch [4/7], Loss: 0.0081, Accuracy: 99.81%, Grad Norm: 0.09266\n",
      "Epoch [992/10044], Batch [5/7], Loss: 0.0059, Accuracy: 99.92%, Grad Norm: 0.06344\n",
      "Epoch [992/10044], Batch [6/7], Loss: 0.0102, Accuracy: 99.73%, Grad Norm: 0.11343\n",
      "Epoch [992/10044], Batch [7/7], Loss: 0.0057, Accuracy: 99.87%, Grad Norm: 0.08871\n",
      "Epoch [992/10044], Loss: 0.0057\n",
      "Epoch [993/10044], Batch [1/7], Loss: 0.0070, Accuracy: 99.87%, Grad Norm: 0.08236\n",
      "Epoch [993/10044], Batch [2/7], Loss: 0.0078, Accuracy: 99.78%, Grad Norm: 0.08815\n",
      "Epoch [993/10044], Batch [3/7], Loss: 0.0071, Accuracy: 99.89%, Grad Norm: 0.08127\n",
      "Epoch [993/10044], Batch [4/7], Loss: 0.0078, Accuracy: 99.80%, Grad Norm: 0.08079\n",
      "Epoch [993/10044], Batch [5/7], Loss: 0.0065, Accuracy: 99.90%, Grad Norm: 0.07606\n",
      "Epoch [993/10044], Batch [6/7], Loss: 0.0091, Accuracy: 99.77%, Grad Norm: 0.10508\n",
      "Epoch [993/10044], Batch [7/7], Loss: 0.0098, Accuracy: 99.78%, Grad Norm: 0.16832\n",
      "Epoch [993/10044], Loss: 0.0098\n",
      "Epoch [994/10044], Batch [1/7], Loss: 0.0076, Accuracy: 99.87%, Grad Norm: 0.07645\n",
      "Epoch [994/10044], Batch [2/7], Loss: 0.0093, Accuracy: 99.76%, Grad Norm: 0.09718\n",
      "Epoch [994/10044], Batch [3/7], Loss: 0.0060, Accuracy: 99.92%, Grad Norm: 0.05740\n",
      "Epoch [994/10044], Batch [4/7], Loss: 0.0075, Accuracy: 99.87%, Grad Norm: 0.07415\n",
      "Epoch [994/10044], Batch [5/7], Loss: 0.0075, Accuracy: 99.87%, Grad Norm: 0.10069\n",
      "Epoch [994/10044], Batch [6/7], Loss: 0.0102, Accuracy: 99.76%, Grad Norm: 0.13383\n",
      "Epoch [994/10044], Batch [7/7], Loss: 0.0059, Accuracy: 99.90%, Grad Norm: 0.10299\n",
      "Epoch [994/10044], Loss: 0.0059\n",
      "Epoch [995/10044], Batch [1/7], Loss: 0.0067, Accuracy: 99.86%, Grad Norm: 0.07209\n",
      "Epoch [995/10044], Batch [2/7], Loss: 0.0084, Accuracy: 99.82%, Grad Norm: 0.08322\n",
      "Epoch [995/10044], Batch [3/7], Loss: 0.0067, Accuracy: 99.89%, Grad Norm: 0.07616\n",
      "Epoch [995/10044], Batch [4/7], Loss: 0.0083, Accuracy: 99.84%, Grad Norm: 0.09698\n",
      "Epoch [995/10044], Batch [5/7], Loss: 0.0066, Accuracy: 99.88%, Grad Norm: 0.09010\n",
      "Epoch [995/10044], Batch [6/7], Loss: 0.0075, Accuracy: 99.86%, Grad Norm: 0.08054\n",
      "Epoch [995/10044], Batch [7/7], Loss: 0.0054, Accuracy: 99.92%, Grad Norm: 0.07398\n",
      "Epoch [995/10044], Loss: 0.0054\n",
      "Epoch [996/10044], Batch [1/7], Loss: 0.0067, Accuracy: 99.87%, Grad Norm: 0.07819\n",
      "Epoch [996/10044], Batch [2/7], Loss: 0.0080, Accuracy: 99.79%, Grad Norm: 0.09748\n",
      "Epoch [996/10044], Batch [3/7], Loss: 0.0069, Accuracy: 99.90%, Grad Norm: 0.06717\n",
      "Epoch [996/10044], Batch [4/7], Loss: 0.0068, Accuracy: 99.88%, Grad Norm: 0.08201\n",
      "Epoch [996/10044], Batch [5/7], Loss: 0.0065, Accuracy: 99.87%, Grad Norm: 0.07002\n",
      "Epoch [996/10044], Batch [6/7], Loss: 0.0082, Accuracy: 99.85%, Grad Norm: 0.08853\n",
      "Epoch [996/10044], Batch [7/7], Loss: 0.0061, Accuracy: 99.88%, Grad Norm: 0.11986\n",
      "Epoch [996/10044], Loss: 0.0061\n",
      "Epoch [997/10044], Batch [1/7], Loss: 0.0072, Accuracy: 99.86%, Grad Norm: 0.07166\n",
      "Epoch [997/10044], Batch [2/7], Loss: 0.0093, Accuracy: 99.78%, Grad Norm: 0.10358\n",
      "Epoch [997/10044], Batch [3/7], Loss: 0.0077, Accuracy: 99.84%, Grad Norm: 0.09811\n",
      "Epoch [997/10044], Batch [4/7], Loss: 0.0068, Accuracy: 99.92%, Grad Norm: 0.07412\n",
      "Epoch [997/10044], Batch [5/7], Loss: 0.0069, Accuracy: 99.89%, Grad Norm: 0.07723\n",
      "Epoch [997/10044], Batch [6/7], Loss: 0.0088, Accuracy: 99.75%, Grad Norm: 0.10653\n",
      "Epoch [997/10044], Batch [7/7], Loss: 0.0074, Accuracy: 99.83%, Grad Norm: 0.12923\n",
      "Epoch [997/10044], Loss: 0.0074\n",
      "Epoch [998/10044], Batch [1/7], Loss: 0.0093, Accuracy: 99.80%, Grad Norm: 0.09882\n",
      "Epoch [998/10044], Batch [2/7], Loss: 0.0100, Accuracy: 99.74%, Grad Norm: 0.11951\n",
      "Epoch [998/10044], Batch [3/7], Loss: 0.0063, Accuracy: 99.90%, Grad Norm: 0.06473\n",
      "Epoch [998/10044], Batch [4/7], Loss: 0.0091, Accuracy: 99.79%, Grad Norm: 0.10995\n",
      "Epoch [998/10044], Batch [5/7], Loss: 0.0059, Accuracy: 99.91%, Grad Norm: 0.07553\n",
      "Epoch [998/10044], Batch [6/7], Loss: 0.0097, Accuracy: 99.73%, Grad Norm: 0.10676\n",
      "Epoch [998/10044], Batch [7/7], Loss: 0.0101, Accuracy: 99.77%, Grad Norm: 0.20402\n",
      "Epoch [998/10044], Loss: 0.0101\n",
      "Epoch [999/10044], Batch [1/7], Loss: 0.0063, Accuracy: 99.87%, Grad Norm: 0.06167\n",
      "Epoch [999/10044], Batch [2/7], Loss: 0.0081, Accuracy: 99.83%, Grad Norm: 0.09904\n",
      "Epoch [999/10044], Batch [3/7], Loss: 0.0057, Accuracy: 99.93%, Grad Norm: 0.05355\n",
      "Epoch [999/10044], Batch [4/7], Loss: 0.0080, Accuracy: 99.83%, Grad Norm: 0.08750\n",
      "Epoch [999/10044], Batch [5/7], Loss: 0.0063, Accuracy: 99.87%, Grad Norm: 0.07765\n",
      "Epoch [999/10044], Batch [6/7], Loss: 0.0095, Accuracy: 99.77%, Grad Norm: 0.11672\n",
      "Epoch [999/10044], Batch [7/7], Loss: 0.0098, Accuracy: 99.80%, Grad Norm: 0.19888\n",
      "Epoch [999/10044], Loss: 0.0098\n",
      "Epoch [1000/10044], Batch [1/7], Loss: 0.0089, Accuracy: 99.81%, Grad Norm: 0.09253\n",
      "Epoch [1000/10044], Batch [2/7], Loss: 0.0096, Accuracy: 99.74%, Grad Norm: 0.10284\n",
      "Epoch [1000/10044], Batch [3/7], Loss: 0.0055, Accuracy: 99.96%, Grad Norm: 0.04739\n",
      "Epoch [1000/10044], Batch [4/7], Loss: 0.0074, Accuracy: 99.85%, Grad Norm: 0.09233\n",
      "Epoch [1000/10044], Batch [5/7], Loss: 0.0075, Accuracy: 99.84%, Grad Norm: 0.08664\n",
      "Epoch [1000/10044], Batch [6/7], Loss: 0.0095, Accuracy: 99.77%, Grad Norm: 0.12804\n",
      "Epoch [1000/10044], Batch [7/7], Loss: 0.0067, Accuracy: 99.90%, Grad Norm: 0.11894\n",
      "Epoch [1000/10044], Loss: 0.0067\n",
      "Epoch [1001/10044], Batch [1/7], Loss: 0.0068, Accuracy: 99.90%, Grad Norm: 0.07437\n",
      "Epoch [1001/10044], Batch [2/7], Loss: 0.0111, Accuracy: 99.65%, Grad Norm: 0.14242\n",
      "Epoch [1001/10044], Batch [3/7], Loss: 0.0070, Accuracy: 99.87%, Grad Norm: 0.07843\n",
      "Epoch [1001/10044], Batch [4/7], Loss: 0.0090, Accuracy: 99.83%, Grad Norm: 0.09698\n",
      "Epoch [1001/10044], Batch [5/7], Loss: 0.0079, Accuracy: 99.81%, Grad Norm: 0.08769\n",
      "Epoch [1001/10044], Batch [6/7], Loss: 0.0074, Accuracy: 99.87%, Grad Norm: 0.08222\n",
      "Epoch [1001/10044], Batch [7/7], Loss: 0.0093, Accuracy: 99.77%, Grad Norm: 0.15207\n",
      "Epoch [1001/10044], Loss: 0.0093\n",
      "Epoch [1002/10044], Batch [1/7], Loss: 0.0100, Accuracy: 99.78%, Grad Norm: 0.10247\n",
      "Epoch [1002/10044], Batch [2/7], Loss: 0.0101, Accuracy: 99.76%, Grad Norm: 0.11663\n",
      "Epoch [1002/10044], Batch [3/7], Loss: 0.0066, Accuracy: 99.90%, Grad Norm: 0.07146\n",
      "Epoch [1002/10044], Batch [4/7], Loss: 0.0089, Accuracy: 99.73%, Grad Norm: 0.09863\n",
      "Epoch [1002/10044], Batch [5/7], Loss: 0.0074, Accuracy: 99.79%, Grad Norm: 0.09201\n",
      "Epoch [1002/10044], Batch [6/7], Loss: 0.0085, Accuracy: 99.78%, Grad Norm: 0.09390\n",
      "Epoch [1002/10044], Batch [7/7], Loss: 0.0073, Accuracy: 99.80%, Grad Norm: 0.12351\n",
      "Epoch [1002/10044], Loss: 0.0073\n",
      "Epoch [1003/10044], Batch [1/7], Loss: 0.0085, Accuracy: 99.80%, Grad Norm: 0.08560\n",
      "Epoch [1003/10044], Batch [2/7], Loss: 0.0080, Accuracy: 99.81%, Grad Norm: 0.08358\n",
      "Epoch [1003/10044], Batch [3/7], Loss: 0.0067, Accuracy: 99.89%, Grad Norm: 0.08222\n",
      "Epoch [1003/10044], Batch [4/7], Loss: 0.0073, Accuracy: 99.87%, Grad Norm: 0.07284\n",
      "Epoch [1003/10044], Batch [5/7], Loss: 0.0062, Accuracy: 99.91%, Grad Norm: 0.06057\n",
      "Epoch [1003/10044], Batch [6/7], Loss: 0.0095, Accuracy: 99.77%, Grad Norm: 0.10956\n",
      "Epoch [1003/10044], Batch [7/7], Loss: 0.0063, Accuracy: 99.85%, Grad Norm: 0.12712\n",
      "Epoch [1003/10044], Loss: 0.0063\n",
      "Epoch [1004/10044], Batch [1/7], Loss: 0.0073, Accuracy: 99.87%, Grad Norm: 0.08556\n",
      "Epoch [1004/10044], Batch [2/7], Loss: 0.0078, Accuracy: 99.78%, Grad Norm: 0.09198\n",
      "Epoch [1004/10044], Batch [3/7], Loss: 0.0061, Accuracy: 99.92%, Grad Norm: 0.05569\n",
      "Epoch [1004/10044], Batch [4/7], Loss: 0.0074, Accuracy: 99.87%, Grad Norm: 0.06843\n",
      "Epoch [1004/10044], Batch [5/7], Loss: 0.0077, Accuracy: 99.82%, Grad Norm: 0.09233\n",
      "Epoch [1004/10044], Batch [6/7], Loss: 0.0083, Accuracy: 99.82%, Grad Norm: 0.10697\n",
      "Epoch [1004/10044], Batch [7/7], Loss: 0.0068, Accuracy: 99.87%, Grad Norm: 0.10058\n",
      "Epoch [1004/10044], Loss: 0.0068\n",
      "Epoch [1005/10044], Batch [1/7], Loss: 0.0070, Accuracy: 99.86%, Grad Norm: 0.07657\n",
      "Epoch [1005/10044], Batch [2/7], Loss: 0.0085, Accuracy: 99.80%, Grad Norm: 0.08935\n",
      "Epoch [1005/10044], Batch [3/7], Loss: 0.0074, Accuracy: 99.87%, Grad Norm: 0.07871\n",
      "Epoch [1005/10044], Batch [4/7], Loss: 0.0077, Accuracy: 99.87%, Grad Norm: 0.07931\n",
      "Epoch [1005/10044], Batch [5/7], Loss: 0.0068, Accuracy: 99.87%, Grad Norm: 0.08638\n",
      "Epoch [1005/10044], Batch [6/7], Loss: 0.0082, Accuracy: 99.78%, Grad Norm: 0.09316\n",
      "Epoch [1005/10044], Batch [7/7], Loss: 0.0063, Accuracy: 99.85%, Grad Norm: 0.11418\n",
      "Epoch [1005/10044], Loss: 0.0063\n",
      "Epoch [1006/10044], Batch [1/7], Loss: 0.0076, Accuracy: 99.81%, Grad Norm: 0.08203\n",
      "Epoch [1006/10044], Batch [2/7], Loss: 0.0077, Accuracy: 99.82%, Grad Norm: 0.08017\n",
      "Epoch [1006/10044], Batch [3/7], Loss: 0.0064, Accuracy: 99.88%, Grad Norm: 0.07443\n",
      "Epoch [1006/10044], Batch [4/7], Loss: 0.0078, Accuracy: 99.83%, Grad Norm: 0.08338\n",
      "Epoch [1006/10044], Batch [5/7], Loss: 0.0065, Accuracy: 99.88%, Grad Norm: 0.07275\n",
      "Epoch [1006/10044], Batch [6/7], Loss: 0.0084, Accuracy: 99.82%, Grad Norm: 0.09827\n",
      "Epoch [1006/10044], Batch [7/7], Loss: 0.0073, Accuracy: 99.78%, Grad Norm: 0.12852\n",
      "Epoch [1006/10044], Loss: 0.0073\n",
      "Epoch [1007/10044], Batch [1/7], Loss: 0.0084, Accuracy: 99.79%, Grad Norm: 0.10117\n",
      "Epoch [1007/10044], Batch [2/7], Loss: 0.0088, Accuracy: 99.81%, Grad Norm: 0.10096\n",
      "Epoch [1007/10044], Batch [3/7], Loss: 0.0050, Accuracy: 99.96%, Grad Norm: 0.05270\n",
      "Epoch [1007/10044], Batch [4/7], Loss: 0.0063, Accuracy: 99.91%, Grad Norm: 0.07495\n",
      "Epoch [1007/10044], Batch [5/7], Loss: 0.0076, Accuracy: 99.86%, Grad Norm: 0.08940\n",
      "Epoch [1007/10044], Batch [6/7], Loss: 0.0070, Accuracy: 99.82%, Grad Norm: 0.08370\n",
      "Epoch [1007/10044], Batch [7/7], Loss: 0.0068, Accuracy: 99.80%, Grad Norm: 0.13305\n",
      "Epoch [1007/10044], Loss: 0.0068\n",
      "Epoch [1008/10044], Batch [1/7], Loss: 0.0070, Accuracy: 99.82%, Grad Norm: 0.07446\n",
      "Epoch [1008/10044], Batch [2/7], Loss: 0.0093, Accuracy: 99.77%, Grad Norm: 0.10763\n",
      "Epoch [1008/10044], Batch [3/7], Loss: 0.0057, Accuracy: 99.94%, Grad Norm: 0.05547\n",
      "Epoch [1008/10044], Batch [4/7], Loss: 0.0069, Accuracy: 99.89%, Grad Norm: 0.06648\n",
      "Epoch [1008/10044], Batch [5/7], Loss: 0.0069, Accuracy: 99.88%, Grad Norm: 0.09156\n",
      "Epoch [1008/10044], Batch [6/7], Loss: 0.0095, Accuracy: 99.80%, Grad Norm: 0.09199\n",
      "Epoch [1008/10044], Batch [7/7], Loss: 0.0088, Accuracy: 99.77%, Grad Norm: 0.16367\n",
      "Epoch [1008/10044], Loss: 0.0088\n",
      "Epoch [1009/10044], Batch [1/7], Loss: 0.0068, Accuracy: 99.88%, Grad Norm: 0.07989\n",
      "Epoch [1009/10044], Batch [2/7], Loss: 0.0087, Accuracy: 99.79%, Grad Norm: 0.10687\n",
      "Epoch [1009/10044], Batch [3/7], Loss: 0.0062, Accuracy: 99.92%, Grad Norm: 0.07826\n",
      "Epoch [1009/10044], Batch [4/7], Loss: 0.0069, Accuracy: 99.84%, Grad Norm: 0.06953\n",
      "Epoch [1009/10044], Batch [5/7], Loss: 0.0057, Accuracy: 99.95%, Grad Norm: 0.06305\n",
      "Epoch [1009/10044], Batch [6/7], Loss: 0.0067, Accuracy: 99.83%, Grad Norm: 0.08279\n",
      "Epoch [1009/10044], Batch [7/7], Loss: 0.0081, Accuracy: 99.85%, Grad Norm: 0.15231\n",
      "Epoch [1009/10044], Loss: 0.0081\n",
      "Epoch [1010/10044], Batch [1/7], Loss: 0.0072, Accuracy: 99.84%, Grad Norm: 0.07340\n",
      "Epoch [1010/10044], Batch [2/7], Loss: 0.0082, Accuracy: 99.82%, Grad Norm: 0.09238\n",
      "Epoch [1010/10044], Batch [3/7], Loss: 0.0060, Accuracy: 99.86%, Grad Norm: 0.06818\n",
      "Epoch [1010/10044], Batch [4/7], Loss: 0.0069, Accuracy: 99.90%, Grad Norm: 0.07738\n",
      "Epoch [1010/10044], Batch [5/7], Loss: 0.0066, Accuracy: 99.84%, Grad Norm: 0.07950\n",
      "Epoch [1010/10044], Batch [6/7], Loss: 0.0097, Accuracy: 99.74%, Grad Norm: 0.11257\n",
      "Epoch [1010/10044], Batch [7/7], Loss: 0.0062, Accuracy: 99.80%, Grad Norm: 0.13248\n",
      "Epoch [1010/10044], Loss: 0.0062\n",
      "Epoch [1011/10044], Batch [1/7], Loss: 0.0062, Accuracy: 99.87%, Grad Norm: 0.06838\n",
      "Epoch [1011/10044], Batch [2/7], Loss: 0.0074, Accuracy: 99.82%, Grad Norm: 0.08746\n",
      "Epoch [1011/10044], Batch [3/7], Loss: 0.0056, Accuracy: 99.92%, Grad Norm: 0.05589\n",
      "Epoch [1011/10044], Batch [4/7], Loss: 0.0062, Accuracy: 99.90%, Grad Norm: 0.06281\n",
      "Epoch [1011/10044], Batch [5/7], Loss: 0.0065, Accuracy: 99.93%, Grad Norm: 0.07956\n",
      "Epoch [1011/10044], Batch [6/7], Loss: 0.0085, Accuracy: 99.80%, Grad Norm: 0.11828\n",
      "Epoch [1011/10044], Batch [7/7], Loss: 0.0092, Accuracy: 99.80%, Grad Norm: 0.15070\n",
      "Epoch [1011/10044], Loss: 0.0092\n",
      "Epoch [1012/10044], Batch [1/7], Loss: 0.0074, Accuracy: 99.82%, Grad Norm: 0.07550\n",
      "Epoch [1012/10044], Batch [2/7], Loss: 0.0062, Accuracy: 99.88%, Grad Norm: 0.07614\n",
      "Epoch [1012/10044], Batch [3/7], Loss: 0.0059, Accuracy: 99.90%, Grad Norm: 0.06001\n",
      "Epoch [1012/10044], Batch [4/7], Loss: 0.0077, Accuracy: 99.85%, Grad Norm: 0.07835\n",
      "Epoch [1012/10044], Batch [5/7], Loss: 0.0068, Accuracy: 99.87%, Grad Norm: 0.09577\n",
      "Epoch [1012/10044], Batch [6/7], Loss: 0.0073, Accuracy: 99.87%, Grad Norm: 0.07660\n",
      "Epoch [1012/10044], Batch [7/7], Loss: 0.0063, Accuracy: 99.85%, Grad Norm: 0.12371\n",
      "Epoch [1012/10044], Loss: 0.0063\n",
      "Epoch [1013/10044], Batch [1/7], Loss: 0.0075, Accuracy: 99.84%, Grad Norm: 0.07738\n",
      "Epoch [1013/10044], Batch [2/7], Loss: 0.0079, Accuracy: 99.84%, Grad Norm: 0.10334\n",
      "Epoch [1013/10044], Batch [3/7], Loss: 0.0056, Accuracy: 99.92%, Grad Norm: 0.05836\n",
      "Epoch [1013/10044], Batch [4/7], Loss: 0.0074, Accuracy: 99.88%, Grad Norm: 0.07699\n",
      "Epoch [1013/10044], Batch [5/7], Loss: 0.0074, Accuracy: 99.79%, Grad Norm: 0.11130\n",
      "Epoch [1013/10044], Batch [6/7], Loss: 0.0081, Accuracy: 99.81%, Grad Norm: 0.09204\n",
      "Epoch [1013/10044], Batch [7/7], Loss: 0.0068, Accuracy: 99.88%, Grad Norm: 0.12482\n",
      "Epoch [1013/10044], Loss: 0.0068\n",
      "Epoch [1014/10044], Batch [1/7], Loss: 0.0077, Accuracy: 99.84%, Grad Norm: 0.08600\n",
      "Epoch [1014/10044], Batch [2/7], Loss: 0.0064, Accuracy: 99.92%, Grad Norm: 0.07665\n",
      "Epoch [1014/10044], Batch [3/7], Loss: 0.0060, Accuracy: 99.90%, Grad Norm: 0.06353\n",
      "Epoch [1014/10044], Batch [4/7], Loss: 0.0093, Accuracy: 99.82%, Grad Norm: 0.10634\n",
      "Epoch [1014/10044], Batch [5/7], Loss: 0.0077, Accuracy: 99.83%, Grad Norm: 0.10786\n",
      "Epoch [1014/10044], Batch [6/7], Loss: 0.0084, Accuracy: 99.80%, Grad Norm: 0.09246\n",
      "Epoch [1014/10044], Batch [7/7], Loss: 0.0084, Accuracy: 99.83%, Grad Norm: 0.16899\n",
      "Epoch [1014/10044], Loss: 0.0084\n",
      "Epoch [1015/10044], Batch [1/7], Loss: 0.0068, Accuracy: 99.88%, Grad Norm: 0.07961\n",
      "Epoch [1015/10044], Batch [2/7], Loss: 0.0081, Accuracy: 99.80%, Grad Norm: 0.10092\n",
      "Epoch [1015/10044], Batch [3/7], Loss: 0.0057, Accuracy: 99.90%, Grad Norm: 0.05967\n",
      "Epoch [1015/10044], Batch [4/7], Loss: 0.0064, Accuracy: 99.89%, Grad Norm: 0.06345\n",
      "Epoch [1015/10044], Batch [5/7], Loss: 0.0059, Accuracy: 99.87%, Grad Norm: 0.06572\n",
      "Epoch [1015/10044], Batch [6/7], Loss: 0.0085, Accuracy: 99.81%, Grad Norm: 0.10005\n",
      "Epoch [1015/10044], Batch [7/7], Loss: 0.0064, Accuracy: 99.83%, Grad Norm: 0.12518\n",
      "Epoch [1015/10044], Loss: 0.0064\n",
      "Epoch [1016/10044], Batch [1/7], Loss: 0.0072, Accuracy: 99.87%, Grad Norm: 0.08391\n",
      "Epoch [1016/10044], Batch [2/7], Loss: 0.0085, Accuracy: 99.81%, Grad Norm: 0.09612\n",
      "Epoch [1016/10044], Batch [3/7], Loss: 0.0053, Accuracy: 99.92%, Grad Norm: 0.05937\n",
      "Epoch [1016/10044], Batch [4/7], Loss: 0.0073, Accuracy: 99.88%, Grad Norm: 0.07542\n",
      "Epoch [1016/10044], Batch [5/7], Loss: 0.0063, Accuracy: 99.87%, Grad Norm: 0.08332\n",
      "Epoch [1016/10044], Batch [6/7], Loss: 0.0089, Accuracy: 99.77%, Grad Norm: 0.12869\n",
      "Epoch [1016/10044], Batch [7/7], Loss: 0.0059, Accuracy: 99.77%, Grad Norm: 0.11951\n",
      "Epoch [1016/10044], Loss: 0.0059\n",
      "Epoch [1017/10044], Batch [1/7], Loss: 0.0079, Accuracy: 99.81%, Grad Norm: 0.09004\n",
      "Epoch [1017/10044], Batch [2/7], Loss: 0.0096, Accuracy: 99.75%, Grad Norm: 0.13425\n",
      "Epoch [1017/10044], Batch [3/7], Loss: 0.0062, Accuracy: 99.88%, Grad Norm: 0.06812\n",
      "Epoch [1017/10044], Batch [4/7], Loss: 0.0077, Accuracy: 99.87%, Grad Norm: 0.09134\n",
      "Epoch [1017/10044], Batch [5/7], Loss: 0.0068, Accuracy: 99.87%, Grad Norm: 0.09515\n",
      "Epoch [1017/10044], Batch [6/7], Loss: 0.0103, Accuracy: 99.76%, Grad Norm: 0.13341\n",
      "Epoch [1017/10044], Batch [7/7], Loss: 0.0044, Accuracy: 99.98%, Grad Norm: 0.08253\n",
      "Epoch [1017/10044], Loss: 0.0044\n",
      "Epoch [1018/10044], Batch [1/7], Loss: 0.0075, Accuracy: 99.80%, Grad Norm: 0.08488\n",
      "Epoch [1018/10044], Batch [2/7], Loss: 0.0098, Accuracy: 99.76%, Grad Norm: 0.13165\n",
      "Epoch [1018/10044], Batch [3/7], Loss: 0.0060, Accuracy: 99.94%, Grad Norm: 0.05527\n",
      "Epoch [1018/10044], Batch [4/7], Loss: 0.0073, Accuracy: 99.86%, Grad Norm: 0.07028\n",
      "Epoch [1018/10044], Batch [5/7], Loss: 0.0071, Accuracy: 99.84%, Grad Norm: 0.09685\n",
      "Epoch [1018/10044], Batch [6/7], Loss: 0.0094, Accuracy: 99.79%, Grad Norm: 0.10333\n",
      "Epoch [1018/10044], Batch [7/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.10772\n",
      "Epoch [1018/10044], Loss: 0.0058\n",
      "Epoch [1019/10044], Batch [1/7], Loss: 0.0080, Accuracy: 99.87%, Grad Norm: 0.07912\n",
      "Epoch [1019/10044], Batch [2/7], Loss: 0.0081, Accuracy: 99.87%, Grad Norm: 0.11774\n",
      "Epoch [1019/10044], Batch [3/7], Loss: 0.0056, Accuracy: 99.92%, Grad Norm: 0.06292\n",
      "Epoch [1019/10044], Batch [4/7], Loss: 0.0064, Accuracy: 99.89%, Grad Norm: 0.07100\n",
      "Epoch [1019/10044], Batch [5/7], Loss: 0.0066, Accuracy: 99.87%, Grad Norm: 0.08207\n",
      "Epoch [1019/10044], Batch [6/7], Loss: 0.0085, Accuracy: 99.81%, Grad Norm: 0.09793\n",
      "Epoch [1019/10044], Batch [7/7], Loss: 0.0065, Accuracy: 99.82%, Grad Norm: 0.13593\n",
      "Epoch [1019/10044], Loss: 0.0065\n",
      "Epoch [1020/10044], Batch [1/7], Loss: 0.0089, Accuracy: 99.81%, Grad Norm: 0.11369\n",
      "Epoch [1020/10044], Batch [2/7], Loss: 0.0092, Accuracy: 99.77%, Grad Norm: 0.11405\n",
      "Epoch [1020/10044], Batch [3/7], Loss: 0.0066, Accuracy: 99.85%, Grad Norm: 0.06972\n",
      "Epoch [1020/10044], Batch [4/7], Loss: 0.0057, Accuracy: 99.90%, Grad Norm: 0.06854\n",
      "Epoch [1020/10044], Batch [5/7], Loss: 0.0061, Accuracy: 99.91%, Grad Norm: 0.06904\n",
      "Epoch [1020/10044], Batch [6/7], Loss: 0.0070, Accuracy: 99.86%, Grad Norm: 0.09396\n",
      "Epoch [1020/10044], Batch [7/7], Loss: 0.0087, Accuracy: 99.73%, Grad Norm: 0.17428\n",
      "Epoch [1020/10044], Loss: 0.0087\n",
      "Epoch [1021/10044], Batch [1/7], Loss: 0.0078, Accuracy: 99.86%, Grad Norm: 0.08980\n",
      "Epoch [1021/10044], Batch [2/7], Loss: 0.0066, Accuracy: 99.87%, Grad Norm: 0.07674\n",
      "Epoch [1021/10044], Batch [3/7], Loss: 0.0058, Accuracy: 99.92%, Grad Norm: 0.06332\n",
      "Epoch [1021/10044], Batch [4/7], Loss: 0.0066, Accuracy: 99.89%, Grad Norm: 0.06592\n",
      "Epoch [1021/10044], Batch [5/7], Loss: 0.0059, Accuracy: 99.87%, Grad Norm: 0.07656\n",
      "Epoch [1021/10044], Batch [6/7], Loss: 0.0081, Accuracy: 99.80%, Grad Norm: 0.11240\n",
      "Epoch [1021/10044], Batch [7/7], Loss: 0.0072, Accuracy: 99.83%, Grad Norm: 0.13079\n",
      "Epoch [1021/10044], Loss: 0.0072\n",
      "Epoch [1022/10044], Batch [1/7], Loss: 0.0066, Accuracy: 99.88%, Grad Norm: 0.06656\n",
      "Epoch [1022/10044], Batch [2/7], Loss: 0.0066, Accuracy: 99.88%, Grad Norm: 0.07061\n",
      "Epoch [1022/10044], Batch [3/7], Loss: 0.0062, Accuracy: 99.90%, Grad Norm: 0.05701\n",
      "Epoch [1022/10044], Batch [4/7], Loss: 0.0067, Accuracy: 99.86%, Grad Norm: 0.07281\n",
      "Epoch [1022/10044], Batch [5/7], Loss: 0.0065, Accuracy: 99.86%, Grad Norm: 0.09644\n",
      "Epoch [1022/10044], Batch [6/7], Loss: 0.0088, Accuracy: 99.77%, Grad Norm: 0.09933\n",
      "Epoch [1022/10044], Batch [7/7], Loss: 0.0044, Accuracy: 99.93%, Grad Norm: 0.06754\n",
      "Epoch [1022/10044], Loss: 0.0044\n",
      "Epoch [1023/10044], Batch [1/7], Loss: 0.0072, Accuracy: 99.82%, Grad Norm: 0.09543\n",
      "Epoch [1023/10044], Batch [2/7], Loss: 0.0082, Accuracy: 99.84%, Grad Norm: 0.09859\n",
      "Epoch [1023/10044], Batch [3/7], Loss: 0.0065, Accuracy: 99.87%, Grad Norm: 0.07375\n",
      "Epoch [1023/10044], Batch [4/7], Loss: 0.0069, Accuracy: 99.86%, Grad Norm: 0.07445\n",
      "Epoch [1023/10044], Batch [5/7], Loss: 0.0061, Accuracy: 99.86%, Grad Norm: 0.09073\n",
      "Epoch [1023/10044], Batch [6/7], Loss: 0.0075, Accuracy: 99.82%, Grad Norm: 0.09260\n",
      "Epoch [1023/10044], Batch [7/7], Loss: 0.0044, Accuracy: 99.95%, Grad Norm: 0.06948\n",
      "Epoch [1023/10044], Loss: 0.0044\n",
      "Epoch [1024/10044], Batch [1/7], Loss: 0.0074, Accuracy: 99.85%, Grad Norm: 0.10698\n",
      "Epoch [1024/10044], Batch [2/7], Loss: 0.0090, Accuracy: 99.76%, Grad Norm: 0.11065\n",
      "Epoch [1024/10044], Batch [3/7], Loss: 0.0055, Accuracy: 99.93%, Grad Norm: 0.05670\n",
      "Epoch [1024/10044], Batch [4/7], Loss: 0.0068, Accuracy: 99.84%, Grad Norm: 0.07214\n",
      "Epoch [1024/10044], Batch [5/7], Loss: 0.0066, Accuracy: 99.89%, Grad Norm: 0.07887\n",
      "Epoch [1024/10044], Batch [6/7], Loss: 0.0071, Accuracy: 99.87%, Grad Norm: 0.08273\n",
      "Epoch [1024/10044], Batch [7/7], Loss: 0.0039, Accuracy: 99.97%, Grad Norm: 0.06116\n",
      "Epoch [1024/10044], Loss: 0.0039\n",
      "Epoch [1025/10044], Batch [1/7], Loss: 0.0074, Accuracy: 99.83%, Grad Norm: 0.08236\n",
      "Epoch [1025/10044], Batch [2/7], Loss: 0.0086, Accuracy: 99.77%, Grad Norm: 0.10962\n",
      "Epoch [1025/10044], Batch [3/7], Loss: 0.0062, Accuracy: 99.91%, Grad Norm: 0.06035\n",
      "Epoch [1025/10044], Batch [4/7], Loss: 0.0060, Accuracy: 99.88%, Grad Norm: 0.07014\n",
      "Epoch [1025/10044], Batch [5/7], Loss: 0.0059, Accuracy: 99.92%, Grad Norm: 0.07759\n",
      "Epoch [1025/10044], Batch [6/7], Loss: 0.0071, Accuracy: 99.82%, Grad Norm: 0.08529\n",
      "Epoch [1025/10044], Batch [7/7], Loss: 0.0058, Accuracy: 99.88%, Grad Norm: 0.11900\n",
      "Epoch [1025/10044], Loss: 0.0058\n",
      "Epoch [1026/10044], Batch [1/7], Loss: 0.0081, Accuracy: 99.82%, Grad Norm: 0.11810\n",
      "Epoch [1026/10044], Batch [2/7], Loss: 0.0089, Accuracy: 99.72%, Grad Norm: 0.12585\n",
      "Epoch [1026/10044], Batch [3/7], Loss: 0.0056, Accuracy: 99.91%, Grad Norm: 0.07366\n",
      "Epoch [1026/10044], Batch [4/7], Loss: 0.0054, Accuracy: 99.93%, Grad Norm: 0.05569\n",
      "Epoch [1026/10044], Batch [5/7], Loss: 0.0078, Accuracy: 99.84%, Grad Norm: 0.10094\n",
      "Epoch [1026/10044], Batch [6/7], Loss: 0.0107, Accuracy: 99.71%, Grad Norm: 0.14428\n",
      "Epoch [1026/10044], Batch [7/7], Loss: 0.0071, Accuracy: 99.78%, Grad Norm: 0.13666\n",
      "Epoch [1026/10044], Loss: 0.0071\n",
      "Epoch [1027/10044], Batch [1/7], Loss: 0.0081, Accuracy: 99.81%, Grad Norm: 0.09442\n",
      "Epoch [1027/10044], Batch [2/7], Loss: 0.0075, Accuracy: 99.79%, Grad Norm: 0.11576\n",
      "Epoch [1027/10044], Batch [3/7], Loss: 0.0055, Accuracy: 99.91%, Grad Norm: 0.06394\n",
      "Epoch [1027/10044], Batch [4/7], Loss: 0.0075, Accuracy: 99.84%, Grad Norm: 0.08956\n",
      "Epoch [1027/10044], Batch [5/7], Loss: 0.0077, Accuracy: 99.87%, Grad Norm: 0.10316\n",
      "Epoch [1027/10044], Batch [6/7], Loss: 0.0081, Accuracy: 99.84%, Grad Norm: 0.09485\n",
      "Epoch [1027/10044], Batch [7/7], Loss: 0.0090, Accuracy: 99.78%, Grad Norm: 0.17008\n",
      "Epoch [1027/10044], Loss: 0.0090\n",
      "Epoch [1028/10044], Batch [1/7], Loss: 0.0082, Accuracy: 99.81%, Grad Norm: 0.10322\n",
      "Epoch [1028/10044], Batch [2/7], Loss: 0.0076, Accuracy: 99.82%, Grad Norm: 0.10202\n",
      "Epoch [1028/10044], Batch [3/7], Loss: 0.0061, Accuracy: 99.90%, Grad Norm: 0.06529\n",
      "Epoch [1028/10044], Batch [4/7], Loss: 0.0087, Accuracy: 99.81%, Grad Norm: 0.09324\n",
      "Epoch [1028/10044], Batch [5/7], Loss: 0.0069, Accuracy: 99.83%, Grad Norm: 0.09294\n",
      "Epoch [1028/10044], Batch [6/7], Loss: 0.0084, Accuracy: 99.78%, Grad Norm: 0.10675\n",
      "Epoch [1028/10044], Batch [7/7], Loss: 0.0059, Accuracy: 99.85%, Grad Norm: 0.11565\n",
      "Epoch [1028/10044], Loss: 0.0059\n",
      "Epoch [1029/10044], Batch [1/7], Loss: 0.0064, Accuracy: 99.90%, Grad Norm: 0.06815\n",
      "Epoch [1029/10044], Batch [2/7], Loss: 0.0081, Accuracy: 99.82%, Grad Norm: 0.11210\n",
      "Epoch [1029/10044], Batch [3/7], Loss: 0.0068, Accuracy: 99.88%, Grad Norm: 0.07193\n",
      "Epoch [1029/10044], Batch [4/7], Loss: 0.0072, Accuracy: 99.84%, Grad Norm: 0.09131\n",
      "Epoch [1029/10044], Batch [5/7], Loss: 0.0083, Accuracy: 99.80%, Grad Norm: 0.09224\n",
      "Epoch [1029/10044], Batch [6/7], Loss: 0.0086, Accuracy: 99.78%, Grad Norm: 0.09945\n",
      "Epoch [1029/10044], Batch [7/7], Loss: 0.0052, Accuracy: 99.87%, Grad Norm: 0.11678\n",
      "Epoch [1029/10044], Loss: 0.0052\n",
      "Epoch [1030/10044], Batch [1/7], Loss: 0.0069, Accuracy: 99.85%, Grad Norm: 0.09889\n",
      "Epoch [1030/10044], Batch [2/7], Loss: 0.0077, Accuracy: 99.86%, Grad Norm: 0.09646\n",
      "Epoch [1030/10044], Batch [3/7], Loss: 0.0064, Accuracy: 99.87%, Grad Norm: 0.07663\n",
      "Epoch [1030/10044], Batch [4/7], Loss: 0.0068, Accuracy: 99.87%, Grad Norm: 0.07664\n",
      "Epoch [1030/10044], Batch [5/7], Loss: 0.0059, Accuracy: 99.90%, Grad Norm: 0.07159\n",
      "Epoch [1030/10044], Batch [6/7], Loss: 0.0079, Accuracy: 99.83%, Grad Norm: 0.11334\n",
      "Epoch [1030/10044], Batch [7/7], Loss: 0.0055, Accuracy: 99.87%, Grad Norm: 0.10229\n",
      "Epoch [1030/10044], Loss: 0.0055\n",
      "Epoch [1031/10044], Batch [1/7], Loss: 0.0095, Accuracy: 99.79%, Grad Norm: 0.11180\n",
      "Epoch [1031/10044], Batch [2/7], Loss: 0.0081, Accuracy: 99.79%, Grad Norm: 0.12262\n",
      "Epoch [1031/10044], Batch [3/7], Loss: 0.0050, Accuracy: 99.92%, Grad Norm: 0.05261\n",
      "Epoch [1031/10044], Batch [4/7], Loss: 0.0068, Accuracy: 99.87%, Grad Norm: 0.07865\n",
      "Epoch [1031/10044], Batch [5/7], Loss: 0.0063, Accuracy: 99.88%, Grad Norm: 0.07993\n",
      "Epoch [1031/10044], Batch [6/7], Loss: 0.0060, Accuracy: 99.89%, Grad Norm: 0.07549\n",
      "Epoch [1031/10044], Batch [7/7], Loss: 0.0071, Accuracy: 99.77%, Grad Norm: 0.16654\n",
      "Epoch [1031/10044], Loss: 0.0071\n",
      "Epoch [1032/10044], Batch [1/7], Loss: 0.0064, Accuracy: 99.86%, Grad Norm: 0.07395\n",
      "Epoch [1032/10044], Batch [2/7], Loss: 0.0073, Accuracy: 99.83%, Grad Norm: 0.08934\n",
      "Epoch [1032/10044], Batch [3/7], Loss: 0.0071, Accuracy: 99.86%, Grad Norm: 0.10504\n",
      "Epoch [1032/10044], Batch [4/7], Loss: 0.0080, Accuracy: 99.80%, Grad Norm: 0.09899\n",
      "Epoch [1032/10044], Batch [5/7], Loss: 0.0057, Accuracy: 99.91%, Grad Norm: 0.07493\n",
      "Epoch [1032/10044], Batch [6/7], Loss: 0.0092, Accuracy: 99.79%, Grad Norm: 0.11967\n",
      "Epoch [1032/10044], Batch [7/7], Loss: 0.0072, Accuracy: 99.87%, Grad Norm: 0.13105\n",
      "Epoch [1032/10044], Loss: 0.0072\n",
      "Epoch [1033/10044], Batch [1/7], Loss: 0.0077, Accuracy: 99.83%, Grad Norm: 0.08927\n",
      "Epoch [1033/10044], Batch [2/7], Loss: 0.0073, Accuracy: 99.87%, Grad Norm: 0.08597\n",
      "Epoch [1033/10044], Batch [3/7], Loss: 0.0060, Accuracy: 99.89%, Grad Norm: 0.07061\n",
      "Epoch [1033/10044], Batch [4/7], Loss: 0.0068, Accuracy: 99.85%, Grad Norm: 0.08081\n",
      "Epoch [1033/10044], Batch [5/7], Loss: 0.0061, Accuracy: 99.89%, Grad Norm: 0.08435\n",
      "Epoch [1033/10044], Batch [6/7], Loss: 0.0094, Accuracy: 99.76%, Grad Norm: 0.11161\n",
      "Epoch [1033/10044], Batch [7/7], Loss: 0.0066, Accuracy: 99.85%, Grad Norm: 0.14226\n",
      "Epoch [1033/10044], Loss: 0.0066\n",
      "Epoch [1034/10044], Batch [1/7], Loss: 0.0065, Accuracy: 99.86%, Grad Norm: 0.07381\n",
      "Epoch [1034/10044], Batch [2/7], Loss: 0.0062, Accuracy: 99.90%, Grad Norm: 0.09354\n",
      "Epoch [1034/10044], Batch [3/7], Loss: 0.0054, Accuracy: 99.90%, Grad Norm: 0.05899\n",
      "Epoch [1034/10044], Batch [4/7], Loss: 0.0078, Accuracy: 99.85%, Grad Norm: 0.08896\n",
      "Epoch [1034/10044], Batch [5/7], Loss: 0.0061, Accuracy: 99.91%, Grad Norm: 0.07397\n",
      "Epoch [1034/10044], Batch [6/7], Loss: 0.0072, Accuracy: 99.82%, Grad Norm: 0.08612\n",
      "Epoch [1034/10044], Batch [7/7], Loss: 0.0062, Accuracy: 99.87%, Grad Norm: 0.10043\n",
      "Epoch [1034/10044], Loss: 0.0062\n",
      "Epoch [1035/10044], Batch [1/7], Loss: 0.0067, Accuracy: 99.82%, Grad Norm: 0.09368\n",
      "Epoch [1035/10044], Batch [2/7], Loss: 0.0077, Accuracy: 99.81%, Grad Norm: 0.09449\n",
      "Epoch [1035/10044], Batch [3/7], Loss: 0.0065, Accuracy: 99.84%, Grad Norm: 0.07871\n",
      "Epoch [1035/10044], Batch [4/7], Loss: 0.0062, Accuracy: 99.87%, Grad Norm: 0.06413\n",
      "Epoch [1035/10044], Batch [5/7], Loss: 0.0070, Accuracy: 99.85%, Grad Norm: 0.08688\n",
      "Epoch [1035/10044], Batch [6/7], Loss: 0.0076, Accuracy: 99.79%, Grad Norm: 0.09461\n",
      "Epoch [1035/10044], Batch [7/7], Loss: 0.0047, Accuracy: 99.93%, Grad Norm: 0.08759\n",
      "Epoch [1035/10044], Loss: 0.0047\n",
      "Epoch [1036/10044], Batch [1/7], Loss: 0.0085, Accuracy: 99.83%, Grad Norm: 0.10509\n",
      "Epoch [1036/10044], Batch [2/7], Loss: 0.0083, Accuracy: 99.82%, Grad Norm: 0.10835\n",
      "Epoch [1036/10044], Batch [3/7], Loss: 0.0051, Accuracy: 99.92%, Grad Norm: 0.06150\n",
      "Epoch [1036/10044], Batch [4/7], Loss: 0.0087, Accuracy: 99.80%, Grad Norm: 0.11537\n",
      "Epoch [1036/10044], Batch [5/7], Loss: 0.0061, Accuracy: 99.87%, Grad Norm: 0.07910\n",
      "Epoch [1036/10044], Batch [6/7], Loss: 0.0065, Accuracy: 99.88%, Grad Norm: 0.07700\n",
      "Epoch [1036/10044], Batch [7/7], Loss: 0.0074, Accuracy: 99.83%, Grad Norm: 0.14243\n",
      "Epoch [1036/10044], Loss: 0.0074\n",
      "Epoch [1037/10044], Batch [1/7], Loss: 0.0074, Accuracy: 99.84%, Grad Norm: 0.10654\n",
      "Epoch [1037/10044], Batch [2/7], Loss: 0.0084, Accuracy: 99.81%, Grad Norm: 0.12145\n",
      "Epoch [1037/10044], Batch [3/7], Loss: 0.0059, Accuracy: 99.88%, Grad Norm: 0.09153\n",
      "Epoch [1037/10044], Batch [4/7], Loss: 0.0074, Accuracy: 99.87%, Grad Norm: 0.09299\n",
      "Epoch [1037/10044], Batch [5/7], Loss: 0.0071, Accuracy: 99.85%, Grad Norm: 0.10449\n",
      "Epoch [1037/10044], Batch [6/7], Loss: 0.0097, Accuracy: 99.79%, Grad Norm: 0.11884\n",
      "Epoch [1037/10044], Batch [7/7], Loss: 0.0076, Accuracy: 99.78%, Grad Norm: 0.13214\n",
      "Epoch [1037/10044], Loss: 0.0076\n",
      "Epoch [1038/10044], Batch [1/7], Loss: 0.0063, Accuracy: 99.90%, Grad Norm: 0.07353\n",
      "Epoch [1038/10044], Batch [2/7], Loss: 0.0079, Accuracy: 99.82%, Grad Norm: 0.10105\n",
      "Epoch [1038/10044], Batch [3/7], Loss: 0.0066, Accuracy: 99.91%, Grad Norm: 0.08346\n",
      "Epoch [1038/10044], Batch [4/7], Loss: 0.0064, Accuracy: 99.87%, Grad Norm: 0.07969\n",
      "Epoch [1038/10044], Batch [5/7], Loss: 0.0067, Accuracy: 99.87%, Grad Norm: 0.08685\n",
      "Epoch [1038/10044], Batch [6/7], Loss: 0.0078, Accuracy: 99.78%, Grad Norm: 0.08878\n",
      "Epoch [1038/10044], Batch [7/7], Loss: 0.0058, Accuracy: 99.88%, Grad Norm: 0.10018\n",
      "Epoch [1038/10044], Loss: 0.0058\n",
      "Epoch [1039/10044], Batch [1/7], Loss: 0.0059, Accuracy: 99.89%, Grad Norm: 0.07134\n",
      "Epoch [1039/10044], Batch [2/7], Loss: 0.0069, Accuracy: 99.83%, Grad Norm: 0.08038\n",
      "Epoch [1039/10044], Batch [3/7], Loss: 0.0060, Accuracy: 99.87%, Grad Norm: 0.10238\n",
      "Epoch [1039/10044], Batch [4/7], Loss: 0.0063, Accuracy: 99.88%, Grad Norm: 0.06952\n",
      "Epoch [1039/10044], Batch [5/7], Loss: 0.0090, Accuracy: 99.81%, Grad Norm: 0.09784\n",
      "Epoch [1039/10044], Batch [6/7], Loss: 0.0075, Accuracy: 99.82%, Grad Norm: 0.09239\n",
      "Epoch [1039/10044], Batch [7/7], Loss: 0.0058, Accuracy: 99.85%, Grad Norm: 0.11473\n",
      "Epoch [1039/10044], Loss: 0.0058\n",
      "Epoch [1040/10044], Batch [1/7], Loss: 0.0074, Accuracy: 99.85%, Grad Norm: 0.09944\n",
      "Epoch [1040/10044], Batch [2/7], Loss: 0.0091, Accuracy: 99.79%, Grad Norm: 0.12937\n",
      "Epoch [1040/10044], Batch [3/7], Loss: 0.0062, Accuracy: 99.85%, Grad Norm: 0.07626\n",
      "Epoch [1040/10044], Batch [4/7], Loss: 0.0071, Accuracy: 99.84%, Grad Norm: 0.07579\n",
      "Epoch [1040/10044], Batch [5/7], Loss: 0.0058, Accuracy: 99.90%, Grad Norm: 0.07184\n",
      "Epoch [1040/10044], Batch [6/7], Loss: 0.0071, Accuracy: 99.86%, Grad Norm: 0.09145\n",
      "Epoch [1040/10044], Batch [7/7], Loss: 0.0057, Accuracy: 99.90%, Grad Norm: 0.11102\n",
      "Epoch [1040/10044], Loss: 0.0057\n",
      "Epoch [1041/10044], Batch [1/7], Loss: 0.0074, Accuracy: 99.80%, Grad Norm: 0.09120\n",
      "Epoch [1041/10044], Batch [2/7], Loss: 0.0092, Accuracy: 99.69%, Grad Norm: 0.11708\n",
      "Epoch [1041/10044], Batch [3/7], Loss: 0.0050, Accuracy: 99.94%, Grad Norm: 0.05263\n",
      "Epoch [1041/10044], Batch [4/7], Loss: 0.0076, Accuracy: 99.82%, Grad Norm: 0.08620\n",
      "Epoch [1041/10044], Batch [5/7], Loss: 0.0056, Accuracy: 99.95%, Grad Norm: 0.07080\n",
      "Epoch [1041/10044], Batch [6/7], Loss: 0.0075, Accuracy: 99.85%, Grad Norm: 0.09491\n",
      "Epoch [1041/10044], Batch [7/7], Loss: 0.0061, Accuracy: 99.80%, Grad Norm: 0.13331\n",
      "Epoch [1041/10044], Loss: 0.0061\n",
      "Epoch [1042/10044], Batch [1/7], Loss: 0.0079, Accuracy: 99.81%, Grad Norm: 0.09659\n",
      "Epoch [1042/10044], Batch [2/7], Loss: 0.0068, Accuracy: 99.88%, Grad Norm: 0.08840\n",
      "Epoch [1042/10044], Batch [3/7], Loss: 0.0049, Accuracy: 99.92%, Grad Norm: 0.04531\n",
      "Epoch [1042/10044], Batch [4/7], Loss: 0.0072, Accuracy: 99.82%, Grad Norm: 0.07456\n",
      "Epoch [1042/10044], Batch [5/7], Loss: 0.0051, Accuracy: 99.92%, Grad Norm: 0.07342\n",
      "Epoch [1042/10044], Batch [6/7], Loss: 0.0071, Accuracy: 99.82%, Grad Norm: 0.08730\n",
      "Epoch [1042/10044], Batch [7/7], Loss: 0.0092, Accuracy: 99.78%, Grad Norm: 0.16042\n",
      "Epoch [1042/10044], Loss: 0.0092\n",
      "Epoch [1043/10044], Batch [1/7], Loss: 0.0060, Accuracy: 99.92%, Grad Norm: 0.07059\n",
      "Epoch [1043/10044], Batch [2/7], Loss: 0.0087, Accuracy: 99.81%, Grad Norm: 0.12248\n",
      "Epoch [1043/10044], Batch [3/7], Loss: 0.0059, Accuracy: 99.88%, Grad Norm: 0.07012\n",
      "Epoch [1043/10044], Batch [4/7], Loss: 0.0073, Accuracy: 99.84%, Grad Norm: 0.08310\n",
      "Epoch [1043/10044], Batch [5/7], Loss: 0.0074, Accuracy: 99.87%, Grad Norm: 0.11009\n",
      "Epoch [1043/10044], Batch [6/7], Loss: 0.0066, Accuracy: 99.85%, Grad Norm: 0.10337\n",
      "Epoch [1043/10044], Batch [7/7], Loss: 0.0055, Accuracy: 99.88%, Grad Norm: 0.12587\n",
      "Epoch [1043/10044], Loss: 0.0055\n",
      "Epoch [1044/10044], Batch [1/7], Loss: 0.0070, Accuracy: 99.87%, Grad Norm: 0.08061\n",
      "Epoch [1044/10044], Batch [2/7], Loss: 0.0080, Accuracy: 99.82%, Grad Norm: 0.08860\n",
      "Epoch [1044/10044], Batch [3/7], Loss: 0.0060, Accuracy: 99.87%, Grad Norm: 0.06643\n",
      "Epoch [1044/10044], Batch [4/7], Loss: 0.0069, Accuracy: 99.85%, Grad Norm: 0.08397\n",
      "Epoch [1044/10044], Batch [5/7], Loss: 0.0071, Accuracy: 99.87%, Grad Norm: 0.10906\n",
      "Epoch [1044/10044], Batch [6/7], Loss: 0.0085, Accuracy: 99.78%, Grad Norm: 0.12006\n",
      "Epoch [1044/10044], Batch [7/7], Loss: 0.0069, Accuracy: 99.82%, Grad Norm: 0.14209\n",
      "Epoch [1044/10044], Loss: 0.0069\n",
      "Epoch [1045/10044], Batch [1/7], Loss: 0.0071, Accuracy: 99.85%, Grad Norm: 0.08719\n",
      "Epoch [1045/10044], Batch [2/7], Loss: 0.0075, Accuracy: 99.82%, Grad Norm: 0.08710\n",
      "Epoch [1045/10044], Batch [3/7], Loss: 0.0054, Accuracy: 99.92%, Grad Norm: 0.05990\n",
      "Epoch [1045/10044], Batch [4/7], Loss: 0.0062, Accuracy: 99.86%, Grad Norm: 0.07351\n",
      "Epoch [1045/10044], Batch [5/7], Loss: 0.0074, Accuracy: 99.86%, Grad Norm: 0.10523\n",
      "Epoch [1045/10044], Batch [6/7], Loss: 0.0079, Accuracy: 99.80%, Grad Norm: 0.11013\n",
      "Epoch [1045/10044], Batch [7/7], Loss: 0.0074, Accuracy: 99.78%, Grad Norm: 0.16306\n",
      "Epoch [1045/10044], Loss: 0.0074\n",
      "Epoch [1046/10044], Batch [1/7], Loss: 0.0060, Accuracy: 99.90%, Grad Norm: 0.07129\n",
      "Epoch [1046/10044], Batch [2/7], Loss: 0.0076, Accuracy: 99.83%, Grad Norm: 0.10660\n",
      "Epoch [1046/10044], Batch [3/7], Loss: 0.0059, Accuracy: 99.90%, Grad Norm: 0.06765\n",
      "Epoch [1046/10044], Batch [4/7], Loss: 0.0066, Accuracy: 99.87%, Grad Norm: 0.08147\n",
      "Epoch [1046/10044], Batch [5/7], Loss: 0.0056, Accuracy: 99.91%, Grad Norm: 0.06791\n",
      "Epoch [1046/10044], Batch [6/7], Loss: 0.0071, Accuracy: 99.79%, Grad Norm: 0.09608\n",
      "Epoch [1046/10044], Batch [7/7], Loss: 0.0074, Accuracy: 99.92%, Grad Norm: 0.12075\n",
      "Epoch [1046/10044], Loss: 0.0074\n",
      "Epoch [1047/10044], Batch [1/7], Loss: 0.0068, Accuracy: 99.87%, Grad Norm: 0.08139\n",
      "Epoch [1047/10044], Batch [2/7], Loss: 0.0066, Accuracy: 99.88%, Grad Norm: 0.08694\n",
      "Epoch [1047/10044], Batch [3/7], Loss: 0.0056, Accuracy: 99.89%, Grad Norm: 0.06076\n",
      "Epoch [1047/10044], Batch [4/7], Loss: 0.0063, Accuracy: 99.89%, Grad Norm: 0.06715\n",
      "Epoch [1047/10044], Batch [5/7], Loss: 0.0073, Accuracy: 99.80%, Grad Norm: 0.09571\n",
      "Epoch [1047/10044], Batch [6/7], Loss: 0.0072, Accuracy: 99.84%, Grad Norm: 0.10790\n",
      "Epoch [1047/10044], Batch [7/7], Loss: 0.0042, Accuracy: 99.92%, Grad Norm: 0.07612\n",
      "Epoch [1047/10044], Loss: 0.0042\n",
      "Epoch [1048/10044], Batch [1/7], Loss: 0.0058, Accuracy: 99.86%, Grad Norm: 0.07637\n",
      "Epoch [1048/10044], Batch [2/7], Loss: 0.0085, Accuracy: 99.80%, Grad Norm: 0.10542\n",
      "Epoch [1048/10044], Batch [3/7], Loss: 0.0054, Accuracy: 99.91%, Grad Norm: 0.06274\n",
      "Epoch [1048/10044], Batch [4/7], Loss: 0.0069, Accuracy: 99.85%, Grad Norm: 0.08485\n",
      "Epoch [1048/10044], Batch [5/7], Loss: 0.0061, Accuracy: 99.85%, Grad Norm: 0.07617\n",
      "Epoch [1048/10044], Batch [6/7], Loss: 0.0090, Accuracy: 99.77%, Grad Norm: 0.14343\n",
      "Epoch [1048/10044], Batch [7/7], Loss: 0.0048, Accuracy: 99.93%, Grad Norm: 0.09190\n",
      "Epoch [1048/10044], Loss: 0.0048\n",
      "Epoch [1049/10044], Batch [1/7], Loss: 0.0067, Accuracy: 99.89%, Grad Norm: 0.07666\n",
      "Epoch [1049/10044], Batch [2/7], Loss: 0.0068, Accuracy: 99.82%, Grad Norm: 0.09654\n",
      "Epoch [1049/10044], Batch [3/7], Loss: 0.0045, Accuracy: 99.98%, Grad Norm: 0.04242\n",
      "Epoch [1049/10044], Batch [4/7], Loss: 0.0066, Accuracy: 99.86%, Grad Norm: 0.08132\n",
      "Epoch [1049/10044], Batch [5/7], Loss: 0.0052, Accuracy: 99.93%, Grad Norm: 0.06290\n",
      "Epoch [1049/10044], Batch [6/7], Loss: 0.0091, Accuracy: 99.80%, Grad Norm: 0.10785\n",
      "Epoch [1049/10044], Batch [7/7], Loss: 0.0062, Accuracy: 99.83%, Grad Norm: 0.11710\n",
      "Epoch [1049/10044], Loss: 0.0062\n",
      "Epoch [1050/10044], Batch [1/7], Loss: 0.0066, Accuracy: 99.87%, Grad Norm: 0.07389\n",
      "Epoch [1050/10044], Batch [2/7], Loss: 0.0075, Accuracy: 99.82%, Grad Norm: 0.09853\n",
      "Epoch [1050/10044], Batch [3/7], Loss: 0.0052, Accuracy: 99.92%, Grad Norm: 0.05610\n",
      "Epoch [1050/10044], Batch [4/7], Loss: 0.0077, Accuracy: 99.79%, Grad Norm: 0.08733\n",
      "Epoch [1050/10044], Batch [5/7], Loss: 0.0064, Accuracy: 99.88%, Grad Norm: 0.07911\n",
      "Epoch [1050/10044], Batch [6/7], Loss: 0.0074, Accuracy: 99.81%, Grad Norm: 0.09644\n",
      "Epoch [1050/10044], Batch [7/7], Loss: 0.0050, Accuracy: 99.92%, Grad Norm: 0.09834\n",
      "Epoch [1050/10044], Loss: 0.0050\n",
      "Epoch [1051/10044], Batch [1/7], Loss: 0.0071, Accuracy: 99.82%, Grad Norm: 0.10705\n",
      "Epoch [1051/10044], Batch [2/7], Loss: 0.0073, Accuracy: 99.84%, Grad Norm: 0.09735\n",
      "Epoch [1051/10044], Batch [3/7], Loss: 0.0049, Accuracy: 99.94%, Grad Norm: 0.05475\n",
      "Epoch [1051/10044], Batch [4/7], Loss: 0.0063, Accuracy: 99.87%, Grad Norm: 0.07182\n",
      "Epoch [1051/10044], Batch [5/7], Loss: 0.0053, Accuracy: 99.89%, Grad Norm: 0.07330\n",
      "Epoch [1051/10044], Batch [6/7], Loss: 0.0061, Accuracy: 99.88%, Grad Norm: 0.08580\n",
      "Epoch [1051/10044], Batch [7/7], Loss: 0.0066, Accuracy: 99.83%, Grad Norm: 0.16000\n",
      "Epoch [1051/10044], Loss: 0.0066\n",
      "Epoch [1052/10044], Batch [1/7], Loss: 0.0060, Accuracy: 99.92%, Grad Norm: 0.07375\n",
      "Epoch [1052/10044], Batch [2/7], Loss: 0.0076, Accuracy: 99.81%, Grad Norm: 0.09907\n",
      "Epoch [1052/10044], Batch [3/7], Loss: 0.0049, Accuracy: 99.92%, Grad Norm: 0.05178\n",
      "Epoch [1052/10044], Batch [4/7], Loss: 0.0053, Accuracy: 99.90%, Grad Norm: 0.05681\n",
      "Epoch [1052/10044], Batch [5/7], Loss: 0.0053, Accuracy: 99.91%, Grad Norm: 0.07345\n",
      "Epoch [1052/10044], Batch [6/7], Loss: 0.0088, Accuracy: 99.77%, Grad Norm: 0.11566\n",
      "Epoch [1052/10044], Batch [7/7], Loss: 0.0069, Accuracy: 99.88%, Grad Norm: 0.12854\n",
      "Epoch [1052/10044], Loss: 0.0069\n",
      "Epoch [1053/10044], Batch [1/7], Loss: 0.0072, Accuracy: 99.85%, Grad Norm: 0.09132\n",
      "Epoch [1053/10044], Batch [2/7], Loss: 0.0069, Accuracy: 99.82%, Grad Norm: 0.09132\n",
      "Epoch [1053/10044], Batch [3/7], Loss: 0.0054, Accuracy: 99.92%, Grad Norm: 0.05422\n",
      "Epoch [1053/10044], Batch [4/7], Loss: 0.0065, Accuracy: 99.87%, Grad Norm: 0.08056\n",
      "Epoch [1053/10044], Batch [5/7], Loss: 0.0059, Accuracy: 99.85%, Grad Norm: 0.08516\n",
      "Epoch [1053/10044], Batch [6/7], Loss: 0.0063, Accuracy: 99.89%, Grad Norm: 0.07848\n",
      "Epoch [1053/10044], Batch [7/7], Loss: 0.0041, Accuracy: 99.93%, Grad Norm: 0.08355\n",
      "Epoch [1053/10044], Loss: 0.0041\n",
      "Epoch [1054/10044], Batch [1/7], Loss: 0.0088, Accuracy: 99.82%, Grad Norm: 0.11068\n",
      "Epoch [1054/10044], Batch [2/7], Loss: 0.0066, Accuracy: 99.85%, Grad Norm: 0.08093\n",
      "Epoch [1054/10044], Batch [3/7], Loss: 0.0050, Accuracy: 99.93%, Grad Norm: 0.04938\n",
      "Epoch [1054/10044], Batch [4/7], Loss: 0.0058, Accuracy: 99.91%, Grad Norm: 0.06369\n",
      "Epoch [1054/10044], Batch [5/7], Loss: 0.0064, Accuracy: 99.85%, Grad Norm: 0.08038\n",
      "Epoch [1054/10044], Batch [6/7], Loss: 0.0082, Accuracy: 99.80%, Grad Norm: 0.10654\n",
      "Epoch [1054/10044], Batch [7/7], Loss: 0.0059, Accuracy: 99.88%, Grad Norm: 0.12868\n",
      "Epoch [1054/10044], Loss: 0.0059\n",
      "Epoch [1055/10044], Batch [1/7], Loss: 0.0060, Accuracy: 99.87%, Grad Norm: 0.06280\n",
      "Epoch [1055/10044], Batch [2/7], Loss: 0.0059, Accuracy: 99.89%, Grad Norm: 0.08228\n",
      "Epoch [1055/10044], Batch [3/7], Loss: 0.0053, Accuracy: 99.92%, Grad Norm: 0.06483\n",
      "Epoch [1055/10044], Batch [4/7], Loss: 0.0064, Accuracy: 99.89%, Grad Norm: 0.07051\n",
      "Epoch [1055/10044], Batch [5/7], Loss: 0.0061, Accuracy: 99.88%, Grad Norm: 0.08492\n",
      "Epoch [1055/10044], Batch [6/7], Loss: 0.0087, Accuracy: 99.77%, Grad Norm: 0.11185\n",
      "Epoch [1055/10044], Batch [7/7], Loss: 0.0066, Accuracy: 99.87%, Grad Norm: 0.11880\n",
      "Epoch [1055/10044], Loss: 0.0066\n",
      "Epoch [1056/10044], Batch [1/7], Loss: 0.0054, Accuracy: 99.92%, Grad Norm: 0.05505\n",
      "Epoch [1056/10044], Batch [2/7], Loss: 0.0066, Accuracy: 99.85%, Grad Norm: 0.07495\n",
      "Epoch [1056/10044], Batch [3/7], Loss: 0.0058, Accuracy: 99.92%, Grad Norm: 0.06168\n",
      "Epoch [1056/10044], Batch [4/7], Loss: 0.0060, Accuracy: 99.87%, Grad Norm: 0.07420\n",
      "Epoch [1056/10044], Batch [5/7], Loss: 0.0056, Accuracy: 99.91%, Grad Norm: 0.07026\n",
      "Epoch [1056/10044], Batch [6/7], Loss: 0.0069, Accuracy: 99.81%, Grad Norm: 0.10647\n",
      "Epoch [1056/10044], Batch [7/7], Loss: 0.0057, Accuracy: 99.87%, Grad Norm: 0.10896\n",
      "Epoch [1056/10044], Loss: 0.0057\n",
      "Epoch [1057/10044], Batch [1/7], Loss: 0.0060, Accuracy: 99.88%, Grad Norm: 0.07253\n",
      "Epoch [1057/10044], Batch [2/7], Loss: 0.0059, Accuracy: 99.87%, Grad Norm: 0.07943\n",
      "Epoch [1057/10044], Batch [3/7], Loss: 0.0053, Accuracy: 99.92%, Grad Norm: 0.06946\n",
      "Epoch [1057/10044], Batch [4/7], Loss: 0.0056, Accuracy: 99.92%, Grad Norm: 0.06687\n",
      "Epoch [1057/10044], Batch [5/7], Loss: 0.0062, Accuracy: 99.87%, Grad Norm: 0.09049\n",
      "Epoch [1057/10044], Batch [6/7], Loss: 0.0079, Accuracy: 99.82%, Grad Norm: 0.09674\n",
      "Epoch [1057/10044], Batch [7/7], Loss: 0.0057, Accuracy: 99.83%, Grad Norm: 0.12102\n",
      "Epoch [1057/10044], Loss: 0.0057\n",
      "Epoch [1058/10044], Batch [1/7], Loss: 0.0061, Accuracy: 99.90%, Grad Norm: 0.07557\n",
      "Epoch [1058/10044], Batch [2/7], Loss: 0.0081, Accuracy: 99.78%, Grad Norm: 0.10067\n",
      "Epoch [1058/10044], Batch [3/7], Loss: 0.0056, Accuracy: 99.91%, Grad Norm: 0.07827\n",
      "Epoch [1058/10044], Batch [4/7], Loss: 0.0066, Accuracy: 99.89%, Grad Norm: 0.07771\n",
      "Epoch [1058/10044], Batch [5/7], Loss: 0.0075, Accuracy: 99.81%, Grad Norm: 0.10367\n",
      "Epoch [1058/10044], Batch [6/7], Loss: 0.0055, Accuracy: 99.87%, Grad Norm: 0.06680\n",
      "Epoch [1058/10044], Batch [7/7], Loss: 0.0047, Accuracy: 99.87%, Grad Norm: 0.11106\n",
      "Epoch [1058/10044], Loss: 0.0047\n",
      "Epoch [1059/10044], Batch [1/7], Loss: 0.0076, Accuracy: 99.82%, Grad Norm: 0.11455\n",
      "Epoch [1059/10044], Batch [2/7], Loss: 0.0059, Accuracy: 99.85%, Grad Norm: 0.08869\n",
      "Epoch [1059/10044], Batch [3/7], Loss: 0.0054, Accuracy: 99.91%, Grad Norm: 0.07452\n",
      "Epoch [1059/10044], Batch [4/7], Loss: 0.0060, Accuracy: 99.91%, Grad Norm: 0.06574\n",
      "Epoch [1059/10044], Batch [5/7], Loss: 0.0063, Accuracy: 99.88%, Grad Norm: 0.07876\n",
      "Epoch [1059/10044], Batch [6/7], Loss: 0.0078, Accuracy: 99.77%, Grad Norm: 0.11240\n",
      "Epoch [1059/10044], Batch [7/7], Loss: 0.0039, Accuracy: 99.93%, Grad Norm: 0.08378\n",
      "Epoch [1059/10044], Loss: 0.0039\n",
      "Epoch [1060/10044], Batch [1/7], Loss: 0.0101, Accuracy: 99.77%, Grad Norm: 0.13563\n",
      "Epoch [1060/10044], Batch [2/7], Loss: 0.0069, Accuracy: 99.83%, Grad Norm: 0.10986\n",
      "Epoch [1060/10044], Batch [3/7], Loss: 0.0046, Accuracy: 99.94%, Grad Norm: 0.05070\n",
      "Epoch [1060/10044], Batch [4/7], Loss: 0.0054, Accuracy: 99.87%, Grad Norm: 0.06653\n",
      "Epoch [1060/10044], Batch [5/7], Loss: 0.0054, Accuracy: 99.89%, Grad Norm: 0.06897\n",
      "Epoch [1060/10044], Batch [6/7], Loss: 0.0068, Accuracy: 99.85%, Grad Norm: 0.07802\n",
      "Epoch [1060/10044], Batch [7/7], Loss: 0.0044, Accuracy: 99.90%, Grad Norm: 0.07428\n",
      "Epoch [1060/10044], Loss: 0.0044\n",
      "Epoch [1061/10044], Batch [1/7], Loss: 0.0072, Accuracy: 99.86%, Grad Norm: 0.08087\n",
      "Epoch [1061/10044], Batch [2/7], Loss: 0.0093, Accuracy: 99.81%, Grad Norm: 0.13192\n",
      "Epoch [1061/10044], Batch [3/7], Loss: 0.0049, Accuracy: 99.93%, Grad Norm: 0.06977\n",
      "Epoch [1061/10044], Batch [4/7], Loss: 0.0069, Accuracy: 99.84%, Grad Norm: 0.09825\n",
      "Epoch [1061/10044], Batch [5/7], Loss: 0.0051, Accuracy: 99.90%, Grad Norm: 0.07072\n",
      "Epoch [1061/10044], Batch [6/7], Loss: 0.0078, Accuracy: 99.80%, Grad Norm: 0.10027\n",
      "Epoch [1061/10044], Batch [7/7], Loss: 0.0051, Accuracy: 99.90%, Grad Norm: 0.08632\n",
      "Epoch [1061/10044], Loss: 0.0051\n",
      "Epoch [1062/10044], Batch [1/7], Loss: 0.0071, Accuracy: 99.85%, Grad Norm: 0.09626\n",
      "Epoch [1062/10044], Batch [2/7], Loss: 0.0070, Accuracy: 99.85%, Grad Norm: 0.10009\n",
      "Epoch [1062/10044], Batch [3/7], Loss: 0.0062, Accuracy: 99.89%, Grad Norm: 0.08991\n",
      "Epoch [1062/10044], Batch [4/7], Loss: 0.0067, Accuracy: 99.85%, Grad Norm: 0.08499\n",
      "Epoch [1062/10044], Batch [5/7], Loss: 0.0061, Accuracy: 99.84%, Grad Norm: 0.09209\n",
      "Epoch [1062/10044], Batch [6/7], Loss: 0.0070, Accuracy: 99.81%, Grad Norm: 0.08838\n",
      "Epoch [1062/10044], Batch [7/7], Loss: 0.0058, Accuracy: 99.90%, Grad Norm: 0.13807\n",
      "Epoch [1062/10044], Loss: 0.0058\n",
      "Epoch [1063/10044], Batch [1/7], Loss: 0.0065, Accuracy: 99.86%, Grad Norm: 0.08177\n",
      "Epoch [1063/10044], Batch [2/7], Loss: 0.0071, Accuracy: 99.83%, Grad Norm: 0.10925\n",
      "Epoch [1063/10044], Batch [3/7], Loss: 0.0060, Accuracy: 99.88%, Grad Norm: 0.07111\n",
      "Epoch [1063/10044], Batch [4/7], Loss: 0.0070, Accuracy: 99.85%, Grad Norm: 0.08249\n",
      "Epoch [1063/10044], Batch [5/7], Loss: 0.0056, Accuracy: 99.87%, Grad Norm: 0.09830\n",
      "Epoch [1063/10044], Batch [6/7], Loss: 0.0081, Accuracy: 99.78%, Grad Norm: 0.10961\n",
      "Epoch [1063/10044], Batch [7/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.12729\n",
      "Epoch [1063/10044], Loss: 0.0058\n",
      "Epoch [1064/10044], Batch [1/7], Loss: 0.0059, Accuracy: 99.88%, Grad Norm: 0.06882\n",
      "Epoch [1064/10044], Batch [2/7], Loss: 0.0068, Accuracy: 99.82%, Grad Norm: 0.09377\n",
      "Epoch [1064/10044], Batch [3/7], Loss: 0.0053, Accuracy: 99.91%, Grad Norm: 0.05658\n",
      "Epoch [1064/10044], Batch [4/7], Loss: 0.0074, Accuracy: 99.83%, Grad Norm: 0.08937\n",
      "Epoch [1064/10044], Batch [5/7], Loss: 0.0067, Accuracy: 99.80%, Grad Norm: 0.10609\n",
      "Epoch [1064/10044], Batch [6/7], Loss: 0.0063, Accuracy: 99.87%, Grad Norm: 0.08239\n",
      "Epoch [1064/10044], Batch [7/7], Loss: 0.0045, Accuracy: 99.92%, Grad Norm: 0.09190\n",
      "Epoch [1064/10044], Loss: 0.0045\n",
      "Epoch [1065/10044], Batch [1/7], Loss: 0.0067, Accuracy: 99.85%, Grad Norm: 0.07996\n",
      "Epoch [1065/10044], Batch [2/7], Loss: 0.0071, Accuracy: 99.87%, Grad Norm: 0.10397\n",
      "Epoch [1065/10044], Batch [3/7], Loss: 0.0060, Accuracy: 99.88%, Grad Norm: 0.07336\n",
      "Epoch [1065/10044], Batch [4/7], Loss: 0.0085, Accuracy: 99.83%, Grad Norm: 0.12014\n",
      "Epoch [1065/10044], Batch [5/7], Loss: 0.0055, Accuracy: 99.90%, Grad Norm: 0.08309\n",
      "Epoch [1065/10044], Batch [6/7], Loss: 0.0065, Accuracy: 99.82%, Grad Norm: 0.08878\n",
      "Epoch [1065/10044], Batch [7/7], Loss: 0.0045, Accuracy: 99.85%, Grad Norm: 0.10768\n",
      "Epoch [1065/10044], Loss: 0.0045\n",
      "Epoch [1066/10044], Batch [1/7], Loss: 0.0046, Accuracy: 99.93%, Grad Norm: 0.05426\n",
      "Epoch [1066/10044], Batch [2/7], Loss: 0.0069, Accuracy: 99.82%, Grad Norm: 0.08797\n",
      "Epoch [1066/10044], Batch [3/7], Loss: 0.0049, Accuracy: 99.93%, Grad Norm: 0.04996\n",
      "Epoch [1066/10044], Batch [4/7], Loss: 0.0062, Accuracy: 99.86%, Grad Norm: 0.07613\n",
      "Epoch [1066/10044], Batch [5/7], Loss: 0.0060, Accuracy: 99.90%, Grad Norm: 0.08116\n",
      "Epoch [1066/10044], Batch [6/7], Loss: 0.0097, Accuracy: 99.74%, Grad Norm: 0.13646\n",
      "Epoch [1066/10044], Batch [7/7], Loss: 0.0050, Accuracy: 99.90%, Grad Norm: 0.10104\n",
      "Epoch [1066/10044], Loss: 0.0050\n",
      "Epoch [1067/10044], Batch [1/7], Loss: 0.0058, Accuracy: 99.88%, Grad Norm: 0.08354\n",
      "Epoch [1067/10044], Batch [2/7], Loss: 0.0068, Accuracy: 99.84%, Grad Norm: 0.09806\n",
      "Epoch [1067/10044], Batch [3/7], Loss: 0.0050, Accuracy: 99.92%, Grad Norm: 0.06447\n",
      "Epoch [1067/10044], Batch [4/7], Loss: 0.0060, Accuracy: 99.87%, Grad Norm: 0.07933\n",
      "Epoch [1067/10044], Batch [5/7], Loss: 0.0065, Accuracy: 99.85%, Grad Norm: 0.09244\n",
      "Epoch [1067/10044], Batch [6/7], Loss: 0.0068, Accuracy: 99.87%, Grad Norm: 0.10740\n",
      "Epoch [1067/10044], Batch [7/7], Loss: 0.0037, Accuracy: 99.98%, Grad Norm: 0.07934\n",
      "Epoch [1067/10044], Loss: 0.0037\n",
      "Epoch [1068/10044], Batch [1/7], Loss: 0.0072, Accuracy: 99.80%, Grad Norm: 0.11138\n",
      "Epoch [1068/10044], Batch [2/7], Loss: 0.0067, Accuracy: 99.88%, Grad Norm: 0.07604\n",
      "Epoch [1068/10044], Batch [3/7], Loss: 0.0059, Accuracy: 99.87%, Grad Norm: 0.06904\n",
      "Epoch [1068/10044], Batch [4/7], Loss: 0.0057, Accuracy: 99.89%, Grad Norm: 0.07219\n",
      "Epoch [1068/10044], Batch [5/7], Loss: 0.0078, Accuracy: 99.82%, Grad Norm: 0.12199\n",
      "Epoch [1068/10044], Batch [6/7], Loss: 0.0097, Accuracy: 99.78%, Grad Norm: 0.11295\n",
      "Epoch [1068/10044], Batch [7/7], Loss: 0.0044, Accuracy: 99.88%, Grad Norm: 0.08710\n",
      "Epoch [1068/10044], Loss: 0.0044\n",
      "Epoch [1069/10044], Batch [1/7], Loss: 0.0078, Accuracy: 99.84%, Grad Norm: 0.08837\n",
      "Epoch [1069/10044], Batch [2/7], Loss: 0.0073, Accuracy: 99.82%, Grad Norm: 0.08953\n",
      "Epoch [1069/10044], Batch [3/7], Loss: 0.0051, Accuracy: 99.92%, Grad Norm: 0.05779\n",
      "Epoch [1069/10044], Batch [4/7], Loss: 0.0069, Accuracy: 99.86%, Grad Norm: 0.08337\n",
      "Epoch [1069/10044], Batch [5/7], Loss: 0.0076, Accuracy: 99.81%, Grad Norm: 0.10421\n",
      "Epoch [1069/10044], Batch [6/7], Loss: 0.0078, Accuracy: 99.77%, Grad Norm: 0.09680\n",
      "Epoch [1069/10044], Batch [7/7], Loss: 0.0059, Accuracy: 99.82%, Grad Norm: 0.12961\n",
      "Epoch [1069/10044], Loss: 0.0059\n",
      "Epoch [1070/10044], Batch [1/7], Loss: 0.0085, Accuracy: 99.82%, Grad Norm: 0.12256\n",
      "Epoch [1070/10044], Batch [2/7], Loss: 0.0083, Accuracy: 99.77%, Grad Norm: 0.10922\n",
      "Epoch [1070/10044], Batch [3/7], Loss: 0.0049, Accuracy: 99.93%, Grad Norm: 0.04961\n",
      "Epoch [1070/10044], Batch [4/7], Loss: 0.0054, Accuracy: 99.91%, Grad Norm: 0.07086\n",
      "Epoch [1070/10044], Batch [5/7], Loss: 0.0062, Accuracy: 99.87%, Grad Norm: 0.10522\n",
      "Epoch [1070/10044], Batch [6/7], Loss: 0.0071, Accuracy: 99.86%, Grad Norm: 0.09025\n",
      "Epoch [1070/10044], Batch [7/7], Loss: 0.0071, Accuracy: 99.85%, Grad Norm: 0.12178\n",
      "Epoch [1070/10044], Loss: 0.0071\n",
      "Epoch [1071/10044], Batch [1/7], Loss: 0.0074, Accuracy: 99.85%, Grad Norm: 0.10182\n",
      "Epoch [1071/10044], Batch [2/7], Loss: 0.0075, Accuracy: 99.86%, Grad Norm: 0.08955\n",
      "Epoch [1071/10044], Batch [3/7], Loss: 0.0044, Accuracy: 99.94%, Grad Norm: 0.04710\n",
      "Epoch [1071/10044], Batch [4/7], Loss: 0.0051, Accuracy: 99.93%, Grad Norm: 0.05468\n",
      "Epoch [1071/10044], Batch [5/7], Loss: 0.0062, Accuracy: 99.92%, Grad Norm: 0.07089\n",
      "Epoch [1071/10044], Batch [6/7], Loss: 0.0084, Accuracy: 99.81%, Grad Norm: 0.11211\n",
      "Epoch [1071/10044], Batch [7/7], Loss: 0.0049, Accuracy: 99.90%, Grad Norm: 0.10170\n",
      "Epoch [1071/10044], Loss: 0.0049\n",
      "Epoch [1072/10044], Batch [1/7], Loss: 0.0057, Accuracy: 99.92%, Grad Norm: 0.06707\n",
      "Epoch [1072/10044], Batch [2/7], Loss: 0.0063, Accuracy: 99.90%, Grad Norm: 0.07966\n",
      "Epoch [1072/10044], Batch [3/7], Loss: 0.0054, Accuracy: 99.91%, Grad Norm: 0.06565\n",
      "Epoch [1072/10044], Batch [4/7], Loss: 0.0056, Accuracy: 99.88%, Grad Norm: 0.06462\n",
      "Epoch [1072/10044], Batch [5/7], Loss: 0.0061, Accuracy: 99.87%, Grad Norm: 0.07978\n",
      "Epoch [1072/10044], Batch [6/7], Loss: 0.0064, Accuracy: 99.88%, Grad Norm: 0.07271\n",
      "Epoch [1072/10044], Batch [7/7], Loss: 0.0055, Accuracy: 99.83%, Grad Norm: 0.10993\n",
      "Epoch [1072/10044], Loss: 0.0055\n",
      "Epoch [1073/10044], Batch [1/7], Loss: 0.0070, Accuracy: 99.86%, Grad Norm: 0.08482\n",
      "Epoch [1073/10044], Batch [2/7], Loss: 0.0065, Accuracy: 99.85%, Grad Norm: 0.10095\n",
      "Epoch [1073/10044], Batch [3/7], Loss: 0.0047, Accuracy: 99.94%, Grad Norm: 0.05260\n",
      "Epoch [1073/10044], Batch [4/7], Loss: 0.0054, Accuracy: 99.86%, Grad Norm: 0.07051\n",
      "Epoch [1073/10044], Batch [5/7], Loss: 0.0045, Accuracy: 99.91%, Grad Norm: 0.05017\n",
      "Epoch [1073/10044], Batch [6/7], Loss: 0.0063, Accuracy: 99.82%, Grad Norm: 0.08083\n",
      "Epoch [1073/10044], Batch [7/7], Loss: 0.0040, Accuracy: 99.92%, Grad Norm: 0.07174\n",
      "Epoch [1073/10044], Loss: 0.0040\n",
      "Epoch [1074/10044], Batch [1/7], Loss: 0.0065, Accuracy: 99.90%, Grad Norm: 0.10151\n",
      "Epoch [1074/10044], Batch [2/7], Loss: 0.0068, Accuracy: 99.83%, Grad Norm: 0.09898\n",
      "Epoch [1074/10044], Batch [3/7], Loss: 0.0042, Accuracy: 99.93%, Grad Norm: 0.04934\n",
      "Epoch [1074/10044], Batch [4/7], Loss: 0.0055, Accuracy: 99.90%, Grad Norm: 0.06493\n",
      "Epoch [1074/10044], Batch [5/7], Loss: 0.0051, Accuracy: 99.88%, Grad Norm: 0.06740\n",
      "Epoch [1074/10044], Batch [6/7], Loss: 0.0078, Accuracy: 99.82%, Grad Norm: 0.12271\n",
      "Epoch [1074/10044], Batch [7/7], Loss: 0.0045, Accuracy: 99.90%, Grad Norm: 0.11681\n",
      "Epoch [1074/10044], Loss: 0.0045\n",
      "Epoch [1075/10044], Batch [1/7], Loss: 0.0072, Accuracy: 99.85%, Grad Norm: 0.09390\n",
      "Epoch [1075/10044], Batch [2/7], Loss: 0.0077, Accuracy: 99.82%, Grad Norm: 0.11976\n",
      "Epoch [1075/10044], Batch [3/7], Loss: 0.0052, Accuracy: 99.91%, Grad Norm: 0.05316\n",
      "Epoch [1075/10044], Batch [4/7], Loss: 0.0061, Accuracy: 99.88%, Grad Norm: 0.07306\n",
      "Epoch [1075/10044], Batch [5/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.07210\n",
      "Epoch [1075/10044], Batch [6/7], Loss: 0.0093, Accuracy: 99.73%, Grad Norm: 0.12717\n",
      "Epoch [1075/10044], Batch [7/7], Loss: 0.0051, Accuracy: 99.90%, Grad Norm: 0.12029\n",
      "Epoch [1075/10044], Loss: 0.0051\n",
      "Epoch [1076/10044], Batch [1/7], Loss: 0.0085, Accuracy: 99.78%, Grad Norm: 0.12683\n",
      "Epoch [1076/10044], Batch [2/7], Loss: 0.0074, Accuracy: 99.82%, Grad Norm: 0.09832\n",
      "Epoch [1076/10044], Batch [3/7], Loss: 0.0057, Accuracy: 99.87%, Grad Norm: 0.06999\n",
      "Epoch [1076/10044], Batch [4/7], Loss: 0.0059, Accuracy: 99.91%, Grad Norm: 0.06528\n",
      "Epoch [1076/10044], Batch [5/7], Loss: 0.0044, Accuracy: 99.96%, Grad Norm: 0.05292\n",
      "Epoch [1076/10044], Batch [6/7], Loss: 0.0074, Accuracy: 99.82%, Grad Norm: 0.10128\n",
      "Epoch [1076/10044], Batch [7/7], Loss: 0.0037, Accuracy: 99.93%, Grad Norm: 0.06867\n",
      "Epoch [1076/10044], Loss: 0.0037\n",
      "Epoch [1077/10044], Batch [1/7], Loss: 0.0069, Accuracy: 99.82%, Grad Norm: 0.09628\n",
      "Epoch [1077/10044], Batch [2/7], Loss: 0.0064, Accuracy: 99.84%, Grad Norm: 0.09078\n",
      "Epoch [1077/10044], Batch [3/7], Loss: 0.0062, Accuracy: 99.87%, Grad Norm: 0.08367\n",
      "Epoch [1077/10044], Batch [4/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.07402\n",
      "Epoch [1077/10044], Batch [5/7], Loss: 0.0055, Accuracy: 99.88%, Grad Norm: 0.07192\n",
      "Epoch [1077/10044], Batch [6/7], Loss: 0.0069, Accuracy: 99.83%, Grad Norm: 0.08707\n",
      "Epoch [1077/10044], Batch [7/7], Loss: 0.0048, Accuracy: 99.88%, Grad Norm: 0.09962\n",
      "Epoch [1077/10044], Loss: 0.0048\n",
      "Epoch [1078/10044], Batch [1/7], Loss: 0.0069, Accuracy: 99.82%, Grad Norm: 0.08682\n",
      "Epoch [1078/10044], Batch [2/7], Loss: 0.0072, Accuracy: 99.84%, Grad Norm: 0.09789\n",
      "Epoch [1078/10044], Batch [3/7], Loss: 0.0057, Accuracy: 99.88%, Grad Norm: 0.06838\n",
      "Epoch [1078/10044], Batch [4/7], Loss: 0.0067, Accuracy: 99.82%, Grad Norm: 0.07866\n",
      "Epoch [1078/10044], Batch [5/7], Loss: 0.0047, Accuracy: 99.91%, Grad Norm: 0.06598\n",
      "Epoch [1078/10044], Batch [6/7], Loss: 0.0070, Accuracy: 99.86%, Grad Norm: 0.07794\n",
      "Epoch [1078/10044], Batch [7/7], Loss: 0.0068, Accuracy: 99.80%, Grad Norm: 0.13969\n",
      "Epoch [1078/10044], Loss: 0.0068\n",
      "Epoch [1079/10044], Batch [1/7], Loss: 0.0082, Accuracy: 99.85%, Grad Norm: 0.13544\n",
      "Epoch [1079/10044], Batch [2/7], Loss: 0.0060, Accuracy: 99.87%, Grad Norm: 0.07420\n",
      "Epoch [1079/10044], Batch [3/7], Loss: 0.0052, Accuracy: 99.92%, Grad Norm: 0.05630\n",
      "Epoch [1079/10044], Batch [4/7], Loss: 0.0072, Accuracy: 99.83%, Grad Norm: 0.10118\n",
      "Epoch [1079/10044], Batch [5/7], Loss: 0.0053, Accuracy: 99.87%, Grad Norm: 0.07719\n",
      "Epoch [1079/10044], Batch [6/7], Loss: 0.0064, Accuracy: 99.86%, Grad Norm: 0.08751\n",
      "Epoch [1079/10044], Batch [7/7], Loss: 0.0056, Accuracy: 99.88%, Grad Norm: 0.11227\n",
      "Epoch [1079/10044], Loss: 0.0056\n",
      "Epoch [1080/10044], Batch [1/7], Loss: 0.0082, Accuracy: 99.82%, Grad Norm: 0.09900\n",
      "Epoch [1080/10044], Batch [2/7], Loss: 0.0072, Accuracy: 99.80%, Grad Norm: 0.11590\n",
      "Epoch [1080/10044], Batch [3/7], Loss: 0.0047, Accuracy: 99.93%, Grad Norm: 0.05871\n",
      "Epoch [1080/10044], Batch [4/7], Loss: 0.0063, Accuracy: 99.88%, Grad Norm: 0.08658\n",
      "Epoch [1080/10044], Batch [5/7], Loss: 0.0062, Accuracy: 99.89%, Grad Norm: 0.08294\n",
      "Epoch [1080/10044], Batch [6/7], Loss: 0.0081, Accuracy: 99.81%, Grad Norm: 0.09711\n",
      "Epoch [1080/10044], Batch [7/7], Loss: 0.0047, Accuracy: 99.87%, Grad Norm: 0.10643\n",
      "Epoch [1080/10044], Loss: 0.0047\n",
      "Epoch [1081/10044], Batch [1/7], Loss: 0.0071, Accuracy: 99.87%, Grad Norm: 0.08065\n",
      "Epoch [1081/10044], Batch [2/7], Loss: 0.0073, Accuracy: 99.77%, Grad Norm: 0.09392\n",
      "Epoch [1081/10044], Batch [3/7], Loss: 0.0048, Accuracy: 99.92%, Grad Norm: 0.05458\n",
      "Epoch [1081/10044], Batch [4/7], Loss: 0.0066, Accuracy: 99.82%, Grad Norm: 0.09140\n",
      "Epoch [1081/10044], Batch [5/7], Loss: 0.0071, Accuracy: 99.81%, Grad Norm: 0.09030\n",
      "Epoch [1081/10044], Batch [6/7], Loss: 0.0067, Accuracy: 99.86%, Grad Norm: 0.08805\n",
      "Epoch [1081/10044], Batch [7/7], Loss: 0.0056, Accuracy: 99.83%, Grad Norm: 0.13341\n",
      "Epoch [1081/10044], Loss: 0.0056\n",
      "Epoch [1082/10044], Batch [1/7], Loss: 0.0062, Accuracy: 99.87%, Grad Norm: 0.07826\n",
      "Epoch [1082/10044], Batch [2/7], Loss: 0.0075, Accuracy: 99.79%, Grad Norm: 0.09270\n",
      "Epoch [1082/10044], Batch [3/7], Loss: 0.0055, Accuracy: 99.90%, Grad Norm: 0.06595\n",
      "Epoch [1082/10044], Batch [4/7], Loss: 0.0069, Accuracy: 99.87%, Grad Norm: 0.08326\n",
      "Epoch [1082/10044], Batch [5/7], Loss: 0.0062, Accuracy: 99.87%, Grad Norm: 0.07872\n",
      "Epoch [1082/10044], Batch [6/7], Loss: 0.0066, Accuracy: 99.83%, Grad Norm: 0.08757\n",
      "Epoch [1082/10044], Batch [7/7], Loss: 0.0043, Accuracy: 99.97%, Grad Norm: 0.07576\n",
      "Epoch [1082/10044], Loss: 0.0043\n",
      "Epoch [1083/10044], Batch [1/7], Loss: 0.0065, Accuracy: 99.89%, Grad Norm: 0.07595\n",
      "Epoch [1083/10044], Batch [2/7], Loss: 0.0072, Accuracy: 99.78%, Grad Norm: 0.10051\n",
      "Epoch [1083/10044], Batch [3/7], Loss: 0.0051, Accuracy: 99.91%, Grad Norm: 0.05989\n",
      "Epoch [1083/10044], Batch [4/7], Loss: 0.0057, Accuracy: 99.87%, Grad Norm: 0.07433\n",
      "Epoch [1083/10044], Batch [5/7], Loss: 0.0062, Accuracy: 99.91%, Grad Norm: 0.07001\n",
      "Epoch [1083/10044], Batch [6/7], Loss: 0.0077, Accuracy: 99.81%, Grad Norm: 0.12338\n",
      "Epoch [1083/10044], Batch [7/7], Loss: 0.0040, Accuracy: 99.92%, Grad Norm: 0.10655\n",
      "Epoch [1083/10044], Loss: 0.0040\n",
      "Epoch [1084/10044], Batch [1/7], Loss: 0.0065, Accuracy: 99.85%, Grad Norm: 0.08967\n",
      "Epoch [1084/10044], Batch [2/7], Loss: 0.0080, Accuracy: 99.82%, Grad Norm: 0.10351\n",
      "Epoch [1084/10044], Batch [3/7], Loss: 0.0067, Accuracy: 99.82%, Grad Norm: 0.08942\n",
      "Epoch [1084/10044], Batch [4/7], Loss: 0.0071, Accuracy: 99.80%, Grad Norm: 0.08952\n",
      "Epoch [1084/10044], Batch [5/7], Loss: 0.0055, Accuracy: 99.86%, Grad Norm: 0.08510\n",
      "Epoch [1084/10044], Batch [6/7], Loss: 0.0069, Accuracy: 99.84%, Grad Norm: 0.11162\n",
      "Epoch [1084/10044], Batch [7/7], Loss: 0.0059, Accuracy: 99.82%, Grad Norm: 0.15576\n",
      "Epoch [1084/10044], Loss: 0.0059\n",
      "Epoch [1085/10044], Batch [1/7], Loss: 0.0058, Accuracy: 99.90%, Grad Norm: 0.07467\n",
      "Epoch [1085/10044], Batch [2/7], Loss: 0.0086, Accuracy: 99.78%, Grad Norm: 0.10216\n",
      "Epoch [1085/10044], Batch [3/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.08101\n",
      "Epoch [1085/10044], Batch [4/7], Loss: 0.0065, Accuracy: 99.87%, Grad Norm: 0.07786\n",
      "Epoch [1085/10044], Batch [5/7], Loss: 0.0058, Accuracy: 99.85%, Grad Norm: 0.09419\n",
      "Epoch [1085/10044], Batch [6/7], Loss: 0.0061, Accuracy: 99.84%, Grad Norm: 0.08461\n",
      "Epoch [1085/10044], Batch [7/7], Loss: 0.0051, Accuracy: 99.90%, Grad Norm: 0.10137\n",
      "Epoch [1085/10044], Loss: 0.0051\n",
      "Epoch [1086/10044], Batch [1/7], Loss: 0.0050, Accuracy: 99.91%, Grad Norm: 0.06461\n",
      "Epoch [1086/10044], Batch [2/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.07066\n",
      "Epoch [1086/10044], Batch [3/7], Loss: 0.0047, Accuracy: 99.92%, Grad Norm: 0.05543\n",
      "Epoch [1086/10044], Batch [4/7], Loss: 0.0082, Accuracy: 99.82%, Grad Norm: 0.09244\n",
      "Epoch [1086/10044], Batch [5/7], Loss: 0.0049, Accuracy: 99.89%, Grad Norm: 0.06277\n",
      "Epoch [1086/10044], Batch [6/7], Loss: 0.0067, Accuracy: 99.87%, Grad Norm: 0.07978\n",
      "Epoch [1086/10044], Batch [7/7], Loss: 0.0033, Accuracy: 99.98%, Grad Norm: 0.05567\n",
      "Epoch [1086/10044], Loss: 0.0033\n",
      "Epoch [1087/10044], Batch [1/7], Loss: 0.0063, Accuracy: 99.85%, Grad Norm: 0.08591\n",
      "Epoch [1087/10044], Batch [2/7], Loss: 0.0070, Accuracy: 99.83%, Grad Norm: 0.09154\n",
      "Epoch [1087/10044], Batch [3/7], Loss: 0.0054, Accuracy: 99.92%, Grad Norm: 0.06215\n",
      "Epoch [1087/10044], Batch [4/7], Loss: 0.0048, Accuracy: 99.91%, Grad Norm: 0.05643\n",
      "Epoch [1087/10044], Batch [5/7], Loss: 0.0052, Accuracy: 99.88%, Grad Norm: 0.07242\n",
      "Epoch [1087/10044], Batch [6/7], Loss: 0.0060, Accuracy: 99.87%, Grad Norm: 0.07629\n",
      "Epoch [1087/10044], Batch [7/7], Loss: 0.0033, Accuracy: 99.95%, Grad Norm: 0.06920\n",
      "Epoch [1087/10044], Loss: 0.0033\n",
      "Epoch [1088/10044], Batch [1/7], Loss: 0.0052, Accuracy: 99.89%, Grad Norm: 0.05822\n",
      "Epoch [1088/10044], Batch [2/7], Loss: 0.0068, Accuracy: 99.82%, Grad Norm: 0.09084\n",
      "Epoch [1088/10044], Batch [3/7], Loss: 0.0050, Accuracy: 99.91%, Grad Norm: 0.06595\n",
      "Epoch [1088/10044], Batch [4/7], Loss: 0.0060, Accuracy: 99.85%, Grad Norm: 0.08044\n",
      "Epoch [1088/10044], Batch [5/7], Loss: 0.0049, Accuracy: 99.91%, Grad Norm: 0.07096\n",
      "Epoch [1088/10044], Batch [6/7], Loss: 0.0072, Accuracy: 99.82%, Grad Norm: 0.09817\n",
      "Epoch [1088/10044], Batch [7/7], Loss: 0.0048, Accuracy: 99.92%, Grad Norm: 0.10836\n",
      "Epoch [1088/10044], Loss: 0.0048\n",
      "Epoch [1089/10044], Batch [1/7], Loss: 0.0060, Accuracy: 99.91%, Grad Norm: 0.07079\n",
      "Epoch [1089/10044], Batch [2/7], Loss: 0.0078, Accuracy: 99.80%, Grad Norm: 0.10293\n",
      "Epoch [1089/10044], Batch [3/7], Loss: 0.0054, Accuracy: 99.91%, Grad Norm: 0.07580\n",
      "Epoch [1089/10044], Batch [4/7], Loss: 0.0065, Accuracy: 99.83%, Grad Norm: 0.07740\n",
      "Epoch [1089/10044], Batch [5/7], Loss: 0.0056, Accuracy: 99.90%, Grad Norm: 0.08050\n",
      "Epoch [1089/10044], Batch [6/7], Loss: 0.0066, Accuracy: 99.80%, Grad Norm: 0.09754\n",
      "Epoch [1089/10044], Batch [7/7], Loss: 0.0032, Accuracy: 99.97%, Grad Norm: 0.07521\n",
      "Epoch [1089/10044], Loss: 0.0032\n",
      "Epoch [1090/10044], Batch [1/7], Loss: 0.0055, Accuracy: 99.91%, Grad Norm: 0.07163\n",
      "Epoch [1090/10044], Batch [2/7], Loss: 0.0071, Accuracy: 99.84%, Grad Norm: 0.09978\n",
      "Epoch [1090/10044], Batch [3/7], Loss: 0.0050, Accuracy: 99.91%, Grad Norm: 0.07359\n",
      "Epoch [1090/10044], Batch [4/7], Loss: 0.0066, Accuracy: 99.86%, Grad Norm: 0.08472\n",
      "Epoch [1090/10044], Batch [5/7], Loss: 0.0064, Accuracy: 99.84%, Grad Norm: 0.11425\n",
      "Epoch [1090/10044], Batch [6/7], Loss: 0.0071, Accuracy: 99.82%, Grad Norm: 0.09158\n",
      "Epoch [1090/10044], Batch [7/7], Loss: 0.0040, Accuracy: 99.92%, Grad Norm: 0.10111\n",
      "Epoch [1090/10044], Loss: 0.0040\n",
      "Epoch [1091/10044], Batch [1/7], Loss: 0.0054, Accuracy: 99.89%, Grad Norm: 0.07557\n",
      "Epoch [1091/10044], Batch [2/7], Loss: 0.0074, Accuracy: 99.77%, Grad Norm: 0.09768\n",
      "Epoch [1091/10044], Batch [3/7], Loss: 0.0045, Accuracy: 99.91%, Grad Norm: 0.06603\n",
      "Epoch [1091/10044], Batch [4/7], Loss: 0.0064, Accuracy: 99.84%, Grad Norm: 0.08779\n",
      "Epoch [1091/10044], Batch [5/7], Loss: 0.0053, Accuracy: 99.91%, Grad Norm: 0.08864\n",
      "Epoch [1091/10044], Batch [6/7], Loss: 0.0062, Accuracy: 99.85%, Grad Norm: 0.07736\n",
      "Epoch [1091/10044], Batch [7/7], Loss: 0.0032, Accuracy: 99.97%, Grad Norm: 0.07402\n",
      "Epoch [1091/10044], Loss: 0.0032\n",
      "Epoch [1092/10044], Batch [1/7], Loss: 0.0049, Accuracy: 99.92%, Grad Norm: 0.07979\n",
      "Epoch [1092/10044], Batch [2/7], Loss: 0.0068, Accuracy: 99.84%, Grad Norm: 0.10009\n",
      "Epoch [1092/10044], Batch [3/7], Loss: 0.0042, Accuracy: 99.95%, Grad Norm: 0.04831\n",
      "Epoch [1092/10044], Batch [4/7], Loss: 0.0064, Accuracy: 99.84%, Grad Norm: 0.08487\n",
      "Epoch [1092/10044], Batch [5/7], Loss: 0.0051, Accuracy: 99.91%, Grad Norm: 0.07785\n",
      "Epoch [1092/10044], Batch [6/7], Loss: 0.0071, Accuracy: 99.81%, Grad Norm: 0.10602\n",
      "Epoch [1092/10044], Batch [7/7], Loss: 0.0041, Accuracy: 99.92%, Grad Norm: 0.09666\n",
      "Epoch [1092/10044], Loss: 0.0041\n",
      "Epoch [1093/10044], Batch [1/7], Loss: 0.0081, Accuracy: 99.78%, Grad Norm: 0.13535\n",
      "Epoch [1093/10044], Batch [2/7], Loss: 0.0054, Accuracy: 99.85%, Grad Norm: 0.09582\n",
      "Epoch [1093/10044], Batch [3/7], Loss: 0.0043, Accuracy: 99.94%, Grad Norm: 0.04739\n",
      "Epoch [1093/10044], Batch [4/7], Loss: 0.0051, Accuracy: 99.92%, Grad Norm: 0.06632\n",
      "Epoch [1093/10044], Batch [5/7], Loss: 0.0055, Accuracy: 99.87%, Grad Norm: 0.07282\n",
      "Epoch [1093/10044], Batch [6/7], Loss: 0.0056, Accuracy: 99.87%, Grad Norm: 0.07333\n",
      "Epoch [1093/10044], Batch [7/7], Loss: 0.0067, Accuracy: 99.82%, Grad Norm: 0.13398\n",
      "Epoch [1093/10044], Loss: 0.0067\n",
      "Epoch [1094/10044], Batch [1/7], Loss: 0.0059, Accuracy: 99.86%, Grad Norm: 0.07763\n",
      "Epoch [1094/10044], Batch [2/7], Loss: 0.0055, Accuracy: 99.87%, Grad Norm: 0.07934\n",
      "Epoch [1094/10044], Batch [3/7], Loss: 0.0043, Accuracy: 99.93%, Grad Norm: 0.05129\n",
      "Epoch [1094/10044], Batch [4/7], Loss: 0.0057, Accuracy: 99.92%, Grad Norm: 0.06829\n",
      "Epoch [1094/10044], Batch [5/7], Loss: 0.0054, Accuracy: 99.88%, Grad Norm: 0.08791\n",
      "Epoch [1094/10044], Batch [6/7], Loss: 0.0073, Accuracy: 99.84%, Grad Norm: 0.09245\n",
      "Epoch [1094/10044], Batch [7/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.10492\n",
      "Epoch [1094/10044], Loss: 0.0058\n",
      "Epoch [1095/10044], Batch [1/7], Loss: 0.0057, Accuracy: 99.89%, Grad Norm: 0.07333\n",
      "Epoch [1095/10044], Batch [2/7], Loss: 0.0060, Accuracy: 99.86%, Grad Norm: 0.08979\n",
      "Epoch [1095/10044], Batch [3/7], Loss: 0.0046, Accuracy: 99.92%, Grad Norm: 0.07206\n",
      "Epoch [1095/10044], Batch [4/7], Loss: 0.0063, Accuracy: 99.84%, Grad Norm: 0.07238\n",
      "Epoch [1095/10044], Batch [5/7], Loss: 0.0052, Accuracy: 99.92%, Grad Norm: 0.06888\n",
      "Epoch [1095/10044], Batch [6/7], Loss: 0.0076, Accuracy: 99.80%, Grad Norm: 0.11514\n",
      "Epoch [1095/10044], Batch [7/7], Loss: 0.0056, Accuracy: 99.85%, Grad Norm: 0.11809\n",
      "Epoch [1095/10044], Loss: 0.0056\n",
      "Epoch [1096/10044], Batch [1/7], Loss: 0.0059, Accuracy: 99.89%, Grad Norm: 0.07754\n",
      "Epoch [1096/10044], Batch [2/7], Loss: 0.0069, Accuracy: 99.82%, Grad Norm: 0.10188\n",
      "Epoch [1096/10044], Batch [3/7], Loss: 0.0046, Accuracy: 99.92%, Grad Norm: 0.05355\n",
      "Epoch [1096/10044], Batch [4/7], Loss: 0.0070, Accuracy: 99.84%, Grad Norm: 0.07923\n",
      "Epoch [1096/10044], Batch [5/7], Loss: 0.0054, Accuracy: 99.85%, Grad Norm: 0.08273\n",
      "Epoch [1096/10044], Batch [6/7], Loss: 0.0076, Accuracy: 99.79%, Grad Norm: 0.12555\n",
      "Epoch [1096/10044], Batch [7/7], Loss: 0.0033, Accuracy: 99.93%, Grad Norm: 0.05956\n",
      "Epoch [1096/10044], Loss: 0.0033\n",
      "Epoch [1097/10044], Batch [1/7], Loss: 0.0072, Accuracy: 99.87%, Grad Norm: 0.08880\n",
      "Epoch [1097/10044], Batch [2/7], Loss: 0.0063, Accuracy: 99.86%, Grad Norm: 0.10176\n",
      "Epoch [1097/10044], Batch [3/7], Loss: 0.0044, Accuracy: 99.92%, Grad Norm: 0.04936\n",
      "Epoch [1097/10044], Batch [4/7], Loss: 0.0060, Accuracy: 99.88%, Grad Norm: 0.06416\n",
      "Epoch [1097/10044], Batch [5/7], Loss: 0.0054, Accuracy: 99.89%, Grad Norm: 0.07097\n",
      "Epoch [1097/10044], Batch [6/7], Loss: 0.0063, Accuracy: 99.85%, Grad Norm: 0.09067\n",
      "Epoch [1097/10044], Batch [7/7], Loss: 0.0044, Accuracy: 99.93%, Grad Norm: 0.09829\n",
      "Epoch [1097/10044], Loss: 0.0044\n",
      "Epoch [1098/10044], Batch [1/7], Loss: 0.0056, Accuracy: 99.91%, Grad Norm: 0.07500\n",
      "Epoch [1098/10044], Batch [2/7], Loss: 0.0065, Accuracy: 99.85%, Grad Norm: 0.08920\n",
      "Epoch [1098/10044], Batch [3/7], Loss: 0.0053, Accuracy: 99.92%, Grad Norm: 0.06425\n",
      "Epoch [1098/10044], Batch [4/7], Loss: 0.0056, Accuracy: 99.89%, Grad Norm: 0.07086\n",
      "Epoch [1098/10044], Batch [5/7], Loss: 0.0060, Accuracy: 99.87%, Grad Norm: 0.08743\n",
      "Epoch [1098/10044], Batch [6/7], Loss: 0.0081, Accuracy: 99.79%, Grad Norm: 0.10059\n",
      "Epoch [1098/10044], Batch [7/7], Loss: 0.0044, Accuracy: 99.88%, Grad Norm: 0.08659\n",
      "Epoch [1098/10044], Loss: 0.0044\n",
      "Epoch [1099/10044], Batch [1/7], Loss: 0.0063, Accuracy: 99.87%, Grad Norm: 0.09163\n",
      "Epoch [1099/10044], Batch [2/7], Loss: 0.0060, Accuracy: 99.84%, Grad Norm: 0.07787\n",
      "Epoch [1099/10044], Batch [3/7], Loss: 0.0048, Accuracy: 99.89%, Grad Norm: 0.05449\n",
      "Epoch [1099/10044], Batch [4/7], Loss: 0.0051, Accuracy: 99.92%, Grad Norm: 0.06263\n",
      "Epoch [1099/10044], Batch [5/7], Loss: 0.0064, Accuracy: 99.84%, Grad Norm: 0.11058\n",
      "Epoch [1099/10044], Batch [6/7], Loss: 0.0063, Accuracy: 99.82%, Grad Norm: 0.07598\n",
      "Epoch [1099/10044], Batch [7/7], Loss: 0.0042, Accuracy: 99.90%, Grad Norm: 0.11201\n",
      "Epoch [1099/10044], Loss: 0.0042\n",
      "Epoch [1100/10044], Batch [1/7], Loss: 0.0081, Accuracy: 99.77%, Grad Norm: 0.10976\n",
      "Epoch [1100/10044], Batch [2/7], Loss: 0.0058, Accuracy: 99.86%, Grad Norm: 0.08972\n",
      "Epoch [1100/10044], Batch [3/7], Loss: 0.0048, Accuracy: 99.92%, Grad Norm: 0.05653\n",
      "Epoch [1100/10044], Batch [4/7], Loss: 0.0050, Accuracy: 99.92%, Grad Norm: 0.06734\n",
      "Epoch [1100/10044], Batch [5/7], Loss: 0.0058, Accuracy: 99.85%, Grad Norm: 0.10183\n",
      "Epoch [1100/10044], Batch [6/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.08099\n",
      "Epoch [1100/10044], Batch [7/7], Loss: 0.0048, Accuracy: 99.88%, Grad Norm: 0.10940\n",
      "Epoch [1100/10044], Loss: 0.0048\n",
      "Epoch [1101/10044], Batch [1/7], Loss: 0.0064, Accuracy: 99.87%, Grad Norm: 0.08943\n",
      "Epoch [1101/10044], Batch [2/7], Loss: 0.0061, Accuracy: 99.87%, Grad Norm: 0.09187\n",
      "Epoch [1101/10044], Batch [3/7], Loss: 0.0045, Accuracy: 99.91%, Grad Norm: 0.05455\n",
      "Epoch [1101/10044], Batch [4/7], Loss: 0.0054, Accuracy: 99.91%, Grad Norm: 0.05978\n",
      "Epoch [1101/10044], Batch [5/7], Loss: 0.0050, Accuracy: 99.94%, Grad Norm: 0.06548\n",
      "Epoch [1101/10044], Batch [6/7], Loss: 0.0064, Accuracy: 99.87%, Grad Norm: 0.07811\n",
      "Epoch [1101/10044], Batch [7/7], Loss: 0.0053, Accuracy: 99.93%, Grad Norm: 0.09549\n",
      "Epoch [1101/10044], Loss: 0.0053\n",
      "Epoch [1102/10044], Batch [1/7], Loss: 0.0068, Accuracy: 99.85%, Grad Norm: 0.09918\n",
      "Epoch [1102/10044], Batch [2/7], Loss: 0.0056, Accuracy: 99.87%, Grad Norm: 0.08034\n",
      "Epoch [1102/10044], Batch [3/7], Loss: 0.0049, Accuracy: 99.92%, Grad Norm: 0.07750\n",
      "Epoch [1102/10044], Batch [4/7], Loss: 0.0051, Accuracy: 99.91%, Grad Norm: 0.05602\n",
      "Epoch [1102/10044], Batch [5/7], Loss: 0.0044, Accuracy: 99.92%, Grad Norm: 0.06586\n",
      "Epoch [1102/10044], Batch [6/7], Loss: 0.0059, Accuracy: 99.88%, Grad Norm: 0.08281\n",
      "Epoch [1102/10044], Batch [7/7], Loss: 0.0065, Accuracy: 99.87%, Grad Norm: 0.18462\n",
      "Epoch [1102/10044], Loss: 0.0065\n",
      "Epoch [1103/10044], Batch [1/7], Loss: 0.0064, Accuracy: 99.85%, Grad Norm: 0.07714\n",
      "Epoch [1103/10044], Batch [2/7], Loss: 0.0057, Accuracy: 99.87%, Grad Norm: 0.08649\n",
      "Epoch [1103/10044], Batch [3/7], Loss: 0.0037, Accuracy: 99.95%, Grad Norm: 0.04056\n",
      "Epoch [1103/10044], Batch [4/7], Loss: 0.0048, Accuracy: 99.93%, Grad Norm: 0.05850\n",
      "Epoch [1103/10044], Batch [5/7], Loss: 0.0052, Accuracy: 99.89%, Grad Norm: 0.07200\n",
      "Epoch [1103/10044], Batch [6/7], Loss: 0.0078, Accuracy: 99.82%, Grad Norm: 0.11480\n",
      "Epoch [1103/10044], Batch [7/7], Loss: 0.0065, Accuracy: 99.83%, Grad Norm: 0.15935\n",
      "Epoch [1103/10044], Loss: 0.0065\n",
      "Epoch [1104/10044], Batch [1/7], Loss: 0.0062, Accuracy: 99.87%, Grad Norm: 0.09310\n",
      "Epoch [1104/10044], Batch [2/7], Loss: 0.0055, Accuracy: 99.88%, Grad Norm: 0.08662\n",
      "Epoch [1104/10044], Batch [3/7], Loss: 0.0043, Accuracy: 99.92%, Grad Norm: 0.05447\n",
      "Epoch [1104/10044], Batch [4/7], Loss: 0.0053, Accuracy: 99.87%, Grad Norm: 0.06477\n",
      "Epoch [1104/10044], Batch [5/7], Loss: 0.0063, Accuracy: 99.83%, Grad Norm: 0.10878\n",
      "Epoch [1104/10044], Batch [6/7], Loss: 0.0092, Accuracy: 99.78%, Grad Norm: 0.14042\n",
      "Epoch [1104/10044], Batch [7/7], Loss: 0.0054, Accuracy: 99.90%, Grad Norm: 0.11738\n",
      "Epoch [1104/10044], Loss: 0.0054\n",
      "Epoch [1105/10044], Batch [1/7], Loss: 0.0076, Accuracy: 99.88%, Grad Norm: 0.10232\n",
      "Epoch [1105/10044], Batch [2/7], Loss: 0.0077, Accuracy: 99.77%, Grad Norm: 0.10987\n",
      "Epoch [1105/10044], Batch [3/7], Loss: 0.0066, Accuracy: 99.84%, Grad Norm: 0.09033\n",
      "Epoch [1105/10044], Batch [4/7], Loss: 0.0062, Accuracy: 99.87%, Grad Norm: 0.08231\n",
      "Epoch [1105/10044], Batch [5/7], Loss: 0.0055, Accuracy: 99.88%, Grad Norm: 0.09709\n",
      "Epoch [1105/10044], Batch [6/7], Loss: 0.0105, Accuracy: 99.74%, Grad Norm: 0.15798\n",
      "Epoch [1105/10044], Batch [7/7], Loss: 0.0035, Accuracy: 99.93%, Grad Norm: 0.06767\n",
      "Epoch [1105/10044], Loss: 0.0035\n",
      "Epoch [1106/10044], Batch [1/7], Loss: 0.0060, Accuracy: 99.87%, Grad Norm: 0.07288\n",
      "Epoch [1106/10044], Batch [2/7], Loss: 0.0072, Accuracy: 99.81%, Grad Norm: 0.11293\n",
      "Epoch [1106/10044], Batch [3/7], Loss: 0.0071, Accuracy: 99.84%, Grad Norm: 0.15187\n",
      "Epoch [1106/10044], Batch [4/7], Loss: 0.0057, Accuracy: 99.91%, Grad Norm: 0.07848\n",
      "Epoch [1106/10044], Batch [5/7], Loss: 0.0063, Accuracy: 99.83%, Grad Norm: 0.09979\n",
      "Epoch [1106/10044], Batch [6/7], Loss: 0.0082, Accuracy: 99.82%, Grad Norm: 0.10233\n",
      "Epoch [1106/10044], Batch [7/7], Loss: 0.0046, Accuracy: 99.88%, Grad Norm: 0.09599\n",
      "Epoch [1106/10044], Loss: 0.0046\n",
      "Epoch [1107/10044], Batch [1/7], Loss: 0.0071, Accuracy: 99.87%, Grad Norm: 0.10359\n",
      "Epoch [1107/10044], Batch [2/7], Loss: 0.0070, Accuracy: 99.83%, Grad Norm: 0.09723\n",
      "Epoch [1107/10044], Batch [3/7], Loss: 0.0050, Accuracy: 99.96%, Grad Norm: 0.09191\n",
      "Epoch [1107/10044], Batch [4/7], Loss: 0.0054, Accuracy: 99.87%, Grad Norm: 0.07090\n",
      "Epoch [1107/10044], Batch [5/7], Loss: 0.0047, Accuracy: 99.92%, Grad Norm: 0.06974\n",
      "Epoch [1107/10044], Batch [6/7], Loss: 0.0062, Accuracy: 99.88%, Grad Norm: 0.07714\n",
      "Epoch [1107/10044], Batch [7/7], Loss: 0.0038, Accuracy: 99.97%, Grad Norm: 0.08860\n",
      "Epoch [1107/10044], Loss: 0.0038\n",
      "Epoch [1108/10044], Batch [1/7], Loss: 0.0075, Accuracy: 99.86%, Grad Norm: 0.08917\n",
      "Epoch [1108/10044], Batch [2/7], Loss: 0.0070, Accuracy: 99.82%, Grad Norm: 0.10022\n",
      "Epoch [1108/10044], Batch [3/7], Loss: 0.0049, Accuracy: 99.92%, Grad Norm: 0.05690\n",
      "Epoch [1108/10044], Batch [4/7], Loss: 0.0047, Accuracy: 99.95%, Grad Norm: 0.05333\n",
      "Epoch [1108/10044], Batch [5/7], Loss: 0.0063, Accuracy: 99.87%, Grad Norm: 0.08993\n",
      "Epoch [1108/10044], Batch [6/7], Loss: 0.0065, Accuracy: 99.82%, Grad Norm: 0.09606\n",
      "Epoch [1108/10044], Batch [7/7], Loss: 0.0037, Accuracy: 99.93%, Grad Norm: 0.08431\n",
      "Epoch [1108/10044], Loss: 0.0037\n",
      "Epoch [1109/10044], Batch [1/7], Loss: 0.0082, Accuracy: 99.82%, Grad Norm: 0.09625\n",
      "Epoch [1109/10044], Batch [2/7], Loss: 0.0073, Accuracy: 99.76%, Grad Norm: 0.10146\n",
      "Epoch [1109/10044], Batch [3/7], Loss: 0.0045, Accuracy: 99.92%, Grad Norm: 0.05422\n",
      "Epoch [1109/10044], Batch [4/7], Loss: 0.0056, Accuracy: 99.89%, Grad Norm: 0.07987\n",
      "Epoch [1109/10044], Batch [5/7], Loss: 0.0058, Accuracy: 99.86%, Grad Norm: 0.08749\n",
      "Epoch [1109/10044], Batch [6/7], Loss: 0.0089, Accuracy: 99.74%, Grad Norm: 0.11980\n",
      "Epoch [1109/10044], Batch [7/7], Loss: 0.0039, Accuracy: 99.90%, Grad Norm: 0.09239\n",
      "Epoch [1109/10044], Loss: 0.0039\n",
      "Epoch [1110/10044], Batch [1/7], Loss: 0.0071, Accuracy: 99.82%, Grad Norm: 0.08916\n",
      "Epoch [1110/10044], Batch [2/7], Loss: 0.0069, Accuracy: 99.89%, Grad Norm: 0.07420\n",
      "Epoch [1110/10044], Batch [3/7], Loss: 0.0047, Accuracy: 99.92%, Grad Norm: 0.06734\n",
      "Epoch [1110/10044], Batch [4/7], Loss: 0.0066, Accuracy: 99.82%, Grad Norm: 0.08989\n",
      "Epoch [1110/10044], Batch [5/7], Loss: 0.0066, Accuracy: 99.83%, Grad Norm: 0.09946\n",
      "Epoch [1110/10044], Batch [6/7], Loss: 0.0081, Accuracy: 99.77%, Grad Norm: 0.11210\n",
      "Epoch [1110/10044], Batch [7/7], Loss: 0.0048, Accuracy: 99.87%, Grad Norm: 0.10432\n",
      "Epoch [1110/10044], Loss: 0.0048\n",
      "Epoch [1111/10044], Batch [1/7], Loss: 0.0070, Accuracy: 99.85%, Grad Norm: 0.10094\n",
      "Epoch [1111/10044], Batch [2/7], Loss: 0.0064, Accuracy: 99.89%, Grad Norm: 0.09122\n",
      "Epoch [1111/10044], Batch [3/7], Loss: 0.0051, Accuracy: 99.89%, Grad Norm: 0.05443\n",
      "Epoch [1111/10044], Batch [4/7], Loss: 0.0061, Accuracy: 99.87%, Grad Norm: 0.08364\n",
      "Epoch [1111/10044], Batch [5/7], Loss: 0.0044, Accuracy: 99.91%, Grad Norm: 0.06890\n",
      "Epoch [1111/10044], Batch [6/7], Loss: 0.0052, Accuracy: 99.90%, Grad Norm: 0.06068\n",
      "Epoch [1111/10044], Batch [7/7], Loss: 0.0043, Accuracy: 99.95%, Grad Norm: 0.07873\n",
      "Epoch [1111/10044], Loss: 0.0043\n",
      "Epoch [1112/10044], Batch [1/7], Loss: 0.0065, Accuracy: 99.87%, Grad Norm: 0.08351\n",
      "Epoch [1112/10044], Batch [2/7], Loss: 0.0071, Accuracy: 99.86%, Grad Norm: 0.09594\n",
      "Epoch [1112/10044], Batch [3/7], Loss: 0.0046, Accuracy: 99.90%, Grad Norm: 0.05537\n",
      "Epoch [1112/10044], Batch [4/7], Loss: 0.0056, Accuracy: 99.87%, Grad Norm: 0.06636\n",
      "Epoch [1112/10044], Batch [5/7], Loss: 0.0055, Accuracy: 99.87%, Grad Norm: 0.08652\n",
      "Epoch [1112/10044], Batch [6/7], Loss: 0.0089, Accuracy: 99.72%, Grad Norm: 0.12616\n",
      "Epoch [1112/10044], Batch [7/7], Loss: 0.0045, Accuracy: 99.92%, Grad Norm: 0.09249\n",
      "Epoch [1112/10044], Loss: 0.0045\n",
      "Epoch [1113/10044], Batch [1/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.07566\n",
      "Epoch [1113/10044], Batch [2/7], Loss: 0.0048, Accuracy: 99.93%, Grad Norm: 0.07033\n",
      "Epoch [1113/10044], Batch [3/7], Loss: 0.0046, Accuracy: 99.92%, Grad Norm: 0.07149\n",
      "Epoch [1113/10044], Batch [4/7], Loss: 0.0063, Accuracy: 99.87%, Grad Norm: 0.08068\n",
      "Epoch [1113/10044], Batch [5/7], Loss: 0.0055, Accuracy: 99.90%, Grad Norm: 0.07116\n",
      "Epoch [1113/10044], Batch [6/7], Loss: 0.0076, Accuracy: 99.85%, Grad Norm: 0.10474\n",
      "Epoch [1113/10044], Batch [7/7], Loss: 0.0051, Accuracy: 99.87%, Grad Norm: 0.10193\n",
      "Epoch [1113/10044], Loss: 0.0051\n",
      "Epoch [1114/10044], Batch [1/7], Loss: 0.0059, Accuracy: 99.91%, Grad Norm: 0.07450\n",
      "Epoch [1114/10044], Batch [2/7], Loss: 0.0078, Accuracy: 99.77%, Grad Norm: 0.11895\n",
      "Epoch [1114/10044], Batch [3/7], Loss: 0.0047, Accuracy: 99.90%, Grad Norm: 0.06010\n",
      "Epoch [1114/10044], Batch [4/7], Loss: 0.0055, Accuracy: 99.89%, Grad Norm: 0.07337\n",
      "Epoch [1114/10044], Batch [5/7], Loss: 0.0054, Accuracy: 99.89%, Grad Norm: 0.07158\n",
      "Epoch [1114/10044], Batch [6/7], Loss: 0.0059, Accuracy: 99.91%, Grad Norm: 0.07861\n",
      "Epoch [1114/10044], Batch [7/7], Loss: 0.0037, Accuracy: 99.92%, Grad Norm: 0.07393\n",
      "Epoch [1114/10044], Loss: 0.0037\n",
      "Epoch [1115/10044], Batch [1/7], Loss: 0.0056, Accuracy: 99.86%, Grad Norm: 0.07435\n",
      "Epoch [1115/10044], Batch [2/7], Loss: 0.0069, Accuracy: 99.82%, Grad Norm: 0.11078\n",
      "Epoch [1115/10044], Batch [3/7], Loss: 0.0054, Accuracy: 99.92%, Grad Norm: 0.06315\n",
      "Epoch [1115/10044], Batch [4/7], Loss: 0.0053, Accuracy: 99.85%, Grad Norm: 0.07513\n",
      "Epoch [1115/10044], Batch [5/7], Loss: 0.0053, Accuracy: 99.85%, Grad Norm: 0.09687\n",
      "Epoch [1115/10044], Batch [6/7], Loss: 0.0061, Accuracy: 99.88%, Grad Norm: 0.07460\n",
      "Epoch [1115/10044], Batch [7/7], Loss: 0.0034, Accuracy: 99.92%, Grad Norm: 0.08545\n",
      "Epoch [1115/10044], Loss: 0.0034\n",
      "Epoch [1116/10044], Batch [1/7], Loss: 0.0059, Accuracy: 99.87%, Grad Norm: 0.07140\n",
      "Epoch [1116/10044], Batch [2/7], Loss: 0.0055, Accuracy: 99.89%, Grad Norm: 0.06667\n",
      "Epoch [1116/10044], Batch [3/7], Loss: 0.0046, Accuracy: 99.92%, Grad Norm: 0.05712\n",
      "Epoch [1116/10044], Batch [4/7], Loss: 0.0067, Accuracy: 99.84%, Grad Norm: 0.09514\n",
      "Epoch [1116/10044], Batch [5/7], Loss: 0.0048, Accuracy: 99.91%, Grad Norm: 0.07702\n",
      "Epoch [1116/10044], Batch [6/7], Loss: 0.0067, Accuracy: 99.82%, Grad Norm: 0.08624\n",
      "Epoch [1116/10044], Batch [7/7], Loss: 0.0050, Accuracy: 99.88%, Grad Norm: 0.14745\n",
      "Epoch [1116/10044], Loss: 0.0050\n",
      "Epoch [1117/10044], Batch [1/7], Loss: 0.0051, Accuracy: 99.89%, Grad Norm: 0.06562\n",
      "Epoch [1117/10044], Batch [2/7], Loss: 0.0073, Accuracy: 99.84%, Grad Norm: 0.09496\n",
      "Epoch [1117/10044], Batch [3/7], Loss: 0.0047, Accuracy: 99.92%, Grad Norm: 0.05839\n",
      "Epoch [1117/10044], Batch [4/7], Loss: 0.0050, Accuracy: 99.91%, Grad Norm: 0.06539\n",
      "Epoch [1117/10044], Batch [5/7], Loss: 0.0052, Accuracy: 99.90%, Grad Norm: 0.07738\n",
      "Epoch [1117/10044], Batch [6/7], Loss: 0.0064, Accuracy: 99.82%, Grad Norm: 0.09048\n",
      "Epoch [1117/10044], Batch [7/7], Loss: 0.0048, Accuracy: 99.88%, Grad Norm: 0.13045\n",
      "Epoch [1117/10044], Loss: 0.0048\n",
      "Epoch [1118/10044], Batch [1/7], Loss: 0.0045, Accuracy: 99.93%, Grad Norm: 0.05658\n",
      "Epoch [1118/10044], Batch [2/7], Loss: 0.0087, Accuracy: 99.78%, Grad Norm: 0.09829\n",
      "Epoch [1118/10044], Batch [3/7], Loss: 0.0041, Accuracy: 99.94%, Grad Norm: 0.04465\n",
      "Epoch [1118/10044], Batch [4/7], Loss: 0.0042, Accuracy: 99.94%, Grad Norm: 0.04830\n",
      "Epoch [1118/10044], Batch [5/7], Loss: 0.0048, Accuracy: 99.89%, Grad Norm: 0.08588\n",
      "Epoch [1118/10044], Batch [6/7], Loss: 0.0060, Accuracy: 99.86%, Grad Norm: 0.08429\n",
      "Epoch [1118/10044], Batch [7/7], Loss: 0.0041, Accuracy: 99.87%, Grad Norm: 0.09013\n",
      "Epoch [1118/10044], Loss: 0.0041\n",
      "Epoch [1119/10044], Batch [1/7], Loss: 0.0052, Accuracy: 99.92%, Grad Norm: 0.06066\n",
      "Epoch [1119/10044], Batch [2/7], Loss: 0.0075, Accuracy: 99.82%, Grad Norm: 0.09000\n",
      "Epoch [1119/10044], Batch [3/7], Loss: 0.0050, Accuracy: 99.92%, Grad Norm: 0.05751\n",
      "Epoch [1119/10044], Batch [4/7], Loss: 0.0056, Accuracy: 99.88%, Grad Norm: 0.07820\n",
      "Epoch [1119/10044], Batch [5/7], Loss: 0.0045, Accuracy: 99.92%, Grad Norm: 0.06507\n",
      "Epoch [1119/10044], Batch [6/7], Loss: 0.0076, Accuracy: 99.84%, Grad Norm: 0.09864\n",
      "Epoch [1119/10044], Batch [7/7], Loss: 0.0055, Accuracy: 99.90%, Grad Norm: 0.13770\n",
      "Epoch [1119/10044], Loss: 0.0055\n",
      "Epoch [1120/10044], Batch [1/7], Loss: 0.0055, Accuracy: 99.89%, Grad Norm: 0.07560\n",
      "Epoch [1120/10044], Batch [2/7], Loss: 0.0064, Accuracy: 99.82%, Grad Norm: 0.08485\n",
      "Epoch [1120/10044], Batch [3/7], Loss: 0.0048, Accuracy: 99.92%, Grad Norm: 0.06015\n",
      "Epoch [1120/10044], Batch [4/7], Loss: 0.0055, Accuracy: 99.89%, Grad Norm: 0.07381\n",
      "Epoch [1120/10044], Batch [5/7], Loss: 0.0058, Accuracy: 99.85%, Grad Norm: 0.08597\n",
      "Epoch [1120/10044], Batch [6/7], Loss: 0.0075, Accuracy: 99.82%, Grad Norm: 0.10804\n",
      "Epoch [1120/10044], Batch [7/7], Loss: 0.0073, Accuracy: 99.83%, Grad Norm: 0.21063\n",
      "Epoch [1120/10044], Loss: 0.0073\n",
      "Epoch [1121/10044], Batch [1/7], Loss: 0.0060, Accuracy: 99.87%, Grad Norm: 0.07554\n",
      "Epoch [1121/10044], Batch [2/7], Loss: 0.0066, Accuracy: 99.84%, Grad Norm: 0.08435\n",
      "Epoch [1121/10044], Batch [3/7], Loss: 0.0043, Accuracy: 99.96%, Grad Norm: 0.05468\n",
      "Epoch [1121/10044], Batch [4/7], Loss: 0.0056, Accuracy: 99.89%, Grad Norm: 0.07803\n",
      "Epoch [1121/10044], Batch [5/7], Loss: 0.0074, Accuracy: 99.83%, Grad Norm: 0.11927\n",
      "Epoch [1121/10044], Batch [6/7], Loss: 0.0099, Accuracy: 99.68%, Grad Norm: 0.15591\n",
      "Epoch [1121/10044], Batch [7/7], Loss: 0.0052, Accuracy: 99.87%, Grad Norm: 0.14963\n",
      "Epoch [1121/10044], Loss: 0.0052\n",
      "Epoch [1122/10044], Batch [1/7], Loss: 0.0054, Accuracy: 99.86%, Grad Norm: 0.07449\n",
      "Epoch [1122/10044], Batch [2/7], Loss: 0.0066, Accuracy: 99.85%, Grad Norm: 0.08878\n",
      "Epoch [1122/10044], Batch [3/7], Loss: 0.0055, Accuracy: 99.92%, Grad Norm: 0.06559\n",
      "Epoch [1122/10044], Batch [4/7], Loss: 0.0075, Accuracy: 99.84%, Grad Norm: 0.10270\n",
      "Epoch [1122/10044], Batch [5/7], Loss: 0.0063, Accuracy: 99.85%, Grad Norm: 0.08157\n",
      "Epoch [1122/10044], Batch [6/7], Loss: 0.0064, Accuracy: 99.87%, Grad Norm: 0.10155\n",
      "Epoch [1122/10044], Batch [7/7], Loss: 0.0048, Accuracy: 99.90%, Grad Norm: 0.10244\n",
      "Epoch [1122/10044], Loss: 0.0048\n",
      "Epoch [1123/10044], Batch [1/7], Loss: 0.0059, Accuracy: 99.88%, Grad Norm: 0.07733\n",
      "Epoch [1123/10044], Batch [2/7], Loss: 0.0056, Accuracy: 99.92%, Grad Norm: 0.08077\n",
      "Epoch [1123/10044], Batch [3/7], Loss: 0.0043, Accuracy: 99.95%, Grad Norm: 0.05815\n",
      "Epoch [1123/10044], Batch [4/7], Loss: 0.0054, Accuracy: 99.90%, Grad Norm: 0.07775\n",
      "Epoch [1123/10044], Batch [5/7], Loss: 0.0043, Accuracy: 99.94%, Grad Norm: 0.06333\n",
      "Epoch [1123/10044], Batch [6/7], Loss: 0.0061, Accuracy: 99.86%, Grad Norm: 0.08472\n",
      "Epoch [1123/10044], Batch [7/7], Loss: 0.0040, Accuracy: 99.92%, Grad Norm: 0.11839\n",
      "Epoch [1123/10044], Loss: 0.0040\n",
      "Epoch [1124/10044], Batch [1/7], Loss: 0.0061, Accuracy: 99.88%, Grad Norm: 0.07369\n",
      "Epoch [1124/10044], Batch [2/7], Loss: 0.0073, Accuracy: 99.82%, Grad Norm: 0.10095\n",
      "Epoch [1124/10044], Batch [3/7], Loss: 0.0043, Accuracy: 99.94%, Grad Norm: 0.04811\n",
      "Epoch [1124/10044], Batch [4/7], Loss: 0.0056, Accuracy: 99.91%, Grad Norm: 0.06248\n",
      "Epoch [1124/10044], Batch [5/7], Loss: 0.0057, Accuracy: 99.91%, Grad Norm: 0.08637\n",
      "Epoch [1124/10044], Batch [6/7], Loss: 0.0063, Accuracy: 99.87%, Grad Norm: 0.09698\n",
      "Epoch [1124/10044], Batch [7/7], Loss: 0.0051, Accuracy: 99.88%, Grad Norm: 0.11971\n",
      "Epoch [1124/10044], Loss: 0.0051\n",
      "Epoch [1125/10044], Batch [1/7], Loss: 0.0057, Accuracy: 99.92%, Grad Norm: 0.07739\n",
      "Epoch [1125/10044], Batch [2/7], Loss: 0.0064, Accuracy: 99.84%, Grad Norm: 0.09134\n",
      "Epoch [1125/10044], Batch [3/7], Loss: 0.0040, Accuracy: 99.95%, Grad Norm: 0.05221\n",
      "Epoch [1125/10044], Batch [4/7], Loss: 0.0051, Accuracy: 99.90%, Grad Norm: 0.07104\n",
      "Epoch [1125/10044], Batch [5/7], Loss: 0.0051, Accuracy: 99.92%, Grad Norm: 0.08684\n",
      "Epoch [1125/10044], Batch [6/7], Loss: 0.0071, Accuracy: 99.84%, Grad Norm: 0.10145\n",
      "Epoch [1125/10044], Batch [7/7], Loss: 0.0070, Accuracy: 99.78%, Grad Norm: 0.21531\n",
      "Epoch [1125/10044], Loss: 0.0070\n",
      "Epoch [1126/10044], Batch [1/7], Loss: 0.0055, Accuracy: 99.92%, Grad Norm: 0.07310\n",
      "Epoch [1126/10044], Batch [2/7], Loss: 0.0075, Accuracy: 99.83%, Grad Norm: 0.10213\n",
      "Epoch [1126/10044], Batch [3/7], Loss: 0.0045, Accuracy: 99.94%, Grad Norm: 0.06679\n",
      "Epoch [1126/10044], Batch [4/7], Loss: 0.0069, Accuracy: 99.81%, Grad Norm: 0.09965\n",
      "Epoch [1126/10044], Batch [5/7], Loss: 0.0052, Accuracy: 99.91%, Grad Norm: 0.07408\n",
      "Epoch [1126/10044], Batch [6/7], Loss: 0.0075, Accuracy: 99.79%, Grad Norm: 0.10847\n",
      "Epoch [1126/10044], Batch [7/7], Loss: 0.0082, Accuracy: 99.78%, Grad Norm: 0.21172\n",
      "Epoch [1126/10044], Loss: 0.0082\n",
      "Epoch [1127/10044], Batch [1/7], Loss: 0.0061, Accuracy: 99.87%, Grad Norm: 0.08532\n",
      "Epoch [1127/10044], Batch [2/7], Loss: 0.0082, Accuracy: 99.77%, Grad Norm: 0.10544\n",
      "Epoch [1127/10044], Batch [3/7], Loss: 0.0064, Accuracy: 99.87%, Grad Norm: 0.08271\n",
      "Epoch [1127/10044], Batch [4/7], Loss: 0.0062, Accuracy: 99.86%, Grad Norm: 0.08543\n",
      "Epoch [1127/10044], Batch [5/7], Loss: 0.0050, Accuracy: 99.91%, Grad Norm: 0.07275\n",
      "Epoch [1127/10044], Batch [6/7], Loss: 0.0060, Accuracy: 99.85%, Grad Norm: 0.10419\n",
      "Epoch [1127/10044], Batch [7/7], Loss: 0.0065, Accuracy: 99.80%, Grad Norm: 0.15108\n",
      "Epoch [1127/10044], Loss: 0.0065\n",
      "Epoch [1128/10044], Batch [1/7], Loss: 0.0054, Accuracy: 99.89%, Grad Norm: 0.07581\n",
      "Epoch [1128/10044], Batch [2/7], Loss: 0.0057, Accuracy: 99.86%, Grad Norm: 0.08265\n",
      "Epoch [1128/10044], Batch [3/7], Loss: 0.0051, Accuracy: 99.92%, Grad Norm: 0.05997\n",
      "Epoch [1128/10044], Batch [4/7], Loss: 0.0061, Accuracy: 99.87%, Grad Norm: 0.08016\n",
      "Epoch [1128/10044], Batch [5/7], Loss: 0.0075, Accuracy: 99.87%, Grad Norm: 0.10063\n",
      "Epoch [1128/10044], Batch [6/7], Loss: 0.0074, Accuracy: 99.80%, Grad Norm: 0.10839\n",
      "Epoch [1128/10044], Batch [7/7], Loss: 0.0053, Accuracy: 99.88%, Grad Norm: 0.12795\n",
      "Epoch [1128/10044], Loss: 0.0053\n",
      "Epoch [1129/10044], Batch [1/7], Loss: 0.0064, Accuracy: 99.86%, Grad Norm: 0.07405\n",
      "Epoch [1129/10044], Batch [2/7], Loss: 0.0053, Accuracy: 99.92%, Grad Norm: 0.06482\n",
      "Epoch [1129/10044], Batch [3/7], Loss: 0.0051, Accuracy: 99.92%, Grad Norm: 0.08146\n",
      "Epoch [1129/10044], Batch [4/7], Loss: 0.0057, Accuracy: 99.88%, Grad Norm: 0.06779\n",
      "Epoch [1129/10044], Batch [5/7], Loss: 0.0050, Accuracy: 99.90%, Grad Norm: 0.08400\n",
      "Epoch [1129/10044], Batch [6/7], Loss: 0.0066, Accuracy: 99.83%, Grad Norm: 0.08754\n",
      "Epoch [1129/10044], Batch [7/7], Loss: 0.0048, Accuracy: 99.90%, Grad Norm: 0.10632\n",
      "Epoch [1129/10044], Loss: 0.0048\n",
      "Epoch [1130/10044], Batch [1/7], Loss: 0.0041, Accuracy: 99.97%, Grad Norm: 0.04645\n",
      "Epoch [1130/10044], Batch [2/7], Loss: 0.0057, Accuracy: 99.88%, Grad Norm: 0.07588\n",
      "Epoch [1130/10044], Batch [3/7], Loss: 0.0049, Accuracy: 99.90%, Grad Norm: 0.05100\n",
      "Epoch [1130/10044], Batch [4/7], Loss: 0.0052, Accuracy: 99.90%, Grad Norm: 0.06333\n",
      "Epoch [1130/10044], Batch [5/7], Loss: 0.0062, Accuracy: 99.87%, Grad Norm: 0.08114\n",
      "Epoch [1130/10044], Batch [6/7], Loss: 0.0074, Accuracy: 99.77%, Grad Norm: 0.10449\n",
      "Epoch [1130/10044], Batch [7/7], Loss: 0.0046, Accuracy: 99.88%, Grad Norm: 0.10190\n",
      "Epoch [1130/10044], Loss: 0.0046\n",
      "Epoch [1131/10044], Batch [1/7], Loss: 0.0056, Accuracy: 99.87%, Grad Norm: 0.09099\n",
      "Epoch [1131/10044], Batch [2/7], Loss: 0.0057, Accuracy: 99.90%, Grad Norm: 0.06968\n",
      "Epoch [1131/10044], Batch [3/7], Loss: 0.0036, Accuracy: 99.94%, Grad Norm: 0.03837\n",
      "Epoch [1131/10044], Batch [4/7], Loss: 0.0049, Accuracy: 99.91%, Grad Norm: 0.05646\n",
      "Epoch [1131/10044], Batch [5/7], Loss: 0.0043, Accuracy: 99.92%, Grad Norm: 0.05805\n",
      "Epoch [1131/10044], Batch [6/7], Loss: 0.0076, Accuracy: 99.77%, Grad Norm: 0.11084\n",
      "Epoch [1131/10044], Batch [7/7], Loss: 0.0038, Accuracy: 99.93%, Grad Norm: 0.08037\n",
      "Epoch [1131/10044], Loss: 0.0038\n",
      "Epoch [1132/10044], Batch [1/7], Loss: 0.0055, Accuracy: 99.87%, Grad Norm: 0.06607\n",
      "Epoch [1132/10044], Batch [2/7], Loss: 0.0046, Accuracy: 99.92%, Grad Norm: 0.07869\n",
      "Epoch [1132/10044], Batch [3/7], Loss: 0.0044, Accuracy: 99.94%, Grad Norm: 0.05908\n",
      "Epoch [1132/10044], Batch [4/7], Loss: 0.0052, Accuracy: 99.90%, Grad Norm: 0.06855\n",
      "Epoch [1132/10044], Batch [5/7], Loss: 0.0059, Accuracy: 99.87%, Grad Norm: 0.08305\n",
      "Epoch [1132/10044], Batch [6/7], Loss: 0.0061, Accuracy: 99.84%, Grad Norm: 0.08838\n",
      "Epoch [1132/10044], Batch [7/7], Loss: 0.0045, Accuracy: 99.95%, Grad Norm: 0.09392\n",
      "Epoch [1132/10044], Loss: 0.0045\n",
      "Epoch [1133/10044], Batch [1/7], Loss: 0.0050, Accuracy: 99.92%, Grad Norm: 0.08240\n",
      "Epoch [1133/10044], Batch [2/7], Loss: 0.0052, Accuracy: 99.89%, Grad Norm: 0.07762\n",
      "Epoch [1133/10044], Batch [3/7], Loss: 0.0047, Accuracy: 99.92%, Grad Norm: 0.06182\n",
      "Epoch [1133/10044], Batch [4/7], Loss: 0.0057, Accuracy: 99.88%, Grad Norm: 0.08142\n",
      "Epoch [1133/10044], Batch [5/7], Loss: 0.0044, Accuracy: 99.88%, Grad Norm: 0.05974\n",
      "Epoch [1133/10044], Batch [6/7], Loss: 0.0059, Accuracy: 99.84%, Grad Norm: 0.08475\n",
      "Epoch [1133/10044], Batch [7/7], Loss: 0.0034, Accuracy: 99.92%, Grad Norm: 0.07338\n",
      "Epoch [1133/10044], Loss: 0.0034\n",
      "Epoch [1134/10044], Batch [1/7], Loss: 0.0068, Accuracy: 99.85%, Grad Norm: 0.08947\n",
      "Epoch [1134/10044], Batch [2/7], Loss: 0.0054, Accuracy: 99.90%, Grad Norm: 0.07108\n",
      "Epoch [1134/10044], Batch [3/7], Loss: 0.0048, Accuracy: 99.89%, Grad Norm: 0.06545\n",
      "Epoch [1134/10044], Batch [4/7], Loss: 0.0052, Accuracy: 99.90%, Grad Norm: 0.06753\n",
      "Epoch [1134/10044], Batch [5/7], Loss: 0.0045, Accuracy: 99.92%, Grad Norm: 0.07431\n",
      "Epoch [1134/10044], Batch [6/7], Loss: 0.0052, Accuracy: 99.86%, Grad Norm: 0.08383\n",
      "Epoch [1134/10044], Batch [7/7], Loss: 0.0035, Accuracy: 99.93%, Grad Norm: 0.09249\n",
      "Epoch [1134/10044], Loss: 0.0035\n",
      "Epoch [1135/10044], Batch [1/7], Loss: 0.0059, Accuracy: 99.86%, Grad Norm: 0.08757\n",
      "Epoch [1135/10044], Batch [2/7], Loss: 0.0056, Accuracy: 99.89%, Grad Norm: 0.09421\n",
      "Epoch [1135/10044], Batch [3/7], Loss: 0.0046, Accuracy: 99.91%, Grad Norm: 0.07382\n",
      "Epoch [1135/10044], Batch [4/7], Loss: 0.0055, Accuracy: 99.88%, Grad Norm: 0.07380\n",
      "Epoch [1135/10044], Batch [5/7], Loss: 0.0049, Accuracy: 99.91%, Grad Norm: 0.07543\n",
      "Epoch [1135/10044], Batch [6/7], Loss: 0.0058, Accuracy: 99.86%, Grad Norm: 0.07980\n",
      "Epoch [1135/10044], Batch [7/7], Loss: 0.0047, Accuracy: 99.90%, Grad Norm: 0.12585\n",
      "Epoch [1135/10044], Loss: 0.0047\n",
      "Epoch [1136/10044], Batch [1/7], Loss: 0.0056, Accuracy: 99.91%, Grad Norm: 0.07151\n",
      "Epoch [1136/10044], Batch [2/7], Loss: 0.0063, Accuracy: 99.83%, Grad Norm: 0.09661\n",
      "Epoch [1136/10044], Batch [3/7], Loss: 0.0045, Accuracy: 99.91%, Grad Norm: 0.06672\n",
      "Epoch [1136/10044], Batch [4/7], Loss: 0.0054, Accuracy: 99.91%, Grad Norm: 0.06947\n",
      "Epoch [1136/10044], Batch [5/7], Loss: 0.0063, Accuracy: 99.86%, Grad Norm: 0.10458\n",
      "Epoch [1136/10044], Batch [6/7], Loss: 0.0068, Accuracy: 99.82%, Grad Norm: 0.09296\n",
      "Epoch [1136/10044], Batch [7/7], Loss: 0.0070, Accuracy: 99.85%, Grad Norm: 0.11963\n",
      "Epoch [1136/10044], Loss: 0.0070\n",
      "Epoch [1137/10044], Batch [1/7], Loss: 0.0057, Accuracy: 99.88%, Grad Norm: 0.07297\n",
      "Epoch [1137/10044], Batch [2/7], Loss: 0.0074, Accuracy: 99.80%, Grad Norm: 0.11192\n",
      "Epoch [1137/10044], Batch [3/7], Loss: 0.0055, Accuracy: 99.92%, Grad Norm: 0.07650\n",
      "Epoch [1137/10044], Batch [4/7], Loss: 0.0054, Accuracy: 99.83%, Grad Norm: 0.07921\n",
      "Epoch [1137/10044], Batch [5/7], Loss: 0.0053, Accuracy: 99.87%, Grad Norm: 0.08675\n",
      "Epoch [1137/10044], Batch [6/7], Loss: 0.0078, Accuracy: 99.77%, Grad Norm: 0.10085\n",
      "Epoch [1137/10044], Batch [7/7], Loss: 0.0045, Accuracy: 99.93%, Grad Norm: 0.09231\n",
      "Epoch [1137/10044], Loss: 0.0045\n",
      "Epoch [1138/10044], Batch [1/7], Loss: 0.0049, Accuracy: 99.90%, Grad Norm: 0.06097\n",
      "Epoch [1138/10044], Batch [2/7], Loss: 0.0062, Accuracy: 99.82%, Grad Norm: 0.10668\n",
      "Epoch [1138/10044], Batch [3/7], Loss: 0.0049, Accuracy: 99.92%, Grad Norm: 0.06746\n",
      "Epoch [1138/10044], Batch [4/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.07102\n",
      "Epoch [1138/10044], Batch [5/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.09969\n",
      "Epoch [1138/10044], Batch [6/7], Loss: 0.0080, Accuracy: 99.76%, Grad Norm: 0.10117\n",
      "Epoch [1138/10044], Batch [7/7], Loss: 0.0035, Accuracy: 99.95%, Grad Norm: 0.07800\n",
      "Epoch [1138/10044], Loss: 0.0035\n",
      "Epoch [1139/10044], Batch [1/7], Loss: 0.0057, Accuracy: 99.87%, Grad Norm: 0.08448\n",
      "Epoch [1139/10044], Batch [2/7], Loss: 0.0051, Accuracy: 99.91%, Grad Norm: 0.07657\n",
      "Epoch [1139/10044], Batch [3/7], Loss: 0.0039, Accuracy: 99.93%, Grad Norm: 0.05102\n",
      "Epoch [1139/10044], Batch [4/7], Loss: 0.0052, Accuracy: 99.89%, Grad Norm: 0.07571\n",
      "Epoch [1139/10044], Batch [5/7], Loss: 0.0050, Accuracy: 99.92%, Grad Norm: 0.06498\n",
      "Epoch [1139/10044], Batch [6/7], Loss: 0.0070, Accuracy: 99.85%, Grad Norm: 0.12065\n",
      "Epoch [1139/10044], Batch [7/7], Loss: 0.0055, Accuracy: 99.83%, Grad Norm: 0.15496\n",
      "Epoch [1139/10044], Loss: 0.0055\n",
      "Epoch [1140/10044], Batch [1/7], Loss: 0.0055, Accuracy: 99.85%, Grad Norm: 0.08977\n",
      "Epoch [1140/10044], Batch [2/7], Loss: 0.0066, Accuracy: 99.82%, Grad Norm: 0.08205\n",
      "Epoch [1140/10044], Batch [3/7], Loss: 0.0046, Accuracy: 99.92%, Grad Norm: 0.06295\n",
      "Epoch [1140/10044], Batch [4/7], Loss: 0.0054, Accuracy: 99.91%, Grad Norm: 0.07100\n",
      "Epoch [1140/10044], Batch [5/7], Loss: 0.0052, Accuracy: 99.88%, Grad Norm: 0.08198\n",
      "Epoch [1140/10044], Batch [6/7], Loss: 0.0065, Accuracy: 99.87%, Grad Norm: 0.09109\n",
      "Epoch [1140/10044], Batch [7/7], Loss: 0.0050, Accuracy: 99.85%, Grad Norm: 0.10966\n",
      "Epoch [1140/10044], Loss: 0.0050\n",
      "Epoch [1141/10044], Batch [1/7], Loss: 0.0059, Accuracy: 99.84%, Grad Norm: 0.10969\n",
      "Epoch [1141/10044], Batch [2/7], Loss: 0.0057, Accuracy: 99.85%, Grad Norm: 0.08608\n",
      "Epoch [1141/10044], Batch [3/7], Loss: 0.0046, Accuracy: 99.92%, Grad Norm: 0.05203\n",
      "Epoch [1141/10044], Batch [4/7], Loss: 0.0068, Accuracy: 99.84%, Grad Norm: 0.10376\n",
      "Epoch [1141/10044], Batch [5/7], Loss: 0.0053, Accuracy: 99.87%, Grad Norm: 0.09208\n",
      "Epoch [1141/10044], Batch [6/7], Loss: 0.0092, Accuracy: 99.77%, Grad Norm: 0.13691\n",
      "Epoch [1141/10044], Batch [7/7], Loss: 0.0043, Accuracy: 99.93%, Grad Norm: 0.09590\n",
      "Epoch [1141/10044], Loss: 0.0043\n",
      "Epoch [1142/10044], Batch [1/7], Loss: 0.0052, Accuracy: 99.87%, Grad Norm: 0.08453\n",
      "Epoch [1142/10044], Batch [2/7], Loss: 0.0054, Accuracy: 99.90%, Grad Norm: 0.07221\n",
      "Epoch [1142/10044], Batch [3/7], Loss: 0.0049, Accuracy: 99.92%, Grad Norm: 0.05020\n",
      "Epoch [1142/10044], Batch [4/7], Loss: 0.0049, Accuracy: 99.91%, Grad Norm: 0.06464\n",
      "Epoch [1142/10044], Batch [5/7], Loss: 0.0050, Accuracy: 99.90%, Grad Norm: 0.07852\n",
      "Epoch [1142/10044], Batch [6/7], Loss: 0.0073, Accuracy: 99.79%, Grad Norm: 0.10771\n",
      "Epoch [1142/10044], Batch [7/7], Loss: 0.0032, Accuracy: 99.95%, Grad Norm: 0.06423\n",
      "Epoch [1142/10044], Loss: 0.0032\n",
      "Epoch [1143/10044], Batch [1/7], Loss: 0.0067, Accuracy: 99.86%, Grad Norm: 0.07524\n",
      "Epoch [1143/10044], Batch [2/7], Loss: 0.0051, Accuracy: 99.88%, Grad Norm: 0.07027\n",
      "Epoch [1143/10044], Batch [3/7], Loss: 0.0047, Accuracy: 99.92%, Grad Norm: 0.06204\n",
      "Epoch [1143/10044], Batch [4/7], Loss: 0.0054, Accuracy: 99.90%, Grad Norm: 0.07013\n",
      "Epoch [1143/10044], Batch [5/7], Loss: 0.0047, Accuracy: 99.91%, Grad Norm: 0.06729\n",
      "Epoch [1143/10044], Batch [6/7], Loss: 0.0072, Accuracy: 99.83%, Grad Norm: 0.11193\n",
      "Epoch [1143/10044], Batch [7/7], Loss: 0.0049, Accuracy: 99.88%, Grad Norm: 0.08791\n",
      "Epoch [1143/10044], Loss: 0.0049\n",
      "Epoch [1144/10044], Batch [1/7], Loss: 0.0048, Accuracy: 99.91%, Grad Norm: 0.06355\n",
      "Epoch [1144/10044], Batch [2/7], Loss: 0.0063, Accuracy: 99.88%, Grad Norm: 0.09575\n",
      "Epoch [1144/10044], Batch [3/7], Loss: 0.0038, Accuracy: 99.96%, Grad Norm: 0.04794\n",
      "Epoch [1144/10044], Batch [4/7], Loss: 0.0053, Accuracy: 99.91%, Grad Norm: 0.07154\n",
      "Epoch [1144/10044], Batch [5/7], Loss: 0.0045, Accuracy: 99.92%, Grad Norm: 0.06620\n",
      "Epoch [1144/10044], Batch [6/7], Loss: 0.0068, Accuracy: 99.82%, Grad Norm: 0.09127\n",
      "Epoch [1144/10044], Batch [7/7], Loss: 0.0036, Accuracy: 99.92%, Grad Norm: 0.09625\n",
      "Epoch [1144/10044], Loss: 0.0036\n",
      "Epoch [1145/10044], Batch [1/7], Loss: 0.0047, Accuracy: 99.91%, Grad Norm: 0.07320\n",
      "Epoch [1145/10044], Batch [2/7], Loss: 0.0073, Accuracy: 99.85%, Grad Norm: 0.09793\n",
      "Epoch [1145/10044], Batch [3/7], Loss: 0.0051, Accuracy: 99.90%, Grad Norm: 0.06623\n",
      "Epoch [1145/10044], Batch [4/7], Loss: 0.0047, Accuracy: 99.89%, Grad Norm: 0.06263\n",
      "Epoch [1145/10044], Batch [5/7], Loss: 0.0046, Accuracy: 99.89%, Grad Norm: 0.07498\n",
      "Epoch [1145/10044], Batch [6/7], Loss: 0.0055, Accuracy: 99.88%, Grad Norm: 0.07074\n",
      "Epoch [1145/10044], Batch [7/7], Loss: 0.0021, Accuracy: 100.00%, Grad Norm: 0.03317\n",
      "Epoch [1145/10044], Loss: 0.0021\n",
      "Epoch [1146/10044], Batch [1/7], Loss: 0.0051, Accuracy: 99.86%, Grad Norm: 0.07879\n",
      "Epoch [1146/10044], Batch [2/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.08670\n",
      "Epoch [1146/10044], Batch [3/7], Loss: 0.0039, Accuracy: 99.94%, Grad Norm: 0.04749\n",
      "Epoch [1146/10044], Batch [4/7], Loss: 0.0050, Accuracy: 99.90%, Grad Norm: 0.08496\n",
      "Epoch [1146/10044], Batch [5/7], Loss: 0.0046, Accuracy: 99.90%, Grad Norm: 0.07579\n",
      "Epoch [1146/10044], Batch [6/7], Loss: 0.0074, Accuracy: 99.81%, Grad Norm: 0.12567\n",
      "Epoch [1146/10044], Batch [7/7], Loss: 0.0032, Accuracy: 99.95%, Grad Norm: 0.08013\n",
      "Epoch [1146/10044], Loss: 0.0032\n",
      "Epoch [1147/10044], Batch [1/7], Loss: 0.0045, Accuracy: 99.88%, Grad Norm: 0.06298\n",
      "Epoch [1147/10044], Batch [2/7], Loss: 0.0047, Accuracy: 99.90%, Grad Norm: 0.07114\n",
      "Epoch [1147/10044], Batch [3/7], Loss: 0.0052, Accuracy: 99.89%, Grad Norm: 0.06559\n",
      "Epoch [1147/10044], Batch [4/7], Loss: 0.0066, Accuracy: 99.86%, Grad Norm: 0.08302\n",
      "Epoch [1147/10044], Batch [5/7], Loss: 0.0090, Accuracy: 99.81%, Grad Norm: 0.13914\n",
      "Epoch [1147/10044], Batch [6/7], Loss: 0.0048, Accuracy: 99.92%, Grad Norm: 0.07188\n",
      "Epoch [1147/10044], Batch [7/7], Loss: 0.0032, Accuracy: 99.92%, Grad Norm: 0.07916\n",
      "Epoch [1147/10044], Loss: 0.0032\n",
      "Epoch [1148/10044], Batch [1/7], Loss: 0.0043, Accuracy: 99.94%, Grad Norm: 0.05978\n",
      "Epoch [1148/10044], Batch [2/7], Loss: 0.0043, Accuracy: 99.92%, Grad Norm: 0.06402\n",
      "Epoch [1148/10044], Batch [3/7], Loss: 0.0041, Accuracy: 99.94%, Grad Norm: 0.06710\n",
      "Epoch [1148/10044], Batch [4/7], Loss: 0.0052, Accuracy: 99.91%, Grad Norm: 0.07065\n",
      "Epoch [1148/10044], Batch [5/7], Loss: 0.0066, Accuracy: 99.88%, Grad Norm: 0.10825\n",
      "Epoch [1148/10044], Batch [6/7], Loss: 0.0076, Accuracy: 99.77%, Grad Norm: 0.11676\n",
      "Epoch [1148/10044], Batch [7/7], Loss: 0.0046, Accuracy: 99.88%, Grad Norm: 0.10878\n",
      "Epoch [1148/10044], Loss: 0.0046\n",
      "Epoch [1149/10044], Batch [1/7], Loss: 0.0043, Accuracy: 99.93%, Grad Norm: 0.06273\n",
      "Epoch [1149/10044], Batch [2/7], Loss: 0.0048, Accuracy: 99.92%, Grad Norm: 0.05974\n",
      "Epoch [1149/10044], Batch [3/7], Loss: 0.0057, Accuracy: 99.86%, Grad Norm: 0.08143\n",
      "Epoch [1149/10044], Batch [4/7], Loss: 0.0052, Accuracy: 99.89%, Grad Norm: 0.08077\n",
      "Epoch [1149/10044], Batch [5/7], Loss: 0.0058, Accuracy: 99.92%, Grad Norm: 0.09476\n",
      "Epoch [1149/10044], Batch [6/7], Loss: 0.0071, Accuracy: 99.82%, Grad Norm: 0.09255\n",
      "Epoch [1149/10044], Batch [7/7], Loss: 0.0026, Accuracy: 99.97%, Grad Norm: 0.07599\n",
      "Epoch [1149/10044], Loss: 0.0026\n",
      "Epoch [1150/10044], Batch [1/7], Loss: 0.0056, Accuracy: 99.89%, Grad Norm: 0.07373\n",
      "Epoch [1150/10044], Batch [2/7], Loss: 0.0054, Accuracy: 99.87%, Grad Norm: 0.09374\n",
      "Epoch [1150/10044], Batch [3/7], Loss: 0.0042, Accuracy: 99.93%, Grad Norm: 0.06644\n",
      "Epoch [1150/10044], Batch [4/7], Loss: 0.0047, Accuracy: 99.91%, Grad Norm: 0.07591\n",
      "Epoch [1150/10044], Batch [5/7], Loss: 0.0057, Accuracy: 99.88%, Grad Norm: 0.07947\n",
      "Epoch [1150/10044], Batch [6/7], Loss: 0.0056, Accuracy: 99.88%, Grad Norm: 0.08620\n",
      "Epoch [1150/10044], Batch [7/7], Loss: 0.0035, Accuracy: 99.93%, Grad Norm: 0.10366\n",
      "Epoch [1150/10044], Loss: 0.0035\n",
      "Epoch [1151/10044], Batch [1/7], Loss: 0.0055, Accuracy: 99.87%, Grad Norm: 0.06168\n",
      "Epoch [1151/10044], Batch [2/7], Loss: 0.0051, Accuracy: 99.91%, Grad Norm: 0.07001\n",
      "Epoch [1151/10044], Batch [3/7], Loss: 0.0036, Accuracy: 99.97%, Grad Norm: 0.04460\n",
      "Epoch [1151/10044], Batch [4/7], Loss: 0.0045, Accuracy: 99.89%, Grad Norm: 0.05944\n",
      "Epoch [1151/10044], Batch [5/7], Loss: 0.0049, Accuracy: 99.89%, Grad Norm: 0.08816\n",
      "Epoch [1151/10044], Batch [6/7], Loss: 0.0065, Accuracy: 99.86%, Grad Norm: 0.08390\n",
      "Epoch [1151/10044], Batch [7/7], Loss: 0.0056, Accuracy: 99.85%, Grad Norm: 0.12142\n",
      "Epoch [1151/10044], Loss: 0.0056\n",
      "Epoch [1152/10044], Batch [1/7], Loss: 0.0054, Accuracy: 99.90%, Grad Norm: 0.09671\n",
      "Epoch [1152/10044], Batch [2/7], Loss: 0.0074, Accuracy: 99.83%, Grad Norm: 0.09469\n",
      "Epoch [1152/10044], Batch [3/7], Loss: 0.0044, Accuracy: 99.89%, Grad Norm: 0.05816\n",
      "Epoch [1152/10044], Batch [4/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.09139\n",
      "Epoch [1152/10044], Batch [5/7], Loss: 0.0037, Accuracy: 99.96%, Grad Norm: 0.04446\n",
      "Epoch [1152/10044], Batch [6/7], Loss: 0.0049, Accuracy: 99.90%, Grad Norm: 0.06622\n",
      "Epoch [1152/10044], Batch [7/7], Loss: 0.0048, Accuracy: 99.88%, Grad Norm: 0.10623\n",
      "Epoch [1152/10044], Loss: 0.0048\n",
      "Epoch [1153/10044], Batch [1/7], Loss: 0.0061, Accuracy: 99.87%, Grad Norm: 0.09123\n",
      "Epoch [1153/10044], Batch [2/7], Loss: 0.0074, Accuracy: 99.79%, Grad Norm: 0.11041\n",
      "Epoch [1153/10044], Batch [3/7], Loss: 0.0037, Accuracy: 99.95%, Grad Norm: 0.05497\n",
      "Epoch [1153/10044], Batch [4/7], Loss: 0.0042, Accuracy: 99.96%, Grad Norm: 0.05347\n",
      "Epoch [1153/10044], Batch [5/7], Loss: 0.0048, Accuracy: 99.89%, Grad Norm: 0.09018\n",
      "Epoch [1153/10044], Batch [6/7], Loss: 0.0059, Accuracy: 99.86%, Grad Norm: 0.09223\n",
      "Epoch [1153/10044], Batch [7/7], Loss: 0.0043, Accuracy: 99.92%, Grad Norm: 0.08592\n",
      "Epoch [1153/10044], Loss: 0.0043\n",
      "Epoch [1154/10044], Batch [1/7], Loss: 0.0043, Accuracy: 99.93%, Grad Norm: 0.05435\n",
      "Epoch [1154/10044], Batch [2/7], Loss: 0.0048, Accuracy: 99.89%, Grad Norm: 0.06783\n",
      "Epoch [1154/10044], Batch [3/7], Loss: 0.0051, Accuracy: 99.89%, Grad Norm: 0.07318\n",
      "Epoch [1154/10044], Batch [4/7], Loss: 0.0044, Accuracy: 99.91%, Grad Norm: 0.06588\n",
      "Epoch [1154/10044], Batch [5/7], Loss: 0.0051, Accuracy: 99.90%, Grad Norm: 0.08016\n",
      "Epoch [1154/10044], Batch [6/7], Loss: 0.0057, Accuracy: 99.86%, Grad Norm: 0.08049\n",
      "Epoch [1154/10044], Batch [7/7], Loss: 0.0032, Accuracy: 99.97%, Grad Norm: 0.05973\n",
      "Epoch [1154/10044], Loss: 0.0032\n",
      "Epoch [1155/10044], Batch [1/7], Loss: 0.0054, Accuracy: 99.83%, Grad Norm: 0.07574\n",
      "Epoch [1155/10044], Batch [2/7], Loss: 0.0046, Accuracy: 99.89%, Grad Norm: 0.06953\n",
      "Epoch [1155/10044], Batch [3/7], Loss: 0.0037, Accuracy: 99.94%, Grad Norm: 0.05603\n",
      "Epoch [1155/10044], Batch [4/7], Loss: 0.0048, Accuracy: 99.89%, Grad Norm: 0.06896\n",
      "Epoch [1155/10044], Batch [5/7], Loss: 0.0048, Accuracy: 99.92%, Grad Norm: 0.06793\n",
      "Epoch [1155/10044], Batch [6/7], Loss: 0.0056, Accuracy: 99.88%, Grad Norm: 0.08236\n",
      "Epoch [1155/10044], Batch [7/7], Loss: 0.0043, Accuracy: 99.92%, Grad Norm: 0.09415\n",
      "Epoch [1155/10044], Loss: 0.0043\n",
      "Epoch [1156/10044], Batch [1/7], Loss: 0.0051, Accuracy: 99.90%, Grad Norm: 0.07208\n",
      "Epoch [1156/10044], Batch [2/7], Loss: 0.0049, Accuracy: 99.90%, Grad Norm: 0.07012\n",
      "Epoch [1156/10044], Batch [3/7], Loss: 0.0041, Accuracy: 99.92%, Grad Norm: 0.05639\n",
      "Epoch [1156/10044], Batch [4/7], Loss: 0.0045, Accuracy: 99.89%, Grad Norm: 0.06018\n",
      "Epoch [1156/10044], Batch [5/7], Loss: 0.0047, Accuracy: 99.92%, Grad Norm: 0.07581\n",
      "Epoch [1156/10044], Batch [6/7], Loss: 0.0062, Accuracy: 99.79%, Grad Norm: 0.08729\n",
      "Epoch [1156/10044], Batch [7/7], Loss: 0.0033, Accuracy: 99.97%, Grad Norm: 0.07624\n",
      "Epoch [1156/10044], Loss: 0.0033\n",
      "Epoch [1157/10044], Batch [1/7], Loss: 0.0047, Accuracy: 99.92%, Grad Norm: 0.06912\n",
      "Epoch [1157/10044], Batch [2/7], Loss: 0.0045, Accuracy: 99.89%, Grad Norm: 0.06624\n",
      "Epoch [1157/10044], Batch [3/7], Loss: 0.0046, Accuracy: 99.92%, Grad Norm: 0.07384\n",
      "Epoch [1157/10044], Batch [4/7], Loss: 0.0043, Accuracy: 99.92%, Grad Norm: 0.06760\n",
      "Epoch [1157/10044], Batch [5/7], Loss: 0.0046, Accuracy: 99.90%, Grad Norm: 0.07415\n",
      "Epoch [1157/10044], Batch [6/7], Loss: 0.0064, Accuracy: 99.82%, Grad Norm: 0.08400\n",
      "Epoch [1157/10044], Batch [7/7], Loss: 0.0031, Accuracy: 99.95%, Grad Norm: 0.08125\n",
      "Epoch [1157/10044], Loss: 0.0031\n",
      "Epoch [1158/10044], Batch [1/7], Loss: 0.0050, Accuracy: 99.87%, Grad Norm: 0.06812\n",
      "Epoch [1158/10044], Batch [2/7], Loss: 0.0039, Accuracy: 99.93%, Grad Norm: 0.05797\n",
      "Epoch [1158/10044], Batch [3/7], Loss: 0.0033, Accuracy: 99.95%, Grad Norm: 0.04120\n",
      "Epoch [1158/10044], Batch [4/7], Loss: 0.0051, Accuracy: 99.91%, Grad Norm: 0.07080\n",
      "Epoch [1158/10044], Batch [5/7], Loss: 0.0051, Accuracy: 99.89%, Grad Norm: 0.08920\n",
      "Epoch [1158/10044], Batch [6/7], Loss: 0.0068, Accuracy: 99.82%, Grad Norm: 0.08417\n",
      "Epoch [1158/10044], Batch [7/7], Loss: 0.0035, Accuracy: 99.98%, Grad Norm: 0.07547\n",
      "Epoch [1158/10044], Loss: 0.0035\n",
      "Epoch [1159/10044], Batch [1/7], Loss: 0.0043, Accuracy: 99.90%, Grad Norm: 0.07145\n",
      "Epoch [1159/10044], Batch [2/7], Loss: 0.0058, Accuracy: 99.80%, Grad Norm: 0.08794\n",
      "Epoch [1159/10044], Batch [3/7], Loss: 0.0039, Accuracy: 99.94%, Grad Norm: 0.05288\n",
      "Epoch [1159/10044], Batch [4/7], Loss: 0.0056, Accuracy: 99.86%, Grad Norm: 0.08296\n",
      "Epoch [1159/10044], Batch [5/7], Loss: 0.0042, Accuracy: 99.92%, Grad Norm: 0.06168\n",
      "Epoch [1159/10044], Batch [6/7], Loss: 0.0054, Accuracy: 99.85%, Grad Norm: 0.08540\n",
      "Epoch [1159/10044], Batch [7/7], Loss: 0.0029, Accuracy: 99.95%, Grad Norm: 0.05781\n",
      "Epoch [1159/10044], Loss: 0.0029\n",
      "Epoch [1160/10044], Batch [1/7], Loss: 0.0053, Accuracy: 99.89%, Grad Norm: 0.08001\n",
      "Epoch [1160/10044], Batch [2/7], Loss: 0.0051, Accuracy: 99.90%, Grad Norm: 0.08422\n",
      "Epoch [1160/10044], Batch [3/7], Loss: 0.0037, Accuracy: 99.97%, Grad Norm: 0.04737\n",
      "Epoch [1160/10044], Batch [4/7], Loss: 0.0063, Accuracy: 99.87%, Grad Norm: 0.08424\n",
      "Epoch [1160/10044], Batch [5/7], Loss: 0.0055, Accuracy: 99.90%, Grad Norm: 0.07780\n",
      "Epoch [1160/10044], Batch [6/7], Loss: 0.0059, Accuracy: 99.87%, Grad Norm: 0.08567\n",
      "Epoch [1160/10044], Batch [7/7], Loss: 0.0035, Accuracy: 99.97%, Grad Norm: 0.07042\n",
      "Epoch [1160/10044], Loss: 0.0035\n",
      "Epoch [1161/10044], Batch [1/7], Loss: 0.0061, Accuracy: 99.84%, Grad Norm: 0.08232\n",
      "Epoch [1161/10044], Batch [2/7], Loss: 0.0059, Accuracy: 99.86%, Grad Norm: 0.07976\n",
      "Epoch [1161/10044], Batch [3/7], Loss: 0.0037, Accuracy: 99.95%, Grad Norm: 0.04806\n",
      "Epoch [1161/10044], Batch [4/7], Loss: 0.0050, Accuracy: 99.93%, Grad Norm: 0.06871\n",
      "Epoch [1161/10044], Batch [5/7], Loss: 0.0042, Accuracy: 99.90%, Grad Norm: 0.06911\n",
      "Epoch [1161/10044], Batch [6/7], Loss: 0.0049, Accuracy: 99.88%, Grad Norm: 0.07265\n",
      "Epoch [1161/10044], Batch [7/7], Loss: 0.0042, Accuracy: 99.92%, Grad Norm: 0.08137\n",
      "Epoch [1161/10044], Loss: 0.0042\n",
      "Epoch [1162/10044], Batch [1/7], Loss: 0.0057, Accuracy: 99.84%, Grad Norm: 0.09945\n",
      "Epoch [1162/10044], Batch [2/7], Loss: 0.0048, Accuracy: 99.90%, Grad Norm: 0.08345\n",
      "Epoch [1162/10044], Batch [3/7], Loss: 0.0035, Accuracy: 99.93%, Grad Norm: 0.04454\n",
      "Epoch [1162/10044], Batch [4/7], Loss: 0.0045, Accuracy: 99.91%, Grad Norm: 0.07237\n",
      "Epoch [1162/10044], Batch [5/7], Loss: 0.0044, Accuracy: 99.91%, Grad Norm: 0.07088\n",
      "Epoch [1162/10044], Batch [6/7], Loss: 0.0049, Accuracy: 99.91%, Grad Norm: 0.05821\n",
      "Epoch [1162/10044], Batch [7/7], Loss: 0.0029, Accuracy: 99.92%, Grad Norm: 0.06271\n",
      "Epoch [1162/10044], Loss: 0.0029\n",
      "Epoch [1163/10044], Batch [1/7], Loss: 0.0047, Accuracy: 99.92%, Grad Norm: 0.07220\n",
      "Epoch [1163/10044], Batch [2/7], Loss: 0.0070, Accuracy: 99.78%, Grad Norm: 0.11736\n",
      "Epoch [1163/10044], Batch [3/7], Loss: 0.0048, Accuracy: 99.91%, Grad Norm: 0.06110\n",
      "Epoch [1163/10044], Batch [4/7], Loss: 0.0049, Accuracy: 99.91%, Grad Norm: 0.06357\n",
      "Epoch [1163/10044], Batch [5/7], Loss: 0.0037, Accuracy: 99.95%, Grad Norm: 0.05675\n",
      "Epoch [1163/10044], Batch [6/7], Loss: 0.0055, Accuracy: 99.90%, Grad Norm: 0.08407\n",
      "Epoch [1163/10044], Batch [7/7], Loss: 0.0024, Accuracy: 99.97%, Grad Norm: 0.03683\n",
      "Epoch [1163/10044], Loss: 0.0024\n",
      "Epoch [1164/10044], Batch [1/7], Loss: 0.0046, Accuracy: 99.91%, Grad Norm: 0.06063\n",
      "Epoch [1164/10044], Batch [2/7], Loss: 0.0050, Accuracy: 99.89%, Grad Norm: 0.07220\n",
      "Epoch [1164/10044], Batch [3/7], Loss: 0.0040, Accuracy: 99.93%, Grad Norm: 0.05403\n",
      "Epoch [1164/10044], Batch [4/7], Loss: 0.0041, Accuracy: 99.93%, Grad Norm: 0.04805\n",
      "Epoch [1164/10044], Batch [5/7], Loss: 0.0040, Accuracy: 99.92%, Grad Norm: 0.07159\n",
      "Epoch [1164/10044], Batch [6/7], Loss: 0.0048, Accuracy: 99.92%, Grad Norm: 0.07531\n",
      "Epoch [1164/10044], Batch [7/7], Loss: 0.0044, Accuracy: 99.93%, Grad Norm: 0.12670\n",
      "Epoch [1164/10044], Loss: 0.0044\n",
      "Epoch [1165/10044], Batch [1/7], Loss: 0.0039, Accuracy: 99.95%, Grad Norm: 0.05801\n",
      "Epoch [1165/10044], Batch [2/7], Loss: 0.0076, Accuracy: 99.80%, Grad Norm: 0.13242\n",
      "Epoch [1165/10044], Batch [3/7], Loss: 0.0035, Accuracy: 99.96%, Grad Norm: 0.04521\n",
      "Epoch [1165/10044], Batch [4/7], Loss: 0.0045, Accuracy: 99.92%, Grad Norm: 0.06114\n",
      "Epoch [1165/10044], Batch [5/7], Loss: 0.0045, Accuracy: 99.92%, Grad Norm: 0.07035\n",
      "Epoch [1165/10044], Batch [6/7], Loss: 0.0067, Accuracy: 99.83%, Grad Norm: 0.08946\n",
      "Epoch [1165/10044], Batch [7/7], Loss: 0.0029, Accuracy: 99.93%, Grad Norm: 0.07228\n",
      "Epoch [1165/10044], Loss: 0.0029\n",
      "Epoch [1166/10044], Batch [1/7], Loss: 0.0052, Accuracy: 99.87%, Grad Norm: 0.09202\n",
      "Epoch [1166/10044], Batch [2/7], Loss: 0.0053, Accuracy: 99.90%, Grad Norm: 0.07627\n",
      "Epoch [1166/10044], Batch [3/7], Loss: 0.0043, Accuracy: 99.93%, Grad Norm: 0.05975\n",
      "Epoch [1166/10044], Batch [4/7], Loss: 0.0044, Accuracy: 99.92%, Grad Norm: 0.06060\n",
      "Epoch [1166/10044], Batch [5/7], Loss: 0.0048, Accuracy: 99.87%, Grad Norm: 0.08772\n",
      "Epoch [1166/10044], Batch [6/7], Loss: 0.0069, Accuracy: 99.81%, Grad Norm: 0.11593\n",
      "Epoch [1166/10044], Batch [7/7], Loss: 0.0047, Accuracy: 99.90%, Grad Norm: 0.14129\n",
      "Epoch [1166/10044], Loss: 0.0047\n",
      "Epoch [1167/10044], Batch [1/7], Loss: 0.0048, Accuracy: 99.89%, Grad Norm: 0.07628\n",
      "Epoch [1167/10044], Batch [2/7], Loss: 0.0064, Accuracy: 99.83%, Grad Norm: 0.10912\n",
      "Epoch [1167/10044], Batch [3/7], Loss: 0.0034, Accuracy: 99.97%, Grad Norm: 0.04954\n",
      "Epoch [1167/10044], Batch [4/7], Loss: 0.0051, Accuracy: 99.87%, Grad Norm: 0.06850\n",
      "Epoch [1167/10044], Batch [5/7], Loss: 0.0046, Accuracy: 99.90%, Grad Norm: 0.07267\n",
      "Epoch [1167/10044], Batch [6/7], Loss: 0.0051, Accuracy: 99.85%, Grad Norm: 0.09250\n",
      "Epoch [1167/10044], Batch [7/7], Loss: 0.0065, Accuracy: 99.82%, Grad Norm: 0.15049\n",
      "Epoch [1167/10044], Loss: 0.0065\n",
      "Epoch [1168/10044], Batch [1/7], Loss: 0.0043, Accuracy: 99.90%, Grad Norm: 0.06383\n",
      "Epoch [1168/10044], Batch [2/7], Loss: 0.0055, Accuracy: 99.86%, Grad Norm: 0.08954\n",
      "Epoch [1168/10044], Batch [3/7], Loss: 0.0035, Accuracy: 99.97%, Grad Norm: 0.04391\n",
      "Epoch [1168/10044], Batch [4/7], Loss: 0.0039, Accuracy: 99.94%, Grad Norm: 0.05472\n",
      "Epoch [1168/10044], Batch [5/7], Loss: 0.0054, Accuracy: 99.89%, Grad Norm: 0.10355\n",
      "Epoch [1168/10044], Batch [6/7], Loss: 0.0053, Accuracy: 99.86%, Grad Norm: 0.08500\n",
      "Epoch [1168/10044], Batch [7/7], Loss: 0.0049, Accuracy: 99.83%, Grad Norm: 0.12693\n",
      "Epoch [1168/10044], Loss: 0.0049\n",
      "Epoch [1169/10044], Batch [1/7], Loss: 0.0065, Accuracy: 99.82%, Grad Norm: 0.11358\n",
      "Epoch [1169/10044], Batch [2/7], Loss: 0.0056, Accuracy: 99.87%, Grad Norm: 0.11520\n",
      "Epoch [1169/10044], Batch [3/7], Loss: 0.0043, Accuracy: 99.94%, Grad Norm: 0.07678\n",
      "Epoch [1169/10044], Batch [4/7], Loss: 0.0036, Accuracy: 99.92%, Grad Norm: 0.05133\n",
      "Epoch [1169/10044], Batch [5/7], Loss: 0.0056, Accuracy: 99.87%, Grad Norm: 0.08870\n",
      "Epoch [1169/10044], Batch [6/7], Loss: 0.0070, Accuracy: 99.77%, Grad Norm: 0.10739\n",
      "Epoch [1169/10044], Batch [7/7], Loss: 0.0041, Accuracy: 99.92%, Grad Norm: 0.11326\n",
      "Epoch [1169/10044], Loss: 0.0041\n",
      "Epoch [1170/10044], Batch [1/7], Loss: 0.0067, Accuracy: 99.87%, Grad Norm: 0.09722\n",
      "Epoch [1170/10044], Batch [2/7], Loss: 0.0064, Accuracy: 99.86%, Grad Norm: 0.13752\n",
      "Epoch [1170/10044], Batch [3/7], Loss: 0.0035, Accuracy: 99.97%, Grad Norm: 0.04887\n",
      "Epoch [1170/10044], Batch [4/7], Loss: 0.0052, Accuracy: 99.90%, Grad Norm: 0.06407\n",
      "Epoch [1170/10044], Batch [5/7], Loss: 0.0049, Accuracy: 99.87%, Grad Norm: 0.07289\n",
      "Epoch [1170/10044], Batch [6/7], Loss: 0.0074, Accuracy: 99.83%, Grad Norm: 0.10826\n",
      "Epoch [1170/10044], Batch [7/7], Loss: 0.0040, Accuracy: 99.90%, Grad Norm: 0.09627\n",
      "Epoch [1170/10044], Loss: 0.0040\n",
      "Epoch [1171/10044], Batch [1/7], Loss: 0.0063, Accuracy: 99.85%, Grad Norm: 0.10591\n",
      "Epoch [1171/10044], Batch [2/7], Loss: 0.0051, Accuracy: 99.89%, Grad Norm: 0.08611\n",
      "Epoch [1171/10044], Batch [3/7], Loss: 0.0050, Accuracy: 99.92%, Grad Norm: 0.07874\n",
      "Epoch [1171/10044], Batch [4/7], Loss: 0.0050, Accuracy: 99.87%, Grad Norm: 0.07078\n",
      "Epoch [1171/10044], Batch [5/7], Loss: 0.0060, Accuracy: 99.85%, Grad Norm: 0.08855\n",
      "Epoch [1171/10044], Batch [6/7], Loss: 0.0070, Accuracy: 99.85%, Grad Norm: 0.08595\n",
      "Epoch [1171/10044], Batch [7/7], Loss: 0.0052, Accuracy: 99.90%, Grad Norm: 0.15041\n",
      "Epoch [1171/10044], Loss: 0.0052\n",
      "Epoch [1172/10044], Batch [1/7], Loss: 0.0052, Accuracy: 99.90%, Grad Norm: 0.07077\n",
      "Epoch [1172/10044], Batch [2/7], Loss: 0.0054, Accuracy: 99.91%, Grad Norm: 0.09594\n",
      "Epoch [1172/10044], Batch [3/7], Loss: 0.0047, Accuracy: 99.89%, Grad Norm: 0.06392\n",
      "Epoch [1172/10044], Batch [4/7], Loss: 0.0048, Accuracy: 99.90%, Grad Norm: 0.06880\n",
      "Epoch [1172/10044], Batch [5/7], Loss: 0.0044, Accuracy: 99.92%, Grad Norm: 0.07900\n",
      "Epoch [1172/10044], Batch [6/7], Loss: 0.0064, Accuracy: 99.84%, Grad Norm: 0.11558\n",
      "Epoch [1172/10044], Batch [7/7], Loss: 0.0060, Accuracy: 99.85%, Grad Norm: 0.16669\n",
      "Epoch [1172/10044], Loss: 0.0060\n",
      "Epoch [1173/10044], Batch [1/7], Loss: 0.0055, Accuracy: 99.90%, Grad Norm: 0.07689\n",
      "Epoch [1173/10044], Batch [2/7], Loss: 0.0054, Accuracy: 99.87%, Grad Norm: 0.08953\n",
      "Epoch [1173/10044], Batch [3/7], Loss: 0.0042, Accuracy: 99.92%, Grad Norm: 0.06482\n",
      "Epoch [1173/10044], Batch [4/7], Loss: 0.0044, Accuracy: 99.92%, Grad Norm: 0.06330\n",
      "Epoch [1173/10044], Batch [5/7], Loss: 0.0064, Accuracy: 99.89%, Grad Norm: 0.10173\n",
      "Epoch [1173/10044], Batch [6/7], Loss: 0.0079, Accuracy: 99.81%, Grad Norm: 0.12295\n",
      "Epoch [1173/10044], Batch [7/7], Loss: 0.0060, Accuracy: 99.85%, Grad Norm: 0.18223\n",
      "Epoch [1173/10044], Loss: 0.0060\n",
      "Epoch [1174/10044], Batch [1/7], Loss: 0.0051, Accuracy: 99.87%, Grad Norm: 0.07652\n",
      "Epoch [1174/10044], Batch [2/7], Loss: 0.0069, Accuracy: 99.85%, Grad Norm: 0.09828\n",
      "Epoch [1174/10044], Batch [3/7], Loss: 0.0033, Accuracy: 99.97%, Grad Norm: 0.03888\n",
      "Epoch [1174/10044], Batch [4/7], Loss: 0.0049, Accuracy: 99.91%, Grad Norm: 0.06745\n",
      "Epoch [1174/10044], Batch [5/7], Loss: 0.0051, Accuracy: 99.89%, Grad Norm: 0.09379\n",
      "Epoch [1174/10044], Batch [6/7], Loss: 0.0080, Accuracy: 99.82%, Grad Norm: 0.12227\n",
      "Epoch [1174/10044], Batch [7/7], Loss: 0.0040, Accuracy: 99.93%, Grad Norm: 0.09601\n",
      "Epoch [1174/10044], Loss: 0.0040\n",
      "Epoch [1175/10044], Batch [1/7], Loss: 0.0041, Accuracy: 99.92%, Grad Norm: 0.06105\n",
      "Epoch [1175/10044], Batch [2/7], Loss: 0.0061, Accuracy: 99.87%, Grad Norm: 0.08708\n",
      "Epoch [1175/10044], Batch [3/7], Loss: 0.0039, Accuracy: 99.94%, Grad Norm: 0.05413\n",
      "Epoch [1175/10044], Batch [4/7], Loss: 0.0054, Accuracy: 99.92%, Grad Norm: 0.06485\n",
      "Epoch [1175/10044], Batch [5/7], Loss: 0.0046, Accuracy: 99.89%, Grad Norm: 0.07221\n",
      "Epoch [1175/10044], Batch [6/7], Loss: 0.0083, Accuracy: 99.80%, Grad Norm: 0.10718\n",
      "Epoch [1175/10044], Batch [7/7], Loss: 0.0036, Accuracy: 99.95%, Grad Norm: 0.09022\n",
      "Epoch [1175/10044], Loss: 0.0036\n",
      "Epoch [1176/10044], Batch [1/7], Loss: 0.0052, Accuracy: 99.87%, Grad Norm: 0.06252\n",
      "Epoch [1176/10044], Batch [2/7], Loss: 0.0049, Accuracy: 99.89%, Grad Norm: 0.08464\n",
      "Epoch [1176/10044], Batch [3/7], Loss: 0.0038, Accuracy: 99.95%, Grad Norm: 0.05120\n",
      "Epoch [1176/10044], Batch [4/7], Loss: 0.0053, Accuracy: 99.89%, Grad Norm: 0.08280\n",
      "Epoch [1176/10044], Batch [5/7], Loss: 0.0049, Accuracy: 99.90%, Grad Norm: 0.07430\n",
      "Epoch [1176/10044], Batch [6/7], Loss: 0.0086, Accuracy: 99.80%, Grad Norm: 0.10994\n",
      "Epoch [1176/10044], Batch [7/7], Loss: 0.0037, Accuracy: 99.93%, Grad Norm: 0.07933\n",
      "Epoch [1176/10044], Loss: 0.0037\n",
      "Epoch [1177/10044], Batch [1/7], Loss: 0.0048, Accuracy: 99.90%, Grad Norm: 0.09125\n",
      "Epoch [1177/10044], Batch [2/7], Loss: 0.0061, Accuracy: 99.87%, Grad Norm: 0.10011\n",
      "Epoch [1177/10044], Batch [3/7], Loss: 0.0040, Accuracy: 99.94%, Grad Norm: 0.05042\n",
      "Epoch [1177/10044], Batch [4/7], Loss: 0.0048, Accuracy: 99.87%, Grad Norm: 0.06593\n",
      "Epoch [1177/10044], Batch [5/7], Loss: 0.0039, Accuracy: 99.93%, Grad Norm: 0.05744\n",
      "Epoch [1177/10044], Batch [6/7], Loss: 0.0064, Accuracy: 99.81%, Grad Norm: 0.10826\n",
      "Epoch [1177/10044], Batch [7/7], Loss: 0.0038, Accuracy: 99.90%, Grad Norm: 0.11996\n",
      "Epoch [1177/10044], Loss: 0.0038\n",
      "Epoch [1178/10044], Batch [1/7], Loss: 0.0071, Accuracy: 99.86%, Grad Norm: 0.10139\n",
      "Epoch [1178/10044], Batch [2/7], Loss: 0.0057, Accuracy: 99.86%, Grad Norm: 0.09067\n",
      "Epoch [1178/10044], Batch [3/7], Loss: 0.0051, Accuracy: 99.87%, Grad Norm: 0.07013\n",
      "Epoch [1178/10044], Batch [4/7], Loss: 0.0050, Accuracy: 99.91%, Grad Norm: 0.07326\n",
      "Epoch [1178/10044], Batch [5/7], Loss: 0.0045, Accuracy: 99.87%, Grad Norm: 0.07783\n",
      "Epoch [1178/10044], Batch [6/7], Loss: 0.0057, Accuracy: 99.89%, Grad Norm: 0.07815\n",
      "Epoch [1178/10044], Batch [7/7], Loss: 0.0037, Accuracy: 99.92%, Grad Norm: 0.08465\n",
      "Epoch [1178/10044], Loss: 0.0037\n",
      "Epoch [1179/10044], Batch [1/7], Loss: 0.0051, Accuracy: 99.88%, Grad Norm: 0.07108\n",
      "Epoch [1179/10044], Batch [2/7], Loss: 0.0056, Accuracy: 99.87%, Grad Norm: 0.10374\n",
      "Epoch [1179/10044], Batch [3/7], Loss: 0.0030, Accuracy: 99.98%, Grad Norm: 0.03308\n",
      "Epoch [1179/10044], Batch [4/7], Loss: 0.0053, Accuracy: 99.90%, Grad Norm: 0.08150\n",
      "Epoch [1179/10044], Batch [5/7], Loss: 0.0043, Accuracy: 99.92%, Grad Norm: 0.06522\n",
      "Epoch [1179/10044], Batch [6/7], Loss: 0.0045, Accuracy: 99.92%, Grad Norm: 0.05752\n",
      "Epoch [1179/10044], Batch [7/7], Loss: 0.0029, Accuracy: 99.95%, Grad Norm: 0.06997\n",
      "Epoch [1179/10044], Loss: 0.0029\n",
      "Epoch [1180/10044], Batch [1/7], Loss: 0.0042, Accuracy: 99.92%, Grad Norm: 0.05851\n",
      "Epoch [1180/10044], Batch [2/7], Loss: 0.0054, Accuracy: 99.87%, Grad Norm: 0.08262\n",
      "Epoch [1180/10044], Batch [3/7], Loss: 0.0054, Accuracy: 99.88%, Grad Norm: 0.08411\n",
      "Epoch [1180/10044], Batch [4/7], Loss: 0.0048, Accuracy: 99.87%, Grad Norm: 0.06781\n",
      "Epoch [1180/10044], Batch [5/7], Loss: 0.0035, Accuracy: 99.95%, Grad Norm: 0.05584\n",
      "Epoch [1180/10044], Batch [6/7], Loss: 0.0050, Accuracy: 99.87%, Grad Norm: 0.07554\n",
      "Epoch [1180/10044], Batch [7/7], Loss: 0.0049, Accuracy: 99.92%, Grad Norm: 0.14405\n",
      "Epoch [1180/10044], Loss: 0.0049\n",
      "Epoch [1181/10044], Batch [1/7], Loss: 0.0054, Accuracy: 99.87%, Grad Norm: 0.08542\n",
      "Epoch [1181/10044], Batch [2/7], Loss: 0.0050, Accuracy: 99.88%, Grad Norm: 0.06806\n",
      "Epoch [1181/10044], Batch [3/7], Loss: 0.0038, Accuracy: 99.94%, Grad Norm: 0.05128\n",
      "Epoch [1181/10044], Batch [4/7], Loss: 0.0047, Accuracy: 99.92%, Grad Norm: 0.06470\n",
      "Epoch [1181/10044], Batch [5/7], Loss: 0.0049, Accuracy: 99.87%, Grad Norm: 0.08586\n",
      "Epoch [1181/10044], Batch [6/7], Loss: 0.0073, Accuracy: 99.80%, Grad Norm: 0.09850\n",
      "Epoch [1181/10044], Batch [7/7], Loss: 0.0048, Accuracy: 99.93%, Grad Norm: 0.16077\n",
      "Epoch [1181/10044], Loss: 0.0048\n",
      "Epoch [1182/10044], Batch [1/7], Loss: 0.0056, Accuracy: 99.87%, Grad Norm: 0.07394\n",
      "Epoch [1182/10044], Batch [2/7], Loss: 0.0050, Accuracy: 99.90%, Grad Norm: 0.07264\n",
      "Epoch [1182/10044], Batch [3/7], Loss: 0.0031, Accuracy: 99.97%, Grad Norm: 0.03695\n",
      "Epoch [1182/10044], Batch [4/7], Loss: 0.0053, Accuracy: 99.87%, Grad Norm: 0.07407\n",
      "Epoch [1182/10044], Batch [5/7], Loss: 0.0040, Accuracy: 99.94%, Grad Norm: 0.06841\n",
      "Epoch [1182/10044], Batch [6/7], Loss: 0.0056, Accuracy: 99.85%, Grad Norm: 0.11173\n",
      "Epoch [1182/10044], Batch [7/7], Loss: 0.0034, Accuracy: 99.92%, Grad Norm: 0.09364\n",
      "Epoch [1182/10044], Loss: 0.0034\n",
      "Epoch [1183/10044], Batch [1/7], Loss: 0.0055, Accuracy: 99.87%, Grad Norm: 0.07822\n",
      "Epoch [1183/10044], Batch [2/7], Loss: 0.0046, Accuracy: 99.91%, Grad Norm: 0.07096\n",
      "Epoch [1183/10044], Batch [3/7], Loss: 0.0036, Accuracy: 99.93%, Grad Norm: 0.05178\n",
      "Epoch [1183/10044], Batch [4/7], Loss: 0.0042, Accuracy: 99.94%, Grad Norm: 0.05472\n",
      "Epoch [1183/10044], Batch [5/7], Loss: 0.0043, Accuracy: 99.92%, Grad Norm: 0.08067\n",
      "Epoch [1183/10044], Batch [6/7], Loss: 0.0060, Accuracy: 99.86%, Grad Norm: 0.09198\n",
      "Epoch [1183/10044], Batch [7/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.12059\n",
      "Epoch [1183/10044], Loss: 0.0058\n",
      "Epoch [1184/10044], Batch [1/7], Loss: 0.0051, Accuracy: 99.89%, Grad Norm: 0.08924\n",
      "Epoch [1184/10044], Batch [2/7], Loss: 0.0050, Accuracy: 99.88%, Grad Norm: 0.07764\n",
      "Epoch [1184/10044], Batch [3/7], Loss: 0.0037, Accuracy: 99.95%, Grad Norm: 0.04480\n",
      "Epoch [1184/10044], Batch [4/7], Loss: 0.0056, Accuracy: 99.82%, Grad Norm: 0.08217\n",
      "Epoch [1184/10044], Batch [5/7], Loss: 0.0034, Accuracy: 99.93%, Grad Norm: 0.04859\n",
      "Epoch [1184/10044], Batch [6/7], Loss: 0.0051, Accuracy: 99.89%, Grad Norm: 0.08254\n",
      "Epoch [1184/10044], Batch [7/7], Loss: 0.0050, Accuracy: 99.87%, Grad Norm: 0.13362\n",
      "Epoch [1184/10044], Loss: 0.0050\n",
      "Epoch [1185/10044], Batch [1/7], Loss: 0.0068, Accuracy: 99.89%, Grad Norm: 0.08506\n",
      "Epoch [1185/10044], Batch [2/7], Loss: 0.0051, Accuracy: 99.91%, Grad Norm: 0.08792\n",
      "Epoch [1185/10044], Batch [3/7], Loss: 0.0042, Accuracy: 99.93%, Grad Norm: 0.05619\n",
      "Epoch [1185/10044], Batch [4/7], Loss: 0.0048, Accuracy: 99.92%, Grad Norm: 0.08044\n",
      "Epoch [1185/10044], Batch [5/7], Loss: 0.0045, Accuracy: 99.89%, Grad Norm: 0.06821\n",
      "Epoch [1185/10044], Batch [6/7], Loss: 0.0047, Accuracy: 99.88%, Grad Norm: 0.06759\n",
      "Epoch [1185/10044], Batch [7/7], Loss: 0.0072, Accuracy: 99.87%, Grad Norm: 0.13473\n",
      "Epoch [1185/10044], Loss: 0.0072\n",
      "Epoch [1186/10044], Batch [1/7], Loss: 0.0042, Accuracy: 99.89%, Grad Norm: 0.06310\n",
      "Epoch [1186/10044], Batch [2/7], Loss: 0.0048, Accuracy: 99.90%, Grad Norm: 0.07548\n",
      "Epoch [1186/10044], Batch [3/7], Loss: 0.0045, Accuracy: 99.92%, Grad Norm: 0.06737\n",
      "Epoch [1186/10044], Batch [4/7], Loss: 0.0054, Accuracy: 99.85%, Grad Norm: 0.09107\n",
      "Epoch [1186/10044], Batch [5/7], Loss: 0.0056, Accuracy: 99.87%, Grad Norm: 0.10488\n",
      "Epoch [1186/10044], Batch [6/7], Loss: 0.0052, Accuracy: 99.87%, Grad Norm: 0.06979\n",
      "Epoch [1186/10044], Batch [7/7], Loss: 0.0037, Accuracy: 99.88%, Grad Norm: 0.08242\n",
      "Epoch [1186/10044], Loss: 0.0037\n",
      "Epoch [1187/10044], Batch [1/7], Loss: 0.0067, Accuracy: 99.82%, Grad Norm: 0.10839\n",
      "Epoch [1187/10044], Batch [2/7], Loss: 0.0058, Accuracy: 99.90%, Grad Norm: 0.08688\n",
      "Epoch [1187/10044], Batch [3/7], Loss: 0.0033, Accuracy: 99.96%, Grad Norm: 0.03879\n",
      "Epoch [1187/10044], Batch [4/7], Loss: 0.0038, Accuracy: 99.93%, Grad Norm: 0.05176\n",
      "Epoch [1187/10044], Batch [5/7], Loss: 0.0047, Accuracy: 99.90%, Grad Norm: 0.07852\n",
      "Epoch [1187/10044], Batch [6/7], Loss: 0.0052, Accuracy: 99.85%, Grad Norm: 0.07969\n",
      "Epoch [1187/10044], Batch [7/7], Loss: 0.0037, Accuracy: 99.92%, Grad Norm: 0.09165\n",
      "Epoch [1187/10044], Loss: 0.0037\n",
      "Epoch [1188/10044], Batch [1/7], Loss: 0.0061, Accuracy: 99.90%, Grad Norm: 0.07783\n",
      "Epoch [1188/10044], Batch [2/7], Loss: 0.0049, Accuracy: 99.87%, Grad Norm: 0.08492\n",
      "Epoch [1188/10044], Batch [3/7], Loss: 0.0039, Accuracy: 99.94%, Grad Norm: 0.06058\n",
      "Epoch [1188/10044], Batch [4/7], Loss: 0.0042, Accuracy: 99.92%, Grad Norm: 0.06369\n",
      "Epoch [1188/10044], Batch [5/7], Loss: 0.0049, Accuracy: 99.89%, Grad Norm: 0.07389\n",
      "Epoch [1188/10044], Batch [6/7], Loss: 0.0061, Accuracy: 99.87%, Grad Norm: 0.09339\n",
      "Epoch [1188/10044], Batch [7/7], Loss: 0.0041, Accuracy: 99.92%, Grad Norm: 0.08607\n",
      "Epoch [1188/10044], Loss: 0.0041\n",
      "Epoch [1189/10044], Batch [1/7], Loss: 0.0041, Accuracy: 99.92%, Grad Norm: 0.06811\n",
      "Epoch [1189/10044], Batch [2/7], Loss: 0.0068, Accuracy: 99.77%, Grad Norm: 0.12290\n",
      "Epoch [1189/10044], Batch [3/7], Loss: 0.0044, Accuracy: 99.93%, Grad Norm: 0.06186\n",
      "Epoch [1189/10044], Batch [4/7], Loss: 0.0045, Accuracy: 99.92%, Grad Norm: 0.05912\n",
      "Epoch [1189/10044], Batch [5/7], Loss: 0.0049, Accuracy: 99.89%, Grad Norm: 0.08480\n",
      "Epoch [1189/10044], Batch [6/7], Loss: 0.0059, Accuracy: 99.84%, Grad Norm: 0.11570\n",
      "Epoch [1189/10044], Batch [7/7], Loss: 0.0044, Accuracy: 99.93%, Grad Norm: 0.09280\n",
      "Epoch [1189/10044], Loss: 0.0044\n",
      "Epoch [1190/10044], Batch [1/7], Loss: 0.0054, Accuracy: 99.90%, Grad Norm: 0.07708\n",
      "Epoch [1190/10044], Batch [2/7], Loss: 0.0049, Accuracy: 99.88%, Grad Norm: 0.08434\n",
      "Epoch [1190/10044], Batch [3/7], Loss: 0.0038, Accuracy: 99.92%, Grad Norm: 0.06370\n",
      "Epoch [1190/10044], Batch [4/7], Loss: 0.0036, Accuracy: 99.94%, Grad Norm: 0.04826\n",
      "Epoch [1190/10044], Batch [5/7], Loss: 0.0064, Accuracy: 99.87%, Grad Norm: 0.09683\n",
      "Epoch [1190/10044], Batch [6/7], Loss: 0.0064, Accuracy: 99.87%, Grad Norm: 0.11544\n",
      "Epoch [1190/10044], Batch [7/7], Loss: 0.0029, Accuracy: 99.95%, Grad Norm: 0.05919\n",
      "Epoch [1190/10044], Loss: 0.0029\n",
      "Epoch [1191/10044], Batch [1/7], Loss: 0.0060, Accuracy: 99.88%, Grad Norm: 0.09669\n",
      "Epoch [1191/10044], Batch [2/7], Loss: 0.0052, Accuracy: 99.87%, Grad Norm: 0.09105\n",
      "Epoch [1191/10044], Batch [3/7], Loss: 0.0032, Accuracy: 99.98%, Grad Norm: 0.04160\n",
      "Epoch [1191/10044], Batch [4/7], Loss: 0.0037, Accuracy: 99.95%, Grad Norm: 0.04963\n",
      "Epoch [1191/10044], Batch [5/7], Loss: 0.0051, Accuracy: 99.90%, Grad Norm: 0.07481\n",
      "Epoch [1191/10044], Batch [6/7], Loss: 0.0063, Accuracy: 99.80%, Grad Norm: 0.09603\n",
      "Epoch [1191/10044], Batch [7/7], Loss: 0.0032, Accuracy: 99.93%, Grad Norm: 0.07153\n",
      "Epoch [1191/10044], Loss: 0.0032\n",
      "Epoch [1192/10044], Batch [1/7], Loss: 0.0048, Accuracy: 99.89%, Grad Norm: 0.08198\n",
      "Epoch [1192/10044], Batch [2/7], Loss: 0.0046, Accuracy: 99.92%, Grad Norm: 0.06757\n",
      "Epoch [1192/10044], Batch [3/7], Loss: 0.0032, Accuracy: 99.94%, Grad Norm: 0.04699\n",
      "Epoch [1192/10044], Batch [4/7], Loss: 0.0057, Accuracy: 99.87%, Grad Norm: 0.09047\n",
      "Epoch [1192/10044], Batch [5/7], Loss: 0.0049, Accuracy: 99.91%, Grad Norm: 0.07167\n",
      "Epoch [1192/10044], Batch [6/7], Loss: 0.0045, Accuracy: 99.88%, Grad Norm: 0.06294\n",
      "Epoch [1192/10044], Batch [7/7], Loss: 0.0046, Accuracy: 99.90%, Grad Norm: 0.12197\n",
      "Epoch [1192/10044], Loss: 0.0046\n",
      "Epoch [1193/10044], Batch [1/7], Loss: 0.0049, Accuracy: 99.91%, Grad Norm: 0.05996\n",
      "Epoch [1193/10044], Batch [2/7], Loss: 0.0045, Accuracy: 99.91%, Grad Norm: 0.08200\n",
      "Epoch [1193/10044], Batch [3/7], Loss: 0.0034, Accuracy: 99.95%, Grad Norm: 0.04858\n",
      "Epoch [1193/10044], Batch [4/7], Loss: 0.0050, Accuracy: 99.86%, Grad Norm: 0.08108\n",
      "Epoch [1193/10044], Batch [5/7], Loss: 0.0047, Accuracy: 99.88%, Grad Norm: 0.07908\n",
      "Epoch [1193/10044], Batch [6/7], Loss: 0.0065, Accuracy: 99.82%, Grad Norm: 0.09109\n",
      "Epoch [1193/10044], Batch [7/7], Loss: 0.0056, Accuracy: 99.88%, Grad Norm: 0.17005\n",
      "Epoch [1193/10044], Loss: 0.0056\n",
      "Epoch [1194/10044], Batch [1/7], Loss: 0.0058, Accuracy: 99.84%, Grad Norm: 0.07778\n",
      "Epoch [1194/10044], Batch [2/7], Loss: 0.0060, Accuracy: 99.85%, Grad Norm: 0.08615\n",
      "Epoch [1194/10044], Batch [3/7], Loss: 0.0049, Accuracy: 99.89%, Grad Norm: 0.07004\n",
      "Epoch [1194/10044], Batch [4/7], Loss: 0.0046, Accuracy: 99.92%, Grad Norm: 0.06462\n",
      "Epoch [1194/10044], Batch [5/7], Loss: 0.0035, Accuracy: 99.96%, Grad Norm: 0.06482\n",
      "Epoch [1194/10044], Batch [6/7], Loss: 0.0053, Accuracy: 99.84%, Grad Norm: 0.07914\n",
      "Epoch [1194/10044], Batch [7/7], Loss: 0.0044, Accuracy: 99.88%, Grad Norm: 0.12902\n",
      "Epoch [1194/10044], Loss: 0.0044\n",
      "Epoch [1195/10044], Batch [1/7], Loss: 0.0038, Accuracy: 99.93%, Grad Norm: 0.04882\n",
      "Epoch [1195/10044], Batch [2/7], Loss: 0.0051, Accuracy: 99.87%, Grad Norm: 0.09504\n",
      "Epoch [1195/10044], Batch [3/7], Loss: 0.0041, Accuracy: 99.93%, Grad Norm: 0.05652\n",
      "Epoch [1195/10044], Batch [4/7], Loss: 0.0063, Accuracy: 99.87%, Grad Norm: 0.08768\n",
      "Epoch [1195/10044], Batch [5/7], Loss: 0.0058, Accuracy: 99.86%, Grad Norm: 0.11534\n",
      "Epoch [1195/10044], Batch [6/7], Loss: 0.0083, Accuracy: 99.77%, Grad Norm: 0.13132\n",
      "Epoch [1195/10044], Batch [7/7], Loss: 0.0082, Accuracy: 99.82%, Grad Norm: 0.18132\n",
      "Epoch [1195/10044], Loss: 0.0082\n",
      "Epoch [1196/10044], Batch [1/7], Loss: 0.0038, Accuracy: 99.94%, Grad Norm: 0.05434\n",
      "Epoch [1196/10044], Batch [2/7], Loss: 0.0080, Accuracy: 99.81%, Grad Norm: 0.15477\n",
      "Epoch [1196/10044], Batch [3/7], Loss: 0.0040, Accuracy: 99.92%, Grad Norm: 0.04995\n",
      "Epoch [1196/10044], Batch [4/7], Loss: 0.0064, Accuracy: 99.87%, Grad Norm: 0.10095\n",
      "Epoch [1196/10044], Batch [5/7], Loss: 0.0085, Accuracy: 99.79%, Grad Norm: 0.13824\n",
      "Epoch [1196/10044], Batch [6/7], Loss: 0.0063, Accuracy: 99.85%, Grad Norm: 0.10705\n",
      "Epoch [1196/10044], Batch [7/7], Loss: 0.0056, Accuracy: 99.85%, Grad Norm: 0.14302\n",
      "Epoch [1196/10044], Loss: 0.0056\n",
      "Epoch [1197/10044], Batch [1/7], Loss: 0.0043, Accuracy: 99.91%, Grad Norm: 0.06662\n",
      "Epoch [1197/10044], Batch [2/7], Loss: 0.0077, Accuracy: 99.77%, Grad Norm: 0.14311\n",
      "Epoch [1197/10044], Batch [3/7], Loss: 0.0034, Accuracy: 99.98%, Grad Norm: 0.04939\n",
      "Epoch [1197/10044], Batch [4/7], Loss: 0.0059, Accuracy: 99.87%, Grad Norm: 0.08806\n",
      "Epoch [1197/10044], Batch [5/7], Loss: 0.0065, Accuracy: 99.86%, Grad Norm: 0.08885\n",
      "Epoch [1197/10044], Batch [6/7], Loss: 0.0051, Accuracy: 99.90%, Grad Norm: 0.07980\n",
      "Epoch [1197/10044], Batch [7/7], Loss: 0.0045, Accuracy: 99.87%, Grad Norm: 0.10289\n",
      "Epoch [1197/10044], Loss: 0.0045\n",
      "Epoch [1198/10044], Batch [1/7], Loss: 0.0044, Accuracy: 99.92%, Grad Norm: 0.06302\n",
      "Epoch [1198/10044], Batch [2/7], Loss: 0.0063, Accuracy: 99.84%, Grad Norm: 0.12020\n",
      "Epoch [1198/10044], Batch [3/7], Loss: 0.0045, Accuracy: 99.90%, Grad Norm: 0.06014\n",
      "Epoch [1198/10044], Batch [4/7], Loss: 0.0040, Accuracy: 99.93%, Grad Norm: 0.05946\n",
      "Epoch [1198/10044], Batch [5/7], Loss: 0.0056, Accuracy: 99.88%, Grad Norm: 0.08338\n",
      "Epoch [1198/10044], Batch [6/7], Loss: 0.0052, Accuracy: 99.86%, Grad Norm: 0.08330\n",
      "Epoch [1198/10044], Batch [7/7], Loss: 0.0039, Accuracy: 99.90%, Grad Norm: 0.08987\n",
      "Epoch [1198/10044], Loss: 0.0039\n",
      "Epoch [1199/10044], Batch [1/7], Loss: 0.0034, Accuracy: 99.92%, Grad Norm: 0.05352\n",
      "Epoch [1199/10044], Batch [2/7], Loss: 0.0048, Accuracy: 99.87%, Grad Norm: 0.07984\n",
      "Epoch [1199/10044], Batch [3/7], Loss: 0.0036, Accuracy: 99.97%, Grad Norm: 0.04714\n",
      "Epoch [1199/10044], Batch [4/7], Loss: 0.0042, Accuracy: 99.93%, Grad Norm: 0.06051\n",
      "Epoch [1199/10044], Batch [5/7], Loss: 0.0059, Accuracy: 99.82%, Grad Norm: 0.11497\n",
      "Epoch [1199/10044], Batch [6/7], Loss: 0.0075, Accuracy: 99.79%, Grad Norm: 0.10413\n",
      "Epoch [1199/10044], Batch [7/7], Loss: 0.0030, Accuracy: 99.95%, Grad Norm: 0.06512\n",
      "Epoch [1199/10044], Loss: 0.0030\n",
      "Epoch [1200/10044], Batch [1/7], Loss: 0.0041, Accuracy: 99.91%, Grad Norm: 0.06657\n",
      "Epoch [1200/10044], Batch [2/7], Loss: 0.0062, Accuracy: 99.87%, Grad Norm: 0.09614\n",
      "Epoch [1200/10044], Batch [3/7], Loss: 0.0035, Accuracy: 99.95%, Grad Norm: 0.05379\n",
      "Epoch [1200/10044], Batch [4/7], Loss: 0.0041, Accuracy: 99.92%, Grad Norm: 0.05479\n",
      "Epoch [1200/10044], Batch [5/7], Loss: 0.0075, Accuracy: 99.82%, Grad Norm: 0.15170\n",
      "Epoch [1200/10044], Batch [6/7], Loss: 0.0061, Accuracy: 99.86%, Grad Norm: 0.10348\n",
      "Epoch [1200/10044], Batch [7/7], Loss: 0.0040, Accuracy: 99.93%, Grad Norm: 0.08257\n",
      "Epoch [1200/10044], Loss: 0.0040\n",
      "Epoch [1201/10044], Batch [1/7], Loss: 0.0048, Accuracy: 99.88%, Grad Norm: 0.08887\n",
      "Epoch [1201/10044], Batch [2/7], Loss: 0.0065, Accuracy: 99.83%, Grad Norm: 0.09570\n",
      "Epoch [1201/10044], Batch [3/7], Loss: 0.0038, Accuracy: 99.95%, Grad Norm: 0.05114\n",
      "Epoch [1201/10044], Batch [4/7], Loss: 0.0045, Accuracy: 99.92%, Grad Norm: 0.07253\n",
      "Epoch [1201/10044], Batch [5/7], Loss: 0.0062, Accuracy: 99.87%, Grad Norm: 0.10768\n",
      "Epoch [1201/10044], Batch [6/7], Loss: 0.0071, Accuracy: 99.74%, Grad Norm: 0.10833\n",
      "Epoch [1201/10044], Batch [7/7], Loss: 0.0055, Accuracy: 99.88%, Grad Norm: 0.12684\n",
      "Epoch [1201/10044], Loss: 0.0055\n",
      "Epoch [1202/10044], Batch [1/7], Loss: 0.0049, Accuracy: 99.92%, Grad Norm: 0.06829\n",
      "Epoch [1202/10044], Batch [2/7], Loss: 0.0059, Accuracy: 99.82%, Grad Norm: 0.09093\n",
      "Epoch [1202/10044], Batch [3/7], Loss: 0.0032, Accuracy: 99.96%, Grad Norm: 0.04954\n",
      "Epoch [1202/10044], Batch [4/7], Loss: 0.0037, Accuracy: 99.95%, Grad Norm: 0.04353\n",
      "Epoch [1202/10044], Batch [5/7], Loss: 0.0041, Accuracy: 99.92%, Grad Norm: 0.06779\n",
      "Epoch [1202/10044], Batch [6/7], Loss: 0.0055, Accuracy: 99.87%, Grad Norm: 0.08733\n",
      "Epoch [1202/10044], Batch [7/7], Loss: 0.0040, Accuracy: 99.90%, Grad Norm: 0.11446\n",
      "Epoch [1202/10044], Loss: 0.0040\n",
      "Epoch [1203/10044], Batch [1/7], Loss: 0.0049, Accuracy: 99.89%, Grad Norm: 0.08666\n",
      "Epoch [1203/10044], Batch [2/7], Loss: 0.0042, Accuracy: 99.89%, Grad Norm: 0.06630\n",
      "Epoch [1203/10044], Batch [3/7], Loss: 0.0038, Accuracy: 99.93%, Grad Norm: 0.04847\n",
      "Epoch [1203/10044], Batch [4/7], Loss: 0.0035, Accuracy: 99.93%, Grad Norm: 0.05181\n",
      "Epoch [1203/10044], Batch [5/7], Loss: 0.0048, Accuracy: 99.91%, Grad Norm: 0.09087\n",
      "Epoch [1203/10044], Batch [6/7], Loss: 0.0074, Accuracy: 99.82%, Grad Norm: 0.09341\n",
      "Epoch [1203/10044], Batch [7/7], Loss: 0.0035, Accuracy: 99.93%, Grad Norm: 0.08919\n",
      "Epoch [1203/10044], Loss: 0.0035\n",
      "Epoch [1204/10044], Batch [1/7], Loss: 0.0057, Accuracy: 99.88%, Grad Norm: 0.09194\n",
      "Epoch [1204/10044], Batch [2/7], Loss: 0.0058, Accuracy: 99.87%, Grad Norm: 0.08080\n",
      "Epoch [1204/10044], Batch [3/7], Loss: 0.0041, Accuracy: 99.92%, Grad Norm: 0.05970\n",
      "Epoch [1204/10044], Batch [4/7], Loss: 0.0039, Accuracy: 99.92%, Grad Norm: 0.05252\n",
      "Epoch [1204/10044], Batch [5/7], Loss: 0.0049, Accuracy: 99.87%, Grad Norm: 0.08447\n",
      "Epoch [1204/10044], Batch [6/7], Loss: 0.0052, Accuracy: 99.89%, Grad Norm: 0.07703\n",
      "Epoch [1204/10044], Batch [7/7], Loss: 0.0046, Accuracy: 99.92%, Grad Norm: 0.10895\n",
      "Epoch [1204/10044], Loss: 0.0046\n",
      "Epoch [1205/10044], Batch [1/7], Loss: 0.0039, Accuracy: 99.94%, Grad Norm: 0.05037\n",
      "Epoch [1205/10044], Batch [2/7], Loss: 0.0059, Accuracy: 99.84%, Grad Norm: 0.09685\n",
      "Epoch [1205/10044], Batch [3/7], Loss: 0.0041, Accuracy: 99.91%, Grad Norm: 0.07641\n",
      "Epoch [1205/10044], Batch [4/7], Loss: 0.0029, Accuracy: 99.97%, Grad Norm: 0.03615\n",
      "Epoch [1205/10044], Batch [5/7], Loss: 0.0053, Accuracy: 99.89%, Grad Norm: 0.10432\n",
      "Epoch [1205/10044], Batch [6/7], Loss: 0.0052, Accuracy: 99.87%, Grad Norm: 0.07284\n",
      "Epoch [1205/10044], Batch [7/7], Loss: 0.0048, Accuracy: 99.87%, Grad Norm: 0.11716\n",
      "Epoch [1205/10044], Loss: 0.0048\n",
      "Epoch [1206/10044], Batch [1/7], Loss: 0.0055, Accuracy: 99.88%, Grad Norm: 0.07921\n",
      "Epoch [1206/10044], Batch [2/7], Loss: 0.0036, Accuracy: 99.93%, Grad Norm: 0.06202\n",
      "Epoch [1206/10044], Batch [3/7], Loss: 0.0038, Accuracy: 99.95%, Grad Norm: 0.05986\n",
      "Epoch [1206/10044], Batch [4/7], Loss: 0.0032, Accuracy: 99.95%, Grad Norm: 0.04955\n",
      "Epoch [1206/10044], Batch [5/7], Loss: 0.0060, Accuracy: 99.89%, Grad Norm: 0.08094\n",
      "Epoch [1206/10044], Batch [6/7], Loss: 0.0067, Accuracy: 99.79%, Grad Norm: 0.08938\n",
      "Epoch [1206/10044], Batch [7/7], Loss: 0.0033, Accuracy: 99.93%, Grad Norm: 0.08233\n",
      "Epoch [1206/10044], Loss: 0.0033\n",
      "Epoch [1207/10044], Batch [1/7], Loss: 0.0051, Accuracy: 99.87%, Grad Norm: 0.06540\n",
      "Epoch [1207/10044], Batch [2/7], Loss: 0.0044, Accuracy: 99.92%, Grad Norm: 0.07640\n",
      "Epoch [1207/10044], Batch [3/7], Loss: 0.0044, Accuracy: 99.92%, Grad Norm: 0.07406\n",
      "Epoch [1207/10044], Batch [4/7], Loss: 0.0036, Accuracy: 99.96%, Grad Norm: 0.04834\n",
      "Epoch [1207/10044], Batch [5/7], Loss: 0.0042, Accuracy: 99.91%, Grad Norm: 0.07597\n",
      "Epoch [1207/10044], Batch [6/7], Loss: 0.0064, Accuracy: 99.81%, Grad Norm: 0.08828\n",
      "Epoch [1207/10044], Batch [7/7], Loss: 0.0025, Accuracy: 99.98%, Grad Norm: 0.07323\n",
      "Epoch [1207/10044], Loss: 0.0025\n",
      "Epoch [1208/10044], Batch [1/7], Loss: 0.0051, Accuracy: 99.88%, Grad Norm: 0.08238\n",
      "Epoch [1208/10044], Batch [2/7], Loss: 0.0054, Accuracy: 99.84%, Grad Norm: 0.08241\n",
      "Epoch [1208/10044], Batch [3/7], Loss: 0.0041, Accuracy: 99.92%, Grad Norm: 0.07362\n",
      "Epoch [1208/10044], Batch [4/7], Loss: 0.0039, Accuracy: 99.96%, Grad Norm: 0.06147\n",
      "Epoch [1208/10044], Batch [5/7], Loss: 0.0042, Accuracy: 99.92%, Grad Norm: 0.07476\n",
      "Epoch [1208/10044], Batch [6/7], Loss: 0.0056, Accuracy: 99.86%, Grad Norm: 0.08866\n",
      "Epoch [1208/10044], Batch [7/7], Loss: 0.0052, Accuracy: 99.92%, Grad Norm: 0.11797\n",
      "Epoch [1208/10044], Loss: 0.0052\n",
      "Epoch [1209/10044], Batch [1/7], Loss: 0.0048, Accuracy: 99.89%, Grad Norm: 0.07433\n",
      "Epoch [1209/10044], Batch [2/7], Loss: 0.0052, Accuracy: 99.88%, Grad Norm: 0.07553\n",
      "Epoch [1209/10044], Batch [3/7], Loss: 0.0044, Accuracy: 99.91%, Grad Norm: 0.06783\n",
      "Epoch [1209/10044], Batch [4/7], Loss: 0.0047, Accuracy: 99.92%, Grad Norm: 0.05870\n",
      "Epoch [1209/10044], Batch [5/7], Loss: 0.0036, Accuracy: 99.92%, Grad Norm: 0.06780\n",
      "Epoch [1209/10044], Batch [6/7], Loss: 0.0046, Accuracy: 99.92%, Grad Norm: 0.07440\n",
      "Epoch [1209/10044], Batch [7/7], Loss: 0.0027, Accuracy: 99.97%, Grad Norm: 0.07149\n",
      "Epoch [1209/10044], Loss: 0.0027\n",
      "Epoch [1210/10044], Batch [1/7], Loss: 0.0041, Accuracy: 99.92%, Grad Norm: 0.06829\n",
      "Epoch [1210/10044], Batch [2/7], Loss: 0.0056, Accuracy: 99.88%, Grad Norm: 0.08318\n",
      "Epoch [1210/10044], Batch [3/7], Loss: 0.0041, Accuracy: 99.96%, Grad Norm: 0.05917\n",
      "Epoch [1210/10044], Batch [4/7], Loss: 0.0034, Accuracy: 99.97%, Grad Norm: 0.04943\n",
      "Epoch [1210/10044], Batch [5/7], Loss: 0.0033, Accuracy: 99.93%, Grad Norm: 0.05177\n",
      "Epoch [1210/10044], Batch [6/7], Loss: 0.0051, Accuracy: 99.88%, Grad Norm: 0.06977\n",
      "Epoch [1210/10044], Batch [7/7], Loss: 0.0025, Accuracy: 99.95%, Grad Norm: 0.05251\n",
      "Epoch [1210/10044], Loss: 0.0025\n",
      "Epoch [1211/10044], Batch [1/7], Loss: 0.0037, Accuracy: 99.94%, Grad Norm: 0.05841\n",
      "Epoch [1211/10044], Batch [2/7], Loss: 0.0058, Accuracy: 99.86%, Grad Norm: 0.09499\n",
      "Epoch [1211/10044], Batch [3/7], Loss: 0.0037, Accuracy: 99.93%, Grad Norm: 0.05289\n",
      "Epoch [1211/10044], Batch [4/7], Loss: 0.0041, Accuracy: 99.92%, Grad Norm: 0.05965\n",
      "Epoch [1211/10044], Batch [5/7], Loss: 0.0050, Accuracy: 99.86%, Grad Norm: 0.09303\n",
      "Epoch [1211/10044], Batch [6/7], Loss: 0.0048, Accuracy: 99.89%, Grad Norm: 0.06682\n",
      "Epoch [1211/10044], Batch [7/7], Loss: 0.0047, Accuracy: 99.87%, Grad Norm: 0.16239\n",
      "Epoch [1211/10044], Loss: 0.0047\n",
      "Epoch [1212/10044], Batch [1/7], Loss: 0.0051, Accuracy: 99.89%, Grad Norm: 0.08227\n",
      "Epoch [1212/10044], Batch [2/7], Loss: 0.0040, Accuracy: 99.92%, Grad Norm: 0.06539\n",
      "Epoch [1212/10044], Batch [3/7], Loss: 0.0048, Accuracy: 99.91%, Grad Norm: 0.06972\n",
      "Epoch [1212/10044], Batch [4/7], Loss: 0.0034, Accuracy: 99.96%, Grad Norm: 0.05529\n",
      "Epoch [1212/10044], Batch [5/7], Loss: 0.0043, Accuracy: 99.90%, Grad Norm: 0.06949\n",
      "Epoch [1212/10044], Batch [6/7], Loss: 0.0061, Accuracy: 99.82%, Grad Norm: 0.09018\n",
      "Epoch [1212/10044], Batch [7/7], Loss: 0.0035, Accuracy: 99.90%, Grad Norm: 0.10240\n",
      "Epoch [1212/10044], Loss: 0.0035\n",
      "Epoch [1213/10044], Batch [1/7], Loss: 0.0038, Accuracy: 99.93%, Grad Norm: 0.06476\n",
      "Epoch [1213/10044], Batch [2/7], Loss: 0.0050, Accuracy: 99.90%, Grad Norm: 0.08262\n",
      "Epoch [1213/10044], Batch [3/7], Loss: 0.0049, Accuracy: 99.87%, Grad Norm: 0.08717\n",
      "Epoch [1213/10044], Batch [4/7], Loss: 0.0042, Accuracy: 99.93%, Grad Norm: 0.05716\n",
      "Epoch [1213/10044], Batch [5/7], Loss: 0.0041, Accuracy: 99.92%, Grad Norm: 0.06429\n",
      "Epoch [1213/10044], Batch [6/7], Loss: 0.0051, Accuracy: 99.86%, Grad Norm: 0.07958\n",
      "Epoch [1213/10044], Batch [7/7], Loss: 0.0050, Accuracy: 99.87%, Grad Norm: 0.14589\n",
      "Epoch [1213/10044], Loss: 0.0050\n",
      "Epoch [1214/10044], Batch [1/7], Loss: 0.0057, Accuracy: 99.88%, Grad Norm: 0.07475\n",
      "Epoch [1214/10044], Batch [2/7], Loss: 0.0064, Accuracy: 99.82%, Grad Norm: 0.09987\n",
      "Epoch [1214/10044], Batch [3/7], Loss: 0.0045, Accuracy: 99.89%, Grad Norm: 0.07273\n",
      "Epoch [1214/10044], Batch [4/7], Loss: 0.0038, Accuracy: 99.92%, Grad Norm: 0.06128\n",
      "Epoch [1214/10044], Batch [5/7], Loss: 0.0043, Accuracy: 99.91%, Grad Norm: 0.06496\n",
      "Epoch [1214/10044], Batch [6/7], Loss: 0.0068, Accuracy: 99.84%, Grad Norm: 0.09867\n",
      "Epoch [1214/10044], Batch [7/7], Loss: 0.0035, Accuracy: 99.92%, Grad Norm: 0.08919\n",
      "Epoch [1214/10044], Loss: 0.0035\n",
      "Epoch [1215/10044], Batch [1/7], Loss: 0.0051, Accuracy: 99.91%, Grad Norm: 0.07546\n",
      "Epoch [1215/10044], Batch [2/7], Loss: 0.0061, Accuracy: 99.86%, Grad Norm: 0.10072\n",
      "Epoch [1215/10044], Batch [3/7], Loss: 0.0036, Accuracy: 99.92%, Grad Norm: 0.05808\n",
      "Epoch [1215/10044], Batch [4/7], Loss: 0.0040, Accuracy: 99.91%, Grad Norm: 0.05134\n",
      "Epoch [1215/10044], Batch [5/7], Loss: 0.0043, Accuracy: 99.91%, Grad Norm: 0.06840\n",
      "Epoch [1215/10044], Batch [6/7], Loss: 0.0055, Accuracy: 99.82%, Grad Norm: 0.08134\n",
      "Epoch [1215/10044], Batch [7/7], Loss: 0.0033, Accuracy: 99.93%, Grad Norm: 0.10326\n",
      "Epoch [1215/10044], Loss: 0.0033\n",
      "Epoch [1216/10044], Batch [1/7], Loss: 0.0049, Accuracy: 99.90%, Grad Norm: 0.06428\n",
      "Epoch [1216/10044], Batch [2/7], Loss: 0.0030, Accuracy: 99.99%, Grad Norm: 0.04758\n",
      "Epoch [1216/10044], Batch [3/7], Loss: 0.0037, Accuracy: 99.93%, Grad Norm: 0.06495\n",
      "Epoch [1216/10044], Batch [4/7], Loss: 0.0042, Accuracy: 99.88%, Grad Norm: 0.06941\n",
      "Epoch [1216/10044], Batch [5/7], Loss: 0.0039, Accuracy: 99.89%, Grad Norm: 0.07970\n",
      "Epoch [1216/10044], Batch [6/7], Loss: 0.0056, Accuracy: 99.84%, Grad Norm: 0.10219\n",
      "Epoch [1216/10044], Batch [7/7], Loss: 0.0026, Accuracy: 99.97%, Grad Norm: 0.06475\n",
      "Epoch [1216/10044], Loss: 0.0026\n",
      "Epoch [1217/10044], Batch [1/7], Loss: 0.0037, Accuracy: 99.95%, Grad Norm: 0.04937\n",
      "Epoch [1217/10044], Batch [2/7], Loss: 0.0042, Accuracy: 99.87%, Grad Norm: 0.07771\n",
      "Epoch [1217/10044], Batch [3/7], Loss: 0.0030, Accuracy: 99.97%, Grad Norm: 0.04165\n",
      "Epoch [1217/10044], Batch [4/7], Loss: 0.0051, Accuracy: 99.88%, Grad Norm: 0.08227\n",
      "Epoch [1217/10044], Batch [5/7], Loss: 0.0046, Accuracy: 99.89%, Grad Norm: 0.07427\n",
      "Epoch [1217/10044], Batch [6/7], Loss: 0.0056, Accuracy: 99.85%, Grad Norm: 0.09994\n",
      "Epoch [1217/10044], Batch [7/7], Loss: 0.0023, Accuracy: 99.97%, Grad Norm: 0.06663\n",
      "Epoch [1217/10044], Loss: 0.0023\n",
      "Epoch [1218/10044], Batch [1/7], Loss: 0.0052, Accuracy: 99.88%, Grad Norm: 0.07389\n",
      "Epoch [1218/10044], Batch [2/7], Loss: 0.0058, Accuracy: 99.83%, Grad Norm: 0.08778\n",
      "Epoch [1218/10044], Batch [3/7], Loss: 0.0032, Accuracy: 99.97%, Grad Norm: 0.04527\n",
      "Epoch [1218/10044], Batch [4/7], Loss: 0.0044, Accuracy: 99.94%, Grad Norm: 0.06713\n",
      "Epoch [1218/10044], Batch [5/7], Loss: 0.0045, Accuracy: 99.89%, Grad Norm: 0.09563\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m tgt[:, :, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, tgt)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m     23\u001b[0m grad_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bucks\\anaconda3\\envs\\encodec\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bucks\\anaconda3\\envs\\encodec\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bucks\\anaconda3\\envs\\encodec\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type=\"cuda\"):\n",
    "            output: torch.Tensor = model(src, tgt[:, :, :-1])\n",
    "            # Reshaping output and target tensors for loss computation\n",
    "            output = output.view(-1, model.vocab_size)\n",
    "            tgt = tgt[:, :, 1:].contiguous().view(-1)\n",
    "            loss = criterion(output, tgt)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        predicted = output.argmax(-1)\n",
    "        non_pad_mask = tgt != 2048\n",
    "        correct = (predicted[non_pad_mask] == tgt[non_pad_mask]).sum().item()\n",
    "        total = non_pad_mask.sum().item()\n",
    "\n",
    "        accuracy = correct / (total + 1e-8)\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1 + previous_epochs}/{NUM_EPOCHS + previous_epochs}], Batch [{batch_idx + 1}/{len(train_loader)}], \"\n",
    "            f\"Loss: {loss.item():.4f}, Accuracy: {100.0 * accuracy:.2f}%, Grad Norm: {grad_norm:.5f}\"\n",
    "        )\n",
    "\n",
    "    # Scheduler step (if using)\n",
    "    # scheduler.step()\n",
    "\n",
    "    # Print the loss every epoch\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch + previous_epochs + 1}/{NUM_EPOCHS + previous_epochs}], Loss: {loss.item():.4f}\"\n",
    "        )\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        save_checkpoint(\n",
    "            model,\n",
    "            optimizer,\n",
    "            epoch + previous_epochs + 1,\n",
    "            loss,\n",
    "            model_config,\n",
    "            filepath=FILEPATH,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bucks\\anaconda3\\envs\\encodec\\Lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
      "c:\\Users\\bucks\\anaconda3\\envs\\encodec\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 1505])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_save_func.<locals>.save() missing 1 required positional argument: 'sample_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m model_out_wav \u001b[38;5;241m=\u001b[39m convert_tensor_to_wav(encodec, model_out\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[0;32m     23\u001b[0m torchaudio\u001b[38;5;241m.\u001b[39msave(\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_model.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_out_wav, sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32000\u001b[39m\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtmp/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_model_combined.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_out_wav\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbacking_wav\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_save_func.<locals>.save() missing 1 required positional argument: 'sample_rate'"
     ]
    }
   ],
   "source": [
    "from transformers import EncodecModel\n",
    "from audio_tokenization import convert_tensor_to_wav, convert_wav_to_tensor\n",
    "import torchaudio\n",
    "\n",
    "train_codes = train_codes\n",
    "encodec = EncodecModel.from_pretrained(\"facebook/encodec_32khz\")\n",
    "\n",
    "i = 150\n",
    "\n",
    "backing_wav = convert_tensor_to_wav(encodec, train_codes[i][0][:, :1500])\n",
    "torchaudio.save(f\"tmp/{i}_backing.wav\", backing_wav, sample_rate=32000)\n",
    "\n",
    "lead_wav = convert_tensor_to_wav(encodec, train_codes[i][1][:, :1500])\n",
    "torchaudio.save(f\"tmp/{i}_lead.wav\", lead_wav, sample_rate=32000)\n",
    "\n",
    "model, _, _, _, _ = load_checkpoint(\n",
    "    lr=1e-5, filepath=\"model_checkpoint/float32/mini_256d_8h_24l_1024ff.pth\"\n",
    ")\n",
    "src = model.add_delay_interleaving(train_codes[i][0][:, :1500].to(device)).unsqueeze(0)\n",
    "print(src.shape)\n",
    "model.eval()\n",
    "model_out = model.generate(src)\n",
    "model_out = model.remove_delay_interleaving(model_out.squeeze())\n",
    "model_out_wav = convert_tensor_to_wav(encodec, model_out.cpu().squeeze())\n",
    "torchaudio.save(f\"tmp/{i}_model.wav\", model_out_wav, sample_rate=32000)\n",
    "torchaudio.save(\n",
    "    f\"tmp/{i}_model_combined.wav\", model_out_wav + backing_wav, sample_rate=32000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encodec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
