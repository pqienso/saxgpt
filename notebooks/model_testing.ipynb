{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "711b0402-2c7d-4ffe-adc6-2dc51ee7b860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ../models/experiment/checkpoints/checkpoint_epoch_350.pt\n",
      "Checkpoint info:\n",
      "  Epoch: 349\n",
      "  Step: 1400\n",
      "  Metrics: {'train_loss': 0.00027132392957961805, 'train_accuracy': 0.9999861121177673, 'val_loss': 9.738258361816406, 'val_accuracy': 0.21627314388751984}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1396"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.training.utils import load_checkpoint_for_inference, load_config, create_model\n",
    "from src.data.tokenization import detokenize\n",
    "from src.data.dataset_util import remove_delay_interleaving\n",
    "import torch\n",
    "import torchaudio\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "config = load_config(\"../config/model/experiment.yaml\")\n",
    "model = create_model(config).to(device)\n",
    "model.eval()\n",
    "load_checkpoint_for_inference(model, \"../models/experiment/checkpoints/checkpoint_epoch_350.pt\", device)\n",
    "\n",
    "test_ds = torch.load(\"../data/main/datasets/test.pt\", weights_only=False)\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75ccfa9a-ed32-44c5-9386-b9f7b875decf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num entries correct: 77 / 6010\n",
      "tensor([[[2048,  166,  166,  ...,   83,   83,   83],\n",
      "         [2048, 2048, 1931,  ..., 1931, 2044, 1931],\n",
      "         [2048, 2048, 2048,  ..., 2019, 2019, 2019],\n",
      "         [2048, 2048, 2048,  ..., 1770, 1951,  779]]])\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_NUMBER = 2\n",
    "\n",
    "def get_generated_audio(\n",
    "    model: torch.nn.Module,\n",
    "    example: Tuple[torch.Tensor, torch.Tensor],\n",
    "    save_dir: str = \"./outputs/\",\n",
    "    greedy: bool = True,\n",
    "    **sampling_args,\n",
    "):\n",
    "    src, tgt = example\n",
    "    src = src.to(device).unsqueeze(0)\n",
    "    tgt = tgt.to(device)\n",
    "    save_dir = Path(save_dir)\n",
    "    start_tokens = torch.tensor(\n",
    "        [[2048 for _ in range(4)]],\n",
    "        dtype=torch.int,\n",
    "        device=device,\n",
    "    )\n",
    "    if greedy:\n",
    "        out = model.generate_greedy(\n",
    "            src,\n",
    "            max_len=1505,\n",
    "            start_tokens=start_tokens,\n",
    "        )\n",
    "    else:\n",
    "        out = model.generate(\n",
    "            src,\n",
    "            max_len=1505,\n",
    "            start_tokens=start_tokens,\n",
    "            **sampling_args,\n",
    "        )\n",
    "    num_nums = 1\n",
    "    for i in out.shape:\n",
    "        num_nums *= i\n",
    "    print(f\"num entries correct: {(out[0] == tgt).sum().item()} / {num_nums - 10}\")\n",
    "    print(out)\n",
    "    src = detokenize(remove_delay_interleaving(src[0]))\n",
    "    out = detokenize(remove_delay_interleaving(out[0]))\n",
    "    tgt = detokenize(remove_delay_interleaving(tgt))\n",
    "    \n",
    "    torchaudio.save(save_dir / \"tgt_model.wav\", out, 32000)\n",
    "    torchaudio.save(save_dir / \"src.wav\", src, 32000)\n",
    "    torchaudio.save(save_dir / \"tgt_ground_truth.wav\", tgt, 32000)\n",
    "\n",
    "    torchaudio.save(\n",
    "        save_dir / \"combined_model.wav\",\n",
    "        torch.clip(out + src, -1.0, 1.0),\n",
    "        32000,\n",
    "    )\n",
    "    torchaudio.save(\n",
    "        save_dir / \"combined_ground_truth.wav\",\n",
    "        torch.clip(tgt + src, -1.0, 1.0),\n",
    "        32000,\n",
    "    )\n",
    "    \n",
    "get_generated_audio(model, test_ds[SAMPLE_NUMBER])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340601c-9eee-40e3-a7c7-038c914ae7a4",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
