{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "711b0402-2c7d-4ffe-adc6-2dc51ee7b860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ../experiments/models/241025_xsmall/checkpoints/checkpoint_interrupt.pt\n",
      "Checkpoint info:\n",
      "  Epoch: 799\n",
      "  Step: 3196\n",
      "  Metrics: {'train_loss': 9.319572563981637e-05, 'val_loss': 26.893381118774414}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.training.util.training import load_checkpoint_for_inference, load_config, create_model\n",
    "from src.data.util.tokenization import detokenize\n",
    "from src.data.util.codes_interleaving import remove_delay_interleaving\n",
    "import torch\n",
    "import torchaudio\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "config = load_config(\"../experiments/models/241025_xsmall/xsmall.yaml\")\n",
    "model = create_model(config).to(device)\n",
    "model.eval()\n",
    "load_checkpoint_for_inference(model, \"../experiments/models/241025_xsmall/checkpoints/checkpoint_interrupt.pt\", device)\n",
    "\n",
    "test_ds = torch.load(\"../experiments/data/241025/train.pt\", weights_only=False)\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ccfa9a-ed32-44c5-9386-b9f7b875decf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num entries correct: 6010 / 6010\n",
      "tensor([[[2048,  289,  289,  ...,   83,   83,   83],\n",
      "         [2048, 2048,  831,  ..., 2044, 2044, 2044],\n",
      "         [2048, 2048, 2048,  ..., 2019, 2019, 2019],\n",
      "         [2048, 2048, 2048,  ..., 1770, 1770, 1770]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuan/Desktop/SaxGPT/.venv/encodec/lib/python3.13/site-packages/torchaudio/_backend/utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_NUMBER = 43\n",
    "\n",
    "def get_generated_audio(\n",
    "    model: torch.nn.Module,\n",
    "    example: Tuple[torch.Tensor, torch.Tensor],\n",
    "    save_dir: str = \"./outputs/\",\n",
    "    greedy: bool = True,\n",
    "    **sampling_args,\n",
    "):\n",
    "    src, tgt = example\n",
    "    src = src.to(device).unsqueeze(0)\n",
    "    tgt = tgt.to(device)\n",
    "    save_dir = Path(save_dir)\n",
    "    start_tokens = torch.tensor(\n",
    "        [[2048 for _ in range(4)]],\n",
    "        dtype=torch.int,\n",
    "        device=device,\n",
    "    )\n",
    "    if greedy:\n",
    "        out = model.generate_greedy(\n",
    "            src,\n",
    "            max_len=1505,\n",
    "            start_tokens=start_tokens,\n",
    "        )\n",
    "    else:\n",
    "        out = model.generate(\n",
    "            src,\n",
    "            max_len=1505,\n",
    "            start_tokens=start_tokens,\n",
    "            **sampling_args,\n",
    "        )\n",
    "    num_nums = 1\n",
    "    for i in out.shape:\n",
    "        num_nums *= i\n",
    "    print(f\"num entries correct: {(out[0] == tgt).sum().item()} / {num_nums - 10}\")\n",
    "    print(out)\n",
    "    src = detokenize(remove_delay_interleaving(src[0]))\n",
    "    out = detokenize(remove_delay_interleaving(out[0]))\n",
    "    tgt = detokenize(remove_delay_interleaving(tgt))\n",
    "    \n",
    "    torchaudio.save(save_dir / \"tgt_model.wav\", out, 32000)\n",
    "    torchaudio.save(save_dir / \"src.wav\", src, 32000)\n",
    "    torchaudio.save(save_dir / \"tgt_ground_truth.wav\", tgt, 32000)\n",
    "\n",
    "    torchaudio.save(\n",
    "        save_dir / \"combined_model.wav\",\n",
    "        torch.clip(out + src, -1.0, 1.0),\n",
    "        32000,\n",
    "    )\n",
    "    torchaudio.save(\n",
    "        save_dir / \"combined_ground_truth.wav\",\n",
    "        torch.clip(tgt + src, -1.0, 1.0), \n",
    "        32000,\n",
    "    )\n",
    "    \n",
    "get_generated_audio(model, test_ds[SAMPLE_NUMBER])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cfc08c-53a7-4ea6-9c63-5c7f946c0128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
