{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "711b0402-2c7d-4ffe-adc6-2dc51ee7b860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ../models/experiment/checkpoints/checkpoint_epoch_450.pt\n",
      "Checkpoint info:\n",
      "  Epoch: 449\n",
      "  Step: 1800\n",
      "  Metrics: {'train_loss': 0.06740963631974799, 'train_accuracy': 0.9766969233751297, 'val_loss': 13.881664085388184, 'val_accuracy': 0.23289999961853028}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.training.utils import load_checkpoint_for_inference, load_config, create_model\n",
    "from src.data.tokenization import detokenize\n",
    "from src.data.dataset_util import remove_delay_interleaving\n",
    "import torch\n",
    "import torchaudio\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "config = load_config(\"../config/model/experiment.yaml\")\n",
    "model = create_model(config).to(device)\n",
    "load_checkpoint_for_inference(model, \"../models/experiment/checkpoints/checkpoint_epoch_450.pt\", device)\n",
    "\n",
    "test_ds = torch.load(\"../data/experiment/datasets/train.pt\", weights_only=False)\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ccfa9a-ed32-44c5-9386-b9f7b875decf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6020 entries\n",
      "num entries correct: 1241\n",
      "tensor([[[2048,  166,  166,  ...,   83,   83,   83],\n",
      "         [2048, 2048, 1931,  ..., 2044, 2044, 2044],\n",
      "         [2048, 2048, 2048,  ..., 2019, 2019, 2019],\n",
      "         [2048, 2048, 2048,  ..., 1770, 1770, 1770]]])\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_NUMBER = 4\n",
    "\n",
    "def get_generated_audio(\n",
    "    model: torch.nn.Module,\n",
    "    example: Tuple[torch.Tensor, torch.Tensor],\n",
    "    save_dir: str = \"./outputs/\",\n",
    "    greedy: bool = True,\n",
    "    **sampling_args,\n",
    "):\n",
    "    src, tgt = example\n",
    "    src = src.to(device).unsqueeze(0)\n",
    "    tgt = tgt.to(device)\n",
    "    save_dir = Path(save_dir)\n",
    "    start_tokens = torch.tensor(\n",
    "        [[2048 for _ in range(4)]],\n",
    "        dtype=torch.int,\n",
    "        device=device,\n",
    "    )\n",
    "    if greedy:\n",
    "        out = model.generate_greedy(\n",
    "            src,\n",
    "            max_len=1505,\n",
    "            start_tokens=start_tokens,\n",
    "        )\n",
    "    else:\n",
    "        out = model.generate(\n",
    "            src,\n",
    "            max_len=1505,\n",
    "            start_tokens=start_tokens,\n",
    "            **sampling_args,\n",
    "        )\n",
    "    num_nums = 1\n",
    "    for i in out.shape:\n",
    "        num_nums *= i\n",
    "    print(f\"{num_nums} entries\")\n",
    "    print(f\"num entries correct: {(out[0] == tgt).sum().item()}\")\n",
    "    print(out)\n",
    "    out = detokenize(remove_delay_interleaving(out[0]))\n",
    "    torchaudio.save(\n",
    "        save_dir / \"model_gen.wav\",\n",
    "        out,\n",
    "        32000,\n",
    "    )\n",
    "    torchaudio.save(\n",
    "        save_dir / \"src.wav\",\n",
    "        detokenize(remove_delay_interleaving(src[0])),\n",
    "        32000,\n",
    "    )\n",
    "    torchaudio.save(\n",
    "        save_dir / \"tgt.wav\",\n",
    "        detokenize(remove_delay_interleaving(tgt)),\n",
    "        32000,\n",
    "    )\n",
    "    \n",
    "get_generated_audio(model, test_ds[SAMPLE_NUMBER])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4d1ec4-489f-4731-abab-9282265689b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Generation Caching Debug Script ---\n",
      "Device: cpu\n",
      "1. Creating and Loading Model...\n",
      "Model weights loaded from ../models/small/checkpoints/checkpoint_epoch_200.pt\n",
      "2. Preparing Test Data...\n",
      "Data shape: (B=1, C=4, T=1505)\n",
      "\n",
      "3. Starting Step-by-Step Decoding Simulation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–Œ                                                           | 13/1504 [00:00<00:12, 119.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Error at T=1 (Position 2), CB=0: Expected=100, Predicted=1673. Previous Token=2048\n",
      "\n",
      "ðŸš¨ Error at T=2 (Position 3), CB=0: Expected=101, Predicted=100. Previous Token=100\n",
      "\n",
      "ðŸš¨ Error at T=2 (Position 3), CB=1: Expected=110, Predicted=1398. Previous Token=2048\n",
      "\n",
      "ðŸš¨ Error at T=3 (Position 4), CB=0: Expected=102, Predicted=641. Previous Token=101\n",
      "\n",
      "ðŸš¨ Error at T=3 (Position 4), CB=1: Expected=111, Predicted=1115. Previous Token=110\n",
      "\n",
      "ðŸš¨ Error at T=3 (Position 4), CB=2: Expected=120, Predicted=754. Previous Token=2048\n",
      "\n",
      "ðŸš¨ Error at T=4 (Position 5), CB=0: Expected=103, Predicted=786. Previous Token=102\n",
      "\n",
      "ðŸš¨ Error at T=4 (Position 5), CB=1: Expected=112, Predicted=940. Previous Token=111\n",
      "\n",
      "ðŸš¨ Error at T=4 (Position 5), CB=2: Expected=121, Predicted=120. Previous Token=120\n",
      "\n",
      "ðŸš¨ Error at T=4 (Position 5), CB=3: Expected=130, Predicted=642. Previous Token=2048\n",
      "\n",
      "ðŸš¨ Error at T=5 (Position 6), CB=0: Expected=104, Predicted=103. Previous Token=103\n",
      "\n",
      "ðŸš¨ Error at T=5 (Position 6), CB=1: Expected=113, Predicted=1646. Previous Token=112\n",
      "\n",
      "ðŸš¨ Error at T=5 (Position 6), CB=2: Expected=122, Predicted=121. Previous Token=121\n",
      "\n",
      "ðŸš¨ Error at T=5 (Position 6), CB=3: Expected=131, Predicted=130. Previous Token=130\n",
      "\n",
      "ðŸš¨ Error at T=6 (Position 7), CB=0: Expected=105, Predicted=1574. Previous Token=104\n",
      "\n",
      "ðŸš¨ Error at T=6 (Position 7), CB=1: Expected=114, Predicted=1011. Previous Token=113\n",
      "\n",
      "ðŸš¨ Error at T=6 (Position 7), CB=2: Expected=123, Predicted=122. Previous Token=122\n",
      "\n",
      "ðŸš¨ Error at T=6 (Position 7), CB=3: Expected=132, Predicted=1196. Previous Token=131\n",
      "\n",
      "ðŸš¨ Error at T=7 (Position 8), CB=0: Expected=106, Predicted=1368. Previous Token=105\n",
      "\n",
      "ðŸš¨ Error at T=7 (Position 8), CB=1: Expected=115, Predicted=114. Previous Token=114\n",
      "\n",
      "ðŸš¨ Error at T=7 (Position 8), CB=2: Expected=124, Predicted=123. Previous Token=123\n",
      "\n",
      "ðŸš¨ Error at T=7 (Position 8), CB=3: Expected=133, Predicted=132. Previous Token=132\n",
      "\n",
      "ðŸš¨ Error at T=8 (Position 9), CB=0: Expected=107, Predicted=4. Previous Token=106\n",
      "\n",
      "ðŸš¨ Error at T=8 (Position 9), CB=1: Expected=116, Predicted=999. Previous Token=115\n",
      "\n",
      "ðŸš¨ Error at T=8 (Position 9), CB=2: Expected=125, Predicted=312. Previous Token=124\n",
      "\n",
      "ðŸš¨ Error at T=8 (Position 9), CB=3: Expected=134, Predicted=133. Previous Token=133\n",
      "\n",
      "ðŸš¨ Error at T=9 (Position 10), CB=0: Expected=108, Predicted=1571. Previous Token=107\n",
      "\n",
      "ðŸš¨ Error at T=9 (Position 10), CB=1: Expected=117, Predicted=797. Previous Token=116\n",
      "\n",
      "ðŸš¨ Error at T=9 (Position 10), CB=2: Expected=126, Predicted=488. Previous Token=125\n",
      "\n",
      "ðŸš¨ Error at T=9 (Position 10), CB=3: Expected=135, Predicted=134. Previous Token=134\n",
      "\n",
      "ðŸš¨ Error at T=10 (Position 11), CB=0: Expected=109, Predicted=108. Previous Token=108\n",
      "\n",
      "ðŸš¨ Error at T=10 (Position 11), CB=1: Expected=118, Predicted=1269. Previous Token=117\n",
      "\n",
      "ðŸš¨ Error at T=10 (Position 11), CB=2: Expected=127, Predicted=126. Previous Token=126\n",
      "\n",
      "ðŸš¨ Error at T=10 (Position 11), CB=3: Expected=136, Predicted=135. Previous Token=135\n",
      "\n",
      "ðŸš¨ Error at T=11 (Position 12), CB=0: Expected=110, Predicted=1367. Previous Token=109\n",
      "\n",
      "ðŸš¨ Error at T=11 (Position 12), CB=1: Expected=119, Predicted=139. Previous Token=118\n",
      "\n",
      "ðŸš¨ Error at T=11 (Position 12), CB=2: Expected=128, Predicted=127. Previous Token=127\n",
      "\n",
      "ðŸš¨ Error at T=11 (Position 12), CB=3: Expected=137, Predicted=136. Previous Token=136\n",
      "\n",
      "ðŸš¨ Error at T=12 (Position 13), CB=0: Expected=111, Predicted=792. Previous Token=110\n",
      "\n",
      "ðŸš¨ Error at T=12 (Position 13), CB=1: Expected=120, Predicted=560. Previous Token=119\n",
      "\n",
      "ðŸš¨ Error at T=12 (Position 13), CB=2: Expected=129, Predicted=128. Previous Token=128\n",
      "\n",
      "ðŸš¨ Error at T=12 (Position 13), CB=3: Expected=138, Predicted=137. Previous Token=137\n",
      "\n",
      "ðŸš¨ Error at T=13 (Position 14), CB=0: Expected=112, Predicted=1755. Previous Token=111\n",
      "\n",
      "ðŸš¨ Error at T=13 (Position 14), CB=1: Expected=121, Predicted=1264. Previous Token=120\n",
      "\n",
      "ðŸš¨ Error at T=13 (Position 14), CB=2: Expected=130, Predicted=1971. Previous Token=129\n",
      "\n",
      "ðŸš¨ Error at T=13 (Position 14), CB=3: Expected=139, Predicted=138. Previous Token=138\n",
      "\n",
      "ðŸš¨ Error at T=14 (Position 15), CB=0: Expected=113, Predicted=1916. Previous Token=112\n",
      "\n",
      "ðŸš¨ Error at T=14 (Position 15), CB=1: Expected=122, Predicted=1215. Previous Token=121\n",
      "\n",
      "ðŸš¨ Error at T=14 (Position 15), CB=2: Expected=131, Predicted=113. Previous Token=130\n",
      "\n",
      "ðŸš¨ Error at T=14 (Position 15), CB=3: Expected=140, Predicted=139. Previous Token=139\n",
      "\n",
      "ðŸš¨ Error at T=15 (Position 16), CB=0: Expected=114, Predicted=113. Previous Token=113\n",
      "\n",
      "ðŸš¨ Error at T=15 (Position 16), CB=1: Expected=123, Predicted=122. Previous Token=122\n",
      "\n",
      "ðŸš¨ Error at T=15 (Position 16), CB=2: Expected=132, Predicted=224. Previous Token=131\n",
      "\n",
      "ðŸš¨ Error at T=15 (Position 16), CB=3: Expected=141, Predicted=140. Previous Token=140\n",
      "\n",
      "ðŸš¨ Error at T=16 (Position 17), CB=0: Expected=115, Predicted=228. Previous Token=114\n",
      "\n",
      "ðŸš¨ Error at T=16 (Position 17), CB=1: Expected=124, Predicted=1713. Previous Token=123\n",
      "\n",
      "ðŸš¨ Error at T=16 (Position 17), CB=2: Expected=133, Predicted=601. Previous Token=132\n",
      "\n",
      "ðŸš¨ Error at T=16 (Position 17), CB=3: Expected=142, Predicted=141. Previous Token=141\n",
      "\n",
      "ðŸš¨ Error at T=17 (Position 18), CB=0: Expected=116, Predicted=754. Previous Token=115\n",
      "\n",
      "ðŸš¨ Error at T=17 (Position 18), CB=1: Expected=125, Predicted=124. Previous Token=124\n",
      "\n",
      "ðŸš¨ Error at T=17 (Position 18), CB=2: Expected=134, Predicted=1487. Previous Token=133\n",
      "\n",
      "ðŸš¨ Error at T=17 (Position 18), CB=3: Expected=143, Predicted=142. Previous Token=142\n",
      "\n",
      "ðŸš¨ Error at T=18 (Position 19), CB=0: Expected=117, Predicted=510. Previous Token=116\n",
      "\n",
      "ðŸš¨ Error at T=18 (Position 19), CB=1: Expected=126, Predicted=962. Previous Token=125\n",
      "\n",
      "ðŸš¨ Error at T=18 (Position 19), CB=2: Expected=135, Predicted=672. Previous Token=134\n",
      "\n",
      "ðŸš¨ Error at T=18 (Position 19), CB=3: Expected=144, Predicted=962. Previous Token=143\n",
      "\n",
      "ðŸš¨ Error at T=19 (Position 20), CB=0: Expected=118, Predicted=50. Previous Token=117\n",
      "\n",
      "ðŸš¨ Error at T=19 (Position 20), CB=1: Expected=127, Predicted=126. Previous Token=126\n",
      "\n",
      "ðŸš¨ Error at T=19 (Position 20), CB=2: Expected=136, Predicted=135. Previous Token=135\n",
      "\n",
      "ðŸš¨ Error at T=19 (Position 20), CB=3: Expected=145, Predicted=1356. Previous Token=144\n",
      "\n",
      "ðŸš¨ Error at T=20 (Position 21), CB=0: Expected=119, Predicted=118. Previous Token=118\n",
      "\n",
      "ðŸš¨ Error at T=20 (Position 21), CB=1: Expected=128, Predicted=1340. Previous Token=127\n",
      "\n",
      "ðŸš¨ Error at T=20 (Position 21), CB=2: Expected=137, Predicted=136. Previous Token=136\n",
      "\n",
      "ðŸš¨ Error at T=20 (Position 21), CB=3: Expected=146, Predicted=817. Previous Token=145\n",
      "\n",
      "ðŸš¨ Error at T=21 (Position 22), CB=0: Expected=120, Predicted=119. Previous Token=119\n",
      "\n",
      "ðŸš¨ Error at T=21 (Position 22), CB=1: Expected=129, Predicted=473. Previous Token=128\n",
      "\n",
      "ðŸš¨ Error at T=21 (Position 22), CB=2: Expected=138, Predicted=782. Previous Token=137\n",
      "\n",
      "ðŸš¨ Error at T=21 (Position 22), CB=3: Expected=147, Predicted=1931. Previous Token=146\n",
      "\n",
      "ðŸš¨ Error at T=22 (Position 23), CB=0: Expected=121, Predicted=1883. Previous Token=120\n",
      "\n",
      "ðŸš¨ Error at T=22 (Position 23), CB=1: Expected=130, Predicted=1862. Previous Token=129\n",
      "\n",
      "ðŸš¨ Error at T=22 (Position 23), CB=2: Expected=139, Predicted=138. Previous Token=138\n",
      "\n",
      "ðŸš¨ Error at T=22 (Position 23), CB=3: Expected=148, Predicted=147. Previous Token=147\n",
      "\n",
      "ðŸš¨ Error at T=23 (Position 24), CB=0: Expected=122, Predicted=121. Previous Token=121\n",
      "\n",
      "ðŸš¨ Error at T=23 (Position 24), CB=1: Expected=131, Predicted=1257. Previous Token=130\n",
      "\n",
      "ðŸš¨ Error at T=23 (Position 24), CB=2: Expected=140, Predicted=139. Previous Token=139\n",
      "\n",
      "ðŸš¨ Error at T=23 (Position 24), CB=3: Expected=149, Predicted=148. Previous Token=148\n",
      "\n",
      "ðŸš¨ Error at T=24 (Position 25), CB=0: Expected=123, Predicted=354. Previous Token=122\n",
      "\n",
      "ðŸš¨ Error at T=24 (Position 25), CB=1: Expected=132, Predicted=1199. Previous Token=131\n",
      "\n",
      "ðŸš¨ Error at T=24 (Position 25), CB=2: Expected=141, Predicted=413. Previous Token=140\n",
      "\n",
      "ðŸš¨ Error at T=24 (Position 25), CB=3: Expected=150, Predicted=149. Previous Token=149\n",
      "\n",
      "ðŸš¨ Error at T=25 (Position 26), CB=0: Expected=124, Predicted=1282. Previous Token=123\n",
      "\n",
      "ðŸš¨ Error at T=25 (Position 26), CB=1: Expected=133, Predicted=1419. Previous Token=132\n",
      "\n",
      "ðŸš¨ Error at T=25 (Position 26), CB=2: Expected=142, Predicted=141. Previous Token=141\n",
      "\n",
      "ðŸš¨ Error at T=25 (Position 26), CB=3: Expected=151, Predicted=1781. Previous Token=150\n",
      "\n",
      "ðŸš¨ Error at T=26 (Position 27), CB=0: Expected=125, Predicted=124. Previous Token=124\n",
      "\n",
      "ðŸš¨ Error at T=26 (Position 27), CB=1: Expected=134, Predicted=1250. Previous Token=133\n",
      "\n",
      "ðŸš¨ Error at T=26 (Position 27), CB=2: Expected=143, Predicted=142. Previous Token=142\n",
      "\n",
      "ðŸš¨ Error at T=26 (Position 27), CB=3: Expected=152, Predicted=151. Previous Token=151\n",
      "\n",
      "ðŸš¨ Error at T=27 (Position 28), CB=0: Expected=126, Predicted=376. Previous Token=125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                    | 161/1504 [00:00<00:07, 178.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Error at T=127 (Position 128), CB=1: Expected=235, Predicted=452. Previous Token=234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                               | 287/1504 [00:01<00:06, 175.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Error at T=252 (Position 253), CB=1: Expected=360, Predicted=359. Previous Token=359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 395/1504 [00:02<00:06, 171.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Error at T=377 (Position 378), CB=1: Expected=485, Predicted=929. Previous Token=484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                      | 535/1504 [00:03<00:05, 162.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Error at T=502 (Position 503), CB=1: Expected=610, Predicted=609. Previous Token=609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                  | 637/1504 [00:03<00:05, 160.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Error at T=627 (Position 628), CB=1: Expected=735, Predicted=1812. Previous Token=734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 763/1504 [00:05<00:07, 101.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Error at T=752 (Position 753), CB=1: Expected=860, Predicted=859. Previous Token=859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 887/1504 [00:06<00:05, 107.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Error at T=877 (Position 878), CB=1: Expected=985, Predicted=984. Previous Token=984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 1016/1504 [00:07<00:03, 132.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Error at T=1002 (Position 1003), CB=1: Expected=1110, Predicted=1109. Previous Token=1109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 1140/1504 [00:08<00:03, 106.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Error at T=1127 (Position 1128), CB=1: Expected=1235, Predicted=1234. Previous Token=1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 1280/1504 [00:09<00:01, 159.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Error at T=1252 (Position 1253), CB=1: Expected=1360, Predicted=1359. Previous Token=1359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1396/1504 [00:10<00:01, 104.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Error at T=1377 (Position 1378), CB=1: Expected=1485, Predicted=1484. Previous Token=1484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1504/1504 [00:11<00:00, 133.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Error at T=1502 (Position 1503), CB=1: Expected=1610, Predicted=1609. Previous Token=1609\n",
      "\n",
      "==================================================\n",
      "         DECODING CACHING SIMULATION REPORT\n",
      "==================================================\n",
      "Total Non-Padding Tokens Checked: 6010\n",
      "Total Correct Predictions (Teacher-Forced Cache): 1\n",
      "Simulation Accuracy: 0.0002 (Expected: ~0.9400)\n",
      "==================================================\n",
      "\n",
      "**Conclusion:** The model's **caching and decode logic is fundamentally flawed**.\n",
      "This confirms the internal decode logic with incremental steps (KV cache) is broken,\n",
      "even when given the correct previous token (teacher forcing).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from src.model.transformer import EncoderDecoderTransformer\n",
    "\n",
    "# Assume these utility functions are imported from your main training script's context\n",
    "# from .utils import load_config, create_model, setup_device, load_checkpoint_for_training\n",
    "\n",
    "# --- Placeholder/Mock Classes for Execution (Replace with your actual imports) ---\n",
    "# NOTE: For the script to run, you need the real imports for your model, utilities, and a mock dataset.\n",
    "\n",
    "# Mock DataLoader and Dataset for easy testing on a single batch\n",
    "class MockDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, config):\n",
    "        # Create mock data based on config\n",
    "        self.num_codebooks = config[\"model\"][\"num_codebooks\"] # 4\n",
    "        self.seq_len = config[\"model\"][\"max_seq_len\"] # 1505\n",
    "        self.padding_idx = config[\"model\"][\"padding_idx\"] # 2048\n",
    "        \n",
    "        # A simple, deterministic sequence for easy checking\n",
    "        # Start with all padding (2048) at t=0\n",
    "        src = torch.full((self.num_codebooks, self.seq_len), self.padding_idx, dtype=torch.long)\n",
    "        tgt = torch.full((self.num_codebooks, self.seq_len), self.padding_idx, dtype=torch.long)\n",
    "        \n",
    "        # Fill in codebook tokens with unique, non-padding values (e.g., 100, 200, 300...)\n",
    "        # CB_idx: 0, 1, 2, 3\n",
    "        # Start Time: 1, 2, 3, 4 (positions 1, 2, 3, 4)\n",
    "        for cb_idx in range(self.num_codebooks):\n",
    "            # Non-padding tokens start at position cb_idx + 1\n",
    "            # Token values: 100+cb_idx * 10 (e.g., 100, 110, 120, 130)\n",
    "            token_value = 100 + cb_idx * 10 \n",
    "            \n",
    "            # Fill tokens from position (cb_idx + 1) to end\n",
    "            src[cb_idx, cb_idx + 1:] = torch.arange(\n",
    "                token_value, token_value + self.seq_len - (cb_idx + 1), dtype=torch.long\n",
    "            ) % (self.padding_idx - 1) # Modulo to keep in range [0, 2047]\n",
    "            tgt[cb_idx, cb_idx + 1:] = src[cb_idx, cb_idx + 1:] # Target is same as source for this test\n",
    "\n",
    "        # Set the first position (t=0) to a sentinel non-padding token if needed, \n",
    "        # but based on your description, t=0 is padding for all codebooks.\n",
    "        # We assume the model expects to predict the sequence from t=1 onwards.\n",
    "        # Let's align the data with a standard [BOS] token (which is often index 0) if padding_idx=2048 is only used for padding.\n",
    "        \n",
    "        # Based on your description: \"the first position is always 2048 for all codebooks\"\n",
    "        # and \"the first codebook has its first actual token at the second position.\"\n",
    "        # This means the target sequence TGT has a start token at T=0 (position 0) and the real sequence starts at T=1.\n",
    "        self.data = [(src.unsqueeze(0), tgt.unsqueeze(0))] # Single batch of size 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# --- Debug Function ---\n",
    "\n",
    "def debug_generation_caching(config: Dict, model_class: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simulates step-by-step teacher-forced decoding with caching\n",
    "    to debug the core decode/cache mechanism.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Generation Caching Debug Script ---\")\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # 1. Instantiate Model\n",
    "    print(\"1. Creating and Loading Model...\")\n",
    "    # NOTE: You must provide the actual 'create_model' and 'load_checkpoint_for_training'\n",
    "    # We mock them here for concept.\n",
    "    try:\n",
    "        # Replace with your actual model instantiation\n",
    "        model = model_class(**config[\"model\"]).to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating model: {e}\")\n",
    "        return\n",
    "\n",
    "    # Load a checkpoint to ensure weights are correct\n",
    "    checkpoint_path = Path(config[\"training\"][\"resume_from_checkpoint\"])\n",
    "    try:\n",
    "        # Load weights only for this debug script.\n",
    "        # This requires the actual structure of your checkpoint file.\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        print(f\"Model weights loaded from {checkpoint_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load checkpoint. Running with random weights. Error: {e}\")\n",
    "        print(\"This test is only valid if you load a trained checkpoint.\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # 2. Prepare Data (A single example from the training set)\n",
    "    # We use a mock dataset here, but ideally, you'd load one batch from your real train_loader\n",
    "    print(\"2. Preparing Test Data...\")\n",
    "    mock_dataset = MockDataset(config)\n",
    "    \n",
    "    # Get the single batch: src, tgt [B=1, C=4, T=1505]\n",
    "    src, tgt = mock_dataset[0] \n",
    "    src = src.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    batch_size, num_codebooks, seq_len = tgt.shape\n",
    "    \n",
    "    print(f\"Data shape: (B={batch_size}, C={num_codebooks}, T={seq_len})\")\n",
    "    \n",
    "    # Initial sequence length to start decoding: 1 (the start token at T=0)\n",
    "    # We will simulate prediction from T=1 up to T=seq_len-1 (1504 total steps)\n",
    "    start_t = 0\n",
    "    end_t = seq_len - 1\n",
    "\n",
    "    # 3. Encode Source (Encoder part of generation)\n",
    "    memory = model.encode(src)\n",
    "\n",
    "    # 4. Step-by-Step Teacher-Forced Decoding\n",
    "    print(\"\\n3. Starting Step-by-Step Decoding Simulation...\")\n",
    "    \n",
    "    # tgt_so_far holds the predicted sequence (initially just the first token)\n",
    "    # Start with the first token of the target sequence (t=0)\n",
    "    tgt_so_far = tgt[:, :, start_t].unsqueeze(-1) # [1, 4, 1]\n",
    "    cache = [\n",
    "        {\"self_attn\": None, \"cross_attn\": None}\n",
    "        for _ in range(model.num_decoder_layers)\n",
    "    ]\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_non_padding = 0\n",
    "\n",
    "    # Loop from t=1 to T-1\n",
    "    for t in tqdm(range(start_t, end_t)):\n",
    "        # Input to decoder is the token at 't' (the last token in tgt_so_far)\n",
    "        # This is the \"teacher-forced\" input.\n",
    "        current_tokens = tgt_so_far[:, :, -1:] \n",
    "        \n",
    "        # Expected correct tokens to be predicted at position t+1\n",
    "        expected_next_tokens = tgt[:, :, t + 1] # [1, 4]\n",
    "        \n",
    "        # Decode one step with caching\n",
    "        logits, cache = model.decode(\n",
    "            current_tokens,\n",
    "            memory,\n",
    "            cache=cache,\n",
    "            use_cache=True,\n",
    "            is_causal=True,\n",
    "        ) # logits: [1, 4, 1, V_proj]\n",
    "\n",
    "        # Get greedy prediction: [1, 4]\n",
    "        # NOTE: argmax is over V_proj (size 2048, indices 0-2047)\n",
    "        predicted_next_tokens = logits[:, :, -1, :].argmax(dim=-1) \n",
    "\n",
    "        # *** CRITICAL CHECK 1: The Padding/Delay Logic Mismatch ***\n",
    "        # The correct target indices are 0-2048. Predicted indices are 0-2047.\n",
    "        # This is the expected mismatch due to the model's output projection size.\n",
    "        # We must align the prediction to the target's vocab space [0, 2048] for comparison.\n",
    "        # Since 2048 is the padding index, we ONLY care about 0-2047.\n",
    "        \n",
    "        # Check all codebooks\n",
    "        for cb_idx in range(num_codebooks):\n",
    "            expected = expected_next_tokens[0, cb_idx].item()\n",
    "            predicted = predicted_next_tokens[0, cb_idx].item()\n",
    "            \n",
    "            # Non-padding token indices are [0, 2047]\n",
    "            if expected != config[\"model\"][\"padding_idx\"]:\n",
    "                total_non_padding += 1\n",
    "                if expected == predicted:\n",
    "                    total_correct += 1\n",
    "                else:\n",
    "                    # Log the first few errors for inspection\n",
    "                    if total_non_padding < 100 or total_non_padding % 500 == 0:\n",
    "                         print(\n",
    "                             f\"\\nðŸš¨ Error at T={t+1} (Position {t+2}), CB={cb_idx}: \"\n",
    "                             f\"Expected={expected}, Predicted={predicted}. \"\n",
    "                             f\"Previous Token={current_tokens[0, cb_idx, 0].item()}\"\n",
    "                         )\n",
    "\n",
    "            # Manually force the prediction to be correct for the next step \n",
    "            # (Teacher forcing step)\n",
    "            # This is crucial: we must ensure the *correct* token is fed back.\n",
    "            predicted_next_tokens[0, cb_idx] = expected_next_tokens[0, cb_idx]\n",
    "\n",
    "        # Append the CORRECT token to the sequence for the next step\n",
    "        tgt_so_far = torch.cat([tgt_so_far, predicted_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    # 5. Final Report\n",
    "    simulation_accuracy = total_correct / total_non_padding if total_non_padding > 0 else 0.0\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"         DECODING CACHING SIMULATION REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total Non-Padding Tokens Checked: {total_non_padding}\")\n",
    "    print(f\"Total Correct Predictions (Teacher-Forced Cache): {total_correct}\")\n",
    "    print(f\"Simulation Accuracy: {simulation_accuracy:.4f} (Expected: ~0.9400)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if simulation_accuracy < 0.90:\n",
    "        print(\"\\n**Conclusion:** The model's **caching and decode logic is fundamentally flawed**.\")\n",
    "        print(\"This confirms the internal decode logic with incremental steps (KV cache) is broken,\")\n",
    "        print(\"even when given the correct previous token (teacher forcing).\")\n",
    "        \n",
    "    elif simulation_accuracy > 0.93:\n",
    "        print(\"\\n**Conclusion:** The model's **caching and decode logic works correctly**.\")\n",
    "        print(\"The 0% generation accuracy is due to the **argmax/sampling** or the **delay-forcing logic**\")\n",
    "        print(\"within your `generate()` function, causing an immediate catastrophic error cascade.\")\n",
    "        \n",
    "        # *** Next Debug Step: The Delay Logic in generate() ***\n",
    "        print(\"\\n--- NEXT DEBUG STEP (Focus on generate() Logic) ---\")\n",
    "        print(\"The most suspicious part of your `generate()` is the delay condition:\")\n",
    "        print(\"`if current_position <= cb_idx:`\")\n",
    "        print(\"If `tgt` starts at length 1 (the start token), then at the first loop step (t=0),\")\n",
    "        print(\"`current_position` is 1. CB0 (cb_idx=0) fails the condition (1 <= 0 is False) and predicts.\")\n",
    "        print(\"CB1 (cb_idx=1) passes the condition (1 <= 1 is True) and forces 2048.\")\n",
    "        print(\"This seems correct for a sequence starting at T=0, but verify against your data preparation.\")\n",
    "        print(\"Try changing `current_position <= cb_idx` to `current_position < cb_idx + 1` or similar.\")\n",
    "\n",
    "\n",
    "# --- Execution Block (You need to run this with your actual environment) ---\n",
    "\n",
    "# Mock the utilities to load the config\n",
    "def load_config(config_path):\n",
    "    # Use the YAML data provided in the prompt\n",
    "    config_yaml = \"\"\"\n",
    "data:\n",
    "  train_path: \"data/main/datasets/train.pt\"\n",
    "  val_path: \"data/main/datasets/val.pt\"\n",
    "\n",
    "# Model architecture\n",
    "model:\n",
    "  vocab_size: 2049\n",
    "  num_codebooks: 4\n",
    "  d_model: 128\n",
    "  nhead: 8\n",
    "  num_encoder_layers: 8\n",
    "  num_decoder_layers: 8\n",
    "  dim_feedforward: 512\n",
    "  dropout: 0.1\n",
    "  activation: \"relu\"         \n",
    "  norm_first: false          \n",
    "  max_seq_len: 1505\n",
    "  padding_idx: 2048\n",
    "  scale_embeddings: true\n",
    "\n",
    "\n",
    "training:\n",
    "  num_epochs: 300\n",
    "  # Effective batch size = batch_size * gradient_accumulation_steps\n",
    "  batch_size: 8\n",
    "  gradient_accumulation_steps: 4\n",
    "  \n",
    "  # Learning rate scheduler\n",
    "  scheduler:\n",
    "    type: \"cosine\"           # Options: cosine, linear, step, plateau\n",
    "    warmup_steps: 1000       # Warmup for 1000 steps\n",
    "    total_steps: 50000       # Total training steps (auto-calculated if omitted)\n",
    "    min_lr: 1.0e-6           # Minimum learning rate\n",
    "    \n",
    "    # For 'step' scheduler:\n",
    "    # step_size: 10          # Decay every N epochs\n",
    "    # gamma: 0.5             # Multiply LR by this factor\n",
    "    \n",
    "    # For 'plateau' scheduler:\n",
    "    # patience: 3            # Wait N epochs before reducing\n",
    "    # factor: 0.5            # Multiply LR by this factor\n",
    "\n",
    "  # Optimizer\n",
    "  optimizer:\n",
    "    type: \"adamw\"\n",
    "    lr: 0.0001 \n",
    "    betas: [0.9, 0.98]\n",
    "    eps: 1.0e-9\n",
    "    weight_decay: 0.01\n",
    "  \n",
    "  # Logging and checkpointing\n",
    "  output_dir: \"models/small\"\n",
    "  log_interval: 2           # Log metrics every N steps\n",
    "  save_interval: 50      # Save checkpoint every N epochs\n",
    "  num_workers: 2             # DataLoader workers\n",
    "  \n",
    "  # Resume training (optional)\n",
    "  resume_from_checkpoint: \"../models/small/checkpoints/checkpoint_epoch_200.pt\" # Assuming you've trained to 200\n",
    "\"\"\"\n",
    "    return yaml.safe_load(config_yaml)\n",
    "\n",
    "# Get the config\n",
    "config_data = load_config(None)\n",
    "\n",
    "# Execute the debug function with your actual EncoderDecoderTransformer class\n",
    "debug_generation_caching(config_data, EncoderDecoderTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02290a42-2a27-4076-b8dc-422d7a3e0512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba3775-75af-4ee2-8bc5-c589660a1f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
