{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711b0402-2c7d-4ffe-adc6-2dc51ee7b860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ../../../Downloads/model/checkpoints/latest.pt\n",
      "Checkpoint info:\n",
      "  Epoch: 945\n",
      "  Step: 118250\n",
      "  Metrics: {'train_loss': 1.6159923076629639, 'train_accuracy': 0.5756251215934753, 'val_loss': 7.692844390869141, 'val_accuracy': 0.10429166257381439}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18688"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.training.util.checkpointing import load_checkpoint_for_inference\n",
    "from src.training.util.config_model import load_config, create_model\n",
    "from src.data.util.tokenization import detokenize\n",
    "from src.data.util.codes_interleaving import remove_delay_interleaving\n",
    "import torch\n",
    "import torchaudio\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "config = load_config(\"../config/model/medium.yaml\")\n",
    "model = create_model(config).to(device)\n",
    "model.eval()\n",
    "load_checkpoint_for_inference(model, \"../../../Downloads/model/checkpoints/latest.pt\", device)\n",
    "\n",
    "test_ds = torch.load(\"../data/main/7_datasets/train.pt\", weights_only=False)\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75ccfa9a-ed32-44c5-9386-b9f7b875decf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num entries correct: 265 / 6010\n",
      "tensor([[[2048,  297,  289,  ...,   83,   83,   83],\n",
      "         [2048, 2048, 1672,  ..., 2044, 2044, 2044],\n",
      "         [2048, 2048, 2048,  ..., 2019, 2019, 2019],\n",
      "         [2048, 2048, 2048,  ..., 1770, 1770,   11]]])\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_NUMBER = 80\n",
    "\n",
    "def get_generated_audio(\n",
    "    model: torch.nn.Module,\n",
    "    example: Tuple[torch.Tensor, torch.Tensor],\n",
    "    save_dir: str = \"./outputs/\",\n",
    "    greedy: bool = True,\n",
    "    teacher_forcing: bool = False,\n",
    "    **sampling_args,\n",
    "):\n",
    "    src, tgt = example\n",
    "    src = src.to(device).unsqueeze(0)\n",
    "    tgt = tgt.to(device)\n",
    "    save_dir = Path(save_dir)\n",
    "    start_tokens = torch.tensor(\n",
    "        [[2048 for _ in range(4)]],\n",
    "        dtype=torch.int,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    if teacher_forcing:\n",
    "        out = model(src, tgt[:, :-1].unsqueeze(0)).argmax(dim=-1, keepdim=False)\n",
    "        out = torch.cat(\n",
    "            [torch.tensor([[[2048] for _ in range(4)]]), out],\n",
    "            dim=-1\n",
    "        )\n",
    "    elif greedy:\n",
    "        out = model.generate_greedy(\n",
    "            src,\n",
    "            max_len=1505,\n",
    "            start_tokens=start_tokens,\n",
    "        )\n",
    "    else:\n",
    "        out = model.generate(\n",
    "            src,\n",
    "            max_len=1505,\n",
    "            start_tokens=start_tokens,\n",
    "            **sampling_args,\n",
    "        )\n",
    "    num_nums = 1\n",
    "    for i in out.shape:\n",
    "        num_nums *= i\n",
    "    print(f\"num entries correct: {(out[0] == tgt).sum().item()} / {num_nums - 10}\")\n",
    "    print(out)\n",
    "    tokens = out\n",
    "    src = detokenize(remove_delay_interleaving(src[0]))\n",
    "    out = detokenize(remove_delay_interleaving(out[0]))\n",
    "    tgt = detokenize(remove_delay_interleaving(tgt))\n",
    "    \n",
    "    torchaudio.save(save_dir / \"tgt_model.wav\", out, 32000)\n",
    "    torchaudio.save(save_dir / \"src.wav\", src, 32000)\n",
    "    torchaudio.save(save_dir / \"tgt_ground_truth.wav\", tgt, 32000)\n",
    "\n",
    "    torchaudio.save(\n",
    "        save_dir / \"combined_model.wav\",\n",
    "        torch.clip(out + src, -1.0, 1.0),\n",
    "        32000,\n",
    "    )\n",
    "    torchaudio.save(\n",
    "        save_dir / \"combined_ground_truth.wav\",\n",
    "        torch.clip(tgt + src, -1.0, 1.0), \n",
    "        32000,\n",
    "    )\n",
    "    return tokens\n",
    "    \n",
    "out = get_generated_audio(model, test_ds[SAMPLE_NUMBER], greedy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08cfc08c-53a7-4ea6-9c63-5c7f946c0128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2048,  297,  289,  289,  609,  248,    8,  609,  443,   83,  289,\n",
       "            83,   83,   83,   83,   83,   83,  166,   83,   83,   83,   83,\n",
       "            83,   83,   83,   83,   83,   83,   83,   83,   83,   83,   83,\n",
       "            83,   83,   83,   83,   83,   83,   83,   83,   83,   83,   83,\n",
       "            83, 1436, 1600,  967, 1915, 1729, 1194, 1044,  658, 1869, 1019,\n",
       "          1652,  472,  289,  233,  233, 1883, 1322,  624, 1701, 1100,  624,\n",
       "          1697, 1458, 1827, 1106, 1126, 1303, 1688,  782, 2047,  918,  310,\n",
       "           113,  310, 1712,  310,  100,  881,  654,  810, 1518,  632, 1279,\n",
       "           632, 1615,  551, 1976, 1424, 1389,   74,  904,  974,  974, 1680,\n",
       "          1322],\n",
       "         [2048, 2048, 1672, 1595, 1972,  959, 2044,  449,  923, 1600, 2044,\n",
       "          1987, 2044, 2044, 1931, 2044, 1931, 1931, 2044, 1931, 1931, 1931,\n",
       "          1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931, 1931,\n",
       "          1931, 1931, 2044, 2044, 2044, 2044, 2044, 2044, 2044, 2044, 2044,\n",
       "          1931, 2044, 1931,  775, 1868, 1990, 1722,  503,  741, 1891,  276,\n",
       "          1078, 1814, 1872,  154,  959, 1874,  161,  916, 1911,  398, 1284,\n",
       "           158,  621, 1649, 1253, 1967, 1963, 1365, 1256, 1867,  649, 1155,\n",
       "          1130,  611, 1400, 1792, 1606, 2021,  485, 1001, 1771, 1605,  611,\n",
       "           404,  404, 1255,  611, 1861, 1221,  963,  469, 1011,  553, 1172,\n",
       "           740],\n",
       "         [2048, 2048, 2048, 1074, 1895, 1242, 1895, 1630, 1895, 2004, 2019,\n",
       "          2003, 2019, 1895, 2019, 2019,  486, 2019, 2019, 2019, 2019, 2019,\n",
       "          2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019,\n",
       "          2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019,\n",
       "          2019, 2019, 1895, 1634, 1238, 1910,  628, 1213, 1064,  496, 1862,\n",
       "          1107, 1598,  887, 1247, 1742, 1246,  860,  254, 1084,   81,   55,\n",
       "          1056, 1071, 1989, 1717,  822,  790, 1146, 1860, 1382, 1936,  331,\n",
       "          1629,  696,  333, 1574,  806, 1860, 1119,  821,  619,  358,  799,\n",
       "          1860,  903,  499, 1704, 1176,  950,  118, 1942, 1826,  518, 1237,\n",
       "           212],\n",
       "         [2048, 2048, 2048, 2048,  242, 1759,  167, 1671,  475, 1740,  412,\n",
       "          1409, 1409, 1854, 1770, 1770, 1770, 1903, 1770, 1770, 1770, 1854,\n",
       "          1770, 1854, 1854, 1854, 1770, 1770, 1854, 1854, 1854, 1854, 1854,\n",
       "          1854, 1854, 1854, 1770, 1770, 1770, 1770, 1770, 1770, 1770, 1770,\n",
       "          1770, 1770, 1770, 1595, 1962, 1247, 2047, 1919,   77, 1739, 1956,\n",
       "          1507, 1485,  805, 1485, 1910,  474, 1289, 1150,  614,  401,  166,\n",
       "          1792,  166, 1792,  755, 1202,  758,  303,  772,  785,  620, 2002,\n",
       "           338, 1029, 1472, 1615, 1925,  644, 1633,  239,  116,  105,  256,\n",
       "           233, 1803, 1112, 1590, 1440,  110,  772,  772, 1305, 1345,  965,\n",
       "           772]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:, :, :100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df674630-9c01-43c4-b4d9-40b3c61f95cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
